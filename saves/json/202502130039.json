[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.07776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07776v1",
                "updated": "2025-02-11T18:58:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    58,
                    4,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T18:58:04Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    58,
                    4,
                    1,
                    42,
                    0
                ],
                "title": "Auditing Prompt Caching in Language Model APIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auditing Prompt Caching in Language Model APIs"
                },
                "summary": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known."
                },
                "authors": [
                    {
                        "name": "Chenchen Gu"
                    },
                    {
                        "name": "Xiang Lisa Li"
                    },
                    {
                        "name": "Rohith Kuditipudi"
                    },
                    {
                        "name": "Percy Liang"
                    },
                    {
                        "name": "Tatsunori Hashimoto"
                    }
                ],
                "author_detail": {
                    "name": "Tatsunori Hashimoto"
                },
                "author": "Tatsunori Hashimoto",
                "arxiv_comment": "20 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19392v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19392v2",
                "updated": "2025-02-11T18:45:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    45,
                    12,
                    1,
                    42,
                    0
                ],
                "published": "2025-01-31T18:47:42Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    47,
                    42,
                    4,
                    31,
                    0
                ],
                "title": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models"
                },
                "summary": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models."
                },
                "authors": [
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "Vladimir Malinovskii"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Denis Mazur"
                    },
                    {
                        "name": "Nikita Surkov"
                    },
                    {
                        "name": "Ivan Ermakov"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19392v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19392v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03743v2",
                "updated": "2025-02-11T17:48:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    48,
                    15,
                    1,
                    42,
                    0
                ],
                "published": "2024-09-05T17:56:19Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    56,
                    19,
                    3,
                    249,
                    0
                ],
                "title": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)"
                },
                "summary": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%."
                },
                "authors": [
                    {
                        "name": "Hans Winderix"
                    },
                    {
                        "name": "Marton Bognar"
                    },
                    {
                        "name": "Lesly-Ann Daniel"
                    },
                    {
                        "name": "Frank Piessens"
                    }
                ],
                "author_detail": {
                    "name": "Frank Piessens"
                },
                "author": "Frank Piessens",
                "arxiv_doi": "10.1145/3658644.3690319",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3658644.3690319",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.03743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11779v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11779v2",
                "updated": "2025-02-11T17:36:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    36,
                    32,
                    1,
                    42,
                    0
                ],
                "published": "2025-01-20T23:10:13Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    23,
                    10,
                    13,
                    0,
                    20,
                    0
                ],
                "title": "Glinthawk: A Two-Tiered Architecture for Offline LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Glinthawk: A Two-Tiered Architecture for Offline LLM Inference"
                },
                "summary": "We introduce Glinthawk, an architecture for offline Large Language Model\n(LLM) inference. By leveraging a two-tiered structure, Glinthawk optimizes the\nutilization of the high-end accelerators (\"Tier 1\") by offloading the attention\nmechanism to lower-end compute tier (\"Tier 2\"). This separation allows the\nmemory demand of the attention, known as the key-value cache, to scale\nindependently from the model weights, enabling larger batch sizes and more\nefficient accelerator usage. Prototyped with NVIDIA T4 GPUs and standard CPU\nVMs, Glinthawk improves throughput by $5.9\\times$ and reduces cost of\ngeneration by $2.8\\times$, compared to paged attention baselines. For long\nsequence lengths, it achieves $16.3\\times$ throughput improvement at\n$2.4\\times$ less cost. Our evaluation shows that this architecture can tolerate\nmoderate network latency with minimal performance degradation, making it highly\neffective for latency-tolerant, throughput-focused applications such as batch\nprocessing. The prototype is publicly available at\nhttps://github.com/microsoft/glinthawk.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Glinthawk, an architecture for offline Large Language Model\n(LLM) inference. By leveraging a two-tiered structure, Glinthawk optimizes the\nutilization of the high-end accelerators (\"Tier 1\") by offloading the attention\nmechanism to lower-end compute tier (\"Tier 2\"). This separation allows the\nmemory demand of the attention, known as the key-value cache, to scale\nindependently from the model weights, enabling larger batch sizes and more\nefficient accelerator usage. Prototyped with NVIDIA T4 GPUs and standard CPU\nVMs, Glinthawk improves throughput by $5.9\\times$ and reduces cost of\ngeneration by $2.8\\times$, compared to paged attention baselines. For long\nsequence lengths, it achieves $16.3\\times$ throughput improvement at\n$2.4\\times$ less cost. Our evaluation shows that this architecture can tolerate\nmoderate network latency with minimal performance degradation, making it highly\neffective for latency-tolerant, throughput-focused applications such as batch\nprocessing. The prototype is publicly available at\nhttps://github.com/microsoft/glinthawk."
                },
                "authors": [
                    {
                        "name": "Pouya Hamadanian"
                    },
                    {
                        "name": "Sadjad Fouladi"
                    }
                ],
                "author_detail": {
                    "name": "Sadjad Fouladi"
                },
                "author": "Sadjad Fouladi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11779v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03736v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03736v3",
                "updated": "2025-02-11T15:42:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    42,
                    19,
                    1,
                    42,
                    0
                ],
                "published": "2024-06-06T04:22:11Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    4,
                    22,
                    11,
                    3,
                    158,
                    0
                ],
                "title": "Your Absorbing Discrete Diffusion Secretly Models the Conditional\n  Distributions of Clean Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Your Absorbing Discrete Diffusion Secretly Models the Conditional\n  Distributions of Clean Data"
                },
                "summary": "Discrete diffusion models with absorbing processes have shown promise in\nlanguage modeling. The key quantities to be estimated are the ratios between\nthe marginal probabilities of two transitive states at all timesteps, called\nthe concrete score. In this paper, we reveal that the concrete score in\nabsorbing diffusion can be expressed as conditional probabilities of clean\ndata, multiplied by a time-dependent scalar in an analytic form. Motivated by\nthis finding, we propose reparameterized absorbing discrete diffusion (RADD), a\ndedicated diffusion model without time-condition that characterizes the\ntime-independent conditional probabilities. Besides its simplicity, RADD can\nreduce the number of function evaluations (NFEs) by caching the output of the\ntime-independent network when the noisy sample remains unchanged in a sampling\ninterval, which enables sampling acceleration. Built upon the new perspective\nof conditional distributions, we further unify absorbing discrete diffusion and\nany-order autoregressive models (AO-ARMs), showing that the upper bound on the\nnegative log-likelihood for the diffusion model can be interpreted as an\nexpected negative log-likelihood for AO-ARMs. Further, our RADD models achieve\nSOTA performance among diffusion models on 5 zero-shot language modeling\nbenchmarks (measured by perplexity) at the GPT-2 scale. Our code is available\nat https://github.com/ML-GSAI/RADD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discrete diffusion models with absorbing processes have shown promise in\nlanguage modeling. The key quantities to be estimated are the ratios between\nthe marginal probabilities of two transitive states at all timesteps, called\nthe concrete score. In this paper, we reveal that the concrete score in\nabsorbing diffusion can be expressed as conditional probabilities of clean\ndata, multiplied by a time-dependent scalar in an analytic form. Motivated by\nthis finding, we propose reparameterized absorbing discrete diffusion (RADD), a\ndedicated diffusion model without time-condition that characterizes the\ntime-independent conditional probabilities. Besides its simplicity, RADD can\nreduce the number of function evaluations (NFEs) by caching the output of the\ntime-independent network when the noisy sample remains unchanged in a sampling\ninterval, which enables sampling acceleration. Built upon the new perspective\nof conditional distributions, we further unify absorbing discrete diffusion and\nany-order autoregressive models (AO-ARMs), showing that the upper bound on the\nnegative log-likelihood for the diffusion model can be interpreted as an\nexpected negative log-likelihood for AO-ARMs. Further, our RADD models achieve\nSOTA performance among diffusion models on 5 zero-shot language modeling\nbenchmarks (measured by perplexity) at the GPT-2 scale. Our code is available\nat https://github.com/ML-GSAI/RADD."
                },
                "authors": [
                    {
                        "name": "Jingyang Ou"
                    },
                    {
                        "name": "Shen Nie"
                    },
                    {
                        "name": "Kaiwen Xue"
                    },
                    {
                        "name": "Fengqi Zhu"
                    },
                    {
                        "name": "Jiacheng Sun"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Chongxuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Chongxuan Li"
                },
                "author": "Chongxuan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03736v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03736v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07578v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07578v1",
                "updated": "2025-02-11T14:25:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    25,
                    20,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T14:25:20Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    25,
                    20,
                    1,
                    42,
                    0
                ],
                "title": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference"
                },
                "summary": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users to pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.3$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users to pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.3$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs."
                },
                "authors": [
                    {
                        "name": "Yufeng Gu"
                    },
                    {
                        "name": "Alireza Khadem"
                    },
                    {
                        "name": "Sumanth Umesh"
                    },
                    {
                        "name": "Ning Liang"
                    },
                    {
                        "name": "Xavier Servot"
                    },
                    {
                        "name": "Onur Mutlu"
                    },
                    {
                        "name": "Ravi Iyer"
                    },
                    {
                        "name": "Reetuparna Das"
                    }
                ],
                "author_detail": {
                    "name": "Reetuparna Das"
                },
                "author": "Reetuparna Das",
                "arxiv_doi": "10.1145/3676641.3716267",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3716267",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.07578v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07578v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, Volume\n  2 (ASPLOS'25)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07115v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07115v1",
                "updated": "2025-02-10T23:11:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    23,
                    11,
                    44,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T23:11:44Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    23,
                    11,
                    44,
                    0,
                    41,
                    0
                ],
                "title": "Online Scheduling for LLM Inference with KV Cache Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Scheduling for LLM Inference with KV Cache Constraints"
                },
                "summary": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment."
                },
                "authors": [
                    {
                        "name": "Patrick Jaillet"
                    },
                    {
                        "name": "Jiashuo Jiang"
                    },
                    {
                        "name": "Chara Podimata"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07115v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07115v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00279v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00279v2",
                "updated": "2025-02-10T18:34:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    34,
                    53,
                    0,
                    41,
                    0
                ],
                "published": "2024-12-31T05:24:30Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "title": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement"
                },
                "summary": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Li"
                },
                "author": "Junjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00279v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00279v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13629v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13629v2",
                "updated": "2025-02-10T17:19:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    17,
                    19,
                    21,
                    0,
                    41,
                    0
                ],
                "published": "2025-01-23T12:58:14Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    58,
                    14,
                    3,
                    23,
                    0
                ],
                "title": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models"
                },
                "summary": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%."
                },
                "authors": [
                    {
                        "name": "Zhenghao Lin"
                    },
                    {
                        "name": "Zihao Tang"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Yeyun Gong"
                    },
                    {
                        "name": "Yi Cheng"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Ying Xin"
                    },
                    {
                        "name": "Ziyue Yang"
                    },
                    {
                        "name": "Kailai Yang"
                    },
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Shuai Lu"
                    },
                    {
                        "name": "Yiming Huang"
                    },
                    {
                        "name": "Zheheng Luo"
                    },
                    {
                        "name": "Lei Qu"
                    },
                    {
                        "name": "Xuan Feng"
                    },
                    {
                        "name": "Yaoxiang Wang"
                    },
                    {
                        "name": "Yuqing Xia"
                    },
                    {
                        "name": "Feiyang Chen"
                    },
                    {
                        "name": "Yuting Jiang"
                    },
                    {
                        "name": "Yasen Hu"
                    },
                    {
                        "name": "Hao Ni"
                    },
                    {
                        "name": "Binyang Li"
                    },
                    {
                        "name": "Guoshuai Zhao"
                    },
                    {
                        "name": "Jui-Hao Chiang"
                    },
                    {
                        "name": "Zhongxin Guo"
                    },
                    {
                        "name": "Chen Lin"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Yelong Shen"
                    },
                    {
                        "name": "Jian Jiao"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13629v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13629v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09425v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09425v2",
                "updated": "2025-02-10T15:17:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    17,
                    49,
                    0,
                    41,
                    0
                ],
                "published": "2024-11-14T13:22:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity"
                },
                "summary": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully."
                },
                "authors": [
                    {
                        "name": "Xiao Lv"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Shijie Guan"
                    },
                    {
                        "name": "Xiaoyou Zhou"
                    },
                    {
                        "name": "Zhiguang Qi"
                    },
                    {
                        "name": "Yaqiang Zang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09425v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09425v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06327v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06327v1",
                "updated": "2025-02-10T10:28:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    10,
                    28,
                    11,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T10:28:11Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    10,
                    28,
                    11,
                    0,
                    41,
                    0
                ],
                "title": "Prompt-Driven Continual Graph Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt-Driven Continual Graph Learning"
                },
                "summary": "Continual Graph Learning (CGL), which aims to accommodate new tasks over\nevolving graph data without forgetting prior knowledge, is garnering\nsignificant research interest. Mainstream solutions adopt the memory\nreplay-based idea, ie, caching representative data from earlier tasks for\nretraining the graph model. However, this strategy struggles with scalability\nissues for constantly evolving graphs and raises concerns regarding data\nprivacy. Inspired by recent advancements in the prompt-based learning paradigm,\nthis paper introduces a novel prompt-driven continual graph learning\n(PROMPTCGL) framework, which learns a separate prompt for each incoming task\nand maintains the underlying graph neural network model fixed. In this way,\nPROMPTCGL naturally avoids catastrophic forgetting of knowledge from previous\ntasks. More specifically, we propose hierarchical prompting to instruct the\nmodel from both feature- and topology-level to fully address the variability of\ntask graphs in dynamic continual learning. Additionally, we develop a\npersonalized prompt generator to generate tailored prompts for each graph node\nwhile minimizing the number of prompts needed, leading to constant memory\nconsumption regardless of the graph scale. Extensive experiments on four\nbenchmarks show that PROMPTCGL achieves superior performance against existing\nCGL approaches while significantly reducing memory consumption. Our code is\navailable at https://github.com/QiWang98/PromptCGL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Graph Learning (CGL), which aims to accommodate new tasks over\nevolving graph data without forgetting prior knowledge, is garnering\nsignificant research interest. Mainstream solutions adopt the memory\nreplay-based idea, ie, caching representative data from earlier tasks for\nretraining the graph model. However, this strategy struggles with scalability\nissues for constantly evolving graphs and raises concerns regarding data\nprivacy. Inspired by recent advancements in the prompt-based learning paradigm,\nthis paper introduces a novel prompt-driven continual graph learning\n(PROMPTCGL) framework, which learns a separate prompt for each incoming task\nand maintains the underlying graph neural network model fixed. In this way,\nPROMPTCGL naturally avoids catastrophic forgetting of knowledge from previous\ntasks. More specifically, we propose hierarchical prompting to instruct the\nmodel from both feature- and topology-level to fully address the variability of\ntask graphs in dynamic continual learning. Additionally, we develop a\npersonalized prompt generator to generate tailored prompts for each graph node\nwhile minimizing the number of prompts needed, leading to constant memory\nconsumption regardless of the graph scale. Extensive experiments on four\nbenchmarks show that PROMPTCGL achieves superior performance against existing\nCGL approaches while significantly reducing memory consumption. Our code is\navailable at https://github.com/QiWang98/PromptCGL."
                },
                "authors": [
                    {
                        "name": "Qi Wang"
                    },
                    {
                        "name": "Tianfei Zhou"
                    },
                    {
                        "name": "Ye Yuan"
                    },
                    {
                        "name": "Rui Mao"
                    }
                ],
                "author_detail": {
                    "name": "Rui Mao"
                },
                "author": "Rui Mao",
                "arxiv_comment": "12 pages, 7figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06327v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06327v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06166v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06166v1",
                "updated": "2025-02-10T05:33:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    5,
                    33,
                    25,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T05:33:25Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    5,
                    33,
                    25,
                    0,
                    41,
                    0
                ],
                "title": "Portable, High-Frequency, and High-Voltage Control Circuits for\n  Untethered Miniature Robots Driven by Dielectric Elastomer Actuators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Portable, High-Frequency, and High-Voltage Control Circuits for\n  Untethered Miniature Robots Driven by Dielectric Elastomer Actuators"
                },
                "summary": "In this work, we propose a high-voltage, high-frequency control circuit for\nthe untethered applications of dielectric elastomer actuators (DEAs). The\ncircuit board leverages low-voltage resistive components connected in series to\ncontrol voltages of up to 1.8 kV within a compact size, suitable for\nfrequencies ranging from 0 to 1 kHz. A single-channel control board weighs only\n2.5 g. We tested the performance of the control circuit under different load\nconditions and power supplies. Based on this control circuit, along with a\ncommercial miniature high-voltage power converter, we construct an untethered\ncrawling robot driven by a cylindrical DEA. The 42-g untethered robots\nsuccessfully obtained crawling locomotion on a bench and within a pipeline at a\ndriving frequency of 15 Hz, while simultaneously transmitting real-time video\ndata via an onboard camera and antenna. Our work provides a practical way to\nuse low-voltage control electronics to achieve the untethered driving of DEAs,\nand therefore portable and wearable devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose a high-voltage, high-frequency control circuit for\nthe untethered applications of dielectric elastomer actuators (DEAs). The\ncircuit board leverages low-voltage resistive components connected in series to\ncontrol voltages of up to 1.8 kV within a compact size, suitable for\nfrequencies ranging from 0 to 1 kHz. A single-channel control board weighs only\n2.5 g. We tested the performance of the control circuit under different load\nconditions and power supplies. Based on this control circuit, along with a\ncommercial miniature high-voltage power converter, we construct an untethered\ncrawling robot driven by a cylindrical DEA. The 42-g untethered robots\nsuccessfully obtained crawling locomotion on a bench and within a pipeline at a\ndriving frequency of 15 Hz, while simultaneously transmitting real-time video\ndata via an onboard camera and antenna. Our work provides a practical way to\nuse low-voltage control electronics to achieve the untethered driving of DEAs,\nand therefore portable and wearable devices."
                },
                "authors": [
                    {
                        "name": "Qi Shao"
                    },
                    {
                        "name": "Xin-Jun Liu"
                    },
                    {
                        "name": "Huichan Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Huichan Zhao"
                },
                "author": "Huichan Zhao",
                "arxiv_comment": "7 pages, 10 figures, accepted by ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06166v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04603v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04603v2",
                "updated": "2025-02-09T20:52:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    9,
                    20,
                    52,
                    26,
                    6,
                    40,
                    0
                ],
                "published": "2024-10-06T19:36:34Z",
                "published_parsed": [
                    2024,
                    10,
                    6,
                    19,
                    36,
                    34,
                    6,
                    280,
                    0
                ],
                "title": "Self-compensating Light Calorimetry with Liquid Argon Time Projection\n  Chamber for GeV Neutrino Physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-compensating Light Calorimetry with Liquid Argon Time Projection\n  Chamber for GeV Neutrino Physics"
                },
                "summary": "The Liquid Argon Time Projection Chamber (LArTPC) is a powerful dual\ncalorimeter capable of estimating particle energy from both ionization charge\nand scintillation light. Our study shows that, due to the recombination\nluminescence, the LArTPC functions as a self-compensating light calorimeter:\nthe missing energy in the hadronic component is compensated for by the\nincreased luminescence relative to the electromagnetic component. Using 0.5--5\nGeV electron neutrino charged current interactions as a case study, we show\nthat good compensation of the electron-to-hadron response ratio (e/h) from\n1--1.05 can be achieved across a broad range of drift electric fields (0.2--1.8\nkV/cm), with better performance for neutrino energies above 2 GeV. This study\nhighlights the potential of light calorimetry in LArTPCs for GeV neutrino\nenergy reconstruction, complementing traditional charge calorimetry. Under\nideal conditions of uniform light collection, we show that LArTPC light\ncalorimetry can achieve an energy resolution comparable to the charge imaging\ncalorimetry. Challenges arising from nonuniform light collection in large\nLArTPCs can be mitigated with a position-dependent light yield correction\nderived from 3D charge signal imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Liquid Argon Time Projection Chamber (LArTPC) is a powerful dual\ncalorimeter capable of estimating particle energy from both ionization charge\nand scintillation light. Our study shows that, due to the recombination\nluminescence, the LArTPC functions as a self-compensating light calorimeter:\nthe missing energy in the hadronic component is compensated for by the\nincreased luminescence relative to the electromagnetic component. Using 0.5--5\nGeV electron neutrino charged current interactions as a case study, we show\nthat good compensation of the electron-to-hadron response ratio (e/h) from\n1--1.05 can be achieved across a broad range of drift electric fields (0.2--1.8\nkV/cm), with better performance for neutrino energies above 2 GeV. This study\nhighlights the potential of light calorimetry in LArTPCs for GeV neutrino\nenergy reconstruction, complementing traditional charge calorimetry. Under\nideal conditions of uniform light collection, we show that LArTPC light\ncalorimetry can achieve an energy resolution comparable to the charge imaging\ncalorimetry. Challenges arising from nonuniform light collection in large\nLArTPCs can be mitigated with a position-dependent light yield correction\nderived from 3D charge signal imaging."
                },
                "authors": [
                    {
                        "name": "Xuyang Ning"
                    },
                    {
                        "name": "Wei Shi"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Ciro Riccio"
                    },
                    {
                        "name": "Jay Hyun Jo"
                    }
                ],
                "author_detail": {
                    "name": "Jay Hyun Jo"
                },
                "author": "Jay Hyun Jo",
                "arxiv_comment": "18 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04603v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04603v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06901v1",
                "updated": "2025-02-09T20:02:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    9,
                    20,
                    2,
                    5,
                    6,
                    40,
                    0
                ],
                "published": "2025-02-09T20:02:05Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    20,
                    2,
                    5,
                    6,
                    40,
                    0
                ],
                "title": "Enabling Autoregressive Models to Fill In Masked Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Autoregressive Models to Fill In Masked Tokens"
                },
                "summary": "Historically, LLMs have been trained using either autoregressive (AR) or\nmasked language modeling (MLM) objectives, with AR models gaining dominance in\nrecent years. However, AR models are inherently incapable of masked infilling,\nwhich is the ability to predict masked tokens between past and future context.\nIn contrast, MLM models suffer from intrinsic computational inefficiencies\nduring both training and inference that hinder their scalability. This work\nintroduces MARIA (Masked and Autoregressive Infilling Architecture), a novel\napproach that leverages the strengths of both paradigms to achieve\nstate-of-the-art masked infilling performance. MARIA combines a pre-trained MLM\nand AR model by training a linear decoder that takes their concatenated hidden\nstates as input. This minimal modification enables the AR model to perform\ninfilling while retaining its inherent advantages in terms of faster inference\nwith KV caching. Our results demonstrate that MARIA significantly outperforms\nexisting methods, namely discrete diffusion models, on masked infilling tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Historically, LLMs have been trained using either autoregressive (AR) or\nmasked language modeling (MLM) objectives, with AR models gaining dominance in\nrecent years. However, AR models are inherently incapable of masked infilling,\nwhich is the ability to predict masked tokens between past and future context.\nIn contrast, MLM models suffer from intrinsic computational inefficiencies\nduring both training and inference that hinder their scalability. This work\nintroduces MARIA (Masked and Autoregressive Infilling Architecture), a novel\napproach that leverages the strengths of both paradigms to achieve\nstate-of-the-art masked infilling performance. MARIA combines a pre-trained MLM\nand AR model by training a linear decoder that takes their concatenated hidden\nstates as input. This minimal modification enables the AR model to perform\ninfilling while retaining its inherent advantages in terms of faster inference\nwith KV caching. Our results demonstrate that MARIA significantly outperforms\nexisting methods, namely discrete diffusion models, on masked infilling tasks."
                },
                "authors": [
                    {
                        "name": "Daniel Israel"
                    },
                    {
                        "name": "Aditya Grover"
                    },
                    {
                        "name": "Guy Van den Broeck"
                    }
                ],
                "author_detail": {
                    "name": "Guy Van den Broeck"
                },
                "author": "Guy Van den Broeck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05960v1",
                "updated": "2025-02-09T17:09:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    9,
                    17,
                    9,
                    20,
                    6,
                    40,
                    0
                ],
                "published": "2025-02-09T17:09:20Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    17,
                    9,
                    20,
                    6,
                    40,
                    0
                ],
                "title": "Electric field control of nonlinear Hall effect in Weyl semimetal\n  TaIrTe4",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electric field control of nonlinear Hall effect in Weyl semimetal\n  TaIrTe4"
                },
                "summary": "The nonlinear Hall effect (NLHE), as an important probe to reveal the\nsymmetry breaking in topological properties of materials, opens up a new\ndimension for exploring the energy band structure and electron transport\nmechanism of quantum materials. Current studies mainly focus on the observation\nof material intrinsic the NLHE or inducing the NLHE response by artificially\nconstructing corrugated/twisted twodimensionalmaterial systems. Notably, the\nmodulation of NLHE signal strength, a core parameter of device performance, has\nattracted much attention, while theoretical predictions suggest that an applied\nelectric field can achieve the NLHE enhancement through modulation of the Berry\ncurvature dipole (BCD). Here we report effective modulation the magnitude and\nsign of the NLHE by applying additional constant electric fields of different\ndirections and magnitudes in the semimetal TaIrTe4. The NLHE response strength\nis enhanced by 168 times compared to the intrinsic one at 4 K when the\nadditional constant electric field of -0.5 kV/cm is applied to the b-axis of\nTaIrTe4 and the through a.c. current is parallel to the TaIrTe4 a-axis. Scaling\nlaw analysis suggests that the enhancement may be the result of the combined\neffect of the electric field on the intrinsic BCD and disorder scattering\neffect of TaIrTe4. This work provides a means to study the properties of\nTaIrTe4, as well as a valuable reference for the study of novel electronic\ndevices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The nonlinear Hall effect (NLHE), as an important probe to reveal the\nsymmetry breaking in topological properties of materials, opens up a new\ndimension for exploring the energy band structure and electron transport\nmechanism of quantum materials. Current studies mainly focus on the observation\nof material intrinsic the NLHE or inducing the NLHE response by artificially\nconstructing corrugated/twisted twodimensionalmaterial systems. Notably, the\nmodulation of NLHE signal strength, a core parameter of device performance, has\nattracted much attention, while theoretical predictions suggest that an applied\nelectric field can achieve the NLHE enhancement through modulation of the Berry\ncurvature dipole (BCD). Here we report effective modulation the magnitude and\nsign of the NLHE by applying additional constant electric fields of different\ndirections and magnitudes in the semimetal TaIrTe4. The NLHE response strength\nis enhanced by 168 times compared to the intrinsic one at 4 K when the\nadditional constant electric field of -0.5 kV/cm is applied to the b-axis of\nTaIrTe4 and the through a.c. current is parallel to the TaIrTe4 a-axis. Scaling\nlaw analysis suggests that the enhancement may be the result of the combined\neffect of the electric field on the intrinsic BCD and disorder scattering\neffect of TaIrTe4. This work provides a means to study the properties of\nTaIrTe4, as well as a valuable reference for the study of novel electronic\ndevices."
                },
                "authors": [
                    {
                        "name": "Jiaju Yang"
                    },
                    {
                        "name": "Lujun Wei"
                    },
                    {
                        "name": "Yanghui Li"
                    },
                    {
                        "name": "Lina Chen"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Feng Li"
                    },
                    {
                        "name": "Ping Liu"
                    },
                    {
                        "name": "Shuang Zhou"
                    },
                    {
                        "name": "Yong Pu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Pu"
                },
                "author": "Yong Pu",
                "arxiv_comment": "19 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05859v1",
                "updated": "2025-02-09T11:36:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    9,
                    11,
                    36,
                    45,
                    6,
                    40,
                    0
                ],
                "published": "2025-02-09T11:36:45Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    11,
                    36,
                    45,
                    6,
                    40,
                    0
                ],
                "title": "SphereFusion: Efficient Panorama Depth Estimation via Gated Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SphereFusion: Efficient Panorama Depth Estimation via Gated Fusion"
                },
                "summary": "Due to the rapid development of panorama cameras, the task of estimating\npanorama depth has attracted significant attention from the computer vision\ncommunity, especially in applications such as robot sensing and autonomous\ndriving. However, existing methods relying on different projection formats\noften encounter challenges, either struggling with distortion and discontinuity\nin the case of equirectangular, cubemap, and tangent projections, or\nexperiencing a loss of texture details with the spherical projection. To tackle\nthese concerns, we present SphereFusion, an end-to-end framework that combines\nthe strengths of various projection methods. Specifically, SphereFusion\ninitially employs 2D image convolution and mesh operations to extract two\ndistinct types of features from the panorama image in both equirectangular and\nspherical projection domains. These features are then projected onto the\nspherical domain, where a gate fusion module selects the most reliable features\nfor fusion. Finally, SphereFusion estimates panorama depth within the spherical\ndomain. Meanwhile, SphereFusion employs a cache strategy to improve the\nefficiency of mesh operation. Extensive experiments on three public panorama\ndatasets demonstrate that SphereFusion achieves competitive results with other\nstate-of-the-art methods, while presenting the fastest inference speed at only\n17 ms on a 512$\\times$1024 panorama image.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the rapid development of panorama cameras, the task of estimating\npanorama depth has attracted significant attention from the computer vision\ncommunity, especially in applications such as robot sensing and autonomous\ndriving. However, existing methods relying on different projection formats\noften encounter challenges, either struggling with distortion and discontinuity\nin the case of equirectangular, cubemap, and tangent projections, or\nexperiencing a loss of texture details with the spherical projection. To tackle\nthese concerns, we present SphereFusion, an end-to-end framework that combines\nthe strengths of various projection methods. Specifically, SphereFusion\ninitially employs 2D image convolution and mesh operations to extract two\ndistinct types of features from the panorama image in both equirectangular and\nspherical projection domains. These features are then projected onto the\nspherical domain, where a gate fusion module selects the most reliable features\nfor fusion. Finally, SphereFusion estimates panorama depth within the spherical\ndomain. Meanwhile, SphereFusion employs a cache strategy to improve the\nefficiency of mesh operation. Extensive experiments on three public panorama\ndatasets demonstrate that SphereFusion achieves competitive results with other\nstate-of-the-art methods, while presenting the fastest inference speed at only\n17 ms on a 512$\\times$1024 panorama image."
                },
                "authors": [
                    {
                        "name": "Qingsong Yan"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Kaiyong Zhao"
                    },
                    {
                        "name": "Jie Chen"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Fei Deng"
                    }
                ],
                "author_detail": {
                    "name": "Fei Deng"
                },
                "author": "Fei Deng",
                "arxiv_comment": "3DV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05763v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05763v1",
                "updated": "2025-02-09T03:49:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    9,
                    3,
                    49,
                    52,
                    6,
                    40,
                    0
                ],
                "published": "2025-02-09T03:49:52Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    3,
                    49,
                    52,
                    6,
                    40,
                    0
                ],
                "title": "Public DNS Resolvers Meet Content Delivery Networks: A Performance\n  Assessment of the Interplay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Public DNS Resolvers Meet Content Delivery Networks: A Performance\n  Assessment of the Interplay"
                },
                "summary": "This paper investigates two key performance aspects of the interplay between\npublic DNS resolution services and content delivery networks -- the latency of\nDNS queries for resolving CDN-accelerated hostnames and the latency between the\nend-user and the CDN's edge server obtained by the user through a given\nresolution service. While these important issues have been considered in the\npast, significant developments, such as the IPv6 finally getting traction, the\nadoption of the ECS extension to DNS by major DNS resolution services, and the\nembracing of anycast by some CDNs warrant a reassessment under these new\nrealities. Among the resolution services we consider, We find Google DNS and\nOpenDNS to lag behind the Cloudflare resolver and, for some CDNs, Quad9 in\nterms of DNS latency, and trace the cause to drastically lower cache hit rates.\nAt the same time, we find that Google and OpenDNS have largely closed the gap\nwith ISP resolvers in the quality of CDNs'client-to-edge-server mappings as\nmeasured by latency, while the Cloudflare resolver still shows some penalty\nwith Akamai, and Quad9 exhibits a noticeable penalty with three of the four\nCDNs in the study, keeping up only for Cloudflare CDN that does not use DNS to\nmap clients to servers. Finally, in several locations, we observe IPv6 penalty\nin the latency of client-to-CDN-edge-server mappings produced by the resolvers.\nMoreover, this penalty does not rise above typical thresholds employed by the\nHappy Eyeballs algorithm for falling back to IPv4 communication. Thus,\ndual-stacked clients in these locations may experience suboptimal performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates two key performance aspects of the interplay between\npublic DNS resolution services and content delivery networks -- the latency of\nDNS queries for resolving CDN-accelerated hostnames and the latency between the\nend-user and the CDN's edge server obtained by the user through a given\nresolution service. While these important issues have been considered in the\npast, significant developments, such as the IPv6 finally getting traction, the\nadoption of the ECS extension to DNS by major DNS resolution services, and the\nembracing of anycast by some CDNs warrant a reassessment under these new\nrealities. Among the resolution services we consider, We find Google DNS and\nOpenDNS to lag behind the Cloudflare resolver and, for some CDNs, Quad9 in\nterms of DNS latency, and trace the cause to drastically lower cache hit rates.\nAt the same time, we find that Google and OpenDNS have largely closed the gap\nwith ISP resolvers in the quality of CDNs'client-to-edge-server mappings as\nmeasured by latency, while the Cloudflare resolver still shows some penalty\nwith Akamai, and Quad9 exhibits a noticeable penalty with three of the four\nCDNs in the study, keeping up only for Cloudflare CDN that does not use DNS to\nmap clients to servers. Finally, in several locations, we observe IPv6 penalty\nin the latency of client-to-CDN-edge-server mappings produced by the resolvers.\nMoreover, this penalty does not rise above typical thresholds employed by the\nHappy Eyeballs algorithm for falling back to IPv4 communication. Thus,\ndual-stacked clients in these locations may experience suboptimal performance."
                },
                "authors": [
                    {
                        "name": "Nicholas Kernan"
                    },
                    {
                        "name": "Joey Li"
                    },
                    {
                        "name": "Rami Al-Dalky"
                    },
                    {
                        "name": "Michael Rabinovich"
                    }
                ],
                "author_detail": {
                    "name": "Michael Rabinovich"
                },
                "author": "Michael Rabinovich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05763v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05763v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05228v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05228v2",
                "updated": "2025-02-08T21:44:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    21,
                    44,
                    24,
                    5,
                    39,
                    0
                ],
                "published": "2024-12-06T17:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    58,
                    57,
                    4,
                    341,
                    0
                ],
                "title": "MC3: Memory Contention based Covert Channel Communication on Shared DRAM\n  System-on-Chips",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MC3: Memory Contention based Covert Channel Communication on Shared DRAM\n  System-on-Chips"
                },
                "summary": "Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a\nwide-range of mobile computing platforms, including edge/IoT devices,\nautonomous systems and smartphones. In SM-SoCs, system-wide shared physical\nmemory enables a convenient and financially-feasible way to make data\naccessible by dozens of processing units (PUs), such as CPU cores and domain\nspecific accelerators. In this study, we investigate vulnerabilities that stem\nfrom the shared use of physical memory in such systems. Due to the diverse\ncomputational characteristics of the PUs they embed, SM-SoCs often do not\nemploy a shared last level cache (LLC). While the literature proposes covert\nchannel attacks for shared memory systems, high-throughput communication is\ncurrently possible by either relying on an LLC or privileged/physical access to\nthe shared memory subsystem.\n  In this study, we introduce a new memory-contention based covert\ncommunication attack, MC3, which specifically targets the shared system memory\nin mobile SoCs. Different from existing attacks, our approach achieves high\nthroughput communication between applications running on CPU and GPU without\nthe need for an LLC or elevated access to the system. We extensively explore\nthe effectiveness of our methodology by demonstrating the trade-off between the\nchannel transmission rate and the robustness of the communication. We\ndemonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to\na transmit rate of 6.4 kbps with less than 1% error rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a\nwide-range of mobile computing platforms, including edge/IoT devices,\nautonomous systems and smartphones. In SM-SoCs, system-wide shared physical\nmemory enables a convenient and financially-feasible way to make data\naccessible by dozens of processing units (PUs), such as CPU cores and domain\nspecific accelerators. In this study, we investigate vulnerabilities that stem\nfrom the shared use of physical memory in such systems. Due to the diverse\ncomputational characteristics of the PUs they embed, SM-SoCs often do not\nemploy a shared last level cache (LLC). While the literature proposes covert\nchannel attacks for shared memory systems, high-throughput communication is\ncurrently possible by either relying on an LLC or privileged/physical access to\nthe shared memory subsystem.\n  In this study, we introduce a new memory-contention based covert\ncommunication attack, MC3, which specifically targets the shared system memory\nin mobile SoCs. Different from existing attacks, our approach achieves high\nthroughput communication between applications running on CPU and GPU without\nthe need for an LLC or elevated access to the system. We extensively explore\nthe effectiveness of our methodology by demonstrating the trade-off between the\nchannel transmission rate and the robustness of the communication. We\ndemonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to\na transmit rate of 6.4 kbps with less than 1% error rate."
                },
                "authors": [
                    {
                        "name": "Ismet Dagli"
                    },
                    {
                        "name": "James Crea"
                    },
                    {
                        "name": "Soner Seckiner"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "Seluk Kse"
                    },
                    {
                        "name": "Mehmet E. Belviranli"
                    }
                ],
                "author_detail": {
                    "name": "Mehmet E. Belviranli"
                },
                "author": "Mehmet E. Belviranli",
                "arxiv_comment": "This paper is accepted to 2025 Design, Automation Test in Europe\n  Conference Exhibition (DATE)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05228v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05228v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22134v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22134v2",
                "updated": "2025-02-08T14:11:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    14,
                    11,
                    25,
                    5,
                    39,
                    0
                ],
                "published": "2024-10-29T15:31:27Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching"
                },
                "summary": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions."
                },
                "authors": [
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Zihang Zhong"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22134v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22134v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09416v2",
                "updated": "2025-02-08T11:51:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    11,
                    51,
                    57,
                    5,
                    39,
                    0
                ],
                "published": "2024-12-12T16:24:35Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    24,
                    35,
                    3,
                    347,
                    0
                ],
                "title": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors"
                },
                "summary": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusions in the mathematical\ndomain. We release MRBench - a new evaluation benchmark containing 192\nconversations and 1,596 responses from seven state-of-the-art LLM-based and\nhuman tutors, providing gold annotations for eight pedagogical dimensions. We\nassess reliability of the popular Prometheus2 and Llama-3.1-8B LLMs as\nevaluators and analyze each tutor's pedagogical abilities, highlighting which\nLLMs are good tutors and which ones are more suitable as question-answering\nsystems. We believe that the presented taxonomy, benchmark, and human-annotated\nlabels will streamline the evaluation process and help track the progress in AI\ntutors' development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusions in the mathematical\ndomain. We release MRBench - a new evaluation benchmark containing 192\nconversations and 1,596 responses from seven state-of-the-art LLM-based and\nhuman tutors, providing gold annotations for eight pedagogical dimensions. We\nassess reliability of the popular Prometheus2 and Llama-3.1-8B LLMs as\nevaluators and analyze each tutor's pedagogical abilities, highlighting which\nLLMs are good tutors and which ones are more suitable as question-answering\nsystems. We believe that the presented taxonomy, benchmark, and human-annotated\nlabels will streamline the evaluation process and help track the progress in AI\ntutors' development."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kseniia Petukhova"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "9 pages",
                "arxiv_journal_ref": "NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05511v1",
                "updated": "2025-02-08T10:14:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    10,
                    14,
                    21,
                    5,
                    39,
                    0
                ],
                "published": "2025-02-08T10:14:21Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    10,
                    14,
                    21,
                    5,
                    39,
                    0
                ],
                "title": "New and Improved Bounds for Markov Paging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "New and Improved Bounds for Markov Paging"
                },
                "summary": "In the Markov paging model, one assumes that page requests are drawn from a\nMarkov chain over the pages in memory, and the goal is to maintain a fast cache\nthat suffers few page faults in expectation. While computing the optimal online\nalgorithm $(\\mathrm{OPT})$ for this problem naively takes time exponential in\nthe size of the cache, the best-known polynomial-time approximation algorithm\nis the dominating distribution algorithm due to Lund, Phillips and Reingold\n(FOCS 1994), who showed that the algorithm is $4$-competitive against\n$\\mathrm{OPT}$. We substantially improve their analysis and show that the\ndominating distribution algorithm is in fact $2$-competitive against\n$\\mathrm{OPT}$. We also show a lower bound of $1.5907$-competitiveness for this\nalgorithm -- to the best of our knowledge, no such lower bound was previously\nknown.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the Markov paging model, one assumes that page requests are drawn from a\nMarkov chain over the pages in memory, and the goal is to maintain a fast cache\nthat suffers few page faults in expectation. While computing the optimal online\nalgorithm $(\\mathrm{OPT})$ for this problem naively takes time exponential in\nthe size of the cache, the best-known polynomial-time approximation algorithm\nis the dominating distribution algorithm due to Lund, Phillips and Reingold\n(FOCS 1994), who showed that the algorithm is $4$-competitive against\n$\\mathrm{OPT}$. We substantially improve their analysis and show that the\ndominating distribution algorithm is in fact $2$-competitive against\n$\\mathrm{OPT}$. We also show a lower bound of $1.5907$-competitiveness for this\nalgorithm -- to the best of our knowledge, no such lower bound was previously\nknown."
                },
                "authors": [
                    {
                        "name": "Chirag Pabbaraju"
                    },
                    {
                        "name": "Ali Vakilian"
                    }
                ],
                "author_detail": {
                    "name": "Ali Vakilian"
                },
                "author": "Ali Vakilian",
                "arxiv_comment": "26 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05433v1",
                "updated": "2025-02-08T03:46:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    46,
                    28,
                    5,
                    39,
                    0
                ],
                "published": "2025-02-08T03:46:28Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    46,
                    28,
                    5,
                    39,
                    0
                ],
                "title": "AdaFlow: Efficient Long Video Editing via Adaptive Attention Slimming\n  And Keyframe Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaFlow: Efficient Long Video Editing via Adaptive Attention Slimming\n  And Keyframe Selection"
                },
                "summary": "Despite great progress, text-driven long video editing is still notoriously\nchallenging mainly due to excessive memory overhead. Although recent efforts\nhave simplified this task into a two-step process of keyframe translation and\ninterpolation generation, the token-wise keyframe translation still plagues the\nupper limit of video length. In this paper, we propose a novel and\ntraining-free approach towards efficient and effective long video editing,\ntermed AdaFlow. We first reveal that not all tokens of video frames hold equal\nimportance for keyframe translation, based on which we propose an Adaptive\nAttention Slimming scheme for AdaFlow to squeeze the $KV$ sequence, thus\nincreasing the number of keyframes for translations by an order of magnitude.\nIn addition, an Adaptive Keyframe Selection scheme is also equipped to select\nthe representative frames for joint editing, further improving generation\nquality. With these innovative designs, AdaFlow achieves high-quality long\nvideo editing of minutes in one inference, i.e., more than 1$k$ frames on one\nA800 GPU, which is about ten times longer than the compared methods, e.g.,\nTokenFlow. To validate AdaFlow, we also build a new benchmark for long video\nediting with high-quality annotations, termed LongV-EVAL. Our code is released\nat: https://github.com/jidantang55/AdaFlow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite great progress, text-driven long video editing is still notoriously\nchallenging mainly due to excessive memory overhead. Although recent efforts\nhave simplified this task into a two-step process of keyframe translation and\ninterpolation generation, the token-wise keyframe translation still plagues the\nupper limit of video length. In this paper, we propose a novel and\ntraining-free approach towards efficient and effective long video editing,\ntermed AdaFlow. We first reveal that not all tokens of video frames hold equal\nimportance for keyframe translation, based on which we propose an Adaptive\nAttention Slimming scheme for AdaFlow to squeeze the $KV$ sequence, thus\nincreasing the number of keyframes for translations by an order of magnitude.\nIn addition, an Adaptive Keyframe Selection scheme is also equipped to select\nthe representative frames for joint editing, further improving generation\nquality. With these innovative designs, AdaFlow achieves high-quality long\nvideo editing of minutes in one inference, i.e., more than 1$k$ frames on one\nA800 GPU, which is about ten times longer than the compared methods, e.g.,\nTokenFlow. To validate AdaFlow, we also build a new benchmark for long video\nediting with high-quality annotations, termed LongV-EVAL. Our code is released\nat: https://github.com/jidantang55/AdaFlow."
                },
                "authors": [
                    {
                        "name": "Shuheng Zhang"
                    },
                    {
                        "name": "Yuqi Liu"
                    },
                    {
                        "name": "Hongbo Zhou"
                    },
                    {
                        "name": "Jun Peng"
                    },
                    {
                        "name": "Yiyi Zhou"
                    },
                    {
                        "name": "Xiaoshuai Sun"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05431v1",
                "updated": "2025-02-08T03:41:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    41,
                    16,
                    5,
                    39,
                    0
                ],
                "published": "2025-02-08T03:41:16Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    41,
                    16,
                    5,
                    39,
                    0
                ],
                "title": "APE: Faster and Longer Context-Augmented Generation via Adaptive\n  Parallel Encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APE: Faster and Longer Context-Augmented Generation via Adaptive\n  Parallel Encoding"
                },
                "summary": "Context-augmented generation (CAG) techniques, including RAG and ICL, require\nthe efficient combination of multiple contexts to generate responses to user\nqueries. Directly inputting these contexts as a sequence introduces a\nconsiderable computational burden by re-encoding the combined selection of\ncontexts for every request. To address this, we explore the promising potential\nof parallel encoding to independently pre-compute and cache each context's KV\nstates. This approach enables the direct loading of cached states during\ninference while accommodating more contexts through position reuse across\ncontexts. However, due to misalignments in attention distribution, directly\napplying parallel encoding results in a significant performance drop. To enable\neffective and efficient CAG, we propose Adaptive Parallel Encoding\n($\\textbf{APE}$), which brings shared prefix, attention temperature, and\nscaling factor to align the distribution of parallel encoding with sequential\nencoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98%\nand 93% sequential encoding performance using the same inputs while\noutperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales\nto many-shot CAG, effectively encoding hundreds of contexts in parallel.\nEfficiency evaluation shows that APE can achieve an end-to-end 4.5$\\times$\nspeedup by reducing 28$\\times$ prefilling time for a 128K-length context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-augmented generation (CAG) techniques, including RAG and ICL, require\nthe efficient combination of multiple contexts to generate responses to user\nqueries. Directly inputting these contexts as a sequence introduces a\nconsiderable computational burden by re-encoding the combined selection of\ncontexts for every request. To address this, we explore the promising potential\nof parallel encoding to independently pre-compute and cache each context's KV\nstates. This approach enables the direct loading of cached states during\ninference while accommodating more contexts through position reuse across\ncontexts. However, due to misalignments in attention distribution, directly\napplying parallel encoding results in a significant performance drop. To enable\neffective and efficient CAG, we propose Adaptive Parallel Encoding\n($\\textbf{APE}$), which brings shared prefix, attention temperature, and\nscaling factor to align the distribution of parallel encoding with sequential\nencoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98%\nand 93% sequential encoding performance using the same inputs while\noutperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales\nto many-shot CAG, effectively encoding hundreds of contexts in parallel.\nEfficiency evaluation shows that APE can achieve an end-to-end 4.5$\\times$\nspeedup by reducing 28$\\times$ prefilling time for a 128K-length context."
                },
                "authors": [
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05429v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05429v1",
                "updated": "2025-02-08T03:35:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    35,
                    55,
                    5,
                    39,
                    0
                ],
                "published": "2025-02-08T03:35:55Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    35,
                    55,
                    5,
                    39,
                    0
                ],
                "title": "SMaCk: Efficient Instruction Cache Attacks via Self-Modifying Code\n  Conflicts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SMaCk: Efficient Instruction Cache Attacks via Self-Modifying Code\n  Conflicts"
                },
                "summary": "Self-modifying code (SMC) allows programs to alter their own instructions,\noptimizing performance and functionality on x86 processors. Despite its\nbenefits, SMC introduces unique microarchitectural behaviors that can be\nexploited for malicious purposes. In this paper, we explore the security\nimplications of SMC by examining how specific x86 instructions affecting\ninstruction cache lines lead to measurable timing discrepancies between cache\nhits and misses. These discrepancies facilitate refined cache attacks, making\nthem less noisy and more effective. We introduce novel attack techniques that\nleverage these timing variations to enhance existing methods such as\nPrime+Probe and Flush+Reload. Our advanced techniques allow adversaries to more\nprecisely attack cryptographic keys and create covert channels akin to Spectre\nacross various x86 platforms. Finally, we propose a dynamic detection\nmethodology utilizing hardware performance counters to mitigate these enhanced\nthreats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-modifying code (SMC) allows programs to alter their own instructions,\noptimizing performance and functionality on x86 processors. Despite its\nbenefits, SMC introduces unique microarchitectural behaviors that can be\nexploited for malicious purposes. In this paper, we explore the security\nimplications of SMC by examining how specific x86 instructions affecting\ninstruction cache lines lead to measurable timing discrepancies between cache\nhits and misses. These discrepancies facilitate refined cache attacks, making\nthem less noisy and more effective. We introduce novel attack techniques that\nleverage these timing variations to enhance existing methods such as\nPrime+Probe and Flush+Reload. Our advanced techniques allow adversaries to more\nprecisely attack cryptographic keys and create covert channels akin to Spectre\nacross various x86 platforms. Finally, we propose a dynamic detection\nmethodology utilizing hardware performance counters to mitigate these enhanced\nthreats."
                },
                "authors": [
                    {
                        "name": "Seonghun Son"
                    },
                    {
                        "name": "Daniel Moghimi"
                    },
                    {
                        "name": "Berk Gulmezoglu"
                    }
                ],
                "author_detail": {
                    "name": "Berk Gulmezoglu"
                },
                "author": "Berk Gulmezoglu",
                "arxiv_doi": "10.1145/3676641.3716274",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3716274",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.05429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05429v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Proceedings of the 30th ACM International Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS) accepted",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12304v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12304v4",
                "updated": "2025-02-07T23:14:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    23,
                    14,
                    10,
                    4,
                    38,
                    0
                ],
                "published": "2024-05-20T18:11:45Z",
                "published_parsed": [
                    2024,
                    5,
                    20,
                    18,
                    11,
                    45,
                    0,
                    141,
                    0
                ],
                "title": "Automatic Hardware Pragma Insertion in High-Level Synthesis: A\n  Non-Linear Programming Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Hardware Pragma Insertion in High-Level Synthesis: A\n  Non-Linear Programming Approach"
                },
                "summary": "High-Level Synthesis enables the rapid prototyping of hardware accelerators,\nby combining a high-level description of the functional behavior of a kernel\nwith a set of micro-architecture optimizations as inputs. Such optimizations\ncan be described by inserting pragmas e.g. pipelining and replication of units,\nor even higher level transformations for HLS such as automatic data caching\nusing the AMD/Xilinx Merlin compiler. Selecting the best combination of\npragmas, even within a restricted set, remains particularly challenging and the\ntypical state-of-practice uses design-space exploration to navigate this space.\nBut due to the highly irregular performance distribution of pragma\nconfigurations, typical DSE approaches are either extremely time consuming, or\noperating on a severely restricted search space. This work proposes a framework\nto automatically insert HLS pragmas in regular loop-based programs, supporting\npipelining, unit replication, and data caching. We develop an analytical\nperformance and resource model as a function of the input program properties\nand pragmas inserted, using non-linear constraints and objectives. We prove\nthis model provides a lower bound on the actual performance after HLS. We then\nencode this model as a Non-Linear Program, by making the pragma configuration\nunknowns of the system, which is computed optimally by solving this NLP. This\napproach can also be used during DSE, to quickly prune points with a (possibly\npartial) pragma configuration, driven by lower bounds on achievable latency. We\nextensively evaluate our end-to-end, fully implemented system, showing it can\neffectively manipulate spaces of billions of designs in seconds to minutes for\nthe kernels evaluated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Level Synthesis enables the rapid prototyping of hardware accelerators,\nby combining a high-level description of the functional behavior of a kernel\nwith a set of micro-architecture optimizations as inputs. Such optimizations\ncan be described by inserting pragmas e.g. pipelining and replication of units,\nor even higher level transformations for HLS such as automatic data caching\nusing the AMD/Xilinx Merlin compiler. Selecting the best combination of\npragmas, even within a restricted set, remains particularly challenging and the\ntypical state-of-practice uses design-space exploration to navigate this space.\nBut due to the highly irregular performance distribution of pragma\nconfigurations, typical DSE approaches are either extremely time consuming, or\noperating on a severely restricted search space. This work proposes a framework\nto automatically insert HLS pragmas in regular loop-based programs, supporting\npipelining, unit replication, and data caching. We develop an analytical\nperformance and resource model as a function of the input program properties\nand pragmas inserted, using non-linear constraints and objectives. We prove\nthis model provides a lower bound on the actual performance after HLS. We then\nencode this model as a Non-Linear Program, by making the pragma configuration\nunknowns of the system, which is computed optimally by solving this NLP. This\napproach can also be used during DSE, to quickly prune points with a (possibly\npartial) pragma configuration, driven by lower bounds on achievable latency. We\nextensively evaluate our end-to-end, fully implemented system, showing it can\neffectively manipulate spaces of billions of designs in seconds to minutes for\nthe kernels evaluated."
                },
                "authors": [
                    {
                        "name": "Stphane Pouget"
                    },
                    {
                        "name": "Louis-Nol Pouchet"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "arxiv_doi": "10.1145/3711847",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3711847",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.12304v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12304v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05370v1",
                "updated": "2025-02-07T22:51:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    22,
                    51,
                    17,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T22:51:17Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    22,
                    51,
                    17,
                    4,
                    38,
                    0
                ],
                "title": "fMoE: Fine-Grained Expert Offloading for Large Mixture-of-Experts\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "fMoE: Fine-Grained Expert Offloading for Large Mixture-of-Experts\n  Serving"
                },
                "summary": "Large Language Models (LLMs) have gained immense success in revolutionizing\nvarious applications, including content generation, search and recommendation,\nand AI-assisted operation. To reduce high training costs, Mixture-of-Experts\n(MoE) architecture has become a popular backbone for modern LLMs. However,\ndespite the benefits, serving MoE-based LLMs experience severe memory\ninefficiency due to sparsely activated experts. Recent studies propose to\noffload inactive experts from GPU memory to CPU memory to improve the serving\nefficiency of MoE models. However, they either incur high inference latency or\nhigh model memory footprints due to coarse-grained designs. To tame the\nlatency-memory trade-off in MoE serving, we present fMoE, a fine-grained expert\noffloading system for MoE serving that achieves low inference latency with\nmemory efficiency. We design fMoE to extract fine-grained expert selection\npatterns from MoE models and semantic hints from input prompts to efficiently\nguide expert prefetching, caching, and offloading decisions. fMoE is prototyped\non top of HuggingFace Transformers and deployed on a six-GPU testbed.\nExperiments with open-source MoE models and real-world workloads show that fMoE\nreduces inference latency by 47% and improves expert hit rate by 36% over\nstate-of-the-art solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained immense success in revolutionizing\nvarious applications, including content generation, search and recommendation,\nand AI-assisted operation. To reduce high training costs, Mixture-of-Experts\n(MoE) architecture has become a popular backbone for modern LLMs. However,\ndespite the benefits, serving MoE-based LLMs experience severe memory\ninefficiency due to sparsely activated experts. Recent studies propose to\noffload inactive experts from GPU memory to CPU memory to improve the serving\nefficiency of MoE models. However, they either incur high inference latency or\nhigh model memory footprints due to coarse-grained designs. To tame the\nlatency-memory trade-off in MoE serving, we present fMoE, a fine-grained expert\noffloading system for MoE serving that achieves low inference latency with\nmemory efficiency. We design fMoE to extract fine-grained expert selection\npatterns from MoE models and semantic hints from input prompts to efficiently\nguide expert prefetching, caching, and offloading decisions. fMoE is prototyped\non top of HuggingFace Transformers and deployed on a six-GPU testbed.\nExperiments with open-source MoE models and real-world workloads show that fMoE\nreduces inference latency by 47% and improves expert hit rate by 36% over\nstate-of-the-art solutions."
                },
                "authors": [
                    {
                        "name": "Hanfei Yu"
                    },
                    {
                        "name": "Xingqi Cui"
                    },
                    {
                        "name": "Hong Zhang"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v2",
                "updated": "2025-02-07T22:00:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    22,
                    0,
                    48,
                    4,
                    38,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPA's memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPA's memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "arxiv_comment": "31 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04923v1",
                "updated": "2025-02-07T13:41:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    41,
                    51,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T13:41:51Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    41,
                    51,
                    4,
                    38,
                    0
                ],
                "title": "Cached Multi-Lora Composition for Multi-Concept Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cached Multi-Lora Composition for Multi-Concept Image Generation"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a widely adopted technique in\ntext-to-image models, enabling precise rendering of multiple distinct elements,\nsuch as characters and styles, in multi-concept image generation. However,\ncurrent approaches face significant challenges when composing these LoRAs for\nmulti-concept image generation, resulting in diminished generated image\nquality. In this paper, we initially investigate the role of LoRAs in the\ndenoising process through the lens of the Fourier frequency domain. Based on\nthe hypothesis that applying multiple LoRAs could lead to \"semantic conflicts\",\nwe find that certain LoRAs amplify high-frequency features such as edges and\ntextures, whereas others mainly focus on low-frequency elements, including the\noverall structure and smooth color gradients. Building on these insights, we\ndevise a frequency domain based sequencing strategy to determine the optimal\norder in which LoRAs should be integrated during inference. This strategy\noffers a methodical and generalizable solution compared to the naive\nintegration commonly found in existing LoRA fusion techniques. To fully\nleverage our proposed LoRA order sequence determination method in multi-LoRA\ncomposition tasks, we introduce a novel, training-free framework, Cached\nMulti-LoRA (CMLoRA), designed to efficiently integrate multiple LoRAs while\nmaintaining cohesive image generation. With its flexible backbone for\nmulti-LoRA fusion and a non-uniform caching strategy tailored to individual\nLoRAs, CMLoRA has the potential to reduce semantic conflicts in LoRA\ncomposition and improve computational efficiency. Our experimental evaluations\ndemonstrate that CMLoRA outperforms state-of-the-art training-free LoRA fusion\nmethods by a significant margin -- it achieves an average improvement of\n$2.19\\%$ in CLIPScore, and $11.25\\%$ in MLLM win rate compared to LoraHub, LoRA\nComposite, and LoRA Switch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a widely adopted technique in\ntext-to-image models, enabling precise rendering of multiple distinct elements,\nsuch as characters and styles, in multi-concept image generation. However,\ncurrent approaches face significant challenges when composing these LoRAs for\nmulti-concept image generation, resulting in diminished generated image\nquality. In this paper, we initially investigate the role of LoRAs in the\ndenoising process through the lens of the Fourier frequency domain. Based on\nthe hypothesis that applying multiple LoRAs could lead to \"semantic conflicts\",\nwe find that certain LoRAs amplify high-frequency features such as edges and\ntextures, whereas others mainly focus on low-frequency elements, including the\noverall structure and smooth color gradients. Building on these insights, we\ndevise a frequency domain based sequencing strategy to determine the optimal\norder in which LoRAs should be integrated during inference. This strategy\noffers a methodical and generalizable solution compared to the naive\nintegration commonly found in existing LoRA fusion techniques. To fully\nleverage our proposed LoRA order sequence determination method in multi-LoRA\ncomposition tasks, we introduce a novel, training-free framework, Cached\nMulti-LoRA (CMLoRA), designed to efficiently integrate multiple LoRAs while\nmaintaining cohesive image generation. With its flexible backbone for\nmulti-LoRA fusion and a non-uniform caching strategy tailored to individual\nLoRAs, CMLoRA has the potential to reduce semantic conflicts in LoRA\ncomposition and improve computational efficiency. Our experimental evaluations\ndemonstrate that CMLoRA outperforms state-of-the-art training-free LoRA fusion\nmethods by a significant margin -- it achieves an average improvement of\n$2.19\\%$ in CLIPScore, and $11.25\\%$ in MLLM win rate compared to LoraHub, LoRA\nComposite, and LoRA Switch."
                },
                "authors": [
                    {
                        "name": "Xiandong Zou"
                    },
                    {
                        "name": "Mingzhu Shen"
                    },
                    {
                        "name": "Christos-Savvas Bouganis"
                    },
                    {
                        "name": "Yiren Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yiren Zhao"
                },
                "author": "Yiren Zhao",
                "arxiv_comment": "The Thirteenth International Conference on Learning Representations\n  (ICLR 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14846v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14846v2",
                "updated": "2025-02-07T13:09:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    9,
                    17,
                    4,
                    38,
                    0
                ],
                "published": "2024-09-23T09:22:59Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    9,
                    22,
                    59,
                    0,
                    267,
                    0
                ],
                "title": "A-VL: Adaptive Attention for Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A-VL: Adaptive Attention for Large Vision-Language Models"
                },
                "summary": "The Large Vision-Language Model (LVLM) integrates computer vision and natural\nlanguage processing techniques, offering substantial application potential.\nHowever, these models demand extensive resources during inference. Adaptive\nattention techniques can dynamically reduce computational redundancy and thus\nimprove efficiency. Although current adaptive attention methods significantly\nreduce the memory requirements of Transformer-based language models, they are\nnot tailored for LVLMs. We observe that LVLMs generate responses from both\nremote image tokens and local text tokens, and different modalities have\ndifferent attention patterns. This observation inspires us to manage the\nattention for each modality separately. Specifically, for visual input, we\nstore the cache of potentially useful information but only compute the most\ncritical parts. For language input, we care more about local information. Based\non our observation and analysis of vision-language attention patterns, we\ndevelop A-VL, a plug-and-play adaptive attention tailored for LVLM inference.\nExtensive evaluations on three vision-language tasks and five datasets show the\neffectiveness of our designs. Our approach A-VL outperforms existing adaptive\nattention methods in reducing memory usage and computational load without\ncompromising performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Large Vision-Language Model (LVLM) integrates computer vision and natural\nlanguage processing techniques, offering substantial application potential.\nHowever, these models demand extensive resources during inference. Adaptive\nattention techniques can dynamically reduce computational redundancy and thus\nimprove efficiency. Although current adaptive attention methods significantly\nreduce the memory requirements of Transformer-based language models, they are\nnot tailored for LVLMs. We observe that LVLMs generate responses from both\nremote image tokens and local text tokens, and different modalities have\ndifferent attention patterns. This observation inspires us to manage the\nattention for each modality separately. Specifically, for visual input, we\nstore the cache of potentially useful information but only compute the most\ncritical parts. For language input, we care more about local information. Based\non our observation and analysis of vision-language attention patterns, we\ndevelop A-VL, a plug-and-play adaptive attention tailored for LVLM inference.\nExtensive evaluations on three vision-language tasks and five datasets show the\neffectiveness of our designs. Our approach A-VL outperforms existing adaptive\nattention methods in reducing memory usage and computational load without\ncompromising performance."
                },
                "authors": [
                    {
                        "name": "Junyang Zhang"
                    },
                    {
                        "name": "Mu Yuan"
                    },
                    {
                        "name": "Ruiguang Zhong"
                    },
                    {
                        "name": "Puhan Luo"
                    },
                    {
                        "name": "Huiyou Zhan"
                    },
                    {
                        "name": "Ningkang Zhang"
                    },
                    {
                        "name": "Chengchen Hu"
                    },
                    {
                        "name": "Xiangyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyang Li"
                },
                "author": "Xiangyang Li",
                "arxiv_comment": "AAAI 2025 Accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14846v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14846v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04760v1",
                "updated": "2025-02-07T08:48:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    48,
                    6,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T08:48:06Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    48,
                    6,
                    4,
                    38,
                    0
                ],
                "title": "Graph Federated Learning Based Proactive Content Caching in Edge\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Federated Learning Based Proactive Content Caching in Edge\n  Computing"
                },
                "summary": "With the rapid growth of mobile data traffic and the increasing prevalence of\nvideo streaming, proactive content caching in edge computing has become crucial\nfor reducing latency and alleviating network congestion. However, traditional\ncaching strategies such as FIFO, LRU, and LFU fail to effectively predict\nfuture content popularity, while existing proactive caching approaches often\nrequire users to upload data to a central server, raising concerns regarding\nprivacy and scalability. To address these challenges, this paper proposes a\nGraph Federated Learning-based Proactive Content Caching (GFPCC) scheme that\nenhances caching efficiency while preserving user privacy. The proposed\napproach integrates federated learning and graph neural networks, enabling\nusers to locally train Light Graph Convolutional Networks (LightGCN) to capture\nuser-item relationships and predict content popularity. Instead of sharing raw\ndata, only the trained model parameters are transmitted to the central server,\nwhere a federated averaging algorithm aggregates updates, refines the global\nmodel, and selects the most popular files for proactive caching. Experimental\nevaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC\noutperforms baseline caching algorithms by achieving higher cache efficiency\nthrough more accurate content popularity predictions. Moreover, the federated\nlearning framework strengthens privacy protection while maintaining efficient\nmodel training; however, scalability remains a challenge in large-scale\nnetworks with dynamic user preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of mobile data traffic and the increasing prevalence of\nvideo streaming, proactive content caching in edge computing has become crucial\nfor reducing latency and alleviating network congestion. However, traditional\ncaching strategies such as FIFO, LRU, and LFU fail to effectively predict\nfuture content popularity, while existing proactive caching approaches often\nrequire users to upload data to a central server, raising concerns regarding\nprivacy and scalability. To address these challenges, this paper proposes a\nGraph Federated Learning-based Proactive Content Caching (GFPCC) scheme that\nenhances caching efficiency while preserving user privacy. The proposed\napproach integrates federated learning and graph neural networks, enabling\nusers to locally train Light Graph Convolutional Networks (LightGCN) to capture\nuser-item relationships and predict content popularity. Instead of sharing raw\ndata, only the trained model parameters are transmitted to the central server,\nwhere a federated averaging algorithm aggregates updates, refines the global\nmodel, and selects the most popular files for proactive caching. Experimental\nevaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC\noutperforms baseline caching algorithms by achieving higher cache efficiency\nthrough more accurate content popularity predictions. Moreover, the federated\nlearning framework strengthens privacy protection while maintaining efficient\nmodel training; however, scalability remains a challenge in large-scale\nnetworks with dynamic user preferences."
                },
                "authors": [
                    {
                        "name": "Rui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Wang"
                },
                "author": "Rui Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21035v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21035v2",
                "updated": "2025-02-06T20:26:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    20,
                    26,
                    24,
                    3,
                    37,
                    0
                ],
                "published": "2024-10-28T13:56:30Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    56,
                    30,
                    0,
                    302,
                    0
                ],
                "title": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time"
                },
                "summary": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, at the 1.3B parameters scale,\ndiffusion models, even without caching, can generate tokens at a rate that is\nup to 8 times faster than AR models employing KV-caching, and we anticipate\nfurther improvements with the inclusion of caching. Moreover, we demonstrate\nthe efficacy of our approach for diffusion language models with up to 860M\nparameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, at the 1.3B parameters scale,\ndiffusion models, even without caching, can generate tokens at a rate that is\nup to 8 times faster than AR models employing KV-caching, and we anticipate\nfurther improvements with the inclusion of caching. Moreover, we demonstrate\nthe efficacy of our approach for diffusion language models with up to 860M\nparameters."
                },
                "authors": [
                    {
                        "name": "Justin Deschenaux"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21035v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21035v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04420v1",
                "updated": "2025-02-06T15:26:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    26,
                    26,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T15:26:26Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    26,
                    26,
                    3,
                    37,
                    0
                ],
                "title": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference"
                },
                "summary": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths."
                },
                "authors": [
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zeyu Xing"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Linping Qu"
                    },
                    {
                        "name": "Hui-Ling Zhen"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Sinno Jialin Pan"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04077v1",
                "updated": "2025-02-06T13:41:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    41,
                    46,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T13:41:46Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    41,
                    46,
                    3,
                    37,
                    0
                ],
                "title": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference"
                },
                "summary": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through heuristic ranking with attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the \\textit{temporal patterns} in attention scores,\nresulting in a noticeable degradation in LLM performance. To address this\nchallenge, we propose AttentionPredictor, which is the first learning-based\ncritical token identification approach. Specifically, AttentionPredictor learns\na lightweight convolution model to capture spatiotemporal patterns and predict\nthe next-token attention score. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score while consuming negligible\nmemory. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n16$\\times$ KV cache compression with comparable LLM performance, significantly\noutperforming the state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through heuristic ranking with attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the \\textit{temporal patterns} in attention scores,\nresulting in a noticeable degradation in LLM performance. To address this\nchallenge, we propose AttentionPredictor, which is the first learning-based\ncritical token identification approach. Specifically, AttentionPredictor learns\na lightweight convolution model to capture spatiotemporal patterns and predict\nthe next-token attention score. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score while consuming negligible\nmemory. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n16$\\times$ KV cache compression with comparable LLM performance, significantly\noutperforming the state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Qingyue Yang"
                    },
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zhihai Wang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Bin Li"
                    }
                ],
                "author_detail": {
                    "name": "Bin Li"
                },
                "author": "Bin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.06893v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.06893v4",
                "updated": "2025-02-06T12:32:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    32,
                    34,
                    3,
                    37,
                    0
                ],
                "published": "2023-12-11T23:34:23Z",
                "published_parsed": [
                    2023,
                    12,
                    11,
                    23,
                    34,
                    23,
                    0,
                    345,
                    0
                ],
                "title": "Styx: Transactional Stateful Functions on Streaming Dataflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Styx: Transactional Stateful Functions on Streaming Dataflows"
                },
                "summary": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches provide weak transactional\nguarantees or perform expensive external state accesses requiring inefficient\ntransactional protocols that increase execution latency.\n  In this paper, we present Styx, a novel dataflow-based SFaaS runtime that\nexecutes serializable transactions consisting of stateful functions that form\narbitrary call-graphs with exactly-once guarantees. Styx extends a\ndeterministic transactional protocol by contributing: i) a function\nacknowledgment scheme to determine transaction boundaries required in SFaaS\nworkloads, ii) a function-execution caching mechanism, and iii) an early-commit\nreply mechanism that substantially reduces transaction execution latency.\nExperiments with the YCSB, TPC-C, and Deathstar benchmarks show that Styx\noutperforms state-of-the-art approaches by achieving at least one order of\nmagnitude higher throughput while exhibiting near-linear scalability and low\nlatency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches provide weak transactional\nguarantees or perform expensive external state accesses requiring inefficient\ntransactional protocols that increase execution latency.\n  In this paper, we present Styx, a novel dataflow-based SFaaS runtime that\nexecutes serializable transactions consisting of stateful functions that form\narbitrary call-graphs with exactly-once guarantees. Styx extends a\ndeterministic transactional protocol by contributing: i) a function\nacknowledgment scheme to determine transaction boundaries required in SFaaS\nworkloads, ii) a function-execution caching mechanism, and iii) an early-commit\nreply mechanism that substantially reduces transaction execution latency.\nExperiments with the YCSB, TPC-C, and Deathstar benchmarks show that Styx\noutperforms state-of-the-art approaches by achieving at least one order of\nmagnitude higher throughput while exhibiting near-linear scalability and low\nlatency."
                },
                "authors": [
                    {
                        "name": "Kyriakos Psarakis"
                    },
                    {
                        "name": "George Christodoulou"
                    },
                    {
                        "name": "George Siachamis"
                    },
                    {
                        "name": "Marios Fragkoulis"
                    },
                    {
                        "name": "Asterios Katsifodimos"
                    }
                ],
                "author_detail": {
                    "name": "Asterios Katsifodimos"
                },
                "author": "Asterios Katsifodimos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.06893v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.06893v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04018v1",
                "updated": "2025-02-06T12:19:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    19,
                    34,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T12:19:34Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    19,
                    34,
                    3,
                    37,
                    0
                ],
                "title": "PINT: Physics-Informed Neural Time Series Models with Applications to\n  Long-term Inference on WeatherBench 2m-Temperature Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PINT: Physics-Informed Neural Time Series Models with Applications to\n  Long-term Inference on WeatherBench 2m-Temperature Data"
                },
                "summary": "This paper introduces PINT (Physics-Informed Neural Time Series Models), a\nframework that integrates physical constraints into neural time series models\nto improve their ability to capture complex dynamics. We apply PINT to the ERA5\nWeatherBench dataset, focusing on long-term forecasting of 2m-temperature data.\nPINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed\nprior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures.\nThis equation's analytical solutions (sine and cosine functions) facilitate\nrigorous evaluation of the benefits of incorporating physics-informed\nconstraints. By benchmarking against a linear regression baseline derived from\nits exact solutions, we quantify the impact of embedding physical principles in\ndata-driven models. Unlike traditional time series models that rely on future\nobservations, PINT is designed for practical forecasting. Using only the first\n90 days of observed data, it iteratively predicts the next two years,\naddressing challenges posed by limited real-time updates. Experiments on the\nWeatherBench dataset demonstrate PINT's ability to generalize, capture periodic\ntrends, and align with physical principles. This study highlights the potential\nof physics-informed neural models in bridging machine learning and\ninterpretable climate applications.\n  Our models and datasets are publicly available on GitHub:\nhttps://github.com/KV-Park.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces PINT (Physics-Informed Neural Time Series Models), a\nframework that integrates physical constraints into neural time series models\nto improve their ability to capture complex dynamics. We apply PINT to the ERA5\nWeatherBench dataset, focusing on long-term forecasting of 2m-temperature data.\nPINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed\nprior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures.\nThis equation's analytical solutions (sine and cosine functions) facilitate\nrigorous evaluation of the benefits of incorporating physics-informed\nconstraints. By benchmarking against a linear regression baseline derived from\nits exact solutions, we quantify the impact of embedding physical principles in\ndata-driven models. Unlike traditional time series models that rely on future\nobservations, PINT is designed for practical forecasting. Using only the first\n90 days of observed data, it iteratively predicts the next two years,\naddressing challenges posed by limited real-time updates. Experiments on the\nWeatherBench dataset demonstrate PINT's ability to generalize, capture periodic\ntrends, and align with physical principles. This study highlights the potential\nof physics-informed neural models in bridging machine learning and\ninterpretable climate applications.\n  Our models and datasets are publicly available on GitHub:\nhttps://github.com/KV-Park."
                },
                "authors": [
                    {
                        "name": "Keon Vin Park"
                    },
                    {
                        "name": "Jisu Kim"
                    },
                    {
                        "name": "Jaemin Seo"
                    }
                ],
                "author_detail": {
                    "name": "Jaemin Seo"
                },
                "author": "Jaemin Seo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01449v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01449v2",
                "updated": "2025-02-06T08:36:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    8,
                    36,
                    44,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-03T15:38:53Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    15,
                    38,
                    53,
                    0,
                    34,
                    0
                ],
                "title": "PlaceIT: Placement-based Inter-Chiplet Interconnect Topologies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PlaceIT: Placement-based Inter-Chiplet Interconnect Topologies"
                },
                "summary": "2.5D integration technology is gaining traction as it copes with the\nexponentially growing design cost of modern integrated circuits. A crucial part\nof a 2.5D stacked chip is a low-latency and high-throughput inter-chiplet\ninterconnect (ICI). Two major factors affecting the latency and throughput are\nthe topology of links between chiplets and the chiplet placement. In this work,\nwe present PlaceIT, a novel methodology to jointly optimize the ICI topology\nand the chiplet placement. While state-of-the-art methods optimize the chiplet\nplacement for a predetermined ICI topology, or they select one topology out of\na set of candidates, we generate a completely new topology for each placement.\nOur process of inferring placement-based ICI topologies connects chiplets that\nare in close proximity to each other, making it particularly attractive for\nchips with silicon bridges or passive silicon interposers with severely limited\nlink lengths. We provide an open-source implementation of our method that\noptimizes the placement of homogeneously or heterogeneously shaped chiplets and\nthe ICI topology connecting them for a user-defined mix of four different\ntraffic types. We evaluate our methodology using synthetic traffic and traces,\nand we compare our results to a 2D mesh baseline. PlaceIT reduces the latency\nof synthetic L1-to-L2 and L2-to-memory traffic, the two most important types\nfor cache coherency traffic, by up to 28% and 62%, respectively. It also\nachieve an average packet latency reduction of up to 18% on traffic traces.\nPlaceIT enables the construction of 2.5D stacked chips with low-latency ICIs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "2.5D integration technology is gaining traction as it copes with the\nexponentially growing design cost of modern integrated circuits. A crucial part\nof a 2.5D stacked chip is a low-latency and high-throughput inter-chiplet\ninterconnect (ICI). Two major factors affecting the latency and throughput are\nthe topology of links between chiplets and the chiplet placement. In this work,\nwe present PlaceIT, a novel methodology to jointly optimize the ICI topology\nand the chiplet placement. While state-of-the-art methods optimize the chiplet\nplacement for a predetermined ICI topology, or they select one topology out of\na set of candidates, we generate a completely new topology for each placement.\nOur process of inferring placement-based ICI topologies connects chiplets that\nare in close proximity to each other, making it particularly attractive for\nchips with silicon bridges or passive silicon interposers with severely limited\nlink lengths. We provide an open-source implementation of our method that\noptimizes the placement of homogeneously or heterogeneously shaped chiplets and\nthe ICI topology connecting them for a user-defined mix of four different\ntraffic types. We evaluate our methodology using synthetic traffic and traces,\nand we compare our results to a 2D mesh baseline. PlaceIT reduces the latency\nof synthetic L1-to-L2 and L2-to-memory traffic, the two most important types\nfor cache coherency traffic, by up to 28% and 62%, respectively. It also\nachieve an average packet latency reduction of up to 18% on traffic traces.\nPlaceIT enables the construction of 2.5D stacked chips with low-latency ICIs."
                },
                "authors": [
                    {
                        "name": "Patrick Iff"
                    },
                    {
                        "name": "Benigna Bruggmann"
                    },
                    {
                        "name": "Maciej Besta"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01449v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01449v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03805v1",
                "updated": "2025-02-06T06:31:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    6,
                    31,
                    47,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T06:31:47Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    6,
                    31,
                    47,
                    3,
                    37,
                    0
                ],
                "title": "Identify Critical KV Cache in LLM Inference from an Output Perturbation\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identify Critical KV Cache in LLM Inference from an Output Perturbation\n  Perspective"
                },
                "summary": "Large language models have revolutionized natural language processing but\nface significant challenges of high storage and runtime costs, due to the\ntransformer architecture's reliance on self-attention, particularly the large\nKey-Value (KV) cache for long-sequence inference. Recent efforts to reduce KV\ncache size by pruning less critical entries based on attention weights remain\nempirical and lack formal grounding. This paper presents a formal study on\nidentifying critical KV cache entries by analyzing attention output\nperturbation. Our analysis reveals that, beyond attention weights, the value\nstates within KV entries and pretrained parameter matrices are also crucial.\nBased on this, we propose a perturbation-constrained selection algorithm that\noptimizes the worst-case output perturbation to identify critical entries.\nEvaluations on the Needle-in-a-Haystack test and Longbench benchmark show our\nalgorithm enhances state-of-the-art cache eviction methods. Further empirical\nanalysis confirms that our algorithm achieves lower output perturbations in\nover 92% attention heads in Llama model, thereby providing a significant\nimprovement over existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have revolutionized natural language processing but\nface significant challenges of high storage and runtime costs, due to the\ntransformer architecture's reliance on self-attention, particularly the large\nKey-Value (KV) cache for long-sequence inference. Recent efforts to reduce KV\ncache size by pruning less critical entries based on attention weights remain\nempirical and lack formal grounding. This paper presents a formal study on\nidentifying critical KV cache entries by analyzing attention output\nperturbation. Our analysis reveals that, beyond attention weights, the value\nstates within KV entries and pretrained parameter matrices are also crucial.\nBased on this, we propose a perturbation-constrained selection algorithm that\noptimizes the worst-case output perturbation to identify critical entries.\nEvaluations on the Needle-in-a-Haystack test and Longbench benchmark show our\nalgorithm enhances state-of-the-art cache eviction methods. Further empirical\nanalysis confirms that our algorithm achieves lower output perturbations in\nover 92% attention heads in Llama model, thereby providing a significant\nimprovement over existing methods."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S Kevin Zhou"
                },
                "author": "S Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03771v1",
                "updated": "2025-02-06T04:16:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T04:16:20Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "title": "Adaptive Semantic Prompt Caching with VectorQ",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Semantic Prompt Caching with VectorQ"
                },
                "summary": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different prompts. We\npropose VectorQ, a framework to learn embedding-specific threshold regions that\nadapt to the complexity and uncertainty of an embedding. Through evaluations on\na combination of four diverse datasets, we show that VectorQ consistently\noutperforms state-of-the-art systems across all static thresholds, achieving up\nto 12x increases in cache hit rate and error rate reductions up to 92%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different prompts. We\npropose VectorQ, a framework to learn embedding-specific threshold regions that\nadapt to the complexity and uncertainty of an embedding. Through evaluations on\na combination of four diverse datasets, we show that VectorQ consistently\noutperforms state-of-the-art systems across all static thresholds, achieving up\nto 12x increases in cache hit rate and error rate reductions up to 92%."
                },
                "authors": [
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Alejandro Cuadron"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Stephan Krusche"
                    },
                    {
                        "name": "Alfons Kemper"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04393v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04393v1",
                "updated": "2025-02-06T03:56:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    3,
                    56,
                    11,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T03:56:11Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    3,
                    56,
                    11,
                    3,
                    37,
                    0
                ],
                "title": "UniCP: A Unified Caching and Pruning Framework for Efficient Video\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniCP: A Unified Caching and Pruning Framework for Efficient Video\n  Generation"
                },
                "summary": "Diffusion Transformers (DiT) excel in video generation but encounter\nsignificant computational challenges due to the quadratic complexity of\nattention. Notably, attention differences between adjacent diffusion steps\nfollow a U-shaped pattern. Current methods leverage this property by caching\nattention blocks, however, they still struggle with sudden error spikes and\nlarge discrepancies. To address these issues, we propose UniCP a unified\ncaching and pruning framework for efficient video generation. UniCP optimizes\nboth temporal and spatial dimensions through. Error Aware Dynamic Cache Window\n(EDCW): Dynamically adjusts cache window sizes for different blocks at various\ntimesteps, adapting to abrupt error changes. PCA based Slicing (PCAS) and\nDynamic Weight Shift (DWS): PCAS prunes redundant attention components, and DWS\nintegrates caching and pruning by enabling dynamic switching between pruned and\ncached outputs. By adjusting cache windows and pruning redundant components,\nUniCP enhances computational efficiency and maintains video detail fidelity.\nExperimental results show that UniCP outperforms existing methods in both\nperformance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) excel in video generation but encounter\nsignificant computational challenges due to the quadratic complexity of\nattention. Notably, attention differences between adjacent diffusion steps\nfollow a U-shaped pattern. Current methods leverage this property by caching\nattention blocks, however, they still struggle with sudden error spikes and\nlarge discrepancies. To address these issues, we propose UniCP a unified\ncaching and pruning framework for efficient video generation. UniCP optimizes\nboth temporal and spatial dimensions through. Error Aware Dynamic Cache Window\n(EDCW): Dynamically adjusts cache window sizes for different blocks at various\ntimesteps, adapting to abrupt error changes. PCA based Slicing (PCAS) and\nDynamic Weight Shift (DWS): PCAS prunes redundant attention components, and DWS\nintegrates caching and pruning by enabling dynamic switching between pruned and\ncached outputs. By adjusting cache windows and pruning redundant components,\nUniCP enhances computational efficiency and maintains video detail fidelity.\nExperimental results show that UniCP outperforms existing methods in both\nperformance and efficiency."
                },
                "authors": [
                    {
                        "name": "Wenzhang Sun"
                    },
                    {
                        "name": "Qirui Hou"
                    },
                    {
                        "name": "Donglin Di"
                    },
                    {
                        "name": "Jiahui Yang"
                    },
                    {
                        "name": "Yongjia Ma"
                    },
                    {
                        "name": "Jianxun Cui"
                    }
                ],
                "author_detail": {
                    "name": "Jianxun Cui"
                },
                "author": "Jianxun Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04393v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04393v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02770v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02770v2",
                "updated": "2025-02-06T03:16:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    3,
                    16,
                    0,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-04T23:26:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    23,
                    26,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning"
                },
                "summary": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding."
                },
                "authors": [
                    {
                        "name": "Chaofan Lin"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Hanshuo Wang"
                    },
                    {
                        "name": "Tian Tang"
                    },
                    {
                        "name": "Boyu Tian"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02770v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02770v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05460v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05460v2",
                "updated": "2025-02-05T22:55:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    22,
                    55,
                    47,
                    2,
                    36,
                    0
                ],
                "published": "2024-12-25T10:11:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    10,
                    11,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "Efficiently Serving Large Multimodal Models Using EPD Disaggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently Serving Large Multimodal Models Using EPD Disaggregation"
                },
                "summary": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step negatively impacting key Service Level Objectives (SLOs)\nlike time to first token (TTFT) and end-to-end throughput (E2ETP). We introduce\nEncode-Prefill-Decode (EPD) Disaggregation, a novel framework that separates\nthe encoding, prefill, and decode stages onto dedicated resources. Unlike\ncurrent systems, which bundle encoding and prefill together, our approach\ndecouple these steps unlocking new opportunities and optimizations. These\ninclude a new mechanism to cache multimedia tokens for efficient transfer, a\nnovel way to parallelize encoding load within a request, a module to find the\noptimal resource allocation for disaggregated serving, and a novel role\nswitching method to handle changing workload characteristics. Experimental\nevaluations with popular LMMs show substantial gains in memory efficiency (up\nto 15$\\times$ less utilization), batch sizes (up to 22$\\times$ larger),\n10$\\times$ more images/request, and 2.2$\\times$ larger KV caches. Further, it\nleads to significant improvements in latency metrics (TTFT up to 71\\%\nreduction) and end-to-end throughput (up to 57\\% reduction), compared to\nsystems that do not disaggregate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step negatively impacting key Service Level Objectives (SLOs)\nlike time to first token (TTFT) and end-to-end throughput (E2ETP). We introduce\nEncode-Prefill-Decode (EPD) Disaggregation, a novel framework that separates\nthe encoding, prefill, and decode stages onto dedicated resources. Unlike\ncurrent systems, which bundle encoding and prefill together, our approach\ndecouple these steps unlocking new opportunities and optimizations. These\ninclude a new mechanism to cache multimedia tokens for efficient transfer, a\nnovel way to parallelize encoding load within a request, a module to find the\noptimal resource allocation for disaggregated serving, and a novel role\nswitching method to handle changing workload characteristics. Experimental\nevaluations with popular LMMs show substantial gains in memory efficiency (up\nto 15$\\times$ less utilization), batch sizes (up to 22$\\times$ larger),\n10$\\times$ more images/request, and 2.2$\\times$ larger KV caches. Further, it\nleads to significant improvements in latency metrics (TTFT up to 71\\%\nreduction) and end-to-end throughput (up to 57\\% reduction), compared to\nsystems that do not disaggregate."
                },
                "authors": [
                    {
                        "name": "Gursimran Singh"
                    },
                    {
                        "name": "Xinglu Wang"
                    },
                    {
                        "name": "Yifan Hu"
                    },
                    {
                        "name": "Timothy Yu"
                    },
                    {
                        "name": "Linzi Xing"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "arxiv_comment": "16 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05460v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05460v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04107v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04107v2",
                "updated": "2025-02-05T21:44:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    21,
                    44,
                    56,
                    2,
                    36,
                    0
                ],
                "published": "2024-08-07T22:10:26Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "title": "ZACK: Zero-Overhead LLM Inference Acceleration via Dimensionality\n  Compression of the Key-Value Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZACK: Zero-Overhead LLM Inference Acceleration via Dimensionality\n  Compression of the Key-Value Cache"
                },
                "summary": "In large-language models, memory constraints in the Key-Value Cache (KVC)\npose a challenge during inference. In this work, we propose ZACK, the first KV\ndimensionality compression system that achieves zero-overhead compression and\ndecompression and also reduces attention computation time. It complements and\ncan be combined with eviction-based and quantization-based methods to further\nenhance KV compression. Moreover, ZACK employs adaptive compression, tailoring\nKV compression rates across heads and layers based on their contributions to\ninference to maximize overall compression while maintaining an accuracy loss\nconstraint. Additionally, ZACK enhances the self-attention kernel to balance\nthe uneven workloads caused by the adaptive compression approach to further\nreduce attention computation latency. Comprehensive experiments demonstrate\nthat when combined with ZACK, state-of-the-art eviction-based and\nquantization-based methods for KV compression further reduce KV size by up to\n68%, Time-To-First-Token (TTFT) by up to 44%, and Time-Between-Tokens (TBT) by\nup to 55% and achieve up to 1.72X throughput under the same latency, while\nmaintaining 99% of the baseline accuracy. We open-sourced the code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-language models, memory constraints in the Key-Value Cache (KVC)\npose a challenge during inference. In this work, we propose ZACK, the first KV\ndimensionality compression system that achieves zero-overhead compression and\ndecompression and also reduces attention computation time. It complements and\ncan be combined with eviction-based and quantization-based methods to further\nenhance KV compression. Moreover, ZACK employs adaptive compression, tailoring\nKV compression rates across heads and layers based on their contributions to\ninference to maximize overall compression while maintaining an accuracy loss\nconstraint. Additionally, ZACK enhances the self-attention kernel to balance\nthe uneven workloads caused by the adaptive compression approach to further\nreduce attention computation latency. Comprehensive experiments demonstrate\nthat when combined with ZACK, state-of-the-art eviction-based and\nquantization-based methods for KV compression further reduce KV size by up to\n68%, Time-To-First-Token (TTFT) by up to 44%, and Time-Between-Tokens (TBT) by\nup to 55% and achieve up to 1.72X throughput under the same latency, while\nmaintaining 99% of the baseline accuracy. We open-sourced the code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04107v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04107v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03589v1",
                "updated": "2025-02-05T20:09:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    20,
                    9,
                    51,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T20:09:51Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    20,
                    9,
                    51,
                    2,
                    36,
                    0
                ],
                "title": "HACK: Homomorphic Acceleration via Compression of the Key-Value Cache\n  for Disaggregated LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HACK: Homomorphic Acceleration via Compression of the Key-Value Cache\n  for Disaggregated LLM Inference"
                },
                "summary": "Disaggregated Large Language Model (LLM) inference has gained popularity as\nit separates the computation-intensive prefill stage from the memory-intensive\ndecode stage, avoiding the prefill-decode interference and improving resource\nutilization. However, transmitting Key-Value (KV) data between the two stages\ncan be a bottleneck, especially for long prompts. Additionally, the computation\ntime overhead for prefill and decode is key for optimizing Job Completion Time\n(JCT), and KV data size can become prohibitive for long prompts and sequences.\nExisting KV quantization methods can alleviate the transmission bottleneck and\nreduce memory requirements, but they introduce significant dequantization\noverhead, exacerbating the computation time.\n  We propose Homomorphic Acceleration via Compression of the KV cache (HACK)\nfor disaggregated LLM inference. HACK eliminates the heavy KV dequantization\nstep, and directly performs computations on quantized KV data to approximate\nand reduce the cost of the expensive matrix-multiplication step. Extensive\ntrace-driven experiments show that HACK reduces JCT by up to 70.9% compared to\ndisaggregated LLM inference baseline and by up to 52.3% compared to\nstate-of-the-art KV quantization methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated Large Language Model (LLM) inference has gained popularity as\nit separates the computation-intensive prefill stage from the memory-intensive\ndecode stage, avoiding the prefill-decode interference and improving resource\nutilization. However, transmitting Key-Value (KV) data between the two stages\ncan be a bottleneck, especially for long prompts. Additionally, the computation\ntime overhead for prefill and decode is key for optimizing Job Completion Time\n(JCT), and KV data size can become prohibitive for long prompts and sequences.\nExisting KV quantization methods can alleviate the transmission bottleneck and\nreduce memory requirements, but they introduce significant dequantization\noverhead, exacerbating the computation time.\n  We propose Homomorphic Acceleration via Compression of the KV cache (HACK)\nfor disaggregated LLM inference. HACK eliminates the heavy KV dequantization\nstep, and directly performs computations on quantized KV data to approximate\nand reduce the cost of the expensive matrix-multiplication step. Extensive\ntrace-driven experiments show that HACK reduces JCT by up to 70.9% compared to\ndisaggregated LLM inference baseline and by up to 52.3% compared to\nstate-of-the-art KV quantization methods."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Shay Vargaftik"
                    },
                    {
                        "name": "Ran Ben Basat"
                    },
                    {
                        "name": "Michael Mitzenmacher"
                    },
                    {
                        "name": "Minlan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Minlan Yu"
                },
                "author": "Minlan Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12959v2",
                "updated": "2025-02-05T09:35:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    9,
                    35,
                    38,
                    2,
                    36,
                    0
                ],
                "published": "2025-01-22T15:33:17Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    33,
                    17,
                    2,
                    22,
                    0
                ],
                "title": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference"
                },
                "summary": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks."
                },
                "authors": [
                    {
                        "name": "Weizhi Fei"
                    },
                    {
                        "name": "Xueyan Niu"
                    },
                    {
                        "name": "Guoqing Xie"
                    },
                    {
                        "name": "Yingqing Liu"
                    },
                    {
                        "name": "Bo Bai"
                    },
                    {
                        "name": "Wei Han"
                    }
                ],
                "author_detail": {
                    "name": "Wei Han"
                },
                "author": "Wei Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14442v2",
                "updated": "2025-02-05T08:22:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    8,
                    22,
                    5,
                    2,
                    36,
                    0
                ],
                "published": "2024-10-18T13:01:14Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    1,
                    14,
                    4,
                    292,
                    0
                ],
                "title": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference"
                },
                "summary": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2$\\times$, most configurations can achieve higher throughput than\nstandard transformers while maintaining competitive performance. When further\nreducing the size of the KV cache, however, pairing queries of all layers with\nKVs of upper layers performs better, at the expense of additional training cost\nand prefilling latency. We hope that this work will help users make more\ninformed choices of cross-layer KV sharing approaches and facilitate future\nresearch on efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2$\\times$, most configurations can achieve higher throughput than\nstandard transformers while maintaining competitive performance. When further\nreducing the size of the KV cache, however, pairing queries of all layers with\nKVs of upper layers performs better, at the expense of additional training cost\nand prefilling latency. We hope that this work will help users make more\ninformed choices of cross-layer KV sharing approaches and facilitate future\nresearch on efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "You Wu"
                    },
                    {
                        "name": "Haoyi Wu"
                    },
                    {
                        "name": "Kewei Tu"
                    }
                ],
                "author_detail": {
                    "name": "Kewei Tu"
                },
                "author": "Kewei Tu",
                "arxiv_comment": "Accepted to NAACL2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13331v2",
                "updated": "2025-02-05T08:10:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    8,
                    10,
                    45,
                    2,
                    36,
                    0
                ],
                "published": "2025-01-23T02:20:08Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    2,
                    20,
                    8,
                    3,
                    23,
                    0
                ],
                "title": "Qrazor: Reliable and Effortless 4-bit LLM Quantization by Significant\n  Data Razoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qrazor: Reliable and Effortless 4-bit LLM Quantization by Significant\n  Data Razoring"
                },
                "summary": "Large-scale language models (LLMs) excel in language processing tasks but\nface deployment challenges due to high memory and computational demands. While\nlow-bit quantization, such as 4-bit techniques, offers a potential solution,\nthese methods often suffer from significant accuracy loss or require\nconsiderable effort for implementation such as reordering, rotation, etc. To\naddress these challenges, we propose QRazor, a simple yet effective\nquantization scheme that enables 4-bit quantization of weights, activations,\nand KV cache in transformer-based LLMs. QRazor operates in two stages: first,\nquantizing data using 8 or 16-bit integers as a basis with absolute max scaling\nto preserve accuracy close to full-precision models, and second, compressing\nthe quantized data to 4-bit using our significant data razoring (SDR)\ntechnique, which retains only the four most salient bits. Without any\nadditional requirment of fine-tuning or additional training, QRazor achieves\nperformance similar or better compared to state-of-the-art in 4-bit\nquantization method, surpassing Smoothquant and QLLM by over 12 points and\nQuarot(RTN) by more than 2.9 points in zero-shot reasoning task accuracy on the\nLLaMA2-7B model. Additionally, we introduce an integer-based arithmetic unit\noptimized for QRazor, allowing direct low-precision operations on SDR data\nwithout decompression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale language models (LLMs) excel in language processing tasks but\nface deployment challenges due to high memory and computational demands. While\nlow-bit quantization, such as 4-bit techniques, offers a potential solution,\nthese methods often suffer from significant accuracy loss or require\nconsiderable effort for implementation such as reordering, rotation, etc. To\naddress these challenges, we propose QRazor, a simple yet effective\nquantization scheme that enables 4-bit quantization of weights, activations,\nand KV cache in transformer-based LLMs. QRazor operates in two stages: first,\nquantizing data using 8 or 16-bit integers as a basis with absolute max scaling\nto preserve accuracy close to full-precision models, and second, compressing\nthe quantized data to 4-bit using our significant data razoring (SDR)\ntechnique, which retains only the four most salient bits. Without any\nadditional requirment of fine-tuning or additional training, QRazor achieves\nperformance similar or better compared to state-of-the-art in 4-bit\nquantization method, surpassing Smoothquant and QLLM by over 12 points and\nQuarot(RTN) by more than 2.9 points in zero-shot reasoning task accuracy on the\nLLaMA2-7B model. Additionally, we introduce an integer-based arithmetic unit\noptimized for QRazor, allowing direct low-precision operations on SDR data\nwithout decompression."
                },
                "authors": [
                    {
                        "name": "Dongyoung Lee"
                    },
                    {
                        "name": "Seungkyu Choi"
                    },
                    {
                        "name": "Ik Joon Chang"
                    }
                ],
                "author_detail": {
                    "name": "Ik Joon Chang"
                },
                "author": "Ik Joon Chang",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02818v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02818v1",
                "updated": "2025-02-05T01:36:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    1,
                    36,
                    40,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T01:36:40Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    1,
                    36,
                    40,
                    2,
                    36,
                    0
                ],
                "title": "Accessible and Portable LLM Inference by Compiling Computational Graphs\n  into SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accessible and Portable LLM Inference by Compiling Computational Graphs\n  into SQL"
                },
                "summary": "Serving large language models (LLMs) often demands specialized hardware,\ndedicated frameworks, and substantial development efforts, which restrict their\naccessibility, especially for edge devices and organizations with limited\ntechnical resources. We propose a novel compiler that translates LLM inference\ngraphs into SQL queries, enabling relational databases, one of the most widely\nused and mature software systems globally, to serve as the runtime. By mapping\nneural operators such as matrix multiplication and attention into relational\nprimitives like joins and aggregations, our approach leverages database\ncapabilities, including disk-based data management and native caching.\nSupporting key transformer components, such as attention mechanisms and\nkey-value caching, our system generates SQL pipelines for end-to-end LLM\ninference. Using the Llama3 family as a case study, we demonstrate up to 30x\nspeedup in token generation for memory-constrained scenarios comparable to\ncompetitive CPU-based frameworks. Our work offers an accessible, portable, and\nefficient solution, facilitating the serving of LLMs across diverse deployment\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) often demands specialized hardware,\ndedicated frameworks, and substantial development efforts, which restrict their\naccessibility, especially for edge devices and organizations with limited\ntechnical resources. We propose a novel compiler that translates LLM inference\ngraphs into SQL queries, enabling relational databases, one of the most widely\nused and mature software systems globally, to serve as the runtime. By mapping\nneural operators such as matrix multiplication and attention into relational\nprimitives like joins and aggregations, our approach leverages database\ncapabilities, including disk-based data management and native caching.\nSupporting key transformer components, such as attention mechanisms and\nkey-value caching, our system generates SQL pipelines for end-to-end LLM\ninference. Using the Llama3 family as a case study, we demonstrate up to 30x\nspeedup in token generation for memory-constrained scenarios comparable to\ncompetitive CPU-based frameworks. Our work offers an accessible, portable, and\nefficient solution, facilitating the serving of LLMs across diverse deployment\nenvironments."
                },
                "authors": [
                    {
                        "name": "Wenbo Sun"
                    },
                    {
                        "name": "Qiming Guo"
                    },
                    {
                        "name": "Wenlu Wang"
                    },
                    {
                        "name": "Rihan Hai"
                    }
                ],
                "author_detail": {
                    "name": "Rihan Hai"
                },
                "author": "Rihan Hai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02818v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02818v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02750v1",
                "updated": "2025-02-04T22:37:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    22,
                    37,
                    17,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T22:37:17Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    22,
                    37,
                    17,
                    1,
                    35,
                    0
                ],
                "title": "Cache is King: Smart Page Eviction with eBPF",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache is King: Smart Page Eviction with eBPF"
                },
                "summary": "The page cache is a central part of an OS. It reduces repeated accesses to\nstorage by deciding which pages to retain in memory. As a result, the page\ncache has a significant impact on the performance of many applications.\nHowever, its one-size-fits-all eviction policy performs poorly in many\nworkloads. While the systems community has experimented with a plethora of new\nand adaptive eviction policies in non-OS settings (e.g., key-value stores,\nCDNs), it is very difficult to implement such policies in the page cache, due\nto the complexity of modifying kernel code. To address these shortcomings, we\ndesign a novel eBPF-based framework for the Linux page cache, called\n$\\texttt{cachebpf}$, that allows developers to customize the page cache without\nmodifying the kernel. $\\texttt{cachebpf}$ enables applications to customize the\npage cache policy for their specific needs, while also ensuring that different\napplications' policies do not interfere with each other and preserving the page\ncache's ability to share memory across different processes. We demonstrate the\nflexibility of $\\texttt{cachebpf}$'s interface by using it to implement several\neviction policies. Our evaluation shows that it is indeed beneficial for\napplications to customize the page cache to match their workloads' unique\nproperties, and that they can achieve up to 70% higher throughput and 58% lower\ntail latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The page cache is a central part of an OS. It reduces repeated accesses to\nstorage by deciding which pages to retain in memory. As a result, the page\ncache has a significant impact on the performance of many applications.\nHowever, its one-size-fits-all eviction policy performs poorly in many\nworkloads. While the systems community has experimented with a plethora of new\nand adaptive eviction policies in non-OS settings (e.g., key-value stores,\nCDNs), it is very difficult to implement such policies in the page cache, due\nto the complexity of modifying kernel code. To address these shortcomings, we\ndesign a novel eBPF-based framework for the Linux page cache, called\n$\\texttt{cachebpf}$, that allows developers to customize the page cache without\nmodifying the kernel. $\\texttt{cachebpf}$ enables applications to customize the\npage cache policy for their specific needs, while also ensuring that different\napplications' policies do not interfere with each other and preserving the page\ncache's ability to share memory across different processes. We demonstrate the\nflexibility of $\\texttt{cachebpf}$'s interface by using it to implement several\neviction policies. Our evaluation shows that it is indeed beneficial for\napplications to customize the page cache to match their workloads' unique\nproperties, and that they can achieve up to 70% higher throughput and 58% lower\ntail latency."
                },
                "authors": [
                    {
                        "name": "Tal Zussman"
                    },
                    {
                        "name": "Ioannis Zarkadas"
                    },
                    {
                        "name": "Jeremy Carin"
                    },
                    {
                        "name": "Andrew Cheng"
                    },
                    {
                        "name": "Hubertus Franke"
                    },
                    {
                        "name": "Jonas Pfefferle"
                    },
                    {
                        "name": "Asaf Cidon"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Cidon"
                },
                "author": "Asaf Cidon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02564v1",
                "updated": "2025-02-04T18:39:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    39,
                    10,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T18:39:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    39,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "CReIS: Computation Reuse through Image Similarity in ICN-Based Edge\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CReIS: Computation Reuse through Image Similarity in ICN-Based Edge\n  Computing"
                },
                "summary": "At the edge, there is a high level of similarity in computing. One approach\nthat has been proposed to enhance the efficiency of edge computing is\ncomputation reuse, which eliminates redundant computations. Edge computing is\nintegrated with the ICN architecture, capitalizing on its inherent intelligence\nto facilitate computation reuse and reduce redundancies in computing\noperations. In many past works, ICN's ability to enable computation reuse\nthrough caching has been limited. In this context, a new approach is proposed\nthat considers computation requests with similar input data, which yield\nidentical results, as equivalent. This method facilitates computation reuse\nthrough caching in ICN. The use of approximate results to reduce redundant\ncomputations without requiring high accuracy in input matching is provided.\nThis concept is termed the Similarity Index, which effectively considers images\nto be similar despite minor changes in the angle of photography. The Similarity\nIndex is determined through an algorithm known as HNSW and utilizes the SIFT\ndescriptor to identify similar data. This approach helps reduce user latency\ntimes by providing quick access to results. The evaluation, simulated using the\nndnSIM tool, showed an 86% improvement in completion time compared to scenarios\nwithout computation reuse, whereas previous works reported only a 70%\nimprovement. To strengthen this method, an analytical model for computing\nrequest transfer considering computation reuse in ICN-based edge computing is\nprovided. To assess the accuracy of the model, several evaluations have been\nconducted in the simulator by varying the parameters, resulting in a maximum\nerror percentage of approximately 16%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At the edge, there is a high level of similarity in computing. One approach\nthat has been proposed to enhance the efficiency of edge computing is\ncomputation reuse, which eliminates redundant computations. Edge computing is\nintegrated with the ICN architecture, capitalizing on its inherent intelligence\nto facilitate computation reuse and reduce redundancies in computing\noperations. In many past works, ICN's ability to enable computation reuse\nthrough caching has been limited. In this context, a new approach is proposed\nthat considers computation requests with similar input data, which yield\nidentical results, as equivalent. This method facilitates computation reuse\nthrough caching in ICN. The use of approximate results to reduce redundant\ncomputations without requiring high accuracy in input matching is provided.\nThis concept is termed the Similarity Index, which effectively considers images\nto be similar despite minor changes in the angle of photography. The Similarity\nIndex is determined through an algorithm known as HNSW and utilizes the SIFT\ndescriptor to identify similar data. This approach helps reduce user latency\ntimes by providing quick access to results. The evaluation, simulated using the\nndnSIM tool, showed an 86% improvement in completion time compared to scenarios\nwithout computation reuse, whereas previous works reported only a 70%\nimprovement. To strengthen this method, an analytical model for computing\nrequest transfer considering computation reuse in ICN-based edge computing is\nprovided. To assess the accuracy of the model, several evaluations have been\nconducted in the simulator by varying the parameters, resulting in a maximum\nerror percentage of approximately 16%."
                },
                "authors": [
                    {
                        "name": "Atiyeh Javaheri"
                    },
                    {
                        "name": "Ali Bohlooli"
                    },
                    {
                        "name": "Kamal Jamshidi"
                    }
                ],
                "author_detail": {
                    "name": "Kamal Jamshidi"
                },
                "author": "Kamal Jamshidi",
                "arxiv_comment": "18 pages, 14 figures, submit to Digital Communications and Networks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14001v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14001v2",
                "updated": "2025-02-04T17:14:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    14,
                    22,
                    1,
                    35,
                    0
                ],
                "published": "2024-08-26T03:58:20Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    58,
                    20,
                    0,
                    239,
                    0
                ],
                "title": "Decentralized Federated Learning with Model Caching on Mobile Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Federated Learning with Model Caching on Mobile Agents"
                },
                "summary": "Federated Learning (FL) trains a shared model using data and computation\npower on distributed agents coordinated by a central server. Decentralized FL\n(DFL) utilizes local model exchange and aggregation between agents to reduce\nthe communication and computation overheads on the central server. However,\nwhen agents are mobile, the communication opportunity between agents can be\nsporadic, largely hindering the convergence and accuracy of DFL. In this paper,\nwe propose Cached Decentralized Federated Learning (Cached-DFL) to investigate\ndelay-tolerant model spreading and aggregation enabled by model caching on\nmobile agents. Each agent stores not only its own model, but also models of\nagents encountered in the recent past. When two agents meet, they exchange\ntheir own models as well as the cached models. Local model aggregation utilizes\nall models stored in the cache. We theoretically analyze the convergence of\nCached-DFL, explicitly taking into account the model staleness introduced by\ncaching. We design and compare different model caching algorithms for different\nDFL and mobility scenarios. We conduct detailed case studies in a vehicular\nnetwork to systematically investigate the interplay between agent mobility,\ncache staleness, and model convergence. In our experiments, Cached-DFL\nconverges quickly, and significantly outperforms DFL without caching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) trains a shared model using data and computation\npower on distributed agents coordinated by a central server. Decentralized FL\n(DFL) utilizes local model exchange and aggregation between agents to reduce\nthe communication and computation overheads on the central server. However,\nwhen agents are mobile, the communication opportunity between agents can be\nsporadic, largely hindering the convergence and accuracy of DFL. In this paper,\nwe propose Cached Decentralized Federated Learning (Cached-DFL) to investigate\ndelay-tolerant model spreading and aggregation enabled by model caching on\nmobile agents. Each agent stores not only its own model, but also models of\nagents encountered in the recent past. When two agents meet, they exchange\ntheir own models as well as the cached models. Local model aggregation utilizes\nall models stored in the cache. We theoretically analyze the convergence of\nCached-DFL, explicitly taking into account the model staleness introduced by\ncaching. We design and compare different model caching algorithms for different\nDFL and mobility scenarios. We conduct detailed case studies in a vehicular\nnetwork to systematically investigate the interplay between agent mobility,\ncache staleness, and model convergence. In our experiments, Cached-DFL\nconverges quickly, and significantly outperforms DFL without caching."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Wang"
                    },
                    {
                        "name": "Guojun Xiong"
                    },
                    {
                        "name": "Houwei Cao"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "Oral Presentation at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14001v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14001v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02493v1",
                "updated": "2025-02-04T17:09:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    9,
                    21,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T17:09:21Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    9,
                    21,
                    1,
                    35,
                    0
                ],
                "title": "EasySpec: Layer-Parallel Speculative Decoding for Efficient Multi-GPU\n  Utilization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EasySpec: Layer-Parallel Speculative Decoding for Efficient Multi-GPU\n  Utilization"
                },
                "summary": "Speculative decoding is an effective and lossless method for Large Language\nModel (LLM) inference acceleration. It employs a smaller model to generate a\ndraft token sequence, which is then verified by the original base model. In\nmulti-GPU systems, inference latency can be further reduced through tensor\nparallelism (TP), while the optimal TP size of the draft model is typically\nsmaller than that of the base model, leading to GPU idling during the drafting\nstage. To solve this problem, we propose EasySpec, a layer-parallel speculation\nstrategy that optimizes the efficiency of multi-GPU utilization.EasySpec breaks\nthe sequential execution order of layers in the drafting model, enabling\nmulti-layer parallelization across devices, albeit with some induced\napproximation errors. After each drafting-and-verification iteration, the draft\nmodel's key-value (KV) cache is calibrated in a single forward pass, preventing\nlong-term error accumulation at minimal additional latency. We evaluated\nEasySpec on several mainstream open-source LLMs, using smaller versions of\nmodels from the same series as drafters. The results demonstrate that EasySpec\ncan achieve a peak speedup of 4.17x compared to vanilla decoding, while\npreserving the original distribution of the base LLMs. Specifically, the\ndrafting stage can be accelerated by up to 1.62x with a maximum accuracy drop\nof only 7%, requiring no training or fine-tuning on the draft models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is an effective and lossless method for Large Language\nModel (LLM) inference acceleration. It employs a smaller model to generate a\ndraft token sequence, which is then verified by the original base model. In\nmulti-GPU systems, inference latency can be further reduced through tensor\nparallelism (TP), while the optimal TP size of the draft model is typically\nsmaller than that of the base model, leading to GPU idling during the drafting\nstage. To solve this problem, we propose EasySpec, a layer-parallel speculation\nstrategy that optimizes the efficiency of multi-GPU utilization.EasySpec breaks\nthe sequential execution order of layers in the drafting model, enabling\nmulti-layer parallelization across devices, albeit with some induced\napproximation errors. After each drafting-and-verification iteration, the draft\nmodel's key-value (KV) cache is calibrated in a single forward pass, preventing\nlong-term error accumulation at minimal additional latency. We evaluated\nEasySpec on several mainstream open-source LLMs, using smaller versions of\nmodels from the same series as drafters. The results demonstrate that EasySpec\ncan achieve a peak speedup of 4.17x compared to vanilla decoding, while\npreserving the original distribution of the base LLMs. Specifically, the\ndrafting stage can be accelerated by up to 1.62x with a maximum accuracy drop\nof only 7%, requiring no training or fine-tuning on the draft models."
                },
                "authors": [
                    {
                        "name": "Yize Wu"
                    },
                    {
                        "name": "Ke Gao"
                    },
                    {
                        "name": "Yanjun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yanjun Wu"
                },
                "author": "Yanjun Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02437v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02437v1",
                "updated": "2025-02-04T16:03:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    3,
                    52,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T16:03:52Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    3,
                    52,
                    1,
                    35,
                    0
                ],
                "title": "H-MBR: Hypervisor-level Memory Bandwidth Reservation for Mixed\n  Criticality Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "H-MBR: Hypervisor-level Memory Bandwidth Reservation for Mixed\n  Criticality Systems"
                },
                "summary": "Recent advancements in fields such as automotive and aerospace have driven a\ngrowing demand for robust computational resources. Applications that were once\ndesigned for basic MCUs are now deployed on highly heterogeneous SoC platforms.\nWhile these platforms deliver the necessary computational performance, they\nalso present challenges related to resource sharing and predictability. These\nchallenges are particularly pronounced when consolidating safety and\nnon-safety-critical systems, the so-called Mixed-Criticality Systems (MCS) to\nadhere to strict SWaP-C requirements. MCS consolidation on shared platforms\nrequires stringent spatial and temporal isolation to comply with functional\nsafety standards. Virtualization, mainly leveraged by hypervisors, is a key\ntechnology that ensures spatial isolation across multiple OSes and\napplications; however, ensuring temporal isolation remains challenging due to\ncontention on shared hardwar resources, which impacts real-time performance and\npredictability. To mitigate this problem, several strategies as cache coloring\nand memory bandwidth reservation have been proposed. Although cache coloring is\ntypically implemented on state-of-the-art hypervisors, memory bandwidth\nreservation approaches are commonly implemented at the Linux kernel level or\nrely on dedicated hardware and typically do not consider the concept of VMs\nthat can run different OSes. To fill the gap between current memory bandwidth\nreservation solutions and the deployment of MCSs that operate on a hypervisor,\nthis work introduces H-MBR, an open-source VM-centric memory bandwidth\nreservation mechanism. H-MBR features (i) VM-centric bandwidth reservation,\n(ii) OS and platform agnosticism, and (iii) reduced overhead. Empirical results\nevidenced no overhead on non-regulated workloads, and negligible overhead (<1%)\nfor regulated workloads for regulation periods of 2 us or higher.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in fields such as automotive and aerospace have driven a\ngrowing demand for robust computational resources. Applications that were once\ndesigned for basic MCUs are now deployed on highly heterogeneous SoC platforms.\nWhile these platforms deliver the necessary computational performance, they\nalso present challenges related to resource sharing and predictability. These\nchallenges are particularly pronounced when consolidating safety and\nnon-safety-critical systems, the so-called Mixed-Criticality Systems (MCS) to\nadhere to strict SWaP-C requirements. MCS consolidation on shared platforms\nrequires stringent spatial and temporal isolation to comply with functional\nsafety standards. Virtualization, mainly leveraged by hypervisors, is a key\ntechnology that ensures spatial isolation across multiple OSes and\napplications; however, ensuring temporal isolation remains challenging due to\ncontention on shared hardwar resources, which impacts real-time performance and\npredictability. To mitigate this problem, several strategies as cache coloring\nand memory bandwidth reservation have been proposed. Although cache coloring is\ntypically implemented on state-of-the-art hypervisors, memory bandwidth\nreservation approaches are commonly implemented at the Linux kernel level or\nrely on dedicated hardware and typically do not consider the concept of VMs\nthat can run different OSes. To fill the gap between current memory bandwidth\nreservation solutions and the deployment of MCSs that operate on a hypervisor,\nthis work introduces H-MBR, an open-source VM-centric memory bandwidth\nreservation mechanism. H-MBR features (i) VM-centric bandwidth reservation,\n(ii) OS and platform agnosticism, and (iii) reduced overhead. Empirical results\nevidenced no overhead on non-regulated workloads, and negligible overhead (<1%)\nfor regulated workloads for regulation periods of 2 us or higher."
                },
                "authors": [
                    {
                        "name": "Afonso Oliveira"
                    },
                    {
                        "name": "Diogo Costa"
                    },
                    {
                        "name": "Gonalo Moreira"
                    },
                    {
                        "name": "Jos Martins"
                    },
                    {
                        "name": "Sandro Pinto"
                    }
                ],
                "author_detail": {
                    "name": "Sandro Pinto"
                },
                "author": "Sandro Pinto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02437v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02437v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02430v1",
                "updated": "2025-02-04T15:55:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    55,
                    10,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T15:55:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    55,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals"
                },
                "summary": "Web refresh crawling is the problem of keeping a cache of web pages fresh,\nthat is, having the most recent copy available when a page is requested, given\na limited bandwidth available to the crawler. Under the assumption that the\nchange and request events, resp., to each web page follow independent Poisson\nprocesses, the optimal scheduling policy was derived by Azar et al. 2018. In\nthis paper, we study an extension of this problem where side information\nindicating content changes, such as various types of web pings, for example,\nsignals from sitemaps, content delivery networks, etc., is available.\nIncorporating such side information into the crawling policy is challenging,\nbecause (i) the signals can be noisy with false positive events and with\nmissing change events; and (ii) the crawler should achieve a fair performance\nover web pages regardless of the quality of the side information, which might\ndiffer from web page to web page. We propose a scalable crawling algorithm\nwhich (i) uses the noisy side information in an optimal way under mild\nassumptions; (ii) can be deployed without heavy centralized computation; (iii)\nis able to crawl web pages at a constant total rate without spikes in the total\nbandwidth usage over any time interval, and automatically adapt to the new\noptimal solution when the total bandwidth changes without centralized\ncomputation. Experiments clearly demonstrate the versatility of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web refresh crawling is the problem of keeping a cache of web pages fresh,\nthat is, having the most recent copy available when a page is requested, given\na limited bandwidth available to the crawler. Under the assumption that the\nchange and request events, resp., to each web page follow independent Poisson\nprocesses, the optimal scheduling policy was derived by Azar et al. 2018. In\nthis paper, we study an extension of this problem where side information\nindicating content changes, such as various types of web pings, for example,\nsignals from sitemaps, content delivery networks, etc., is available.\nIncorporating such side information into the crawling policy is challenging,\nbecause (i) the signals can be noisy with false positive events and with\nmissing change events; and (ii) the crawler should achieve a fair performance\nover web pages regardless of the quality of the side information, which might\ndiffer from web page to web page. We propose a scalable crawling algorithm\nwhich (i) uses the noisy side information in an optimal way under mild\nassumptions; (ii) can be deployed without heavy centralized computation; (iii)\nis able to crawl web pages at a constant total rate without spikes in the total\nbandwidth usage over any time interval, and automatically adapt to the new\noptimal solution when the total bandwidth changes without centralized\ncomputation. Experiments clearly demonstrate the versatility of our approach."
                },
                "authors": [
                    {
                        "name": "Rbert Busa-Fekete"
                    },
                    {
                        "name": "Julian Zimmert"
                    },
                    {
                        "name": "Andrs Gyrgy"
                    },
                    {
                        "name": "Linhai Qiu"
                    },
                    {
                        "name": "Tzu-Wei Sung"
                    },
                    {
                        "name": "Hao Shen"
                    },
                    {
                        "name": "Hyomin Choi"
                    },
                    {
                        "name": "Sharmila Subramaniam"
                    },
                    {
                        "name": "Li Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Li Xiao"
                },
                "author": "Li Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02349v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02349v1",
                "updated": "2025-02-04T14:33:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    33,
                    44,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T14:33:44Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    33,
                    44,
                    1,
                    35,
                    0
                ],
                "title": "Random Adaptive Cache Placement Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random Adaptive Cache Placement Policy"
                },
                "summary": "This paper presents a new hybrid cache replacement algorithm that combines\nrandom allocation with a modified V-Way cache implementation. Our RAC adapts to\ncomplex cache access patterns and optimizes cache usage by improving the\nutilization of cache sets, unlike traditional cache policies. The algorithm\nutilizes a 16-way set-associative cache with 2048 sets, incorporating dynamic\nallocation and flexible tag management. RAC extends the V-Way cache design and\nits variants by optimizing tag and data storage for enhanced efficiency.\n  We evaluated the algorithm using the ChampSim simulator with four diverse\nbenchmark traces and observed significant improvements in cache hit rates up to\n80.82% hit rate. Although the improvements in the instructions per cycle (IPC)\nwere moderate, our findings emphasize the algorithm's potential to enhance\ncache utilization and reduce memory access times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a new hybrid cache replacement algorithm that combines\nrandom allocation with a modified V-Way cache implementation. Our RAC adapts to\ncomplex cache access patterns and optimizes cache usage by improving the\nutilization of cache sets, unlike traditional cache policies. The algorithm\nutilizes a 16-way set-associative cache with 2048 sets, incorporating dynamic\nallocation and flexible tag management. RAC extends the V-Way cache design and\nits variants by optimizing tag and data storage for enhanced efficiency.\n  We evaluated the algorithm using the ChampSim simulator with four diverse\nbenchmark traces and observed significant improvements in cache hit rates up to\n80.82% hit rate. Although the improvements in the instructions per cycle (IPC)\nwere moderate, our findings emphasize the algorithm's potential to enhance\ncache utilization and reduce memory access times."
                },
                "authors": [
                    {
                        "name": "Vrushank Ahire"
                    },
                    {
                        "name": "Pranav Menon"
                    },
                    {
                        "name": "Aniruddh Muley"
                    },
                    {
                        "name": "Abhinandan S. Prasad"
                    }
                ],
                "author_detail": {
                    "name": "Abhinandan S. Prasad"
                },
                "author": "Abhinandan S. Prasad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02349v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02349v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13846v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13846v2",
                "updated": "2025-02-04T13:45:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    13,
                    45,
                    37,
                    1,
                    35,
                    0
                ],
                "published": "2024-10-17T17:58:14Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    58,
                    14,
                    3,
                    291,
                    0
                ],
                "title": "LightTransfer: Your Long-Context LLM is Secretly a Hybrid Model with\n  Effortless Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightTransfer: Your Long-Context LLM is Secretly a Hybrid Model with\n  Effortless Adaptation"
                },
                "summary": "Scaling language models to handle longer contexts introduces substantial\nmemory challenges due to the growing cost of key-value (KV) caches. Motivated\nby the efficiency gains of hybrid models and the broad availability of\npretrained large transformer backbones, we explore transitioning transformer\nmodels into hybrid architectures for a more efficient generation. In this work,\nwe propose LightTransfer, a lightweight method that transforms models such as\nLLaMA into hybrid variants. Our approach identifies lazy layers -- those\nfocusing on recent or initial tokens -- and replaces their full attention with\nstreaming attention. This transformation can be performed without any training\nfor long-context understanding tasks or with minimal fine-tuning for o1-like\nlong reasoning generation tasks that require stronger reasoning capabilities.\nExperiments across diverse benchmarks and models (e.g., LLaMA, Mistral,\nQwQ-STILL) demonstrate that, even when half of the layers are identified as\nlazy, LightTransfer achieves up to 2.17$\\times$ throughput improvement with\nminimal performance loss ($<1.5\\%$ on LongBench) and achieves 53.3\\% on math\nbenchmark AIME24 of advanced o1-like long reasoning model QwQ-STILL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer contexts introduces substantial\nmemory challenges due to the growing cost of key-value (KV) caches. Motivated\nby the efficiency gains of hybrid models and the broad availability of\npretrained large transformer backbones, we explore transitioning transformer\nmodels into hybrid architectures for a more efficient generation. In this work,\nwe propose LightTransfer, a lightweight method that transforms models such as\nLLaMA into hybrid variants. Our approach identifies lazy layers -- those\nfocusing on recent or initial tokens -- and replaces their full attention with\nstreaming attention. This transformation can be performed without any training\nfor long-context understanding tasks or with minimal fine-tuning for o1-like\nlong reasoning generation tasks that require stronger reasoning capabilities.\nExperiments across diverse benchmarks and models (e.g., LLaMA, Mistral,\nQwQ-STILL) demonstrate that, even when half of the layers are identified as\nlazy, LightTransfer achieves up to 2.17$\\times$ throughput improvement with\nminimal performance loss ($<1.5\\%$ on LongBench) and achieves 53.3\\% on math\nbenchmark AIME24 of advanced o1-like long reasoning model QwQ-STILL."
                },
                "authors": [
                    {
                        "name": "Xuan Zhang"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13846v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13846v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02175v1",
                "updated": "2025-02-04T09:48:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    48,
                    14,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T09:48:14Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    48,
                    14,
                    1,
                    35,
                    0
                ],
                "title": "VLA-Cache: Towards Efficient Vision-Language-Action Model via Adaptive\n  Token Caching in Robotic Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLA-Cache: Towards Efficient Vision-Language-Action Model via Adaptive\n  Token Caching in Robotic Manipulation"
                },
                "summary": "Vision-Language-Action (VLA) model can process instructions and visual\nperception to directly generate actions as output in an end-to-end fashion due\nto its strong multi-modal reasoning capabilities. While the performance of VLA\nmodels is promising, their computational cost can be substantial. This raises\nchallenge for applying them on robotics tasks, which requires real-time\ndecision-making to respond quickly to environmental changes. Since robotic\ncontrol involves sequential decision-making, the visual input often exhibits\nminimal variation between successive steps. A natural idea is to reuse the\ncomputational results of unchanged visual tokens from the last step. Motivated\nby this idea, we propose VLA-Cache, an efficient vision-language-action model.\nVLA-Cache incorporates a token-selection mechanism that compares the visual\ninput at each step with the input from the previous step, adaptively\nidentifying visual tokens with minimal changes. The computational results for\nthese unchanged tokens are then reused in subsequent steps via KV-cache,\nthereby significantly improving the efficiency of the VLA-Cache model.\nExperimental results on both simulation (e.g., LIBERO benchmark and SIMPLER)\nand real-world robot valid VLA-Cache can achieve practical acceleration with\nminimal sacrifice in success rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) model can process instructions and visual\nperception to directly generate actions as output in an end-to-end fashion due\nto its strong multi-modal reasoning capabilities. While the performance of VLA\nmodels is promising, their computational cost can be substantial. This raises\nchallenge for applying them on robotics tasks, which requires real-time\ndecision-making to respond quickly to environmental changes. Since robotic\ncontrol involves sequential decision-making, the visual input often exhibits\nminimal variation between successive steps. A natural idea is to reuse the\ncomputational results of unchanged visual tokens from the last step. Motivated\nby this idea, we propose VLA-Cache, an efficient vision-language-action model.\nVLA-Cache incorporates a token-selection mechanism that compares the visual\ninput at each step with the input from the previous step, adaptively\nidentifying visual tokens with minimal changes. The computational results for\nthese unchanged tokens are then reused in subsequent steps via KV-cache,\nthereby significantly improving the efficiency of the VLA-Cache model.\nExperimental results on both simulation (e.g., LIBERO benchmark and SIMPLER)\nand real-world robot valid VLA-Cache can achieve practical acceleration with\nminimal sacrifice in success rate."
                },
                "authors": [
                    {
                        "name": "Siyu Xu"
                    },
                    {
                        "name": "Yunke Wang"
                    },
                    {
                        "name": "Chenghao Xia"
                    },
                    {
                        "name": "Dihao Zhu"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Chang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xu"
                },
                "author": "Chang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02617v1",
                "updated": "2025-02-04T08:52:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    8,
                    52,
                    13,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T08:52:13Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    8,
                    52,
                    13,
                    1,
                    35,
                    0
                ],
                "title": "PolarQuant: Quantizing KV Caches with Polar Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PolarQuant: Quantizing KV Caches with Polar Transformation"
                },
                "summary": "Large language models (LLMs) require significant memory to store Key-Value\n(KV) embeddings in their KV cache, especially when handling long-range\ncontexts. Quantization of these KV embeddings is a common technique to reduce\nmemory consumption. This work introduces PolarQuant, a novel quantization\nmethod employing random preconditioning and polar transformation. Our method\ntransforms the KV embeddings into polar coordinates using an efficient\nrecursive algorithm and then quantizes resulting angles. Our key insight is\nthat, after random preconditioning, the angles in the polar representation\nexhibit a tightly bounded and highly concentrated distribution with an\nanalytically computable form. This nice distribution eliminates the need for\nexplicit normalization, a step required by traditional quantization methods\nwhich introduces significant memory overhead because quantization parameters\n(e.g., zero point and scale) must be stored in full precision per each data\nblock. PolarQuant bypasses this normalization step, enabling substantial memory\nsavings. The long-context evaluation demonstrates that PolarQuant compresses\nthe KV cache by over x4.2 while achieving the best quality scores compared to\nthe state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) require significant memory to store Key-Value\n(KV) embeddings in their KV cache, especially when handling long-range\ncontexts. Quantization of these KV embeddings is a common technique to reduce\nmemory consumption. This work introduces PolarQuant, a novel quantization\nmethod employing random preconditioning and polar transformation. Our method\ntransforms the KV embeddings into polar coordinates using an efficient\nrecursive algorithm and then quantizes resulting angles. Our key insight is\nthat, after random preconditioning, the angles in the polar representation\nexhibit a tightly bounded and highly concentrated distribution with an\nanalytically computable form. This nice distribution eliminates the need for\nexplicit normalization, a step required by traditional quantization methods\nwhich introduces significant memory overhead because quantization parameters\n(e.g., zero point and scale) must be stored in full precision per each data\nblock. PolarQuant bypasses this normalization step, enabling substantial memory\nsavings. The long-context evaluation demonstrates that PolarQuant compresses\nthe KV cache by over x4.2 while achieving the best quality scores compared to\nthe state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Insu Han"
                    },
                    {
                        "name": "Praneeth Kacham"
                    },
                    {
                        "name": "Amin Karbasi"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    },
                    {
                        "name": "Amir Zandieh"
                    }
                ],
                "author_detail": {
                    "name": "Amir Zandieh"
                },
                "author": "Amir Zandieh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v4",
                "updated": "2025-02-04T08:16:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    8,
                    16,
                    31,
                    1,
                    35,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on\n  GitHub^_^. Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02069v1",
                "updated": "2025-02-04T07:40:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    7,
                    40,
                    26,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T07:40:26Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    7,
                    40,
                    26,
                    1,
                    35,
                    0
                ],
                "title": "LoRA-TTT: Low-Rank Test-Time Training for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRA-TTT: Low-Rank Test-Time Training for Vision-Language Models"
                },
                "summary": "The rapid advancements in vision-language models (VLMs), such as CLIP, have\nintensified the need to address distribution shifts between training and\ntesting datasets. Although prior Test-Time Training (TTT) techniques for VLMs\nhave demonstrated robust performance, they predominantly rely on tuning text\nprompts, a process that demands substantial computational resources and is\nheavily dependent on entropy-based loss. In this paper, we propose LoRA-TTT, a\nnovel TTT method that leverages Low-Rank Adaptation (LoRA), applied exclusively\nto the image encoder of VLMs. By introducing LoRA and updating only its\nparameters during test time, our method offers a simple yet effective TTT\napproach, retaining the model's initial generalization capability while\nachieving substantial performance gains with minimal memory and runtime\noverhead. Additionally, we introduce a highly efficient reconstruction loss\ntailored for TTT. Our method can adapt to diverse domains by combining these\ntwo losses, without increasing memory consumption or runtime. Extensive\nexperiments on two benchmarks, covering 15 datasets, demonstrate that our\nmethod improves the zero-shot top-1 accuracy of CLIP-ViT-B/16 by an average of\n5.79% on the OOD benchmark and 1.36% on the fine-grained benchmark, efficiently\nsurpassing test-time prompt tuning, without relying on any external models or\ncache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancements in vision-language models (VLMs), such as CLIP, have\nintensified the need to address distribution shifts between training and\ntesting datasets. Although prior Test-Time Training (TTT) techniques for VLMs\nhave demonstrated robust performance, they predominantly rely on tuning text\nprompts, a process that demands substantial computational resources and is\nheavily dependent on entropy-based loss. In this paper, we propose LoRA-TTT, a\nnovel TTT method that leverages Low-Rank Adaptation (LoRA), applied exclusively\nto the image encoder of VLMs. By introducing LoRA and updating only its\nparameters during test time, our method offers a simple yet effective TTT\napproach, retaining the model's initial generalization capability while\nachieving substantial performance gains with minimal memory and runtime\noverhead. Additionally, we introduce a highly efficient reconstruction loss\ntailored for TTT. Our method can adapt to diverse domains by combining these\ntwo losses, without increasing memory consumption or runtime. Extensive\nexperiments on two benchmarks, covering 15 datasets, demonstrate that our\nmethod improves the zero-shot top-1 accuracy of CLIP-ViT-B/16 by an average of\n5.79% on the OOD benchmark and 1.36% on the fine-grained benchmark, efficiently\nsurpassing test-time prompt tuning, without relying on any external models or\ncache."
                },
                "authors": [
                    {
                        "name": "Yuto Kojima"
                    },
                    {
                        "name": "Jiarui Xu"
                    },
                    {
                        "name": "Xueyan Zou"
                    },
                    {
                        "name": "Xiaolong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaolong Wang"
                },
                "author": "Xiaolong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01960v1",
                "updated": "2025-02-04T03:13:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    3,
                    13,
                    9,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T03:13:09Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    3,
                    13,
                    9,
                    1,
                    35,
                    0
                ],
                "title": "MPIC: Position-Independent Multimodal Context Caching System for\n  Efficient MLLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPIC: Position-Independent Multimodal Context Caching System for\n  Efficient MLLM Serving"
                },
                "summary": "The context caching technique is employed to accelerate the Multimodal Large\nLanguage Model (MLLM) inference by prevailing serving platforms currently.\nHowever, this approach merely reuses the Key-Value (KV) cache of the initial\nsequence of prompt, resulting in full KV cache recomputation even if the prefix\ndiffers slightly. This becomes particularly inefficient in the context of\ninterleaved text and images, as well as multimodal retrieval-augmented\ngeneration. This paper proposes position-independent caching as a more\neffective approach for multimodal information management. We have designed and\nimplemented a caching system, named MPIC, to address both system-level and\nalgorithm-level challenges. MPIC stores the KV cache on local or remote disks\nwhen receiving multimodal data, and calculates and loads the KV cache in\nparallel during inference. To mitigate accuracy degradation, we have\nincorporated integrated reuse and recompute mechanisms within the system. The\nexperimental results demonstrate that MPIC can achieve up to 54% reduction in\nresponse time compared to existing context caching systems, while maintaining\nnegligible or no accuracy loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The context caching technique is employed to accelerate the Multimodal Large\nLanguage Model (MLLM) inference by prevailing serving platforms currently.\nHowever, this approach merely reuses the Key-Value (KV) cache of the initial\nsequence of prompt, resulting in full KV cache recomputation even if the prefix\ndiffers slightly. This becomes particularly inefficient in the context of\ninterleaved text and images, as well as multimodal retrieval-augmented\ngeneration. This paper proposes position-independent caching as a more\neffective approach for multimodal information management. We have designed and\nimplemented a caching system, named MPIC, to address both system-level and\nalgorithm-level challenges. MPIC stores the KV cache on local or remote disks\nwhen receiving multimodal data, and calculates and loads the KV cache in\nparallel during inference. To mitigate accuracy degradation, we have\nincorporated integrated reuse and recompute mechanisms within the system. The\nexperimental results demonstrate that MPIC can achieve up to 54% reduction in\nresponse time compared to existing context caching systems, while maintaining\nnegligible or no accuracy loss."
                },
                "authors": [
                    {
                        "name": "Shiju Zhao"
                    },
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Rongxiao Huang"
                    },
                    {
                        "name": "Jiaqi Zheng"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "arxiv_comment": "14 pages, 11 figures, the first version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01941v1",
                "updated": "2025-02-04T02:23:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    2,
                    23,
                    6,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T02:23:06Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    2,
                    23,
                    6,
                    1,
                    35,
                    0
                ],
                "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?"
                },
                "summary": "This paper investigates an under-explored challenge in large language models\n(LLMs): the impact of KV cache compression methods on LLMs' fundamental\ncapabilities. While existing methods achieve impressive compression ratios on\nlong-context benchmarks, their effects on core model capabilities remain\nunderstudied. We present a comprehensive empirical study evaluating prominent\nKV cache compression methods across diverse tasks, spanning world knowledge,\ncommonsense reasoning, arithmetic reasoning, code generation, safety, and\nlong-context understanding and generation.Our analysis reveals that KV cache\ncompression methods exhibit task-specific performance degradation. Arithmetic\nreasoning tasks prove particularly sensitive to aggressive compression, with\ndifferent methods showing performance drops of $17.4\\%$-$43.3\\%$. Notably, the\nDeepSeek R1 Distill model exhibits more robust compression tolerance compared\nto instruction-tuned models, showing only $9.67\\%$-$25.53\\%$ performance\ndegradation. Based on our analysis of attention patterns and cross-task\ncompression performance, we propose ShotKV, a novel compression approach that\ndistinctly handles prefill and decoding phases while maintaining shot-level\nsemantic coherence. Empirical results show that ShotKV achieves $9\\%$-$18\\%$\nperformance improvements on long-context generation tasks under aggressive\ncompression ratios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates an under-explored challenge in large language models\n(LLMs): the impact of KV cache compression methods on LLMs' fundamental\ncapabilities. While existing methods achieve impressive compression ratios on\nlong-context benchmarks, their effects on core model capabilities remain\nunderstudied. We present a comprehensive empirical study evaluating prominent\nKV cache compression methods across diverse tasks, spanning world knowledge,\ncommonsense reasoning, arithmetic reasoning, code generation, safety, and\nlong-context understanding and generation.Our analysis reveals that KV cache\ncompression methods exhibit task-specific performance degradation. Arithmetic\nreasoning tasks prove particularly sensitive to aggressive compression, with\ndifferent methods showing performance drops of $17.4\\%$-$43.3\\%$. Notably, the\nDeepSeek R1 Distill model exhibits more robust compression tolerance compared\nto instruction-tuned models, showing only $9.67\\%$-$25.53\\%$ performance\ndegradation. Based on our analysis of attention patterns and cross-task\ncompression performance, we propose ShotKV, a novel compression approach that\ndistinctly handles prefill and decoding phases while maintaining shot-level\nsemantic coherence. Empirical results show that ShotKV achieves $9\\%$-$18\\%$\nperformance improvements on long-context generation tasks under aggressive\ncompression ratios."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Xiuze Zhou"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14363v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14363v2",
                "updated": "2025-02-03T21:45:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    21,
                    45,
                    32,
                    0,
                    34,
                    0
                ],
                "published": "2024-12-18T22:01:55Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    1,
                    55,
                    2,
                    353,
                    0
                ],
                "title": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals"
                },
                "summary": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama and Qwen2.5 families of models, we\ndemonstrate that ResQ outperforms recent uniform and mixed precision PTQ\nmethods on a variety of benchmarks, achieving up to 33\\% lower perplexity on\nWikitext than the next best method SpinQuant, and upto 3\\times speedup over\n16-bit baseline. Code is available at\nhttps://github.com/utkarsh-dmx/project-resq.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama and Qwen2.5 families of models, we\ndemonstrate that ResQ outperforms recent uniform and mixed precision PTQ\nmethods on a variety of benchmarks, achieving up to 33\\% lower perplexity on\nWikitext than the next best method SpinQuant, and upto 3\\times speedup over\n16-bit baseline. Code is available at\nhttps://github.com/utkarsh-dmx/project-resq."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Sayeh Sharify"
                    },
                    {
                        "name": "Kaushik Roy"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "arxiv_comment": "18 pages, 7 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14363v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14363v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01802v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01802v1",
                "updated": "2025-02-03T20:30:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    20,
                    30,
                    25,
                    0,
                    34,
                    0
                ],
                "published": "2025-02-03T20:30:25Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    20,
                    30,
                    25,
                    0,
                    34,
                    0
                ],
                "title": "General kinetic ion induced electron emission model for metallic walls\n  applied to biased Z-pinch electrodes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General kinetic ion induced electron emission model for metallic walls\n  applied to biased Z-pinch electrodes"
                },
                "summary": "A generalized kinetic ion induced electron emission (IIEE) model is developed\nto obtain the emitted electron energy spectrum for a distribution of ion\nimpacts on a metallic surface. This framework is implemented as a boundary\ncondition for the continuum kinetic Boltzmann equation. The IIEE model is used\nto study how emissions affect sheath formation near biased Z-pinch electrodes.\n1X-1V (one spatial and one velocity dimension) Boltzmann-Poisson simulations\nare performed for a proton-electron plasma doubly bounded by two biased copper\nelectrodes with and without IIEE at bias potentials from 0 kV to 9 kV. The ions\nare accelerated to higher energies by the sheath potentials at the electrodes\ninducing electron emission. The secondary electron yield (SEY), defined as the\nratio of the flux of emitted electrons to impacting ions, increases with bias\npotential at both electrodes, but more significantly at the cathode. Despite\nthe SEY crossing 1 at 7 kV, a classical sheath, rather than a space-charge\nlimited or inverse sheath, forms for all cases. The emitted electrons present\nas a beam that is accelerated by the sheath potential into the domain resulting\nin increased electron temperatures due to collisions. For bias potentials\ngreater than 2 kV, the potential difference at the cathode is sufficiently\nstrong for emissive heating to increase the plasma potential compared to\nemissionless simulations. The emitted electrons increase the current in the\ndomain from 130 kA to 199 kA closely matching the experimental value of 200 kA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A generalized kinetic ion induced electron emission (IIEE) model is developed\nto obtain the emitted electron energy spectrum for a distribution of ion\nimpacts on a metallic surface. This framework is implemented as a boundary\ncondition for the continuum kinetic Boltzmann equation. The IIEE model is used\nto study how emissions affect sheath formation near biased Z-pinch electrodes.\n1X-1V (one spatial and one velocity dimension) Boltzmann-Poisson simulations\nare performed for a proton-electron plasma doubly bounded by two biased copper\nelectrodes with and without IIEE at bias potentials from 0 kV to 9 kV. The ions\nare accelerated to higher energies by the sheath potentials at the electrodes\ninducing electron emission. The secondary electron yield (SEY), defined as the\nratio of the flux of emitted electrons to impacting ions, increases with bias\npotential at both electrodes, but more significantly at the cathode. Despite\nthe SEY crossing 1 at 7 kV, a classical sheath, rather than a space-charge\nlimited or inverse sheath, forms for all cases. The emitted electrons present\nas a beam that is accelerated by the sheath potential into the domain resulting\nin increased electron temperatures due to collisions. For bias potentials\ngreater than 2 kV, the potential difference at the cathode is sufficiently\nstrong for emissive heating to increase the plasma potential compared to\nemissionless simulations. The emitted electrons increase the current in the\ndomain from 130 kA to 199 kA closely matching the experimental value of 200 kA."
                },
                "authors": [
                    {
                        "name": "Chirag R. Skolar"
                    },
                    {
                        "name": "Kolter Bradshaw"
                    },
                    {
                        "name": "Manaure Francisquez"
                    },
                    {
                        "name": "Lucio Murillo"
                    },
                    {
                        "name": "Vignesh Krishna Kumar"
                    },
                    {
                        "name": "Bhuvana Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Bhuvana Srinivasan"
                },
                "author": "Bhuvana Srinivasan",
                "arxiv_comment": "19 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01802v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01802v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01637v1",
                "updated": "2025-02-03T18:59:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    18,
                    59,
                    32,
                    0,
                    34,
                    0
                ],
                "published": "2025-02-03T18:59:32Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    18,
                    59,
                    32,
                    0,
                    34,
                    0
                ],
                "title": "Scaling Embedding Layers in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Embedding Layers in Language Models"
                },
                "summary": "We propose SCONE ($\\textbf{S}$calable, $\\textbf{C}$ontextualized,\n$\\textbf{O}$ffloaded, $\\textbf{N}$-gram $\\textbf{E}$mbedding), a method for\nextending input embedding layers to enhance language model performance as layer\nsize scales. To avoid increased decoding costs, SCONE retains the original\nvocabulary while introducing embeddings for a set of frequent $n$-grams. These\nembeddings provide contextualized representation for each input token and are\nlearned with a separate model during training. During inference, they are\nprecomputed and stored in off-accelerator memory with minimal impact on\ninference speed. SCONE enables two new scaling strategies: increasing the\nnumber of cached $n$-gram embeddings and scaling the model used to learn them,\nall while maintaining fixed inference-time FLOPS. We show that scaling both\naspects allows SCONE to outperform a 1.9B parameter baseline across diverse\ncorpora, while using only half the inference-time FLOPS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose SCONE ($\\textbf{S}$calable, $\\textbf{C}$ontextualized,\n$\\textbf{O}$ffloaded, $\\textbf{N}$-gram $\\textbf{E}$mbedding), a method for\nextending input embedding layers to enhance language model performance as layer\nsize scales. To avoid increased decoding costs, SCONE retains the original\nvocabulary while introducing embeddings for a set of frequent $n$-grams. These\nembeddings provide contextualized representation for each input token and are\nlearned with a separate model during training. During inference, they are\nprecomputed and stored in off-accelerator memory with minimal impact on\ninference speed. SCONE enables two new scaling strategies: increasing the\nnumber of cached $n$-gram embeddings and scaling the model used to learn them,\nall while maintaining fixed inference-time FLOPS. We show that scaling both\naspects allows SCONE to outperform a 1.9B parameter baseline across diverse\ncorpora, while using only half the inference-time FLOPS."
                },
                "authors": [
                    {
                        "name": "Da Yu"
                    },
                    {
                        "name": "Edith Cohen"
                    },
                    {
                        "name": "Badih Ghazi"
                    },
                    {
                        "name": "Yangsibo Huang"
                    },
                    {
                        "name": "Pritish Kamath"
                    },
                    {
                        "name": "Ravi Kumar"
                    },
                    {
                        "name": "Daogao Liu"
                    },
                    {
                        "name": "Chiyuan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chiyuan Zhang"
                },
                "author": "Chiyuan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14201v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14201v2",
                "updated": "2025-02-03T15:15:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    15,
                    15,
                    58,
                    0,
                    34,
                    0
                ],
                "published": "2024-12-15T21:02:16Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    21,
                    2,
                    16,
                    6,
                    350,
                    0
                ],
                "title": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models"
                },
                "summary": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint."
                },
                "authors": [
                    {
                        "name": "Boris Ruf"
                    },
                    {
                        "name": "Marcin Detyniecki"
                    }
                ],
                "author_detail": {
                    "name": "Marcin Detyniecki"
                },
                "author": "Marcin Detyniecki",
                "arxiv_comment": "Presented at the 18th IEEE International Workshop on Multimedia\n  Technologies for E-Learning (MTEL), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14201v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14201v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01068v1",
                "updated": "2025-02-03T05:25:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    5,
                    25,
                    9,
                    0,
                    34,
                    0
                ],
                "published": "2025-02-03T05:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    5,
                    25,
                    9,
                    0,
                    34,
                    0
                ],
                "title": "FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation"
                },
                "summary": "While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial key-value (KV) caches to store contextual information,\nwhich can heavily burden computational efficiency and memory usage. Previous\nefforts to compress these KV caches primarily focused on reducing memory\ndemands but were limited in enhancing latency. To address this issue, we\nintroduce FastKV, a KV cache compression method designed to enhance latency for\nlong-context sequences. To enhance processing speeds while maintaining\naccuracy, FastKV adopts a novel Token-Selective Propagation (TSP) approach that\nretains the full context information in the initial layers of LLMs and\nselectively propagates only a portion of this information in deeper layers even\nin the prefill stage. Additionally, FastKV incorporates grouped-query attention\n(GQA)-aware KV cache compression to exploit the advantages of GQA in both\nmemory and computational efficiency. Our experimental results show that FastKV\nachieves 2.00$\\times$ and 1.40$\\times$ improvements in time-to-first-token\n(TTFT) and throughput, respectively, compared to HeadKV, the state-of-the-art\nKV cache compression method. Moreover, FastKV successfully maintains accuracy\non long-context benchmarks at levels comparable to the baselines. Our code is\navailable at https://github.com/dongwonjo/FastKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial key-value (KV) caches to store contextual information,\nwhich can heavily burden computational efficiency and memory usage. Previous\nefforts to compress these KV caches primarily focused on reducing memory\ndemands but were limited in enhancing latency. To address this issue, we\nintroduce FastKV, a KV cache compression method designed to enhance latency for\nlong-context sequences. To enhance processing speeds while maintaining\naccuracy, FastKV adopts a novel Token-Selective Propagation (TSP) approach that\nretains the full context information in the initial layers of LLMs and\nselectively propagates only a portion of this information in deeper layers even\nin the prefill stage. Additionally, FastKV incorporates grouped-query attention\n(GQA)-aware KV cache compression to exploit the advantages of GQA in both\nmemory and computational efficiency. Our experimental results show that FastKV\nachieves 2.00$\\times$ and 1.40$\\times$ improvements in time-to-first-token\n(TTFT) and throughput, respectively, compared to HeadKV, the state-of-the-art\nKV cache compression method. Moreover, FastKV successfully maintains accuracy\non long-context benchmarks at levels comparable to the baselines. Our code is\navailable at https://github.com/dongwonjo/FastKV."
                },
                "authors": [
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Jiwon Song"
                    },
                    {
                        "name": "Yulhwa Kim"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08784v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08784v2",
                "updated": "2025-02-02T14:38:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    14,
                    38,
                    15,
                    6,
                    33,
                    0
                ],
                "published": "2023-10-12T07:35:30Z",
                "published_parsed": [
                    2023,
                    10,
                    12,
                    7,
                    35,
                    30,
                    3,
                    285,
                    0
                ],
                "title": "Implicit Shape and Appearance Priors for Few-Shot Full Head\n  Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit Shape and Appearance Priors for Few-Shot Full Head\n  Reconstruction"
                },
                "summary": "Recent advancements in learning techniques that employ coordinate-based\nneural representations have yielded remarkable results in multi-view 3D\nreconstruction tasks. However, these approaches often require a substantial\nnumber of input views (typically several tens) and computationally intensive\noptimization procedures to achieve their effectiveness. In this paper, we\naddress these limitations specifically for the problem of few-shot full 3D head\nreconstruction. We accomplish this by incorporating a probabilistic shape and\nappearance prior into coordinate-based representations, enabling faster\nconvergence and improved generalization when working with only a few input\nimages (even as low as a single image). During testing, we leverage this prior\nto guide the fitting process of a signed distance function using a\ndifferentiable renderer. By incorporating the statistical prior alongside\nparallelizable ray tracing and dynamic caching strategies, we achieve an\nefficient and accurate approach to few-shot full 3D head reconstruction.\nMoreover, we extend the H3DS dataset, which now comprises 60 high-resolution 3D\nfull head scans and their corresponding posed images and masks, which we use\nfor evaluation purposes. By leveraging this dataset, we demonstrate the\nremarkable capabilities of our approach in achieving state-of-the-art results\nin geometry reconstruction while being an order of magnitude faster than\nprevious approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in learning techniques that employ coordinate-based\nneural representations have yielded remarkable results in multi-view 3D\nreconstruction tasks. However, these approaches often require a substantial\nnumber of input views (typically several tens) and computationally intensive\noptimization procedures to achieve their effectiveness. In this paper, we\naddress these limitations specifically for the problem of few-shot full 3D head\nreconstruction. We accomplish this by incorporating a probabilistic shape and\nappearance prior into coordinate-based representations, enabling faster\nconvergence and improved generalization when working with only a few input\nimages (even as low as a single image). During testing, we leverage this prior\nto guide the fitting process of a signed distance function using a\ndifferentiable renderer. By incorporating the statistical prior alongside\nparallelizable ray tracing and dynamic caching strategies, we achieve an\nefficient and accurate approach to few-shot full 3D head reconstruction.\nMoreover, we extend the H3DS dataset, which now comprises 60 high-resolution 3D\nfull head scans and their corresponding posed images and masks, which we use\nfor evaluation purposes. By leveraging this dataset, we demonstrate the\nremarkable capabilities of our approach in achieving state-of-the-art results\nin geometry reconstruction while being an order of magnitude faster than\nprevious approaches."
                },
                "authors": [
                    {
                        "name": "Pol Caselles"
                    },
                    {
                        "name": "Eduard Ramon"
                    },
                    {
                        "name": "Jaime Garcia"
                    },
                    {
                        "name": "Gil Triginer"
                    },
                    {
                        "name": "Francesc Moreno-Noguer"
                    }
                ],
                "author_detail": {
                    "name": "Francesc Moreno-Noguer"
                },
                "author": "Francesc Moreno-Noguer",
                "arxiv_doi": "10.1109/TPAMI.2025.3540542",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TPAMI.2025.3540542",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.08784v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08784v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI) 2025",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16383v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16383v2",
                "updated": "2025-02-02T03:04:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    3,
                    4,
                    54,
                    6,
                    33,
                    0
                ],
                "published": "2025-01-25T01:45:29Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    1,
                    45,
                    29,
                    5,
                    25,
                    0
                ],
                "title": "RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via\n  Outlier-Aware Adaptive Rotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via\n  Outlier-Aware Adaptive Rotations"
                },
                "summary": "Key-Value (KV) cache facilitates efficient large language models (LLMs)\ninference by avoiding recomputation of past KVs. As the batch size and context\nlength increase, the oversized KV caches become a significant memory\nbottleneck, highlighting the need for efficient compression. Existing KV\nquantization rely on fine-grained quantization or the retention of a\nsignificant portion of high bit-widths caches, both of which compromise\ncompression ratio and often fail to maintain robustness at extremely low\naverage bit-widths. In this work, we explore the potential of rotation\ntechnique for 2-bit KV quantization and propose RotateKV, which achieves\naccurate and robust performance through the following innovations: (i)\nOutlier-Aware Rotation, which utilizes channel-reordering to adapt the\nrotations to varying channel-wise outlier distributions without sacrificing the\ncomputational efficiency of the fast Walsh-Hadamard transform (FWHT); (ii)\nPre-RoPE Grouped-Head Rotation, which mitigates the impact of rotary position\nembedding (RoPE) on proposed outlier-aware rotation and further smooths\noutliers across heads; (iii) Attention-Sink-Aware Quantization, which leverages\nthe massive activations to precisely identify and protect attention sinks.\nRotateKV achieves less than 0.3 perplexity (PPL) degradation with 2-bit\nquantization on WikiText-2 using LLaMA-2-13B, maintains strong CoT reasoning\nand long-context capabilities, with less than 1.7\\% degradation on GSM8K,\noutperforming existing methods even at lower average bit-widths. RotateKV also\nshowcases a 3.97x reduction in peak memory usage, supports 5.75x larger batch\nsizes, and achieves a 2.32x speedup in decoding stage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache facilitates efficient large language models (LLMs)\ninference by avoiding recomputation of past KVs. As the batch size and context\nlength increase, the oversized KV caches become a significant memory\nbottleneck, highlighting the need for efficient compression. Existing KV\nquantization rely on fine-grained quantization or the retention of a\nsignificant portion of high bit-widths caches, both of which compromise\ncompression ratio and often fail to maintain robustness at extremely low\naverage bit-widths. In this work, we explore the potential of rotation\ntechnique for 2-bit KV quantization and propose RotateKV, which achieves\naccurate and robust performance through the following innovations: (i)\nOutlier-Aware Rotation, which utilizes channel-reordering to adapt the\nrotations to varying channel-wise outlier distributions without sacrificing the\ncomputational efficiency of the fast Walsh-Hadamard transform (FWHT); (ii)\nPre-RoPE Grouped-Head Rotation, which mitigates the impact of rotary position\nembedding (RoPE) on proposed outlier-aware rotation and further smooths\noutliers across heads; (iii) Attention-Sink-Aware Quantization, which leverages\nthe massive activations to precisely identify and protect attention sinks.\nRotateKV achieves less than 0.3 perplexity (PPL) degradation with 2-bit\nquantization on WikiText-2 using LLaMA-2-13B, maintains strong CoT reasoning\nand long-context capabilities, with less than 1.7\\% degradation on GSM8K,\noutperforming existing methods even at lower average bit-widths. RotateKV also\nshowcases a 3.97x reduction in peak memory usage, supports 5.75x larger batch\nsizes, and achieves a 2.32x speedup in decoding stage."
                },
                "authors": [
                    {
                        "name": "Zunhai Su"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Wang Shen"
                    },
                    {
                        "name": "Hanyu Wei"
                    },
                    {
                        "name": "Linge Li"
                    },
                    {
                        "name": "Huangqi Yu"
                    },
                    {
                        "name": "Kehong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Kehong Yuan"
                },
                "author": "Kehong Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16383v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16383v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00527v1",
                "updated": "2025-02-01T18:59:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    18,
                    59,
                    3,
                    5,
                    32,
                    0
                ],
                "published": "2025-02-01T18:59:03Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    18,
                    59,
                    3,
                    5,
                    32,
                    0
                ],
                "title": "PolarQuant: Leveraging Polar Transformation for Efficient Key Cache\n  Quantization and Decoding Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PolarQuant: Leveraging Polar Transformation for Efficient Key Cache\n  Quantization and Decoding Acceleration"
                },
                "summary": "The KV cache in large language models is a dominant factor in memory usage,\nlimiting their broader applicability. Quantizing the cache to lower bit widths\nis an effective way to reduce computational costs; however, previous methods\nstruggle with quantizing key vectors due to outliers, resulting in excessive\noverhead. We propose a novel quantization approach called PolarQuant, which\nefficiently addresses the outlier challenge. We observe that outliers typically\nappear in only one of two dimensions, which are rotated together by a specific\nangle when rotary position embeddings are applied. When represented as\ntwo-dimensional vectors, these dimensions exhibit well-structured patterns,\nwith radii and angles smoothly distributed in polar coordinates. This\nalleviates the challenge of outliers on per-channel quantization, making them\nwell-suited for quantization. Thus, PolarQuant divides key vectors into groups\nof two-dimensional sub-vectors, encoding them as the corresponding quantized\nradius and the polar angle, rather than quantizing original key vectors\ndirectly. PolarQuant achieves the superior efficiency in KV cache quantization\nand accelerates the decoding process by turning the query-key inner product\ninto a table lookup, all while maintaining the downstream performance of\nfull-precision models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The KV cache in large language models is a dominant factor in memory usage,\nlimiting their broader applicability. Quantizing the cache to lower bit widths\nis an effective way to reduce computational costs; however, previous methods\nstruggle with quantizing key vectors due to outliers, resulting in excessive\noverhead. We propose a novel quantization approach called PolarQuant, which\nefficiently addresses the outlier challenge. We observe that outliers typically\nappear in only one of two dimensions, which are rotated together by a specific\nangle when rotary position embeddings are applied. When represented as\ntwo-dimensional vectors, these dimensions exhibit well-structured patterns,\nwith radii and angles smoothly distributed in polar coordinates. This\nalleviates the challenge of outliers on per-channel quantization, making them\nwell-suited for quantization. Thus, PolarQuant divides key vectors into groups\nof two-dimensional sub-vectors, encoding them as the corresponding quantized\nradius and the polar angle, rather than quantizing original key vectors\ndirectly. PolarQuant achieves the superior efficiency in KV cache quantization\nand accelerates the decoding process by turning the query-key inner product\ninto a table lookup, all while maintaining the downstream performance of\nfull-precision models."
                },
                "authors": [
                    {
                        "name": "Songhao Wu"
                    },
                    {
                        "name": "Ang Lv"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Yufei Zhang"
                    },
                    {
                        "name": "Xun Zhang"
                    },
                    {
                        "name": "Guojun Yin"
                    },
                    {
                        "name": "Wei Lin"
                    },
                    {
                        "name": "Rui Yan"
                    }
                ],
                "author_detail": {
                    "name": "Rui Yan"
                },
                "author": "Rui Yan",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05262v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05262v3",
                "updated": "2025-02-01T16:00:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    16,
                    0,
                    50,
                    5,
                    32,
                    0
                ],
                "published": "2025-01-09T14:16:43Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    14,
                    16,
                    43,
                    3,
                    9,
                    0
                ],
                "title": "QMDB: Quick Merkle Database",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QMDB: Quick Merkle Database"
                },
                "summary": "Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain\nstate management by integrating key-value (KV) and Merkle tree storage into a\nsingle unified architecture. QMDB delivers a significant throughput improvement\nover existing architectures, achieving up to 6X over the widely used RocksDB\nand 8X over NOMT, a leading verifiable database. Its novel append-only\ntwig-based design enables one SSD read per state access, O(1) IOs for updates,\nand in-memory Merkleization on a memory footprint as small as 2.3 bytes per\nentry, enabling it to run on even modest consumer-grade PCs. QMDB scales\nseamlessly across both commodity and enterprise hardware, achieving up to 2.28\nmillion state updates per second. This performance enables support for 1\nmillion token transfers per second (TPS), marking QMDB as the first solution\nachieving such a milestone. QMDB has been benchmarked with workloads exceeding\n15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to\nscale to 280 billion entries on a single server. Furthermore, QMDB introduces\nhistorical proofs, unlocking the ability to query its blockchain's historical\nstate at the latest block. QMDB not only meets the demands of current\nblockchains but also provides a robust foundation for building scalable,\nefficient, and verifiable decentralized applications across diverse use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain\nstate management by integrating key-value (KV) and Merkle tree storage into a\nsingle unified architecture. QMDB delivers a significant throughput improvement\nover existing architectures, achieving up to 6X over the widely used RocksDB\nand 8X over NOMT, a leading verifiable database. Its novel append-only\ntwig-based design enables one SSD read per state access, O(1) IOs for updates,\nand in-memory Merkleization on a memory footprint as small as 2.3 bytes per\nentry, enabling it to run on even modest consumer-grade PCs. QMDB scales\nseamlessly across both commodity and enterprise hardware, achieving up to 2.28\nmillion state updates per second. This performance enables support for 1\nmillion token transfers per second (TPS), marking QMDB as the first solution\nachieving such a milestone. QMDB has been benchmarked with workloads exceeding\n15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to\nscale to 280 billion entries on a single server. Furthermore, QMDB introduces\nhistorical proofs, unlocking the ability to query its blockchain's historical\nstate at the latest block. QMDB not only meets the demands of current\nblockchains but also provides a robust foundation for building scalable,\nefficient, and verifiable decentralized applications across diverse use cases."
                },
                "authors": [
                    {
                        "name": "Isaac Zhang"
                    },
                    {
                        "name": "Ryan Zarick"
                    },
                    {
                        "name": "Daniel Wong"
                    },
                    {
                        "name": "Thomas Kim"
                    },
                    {
                        "name": "Bryan Pellegrino"
                    },
                    {
                        "name": "Mignon Li"
                    },
                    {
                        "name": "Kelvin Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kelvin Wong"
                },
                "author": "Kelvin Wong",
                "arxiv_comment": "11 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05262v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05262v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00439v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00439v1",
                "updated": "2025-02-01T14:16:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    14,
                    16,
                    31,
                    5,
                    32,
                    0
                ],
                "published": "2025-02-01T14:16:31Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    14,
                    16,
                    31,
                    5,
                    32,
                    0
                ],
                "title": "UniAttn: Reducing Inference Costs via Softmax Unification for\n  Post-Training LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniAttn: Reducing Inference Costs via Softmax Unification for\n  Post-Training LLMs"
                },
                "summary": "Post-training is essential for adapting Large Language Models (LLMs) to\nreal-world applications. Deploying post-trained models faces significant\nchallenges due to substantial memory overhead and noticeable inference latency.\nExisting work has identified significant redundancies in LLMs and proposed\nefficient architectures, namely intra-layer KV sharing and cross-layer KV\nsharing. However, intra-layer KV sharing still results in high inference costs,\nwhile cross-layer KV sharing leads to significant performance degradation. As a\nresult, both methods remain suboptimal for post-training pre-trained LLMs. In\nthis paper, we identify that the \\texttt{Softmax} operation is a primary\nbottleneck for LLM inference and discover that it is actually highly redundant\nduring post-training. We propose Softmax \\textbf{Uni}fication in\n\\textbf{Att}e\\textbf{n}tion (\\textbf{UniAttn}), a novel post-training method\nthat unifies Softmax activations across transformer blocks to reduce LLM\ninference costs. Additionally, UniAttn adopts a linear projection to compensate\nfor the errors induced by Softmax unification. Experiments show that UniAttn\nmatches the performance of standard post-training while significantly reducing\ninference costs, outperforming existing efficient architectures during\npost-training. Our code will be available at\n\\url{https://github.com/Bostoncake/UniAttn}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training is essential for adapting Large Language Models (LLMs) to\nreal-world applications. Deploying post-trained models faces significant\nchallenges due to substantial memory overhead and noticeable inference latency.\nExisting work has identified significant redundancies in LLMs and proposed\nefficient architectures, namely intra-layer KV sharing and cross-layer KV\nsharing. However, intra-layer KV sharing still results in high inference costs,\nwhile cross-layer KV sharing leads to significant performance degradation. As a\nresult, both methods remain suboptimal for post-training pre-trained LLMs. In\nthis paper, we identify that the \\texttt{Softmax} operation is a primary\nbottleneck for LLM inference and discover that it is actually highly redundant\nduring post-training. We propose Softmax \\textbf{Uni}fication in\n\\textbf{Att}e\\textbf{n}tion (\\textbf{UniAttn}), a novel post-training method\nthat unifies Softmax activations across transformer blocks to reduce LLM\ninference costs. Additionally, UniAttn adopts a linear projection to compensate\nfor the errors induced by Softmax unification. Experiments show that UniAttn\nmatches the performance of standard post-training while significantly reducing\ninference costs, outperforming existing efficient architectures during\npost-training. Our code will be available at\n\\url{https://github.com/Bostoncake/UniAttn}."
                },
                "authors": [
                    {
                        "name": "Yizhe Xiong"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Xin Ye"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Haoran Lian"
                    },
                    {
                        "name": "Zhenpeng Su"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "11 pages, 4 figures. Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00439v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00439v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00433v1",
                "updated": "2025-02-01T13:46:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    13,
                    46,
                    2,
                    5,
                    32,
                    0
                ],
                "published": "2025-02-01T13:46:02Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    13,
                    46,
                    2,
                    5,
                    32,
                    0
                ],
                "title": "CAT Pruning: Cluster-Aware Token Pruning For Text-to-Image Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAT Pruning: Cluster-Aware Token Pruning For Text-to-Image Diffusion\n  Models"
                },
                "summary": "Diffusion models have revolutionized generative tasks, especially in the\ndomain of text-to-image synthesis; however, their iterative denoising process\ndemands substantial computational resources. In this paper, we present a novel\nacceleration strategy that integrates token-level pruning with caching\ntechniques to tackle this computational challenge. By employing noise relative\nmagnitude, we identify significant token changes across denoising iterations.\nAdditionally, we enhance token selection by incorporating spatial clustering\nand ensuring distributional balance. Our experiments demonstrate reveal a\n50%-60% reduction in computational costs while preserving the performance of\nthe model, thereby markedly increasing the efficiency of diffusion models. The\ncode is available at https://github.com/ada-cheng/CAT-Pruning",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have revolutionized generative tasks, especially in the\ndomain of text-to-image synthesis; however, their iterative denoising process\ndemands substantial computational resources. In this paper, we present a novel\nacceleration strategy that integrates token-level pruning with caching\ntechniques to tackle this computational challenge. By employing noise relative\nmagnitude, we identify significant token changes across denoising iterations.\nAdditionally, we enhance token selection by incorporating spatial clustering\nand ensuring distributional balance. Our experiments demonstrate reveal a\n50%-60% reduction in computational costs while preserving the performance of\nthe model, thereby markedly increasing the efficiency of diffusion models. The\ncode is available at https://github.com/ada-cheng/CAT-Pruning"
                },
                "authors": [
                    {
                        "name": "Xinle Cheng"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Zhihao Jia"
                    }
                ],
                "author_detail": {
                    "name": "Zhihao Jia"
                },
                "author": "Zhihao Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00382v1",
                "updated": "2025-02-01T09:41:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    9,
                    41,
                    1,
                    5,
                    32,
                    0
                ],
                "published": "2025-02-01T09:41:01Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    9,
                    41,
                    1,
                    5,
                    32,
                    0
                ],
                "title": "Masked Generative Nested Transformers with Decode Time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked Generative Nested Transformers with Decode Time Scaling"
                },
                "summary": "Recent advances in visual generation have made significant strides in\nproducing content of exceptional quality. However, most methods suffer from a\nfundamental problem - a bottleneck of inference computational efficiency. Most\nof these algorithms involve multiple passes over a transformer model to\ngenerate tokens or denoise inputs. However, the model size is kept consistent\nthroughout all iterations, which makes it computationally expensive. In this\nwork, we aim to address this issue primarily through two key ideas - (a) not\nall parts of the generation process need equal compute, and we design a decode\ntime model scaling schedule to utilize compute effectively, and (b) we can\ncache and reuse some of the computation. Combining these two ideas leads to\nusing smaller models to process more tokens while large models process fewer\ntokens. These different-sized models do not increase the parameter size, as\nthey share parameters. We rigorously experiment with ImageNet256$\\times$256 ,\nUCF101, and Kinetics600 to showcase the efficacy of the proposed method for\nimage/video generation and frame prediction. Our experiments show that with\nalmost $3\\times$ less compute than baseline, our model obtains competitive\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in visual generation have made significant strides in\nproducing content of exceptional quality. However, most methods suffer from a\nfundamental problem - a bottleneck of inference computational efficiency. Most\nof these algorithms involve multiple passes over a transformer model to\ngenerate tokens or denoise inputs. However, the model size is kept consistent\nthroughout all iterations, which makes it computationally expensive. In this\nwork, we aim to address this issue primarily through two key ideas - (a) not\nall parts of the generation process need equal compute, and we design a decode\ntime model scaling schedule to utilize compute effectively, and (b) we can\ncache and reuse some of the computation. Combining these two ideas leads to\nusing smaller models to process more tokens while large models process fewer\ntokens. These different-sized models do not increase the parameter size, as\nthey share parameters. We rigorously experiment with ImageNet256$\\times$256 ,\nUCF101, and Kinetics600 to showcase the efficacy of the proposed method for\nimage/video generation and frame prediction. Our experiments show that with\nalmost $3\\times$ less compute than baseline, our model obtains competitive\nperformance."
                },
                "authors": [
                    {
                        "name": "Sahil Goyal"
                    },
                    {
                        "name": "Debapriya Tula"
                    },
                    {
                        "name": "Gagan Jain"
                    },
                    {
                        "name": "Pradeep Shenoy"
                    },
                    {
                        "name": "Prateek Jain"
                    },
                    {
                        "name": "Sujoy Paul"
                    }
                ],
                "author_detail": {
                    "name": "Sujoy Paul"
                },
                "author": "Sujoy Paul",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11305v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11305v2",
                "updated": "2025-02-01T04:24:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    4,
                    24,
                    16,
                    5,
                    32,
                    0
                ],
                "published": "2024-10-15T05:57:51Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    57,
                    51,
                    1,
                    289,
                    0
                ],
                "title": "QSpec: Speculative Decoding with Complementary Quantization Schemes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QSpec: Speculative Decoding with Complementary Quantization Schemes"
                },
                "summary": "Quantization has been substantially adopted to accelerate inference and\nreduce memory consumption of large language models (LLMs). While\nactivation-weight joint quantization speeds up the inference process through\nlow-precision kernels, we demonstrate that it suffers severe performance\ndegradation on multi-step reasoning tasks, rendering it ineffective. We propose\na novel quantization paradigm called QSPEC, which seamlessly integrates two\ncomplementary quantization schemes for speculative decoding. Leveraging nearly\ncost-free execution switching, QSPEC drafts tokens with low-precision, fast\nactivation-weight quantization, and verifies them with high-precision\nweight-only quantization, effectively combining the strengths of both\nquantization schemes. Compared to high-precision quantization methods, QSPEC\nempirically boosts token generation throughput by up to 1.64x without any\nquality compromise, distinguishing it from other low-precision quantization\napproaches. This enhancement is also consistent across various serving tasks,\nmodel sizes, quantization methods, and batch sizes. Compared to state-of-art\nspeculative decoding methods, our approach reuses weights and the KV cache,\navoiding extra memory overhead while achieving up to 1.55x speedup in batched\nserving with a high acceptance rate. Furthermore, QSPEC offers a plug-and-play\nadvantage without requiring any training. We believe that QSPEC demonstrates\nunique strengths for future deployment of high-fidelity quantization schemes,\nparticularly in memory-constrained scenarios (e.g., edge devices).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization has been substantially adopted to accelerate inference and\nreduce memory consumption of large language models (LLMs). While\nactivation-weight joint quantization speeds up the inference process through\nlow-precision kernels, we demonstrate that it suffers severe performance\ndegradation on multi-step reasoning tasks, rendering it ineffective. We propose\na novel quantization paradigm called QSPEC, which seamlessly integrates two\ncomplementary quantization schemes for speculative decoding. Leveraging nearly\ncost-free execution switching, QSPEC drafts tokens with low-precision, fast\nactivation-weight quantization, and verifies them with high-precision\nweight-only quantization, effectively combining the strengths of both\nquantization schemes. Compared to high-precision quantization methods, QSPEC\nempirically boosts token generation throughput by up to 1.64x without any\nquality compromise, distinguishing it from other low-precision quantization\napproaches. This enhancement is also consistent across various serving tasks,\nmodel sizes, quantization methods, and batch sizes. Compared to state-of-art\nspeculative decoding methods, our approach reuses weights and the KV cache,\navoiding extra memory overhead while achieving up to 1.55x speedup in batched\nserving with a high acceptance rate. Furthermore, QSPEC offers a plug-and-play\nadvantage without requiring any training. We believe that QSPEC demonstrates\nunique strengths for future deployment of high-fidelity quantization schemes,\nparticularly in memory-constrained scenarios (e.g., edge devices)."
                },
                "authors": [
                    {
                        "name": "Juntao Zhao"
                    },
                    {
                        "name": "Wenhao Lu"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Chuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Wu"
                },
                "author": "Chuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11305v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11305v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00299v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00299v1",
                "updated": "2025-02-01T03:49:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "published": "2025-02-01T03:49:47Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference"
                },
                "summary": "To reduce memory costs in long-context inference with Large Language Models\n(LLMs), many recent works focus on compressing the key-value (KV) cache of\ndifferent tokens. However, we identify that the previous KV cache compression\nmethods measure token importance individually, neglecting the dependency\nbetween different tokens in the real-world language characterics. In light of\nthis, we introduce ChunkKV, grouping the tokens in a chunk as a basic\ncompressing unit, and retaining the most informative semantic chunks while\ndiscarding the less important ones. Furthermore, observing that ChunkKV\nexhibits higher similarity in the preserved indices across different layers, we\npropose layer-wise index reuse to further reduce computational overhead. We\nevaluated ChunkKV on cutting-edge long-context benchmarks including LongBench\nand Needle-In-A-HayStack, as well as the GSM8K and JailbreakV in-context\nlearning benchmark. Our experiments with instruction tuning and multi-step\nreasoning (O1 and R1) LLMs, achieve up to 10\\% performance improvement under\naggressive compression ratios compared to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To reduce memory costs in long-context inference with Large Language Models\n(LLMs), many recent works focus on compressing the key-value (KV) cache of\ndifferent tokens. However, we identify that the previous KV cache compression\nmethods measure token importance individually, neglecting the dependency\nbetween different tokens in the real-world language characterics. In light of\nthis, we introduce ChunkKV, grouping the tokens in a chunk as a basic\ncompressing unit, and retaining the most informative semantic chunks while\ndiscarding the less important ones. Furthermore, observing that ChunkKV\nexhibits higher similarity in the preserved indices across different layers, we\npropose layer-wise index reuse to further reduce computational overhead. We\nevaluated ChunkKV on cutting-edge long-context benchmarks including LongBench\nand Needle-In-A-HayStack, as well as the GSM8K and JailbreakV in-context\nlearning benchmark. Our experiments with instruction tuning and multi-step\nreasoning (O1 and R1) LLMs, achieve up to 10\\% performance improvement under\naggressive compression ratios compared to existing methods."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "35 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00299v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07331v2",
                "updated": "2025-02-01T03:40:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    40,
                    37,
                    5,
                    32,
                    0
                ],
                "published": "2024-09-11T15:11:39Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    11,
                    39,
                    2,
                    255,
                    0
                ],
                "title": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering"
                },
                "summary": "Multimodal large language models (MLLMs) have demonstrated great performance\non visual question answering (VQA). When it comes to knowledge-based Visual\nQuestion Answering (KB-VQA), MLLMs may lack the specialized domain knowledge\nneeded to answer questions, necessitating the retrieval of necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose \\textbf{R}etrieval-\\textbf{A}ugmented MLLMs with\nCompressed Contexts (RACC). RACC learns to compress and aggregate retrieved\nknowledge for a given image-question pair, generating a compact modulation in\nthe form of Key-Value (KV) cache to adapt the downstream frozen MLLM, thereby\nachieving effective and efficient inference. RACC achieves a state-of-the-art\n(SOTA) performance of 63.92\\% on OK-VQA. Moreover, it significantly reduces\ninference latency by 22.0\\%-59.7\\% compared to the prominent RAVQA-v2. Abundant\nexperiments show RACC's broad applicability. It is compatible with various\noff-the-shelf MLLMs and can also handle different knowledge sources including\ntextual and multimodal documents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have demonstrated great performance\non visual question answering (VQA). When it comes to knowledge-based Visual\nQuestion Answering (KB-VQA), MLLMs may lack the specialized domain knowledge\nneeded to answer questions, necessitating the retrieval of necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose \\textbf{R}etrieval-\\textbf{A}ugmented MLLMs with\nCompressed Contexts (RACC). RACC learns to compress and aggregate retrieved\nknowledge for a given image-question pair, generating a compact modulation in\nthe form of Key-Value (KV) cache to adapt the downstream frozen MLLM, thereby\nachieving effective and efficient inference. RACC achieves a state-of-the-art\n(SOTA) performance of 63.92\\% on OK-VQA. Moreover, it significantly reduces\ninference latency by 22.0\\%-59.7\\% compared to the prominent RAVQA-v2. Abundant\nexperiments show RACC's broad applicability. It is compatible with various\noff-the-shelf MLLMs and can also handle different knowledge sources including\ntextual and multimodal documents."
                },
                "authors": [
                    {
                        "name": "Weixi Weng"
                    },
                    {
                        "name": "Jieming Zhu"
                    },
                    {
                        "name": "Xiaojun Meng"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Chun Yuan"
                },
                "author": "Chun Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12178v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12178v2",
                "updated": "2025-01-31T19:09:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    19,
                    9,
                    19,
                    4,
                    31,
                    0
                ],
                "published": "2024-12-13T02:26:54Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    26,
                    54,
                    4,
                    348,
                    0
                ],
                "title": "Activation Sparsity Opportunities for Compressing General Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation Sparsity Opportunities for Compressing General Large Language\n  Models"
                },
                "summary": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize the\ncompression rate while maintaining great accuracy. LLMs' Feed-Forward Network\n(FFN) components, which typically comprise a large proportion of parameters\n(around 2/3), ensure that our FFN optimizations would have a better chance of\nachieving effective compression. Moreover, our findings are beneficial to\ngeneral LLMs and are not restricted to ReLU-based models. This work\nsystematically investigates the tradeoff between enforcing activation sparsity\nand perplexity (accuracy) on state-of-the-art LLMs. Our empirical analysis\ndemonstrates that we can obtain around 50% of main memory and computing\nreductions for critical FFN components with negligible accuracy degradation.\nThis extra 50% sparsity does not naturally exist in the current LLMs, which\nrequire tuning LLMs' activation outputs by injecting zero-enforcing thresholds.\nTo obtain the benefits of activation sparsity, we provide a guideline for the\nsystem architect for LLM prediction and prefetching. The success prediction\nallows the system to prefetch the necessary weights while omitting the inactive\nones and their successors, therefore lowering cache and memory pollution and\nreducing LLM execution time on resource-constrained edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize the\ncompression rate while maintaining great accuracy. LLMs' Feed-Forward Network\n(FFN) components, which typically comprise a large proportion of parameters\n(around 2/3), ensure that our FFN optimizations would have a better chance of\nachieving effective compression. Moreover, our findings are beneficial to\ngeneral LLMs and are not restricted to ReLU-based models. This work\nsystematically investigates the tradeoff between enforcing activation sparsity\nand perplexity (accuracy) on state-of-the-art LLMs. Our empirical analysis\ndemonstrates that we can obtain around 50% of main memory and computing\nreductions for critical FFN components with negligible accuracy degradation.\nThis extra 50% sparsity does not naturally exist in the current LLMs, which\nrequire tuning LLMs' activation outputs by injecting zero-enforcing thresholds.\nTo obtain the benefits of activation sparsity, we provide a guideline for the\nsystem architect for LLM prediction and prefetching. The success prediction\nallows the system to prefetch the necessary weights while omitting the inactive\nones and their successors, therefore lowering cache and memory pollution and\nreducing LLM execution time on resource-constrained edge devices."
                },
                "authors": [
                    {
                        "name": "Nobel Dhar"
                    },
                    {
                        "name": "Bobin Deng"
                    },
                    {
                        "name": "Md Romyull Islam"
                    },
                    {
                        "name": "Kazi Fahim Ahmad Nasif"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Kun Suo"
                    }
                ],
                "author_detail": {
                    "name": "Kun Suo"
                },
                "author": "Kun Suo",
                "arxiv_doi": "10.1109/IPCCC59868.2024.10850382",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IPCCC59868.2024.10850382",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.12178v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12178v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "pp. 1-9, doi: 10.1109/IPCCC59868.2024.10850382. keywords:\n  {Accuracy;Prefetching;Large language models;Computational\n  modeling;Companies;Transformers;User experience;Time\n  factors;Tuning;Guidelines;Large Language Models (LLMs);AI\n  Compression;Activation Sparsity;Edge LLM},",
                "arxiv_journal_ref": "2024 IEEE International Performance, Computing, and Communications\n  Conference (IPCCC), Orlando, FL, USA, 2024",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19300v1",
                "updated": "2025-01-31T16:56:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    56,
                    18,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T16:56:18Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    56,
                    18,
                    4,
                    31,
                    0
                ],
                "title": "Offline Learning for Combinatorial Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline Learning for Combinatorial Multi-armed Bandits"
                },
                "summary": "The combinatorial multi-armed bandit (CMAB) is a fundamental sequential\ndecision-making framework, extensively studied over the past decade. However,\nexisting work primarily focuses on the online setting, overlooking the\nsubstantial costs of online interactions and the readily available offline\ndatasets. To overcome these limitations, we introduce Off-CMAB, the first\noffline learning framework for CMAB. Central to our framework is the\ncombinatorial lower confidence bound (CLCB) algorithm, which combines\npessimistic reward estimations with combinatorial solvers. To characterize the\nquality of offline datasets, we propose two novel data coverage conditions and\nprove that, under these conditions, CLCB achieves a near-optimal suboptimality\ngap, matching the theoretical lower bound up to a logarithmic factor. We\nvalidate Off-CMAB through practical applications, including learning to rank,\nlarge language model (LLM) caching, and social influence maximization, showing\nits ability to handle nonlinear reward functions, general feedback models, and\nout-of-distribution action samples that excludes optimal or even feasible\nactions. Extensive experiments on synthetic and real-world datasets further\nhighlight the superior performance of CLCB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The combinatorial multi-armed bandit (CMAB) is a fundamental sequential\ndecision-making framework, extensively studied over the past decade. However,\nexisting work primarily focuses on the online setting, overlooking the\nsubstantial costs of online interactions and the readily available offline\ndatasets. To overcome these limitations, we introduce Off-CMAB, the first\noffline learning framework for CMAB. Central to our framework is the\ncombinatorial lower confidence bound (CLCB) algorithm, which combines\npessimistic reward estimations with combinatorial solvers. To characterize the\nquality of offline datasets, we propose two novel data coverage conditions and\nprove that, under these conditions, CLCB achieves a near-optimal suboptimality\ngap, matching the theoretical lower bound up to a logarithmic factor. We\nvalidate Off-CMAB through practical applications, including learning to rank,\nlarge language model (LLM) caching, and social influence maximization, showing\nits ability to handle nonlinear reward functions, general feedback models, and\nout-of-distribution action samples that excludes optimal or even feasible\nactions. Extensive experiments on synthetic and real-world datasets further\nhighlight the superior performance of CLCB."
                },
                "authors": [
                    {
                        "name": "Xutong Liu"
                    },
                    {
                        "name": "Xiangxiang Dai"
                    },
                    {
                        "name": "Jinhang Zuo"
                    },
                    {
                        "name": "Siwei Wang"
                    },
                    {
                        "name": "Carlee-Joe Wong"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Wei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wei Chen"
                },
                "author": "Wei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00085v1",
                "updated": "2025-01-31T16:22:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    22,
                    36,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T16:22:36Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    22,
                    36,
                    4,
                    31,
                    0
                ],
                "title": "Efficient Beam Search for Large Language Models Using Trie-Based\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Beam Search for Large Language Models Using Trie-Based\n  Decoding"
                },
                "summary": "In Transformer-based sequence-to-sequence generation, beam search has proven\neffective in enhancing the quality of generated sequences compared to greedy\ndecoding. Conventional beam search methods typically adopt either a sequential\nor batch-based approach. The sequential approach, while memory-efficient,\nrequires multiple decoding passes to construct a complete search tree, leading\nto significantly slower inference. On the other hand, the batch-based approach\nenables parallel computation across beams, but at the expense of high memory\nconsumption due to the need to maintain separate key-value (KV) caches for each\nbeam. In this study, we introduce a novel trie (prefix-tree)-based parallel\ndecoding method that addresses the memory inefficiency of batch-based beam\nsearch. By sharing a single KV cache among all beams that share the same\nprefix, the proposed method not only reduces memory consumption dramatically\nbut also enables parallel decoding across all branches. This innovative use of\na prefix tree offers an efficient alternative for beam search, achieving\nsignificant memory savings while preserving inference speed, making it\nparticularly well-suited for memory-constrained environments or large-scale\nmodel deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Transformer-based sequence-to-sequence generation, beam search has proven\neffective in enhancing the quality of generated sequences compared to greedy\ndecoding. Conventional beam search methods typically adopt either a sequential\nor batch-based approach. The sequential approach, while memory-efficient,\nrequires multiple decoding passes to construct a complete search tree, leading\nto significantly slower inference. On the other hand, the batch-based approach\nenables parallel computation across beams, but at the expense of high memory\nconsumption due to the need to maintain separate key-value (KV) caches for each\nbeam. In this study, we introduce a novel trie (prefix-tree)-based parallel\ndecoding method that addresses the memory inefficiency of batch-based beam\nsearch. By sharing a single KV cache among all beams that share the same\nprefix, the proposed method not only reduces memory consumption dramatically\nbut also enables parallel decoding across all branches. This innovative use of\na prefix tree offers an efficient alternative for beam search, achieving\nsignificant memory savings while preserving inference speed, making it\nparticularly well-suited for memory-constrained environments or large-scale\nmodel deployments."
                },
                "authors": [
                    {
                        "name": "Brian J Chan"
                    },
                    {
                        "name": "Jui-Hung Cheng"
                    },
                    {
                        "name": "Mao Xun Huang"
                    },
                    {
                        "name": "Chao-Ting Chen"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hen-Hsen Huang"
                },
                "author": "Hen-Hsen Huang",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19243v1",
                "updated": "2025-01-31T15:58:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    58,
                    15,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T15:58:15Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    58,
                    15,
                    4,
                    31,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Error-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Error-Optimized Cache"
                },
                "summary": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the Error-Optimized Cache (EOC). This method\nintroduces three key improvements: (1) Prior knowledge extraction: Extract and\nprocess the caching differences; (2) A judgment method for cache optimization:\nDetermine whether certain caching steps need to be optimized; (3) Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching (especially\nover-caching). On the ImageNet dataset, without significantly increasing the\ncomputational burden, this method improves the quality of the generated images\nunder the over-caching, rule-based, and training-based methods. Specifically,\nthe Fr\\'echet Inception Distance (FID) values are improved as follows: from\n6.857 to 5.821, from 3.870 to 3.692 and form 3.539 to 3.451 respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the Error-Optimized Cache (EOC). This method\nintroduces three key improvements: (1) Prior knowledge extraction: Extract and\nprocess the caching differences; (2) A judgment method for cache optimization:\nDetermine whether certain caching steps need to be optimized; (3) Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching (especially\nover-caching). On the ImageNet dataset, without significantly increasing the\ncomputational burden, this method improves the quality of the generated images\nunder the over-caching, rule-based, and training-based methods. Specifically,\nthe Fr\\'echet Inception Distance (FID) values are improved as follows: from\n6.857 to 5.821, from 3.870 to 3.692 and form 3.539 to 3.451 respectively."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Houcheng Jiang"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01723v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01723v3",
                "updated": "2025-01-31T14:26:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    14,
                    26,
                    5,
                    4,
                    31,
                    0
                ],
                "published": "2024-10-02T16:34:29Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    34,
                    29,
                    2,
                    276,
                    0
                ],
                "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Xinjie Zhang"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "Our code will be released upon acceptance. The Change Logs on Page 9\n  reveal our significant changes compared with v1 and v2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01723v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01723v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17426v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17426v3",
                "updated": "2025-01-31T14:13:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    14,
                    13,
                    49,
                    4,
                    31,
                    0
                ],
                "published": "2024-11-26T13:34:02Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    13,
                    34,
                    2,
                    1,
                    331,
                    0
                ],
                "title": "CLOVER: Cross-Layer Orthogonal Vectors Pruning and Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLOVER: Cross-Layer Orthogonal Vectors Pruning and Fine-Tuning"
                },
                "summary": "Decoder-only models generate tokens autoregressively by caching key/value\nvectors, but as the cache grows, inference becomes memory-bound. To address\nthis issue, we introduce CLOVER (Cross-Layer Orthogonal Vectors), a novel\napproach that treats pairs of attention layers as a set of low-rank\ndecompositions. CLOVER applies Singular Value Decomposition (SVD) to the \\( Q\n\\)-\\( K \\) and \\( V \\)-\\( O \\) pairs within each attention head. The resulting\nsingular values can either guide pruning or serve as trainable parameters for\nefficient fine-tuning of all orthogonal vectors. After pruning or fine-tuning,\nthese values are reintegrated into the model without increasing its parameter\ncount. We apply CLOVER to various models, including GPT-2 XL, DeepSeek-V2-Lite,\nWhisper-Large-v3, Stable Diffusion XL, and LLaMA-3.2-11B-Vision. Our results\ndemonstrate that CLOVER significantly improves pruning efficiency. For\ninstance, the perplexity of pruning 70\\% of the \\( Q \\)-\\( K \\) pairs in GPT-2\nXL is similar to that of pruning just 8\\% with vanilla methods. Fine-tuning the\nsingular values further results in a full-rank update, outperforming\nstate-of-the-art methods (LoRA, DoRA, HiRA, and PiSSA) by 7.6\\%, 5.5\\%, 3.8\\%,\nand 0.7\\%, respectively, on eight commonsense tasks for LLaMA-2 7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoder-only models generate tokens autoregressively by caching key/value\nvectors, but as the cache grows, inference becomes memory-bound. To address\nthis issue, we introduce CLOVER (Cross-Layer Orthogonal Vectors), a novel\napproach that treats pairs of attention layers as a set of low-rank\ndecompositions. CLOVER applies Singular Value Decomposition (SVD) to the \\( Q\n\\)-\\( K \\) and \\( V \\)-\\( O \\) pairs within each attention head. The resulting\nsingular values can either guide pruning or serve as trainable parameters for\nefficient fine-tuning of all orthogonal vectors. After pruning or fine-tuning,\nthese values are reintegrated into the model without increasing its parameter\ncount. We apply CLOVER to various models, including GPT-2 XL, DeepSeek-V2-Lite,\nWhisper-Large-v3, Stable Diffusion XL, and LLaMA-3.2-11B-Vision. Our results\ndemonstrate that CLOVER significantly improves pruning efficiency. For\ninstance, the perplexity of pruning 70\\% of the \\( Q \\)-\\( K \\) pairs in GPT-2\nXL is similar to that of pruning just 8\\% with vanilla methods. Fine-tuning the\nsingular values further results in a full-rank update, outperforming\nstate-of-the-art methods (LoRA, DoRA, HiRA, and PiSSA) by 7.6\\%, 5.5\\%, 3.8\\%,\nand 0.7\\%, respectively, on eight commonsense tasks for LLaMA-2 7B."
                },
                "authors": [
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Pingzhi Tang"
                    },
                    {
                        "name": "Fan jiang"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "https://github.com/GraphPKU/PiSSA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17426v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17426v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19051v1",
                "updated": "2025-01-31T11:25:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    25,
                    40,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T11:25:40Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    25,
                    40,
                    4,
                    31,
                    0
                ],
                "title": "Swift: Rethinking RDMA Control Plane for Elastic Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Swift: Rethinking RDMA Control Plane for Elastic Computing"
                },
                "summary": "Elastic computing enables dynamic scaling to meet workload demands, and\nRemote Direct Memory Access (RDMA) enhances this by providing high-throughput,\nlow-latency network communication. However, integrating RDMA into elastic\ncomputing remains a challenge, particularly in control plane operations for\nRDMA connection setup.\n  This paper revisits the assumptions of prior work on high-performance RDMA\nfor elastic computing, and reveals that extreme microsecond-level control plane\noptimizations are often unnecessary. By challenging the conventional beliefs on\nthe slowness of user-space RDMA control plane and the difficulty of user-space\nRDMA resource sharing, we uncover new design opportunities. Our key insight is\nthat user-space RDMA connection setup can be significantly improved with\ncaching, while RDMA resources can be efficiently shared among processes using\nfork. In light of this, we propose Swift, a simple yet effective solution that\nco-designs RDMA with a serverless framework to optimize performance for elastic\ncomputing. At its very core, Swift handles cold and warm serverless requests by\nswiftly initializing the RDMA control plane with cache-optimized libibverbs,\nand manages fork requests by leveraging the RDMA's fork capability. Implemented\nwith OpenWhisk, Swift delivers 30.56-46.50% higher average throughput and\n18.55-37.21% lower latency, at a cost of 6.5% control plane overhead, compared\nto prior solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Elastic computing enables dynamic scaling to meet workload demands, and\nRemote Direct Memory Access (RDMA) enhances this by providing high-throughput,\nlow-latency network communication. However, integrating RDMA into elastic\ncomputing remains a challenge, particularly in control plane operations for\nRDMA connection setup.\n  This paper revisits the assumptions of prior work on high-performance RDMA\nfor elastic computing, and reveals that extreme microsecond-level control plane\noptimizations are often unnecessary. By challenging the conventional beliefs on\nthe slowness of user-space RDMA control plane and the difficulty of user-space\nRDMA resource sharing, we uncover new design opportunities. Our key insight is\nthat user-space RDMA connection setup can be significantly improved with\ncaching, while RDMA resources can be efficiently shared among processes using\nfork. In light of this, we propose Swift, a simple yet effective solution that\nco-designs RDMA with a serverless framework to optimize performance for elastic\ncomputing. At its very core, Swift handles cold and warm serverless requests by\nswiftly initializing the RDMA control plane with cache-optimized libibverbs,\nand manages fork requests by leveraging the RDMA's fork capability. Implemented\nwith OpenWhisk, Swift delivers 30.56-46.50% higher average throughput and\n18.55-37.21% lower latency, at a cost of 6.5% control plane overhead, compared\nto prior solutions."
                },
                "authors": [
                    {
                        "name": "Junxue Zhang"
                    },
                    {
                        "name": "Han Tian"
                    },
                    {
                        "name": "Xinyang Huang"
                    },
                    {
                        "name": "Wenxue Li"
                    },
                    {
                        "name": "Kaiqiang Xu"
                    },
                    {
                        "name": "Dian Shen"
                    },
                    {
                        "name": "Yong Wang"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19021v1",
                "updated": "2025-01-31T10:43:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    10,
                    43,
                    0,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T10:43:00Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    10,
                    43,
                    0,
                    4,
                    31,
                    0
                ],
                "title": "The development of IBIC microscopy at the 100 kV ion implanter of the\n  University of Torino (LIUTo) and the application for the assessment of the\n  radiation hardness of a silicon photodiode",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of IBIC microscopy at the 100 kV ion implanter of the\n  University of Torino (LIUTo) and the application for the assessment of the\n  radiation hardness of a silicon photodiode"
                },
                "summary": "The Ion Beam Induced Charge (IBIC) technique is widely used to characterize\nthe electronic properties of semiconductor materials and devices. Its main\nadvantage over other charge collection microscopies stems in the use of MeV ion\nprobes, which provide both measurable induced charge signals from single ions,\nand high spatial resolution, which is maintained along the ion range. It is a\nfact, however, that the use of low-energy ions in the keV range can provide the\nIBIC technique with complementary analytical capabilities, that are not\navailable with MeV ions, for example the higher sensitivity to the status,\ncontamination and morphology of the surface and the fact that the induced\nsignal depends on the transport of only one type of charge carrier. This paper\noutlines the upgrade that was made at the 100 kV ion implanter of the\nUniversity of Torino, originally installed for material and surface\nmodification, to explore the rather unexplored keV-IBIC field and to assess its\npotential to characterize semiconductor devices. Finally, we report the first\nIBIC application of our apparatus, which regards the assessment of the\nradiation damage of a commercially available silicon photodiode, adopting the\nIAEA experimental protocol and the relevant interpretative model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Ion Beam Induced Charge (IBIC) technique is widely used to characterize\nthe electronic properties of semiconductor materials and devices. Its main\nadvantage over other charge collection microscopies stems in the use of MeV ion\nprobes, which provide both measurable induced charge signals from single ions,\nand high spatial resolution, which is maintained along the ion range. It is a\nfact, however, that the use of low-energy ions in the keV range can provide the\nIBIC technique with complementary analytical capabilities, that are not\navailable with MeV ions, for example the higher sensitivity to the status,\ncontamination and morphology of the surface and the fact that the induced\nsignal depends on the transport of only one type of charge carrier. This paper\noutlines the upgrade that was made at the 100 kV ion implanter of the\nUniversity of Torino, originally installed for material and surface\nmodification, to explore the rather unexplored keV-IBIC field and to assess its\npotential to characterize semiconductor devices. Finally, we report the first\nIBIC application of our apparatus, which regards the assessment of the\nradiation damage of a commercially available silicon photodiode, adopting the\nIAEA experimental protocol and the relevant interpretative model."
                },
                "authors": [
                    {
                        "name": "Emilio Corte"
                    },
                    {
                        "name": "Alberto Bortone"
                    },
                    {
                        "name": "Elena Nieto Hernndez"
                    },
                    {
                        "name": "Carlo Ceresa"
                    },
                    {
                        "name": "Georgios Provatas"
                    },
                    {
                        "name": "Karla Ivankovi Nizi"
                    },
                    {
                        "name": "Milko Jaksi"
                    },
                    {
                        "name": "Ettore Vittone"
                    },
                    {
                        "name": "Sviatoslav Ditalia Tchernij"
                    }
                ],
                "author_detail": {
                    "name": "Sviatoslav Ditalia Tchernij"
                },
                "author": "Sviatoslav Ditalia Tchernij",
                "arxiv_comment": "15 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18824v1",
                "updated": "2025-01-31T00:43:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    0,
                    43,
                    50,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T00:43:50Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    0,
                    43,
                    50,
                    4,
                    31,
                    0
                ],
                "title": "Memory-Efficient Fine-Tuning of Transformers via Token Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Efficient Fine-Tuning of Transformers via Token Selection"
                },
                "summary": "Fine-tuning provides an effective means to specialize pre-trained models for\nvarious downstream tasks. However, fine-tuning often incurs high memory\noverhead, especially for large transformer-based models, such as LLMs. While\nexisting methods may reduce certain parts of the memory required for\nfine-tuning, they still require caching all intermediate activations computed\nin the forward pass to update weights during the backward pass. In this work,\nwe develop TokenTune, a method to reduce memory usage, specifically the memory\nto store intermediate activations, in the fine-tuning of transformer-based\nmodels. During the backward pass, TokenTune approximates the gradient\ncomputation by backpropagating through just a subset of input tokens. Thus,\nwith TokenTune, only a subset of intermediate activations are cached during the\nforward pass. Also, TokenTune can be easily combined with existing methods like\nLoRA, further reducing the memory cost. We evaluate our approach on pre-trained\ntransformer models with up to billions of parameters, considering the\nperformance on multiple downstream tasks such as text classification and\nquestion answering in a few-shot learning setup. Overall, TokenTune achieves\nperformance on par with full fine-tuning or representative memory-efficient\nfine-tuning methods, while greatly reducing the memory footprint, especially\nwhen combined with other methods with complementary memory reduction\nmechanisms. We hope that our approach will facilitate the fine-tuning of large\ntransformers, in specializing them for specific domains or co-training them\nwith other neural components from a larger system. Our code is available at\nhttps://github.com/facebookresearch/tokentune.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning provides an effective means to specialize pre-trained models for\nvarious downstream tasks. However, fine-tuning often incurs high memory\noverhead, especially for large transformer-based models, such as LLMs. While\nexisting methods may reduce certain parts of the memory required for\nfine-tuning, they still require caching all intermediate activations computed\nin the forward pass to update weights during the backward pass. In this work,\nwe develop TokenTune, a method to reduce memory usage, specifically the memory\nto store intermediate activations, in the fine-tuning of transformer-based\nmodels. During the backward pass, TokenTune approximates the gradient\ncomputation by backpropagating through just a subset of input tokens. Thus,\nwith TokenTune, only a subset of intermediate activations are cached during the\nforward pass. Also, TokenTune can be easily combined with existing methods like\nLoRA, further reducing the memory cost. We evaluate our approach on pre-trained\ntransformer models with up to billions of parameters, considering the\nperformance on multiple downstream tasks such as text classification and\nquestion answering in a few-shot learning setup. Overall, TokenTune achieves\nperformance on par with full fine-tuning or representative memory-efficient\nfine-tuning methods, while greatly reducing the memory footprint, especially\nwhen combined with other methods with complementary memory reduction\nmechanisms. We hope that our approach will facilitate the fine-tuning of large\ntransformers, in specializing them for specific domains or co-training them\nwith other neural components from a larger system. Our code is available at\nhttps://github.com/facebookresearch/tokentune."
                },
                "authors": [
                    {
                        "name": "Antoine Simoulin"
                    },
                    {
                        "name": "Namyong Park"
                    },
                    {
                        "name": "Xiaoyi Liu"
                    },
                    {
                        "name": "Grey Yang"
                    }
                ],
                "author_detail": {
                    "name": "Grey Yang"
                },
                "author": "Grey Yang",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21625v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21625v3",
                "updated": "2025-01-30T18:23:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    23,
                    46,
                    3,
                    30,
                    0
                ],
                "published": "2024-07-31T14:17:49Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    17,
                    49,
                    2,
                    213,
                    0
                ],
                "title": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation"
                },
                "summary": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. Existing solutions designed for Ethernet, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilizations as datacenter topologies (and network\nfailures as a consequence) continue to grow. To address these limitations, we\npropose REPS, a lightweight decentralized per-packet adaptive load balancing\nalgorithm designed to optimize network utilization while ensuring rapid\nrecovery from link failures. REPS adapts to network conditions by caching\ngood-performing paths. In case of a network failure, REPS re-routes traffic\naway from it in less than 100 microseconds. REPS is designed to be deployed\nwith next-generation out-of-order transports, such as Ultra Ethernet, and\nintroduces less than 25 bytes of per-connection state. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. Existing solutions designed for Ethernet, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilizations as datacenter topologies (and network\nfailures as a consequence) continue to grow. To address these limitations, we\npropose REPS, a lightweight decentralized per-packet adaptive load balancing\nalgorithm designed to optimize network utilization while ensuring rapid\nrecovery from link failures. REPS adapts to network conditions by caching\ngood-performing paths. In case of a network failure, REPS re-routes traffic\naway from it in less than 100 microseconds. REPS is designed to be deployed\nwith next-generation out-of-order transports, such as Ultra Ethernet, and\nintroduces less than 25 bytes of per-connection state. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs."
                },
                "authors": [
                    {
                        "name": "Tommaso Bonato"
                    },
                    {
                        "name": "Abdul Kabbani"
                    },
                    {
                        "name": "Ahmad Ghalayini"
                    },
                    {
                        "name": "Michael Papamichael"
                    },
                    {
                        "name": "Mohammad Dohadwala"
                    },
                    {
                        "name": "Lukas Gianinazzi"
                    },
                    {
                        "name": "Mikhail Khalilov"
                    },
                    {
                        "name": "Elias Achermann"
                    },
                    {
                        "name": "Daniele De Sensi"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21625v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21625v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18356v1",
                "updated": "2025-01-30T14:03:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    14,
                    3,
                    36,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T14:03:36Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    14,
                    3,
                    36,
                    3,
                    30,
                    0
                ],
                "title": "State Stream Transformer (SST) : Emergent Metacognitive Behaviours\n  Through Latent State Persistence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State Stream Transformer (SST) : Emergent Metacognitive Behaviours\n  Through Latent State Persistence"
                },
                "summary": "We introduce the State Stream Transformer (SST), a novel LLM architecture\nthat reveals emergent reasoning behaviours and capabilities latent in\npretrained weights through addressing a fundamental limitation in traditional\ntransformer models: the lack of latent computational continuity across\nautoregressive generations in the state space. SST introduces a sliding window\nlatent state (FFN) cache with weighted decay that maintains and evolves\npersistent latent processes throughout autoregressive generations. Through\ncontrolled experiments comparing base and SST architectures using the same\nfrozen weights, we demonstrate that this architectural modification alone\nenables enhanced reasoning capabilities which appear best explained by some\nform of potential higher-order processing, as evidenced by emergent\nmetacognitive behaviours. These behaviours persist under controlled conditions\ndesigned to eliminate confounding factors such as stochastic variation or\nlearned response patterns. Analysis of latent state distributions and\nprocessing dynamics provides evidence that it is solely the 'state stream' that\nis responsible for these phenomena. In quantitative evaluations, the SST\nachieves substantial performance improvements over the base model on two\nreasoning benchmarks, reaching 89.01\\% accuracy on GSM-8K (0-shot) and 91.04\\%\non ARC Challenge (0-shot CoT). These findings indicate that persistent\ncomputation in the latent state space enables fundamentally different\ninformation processing and internal reasoning strategies, with implications for\nour understanding of artificial intelligence systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the State Stream Transformer (SST), a novel LLM architecture\nthat reveals emergent reasoning behaviours and capabilities latent in\npretrained weights through addressing a fundamental limitation in traditional\ntransformer models: the lack of latent computational continuity across\nautoregressive generations in the state space. SST introduces a sliding window\nlatent state (FFN) cache with weighted decay that maintains and evolves\npersistent latent processes throughout autoregressive generations. Through\ncontrolled experiments comparing base and SST architectures using the same\nfrozen weights, we demonstrate that this architectural modification alone\nenables enhanced reasoning capabilities which appear best explained by some\nform of potential higher-order processing, as evidenced by emergent\nmetacognitive behaviours. These behaviours persist under controlled conditions\ndesigned to eliminate confounding factors such as stochastic variation or\nlearned response patterns. Analysis of latent state distributions and\nprocessing dynamics provides evidence that it is solely the 'state stream' that\nis responsible for these phenomena. In quantitative evaluations, the SST\nachieves substantial performance improvements over the base model on two\nreasoning benchmarks, reaching 89.01\\% accuracy on GSM-8K (0-shot) and 91.04\\%\non ARC Challenge (0-shot CoT). These findings indicate that persistent\ncomputation in the latent state space enables fundamentally different\ninformation processing and internal reasoning strategies, with implications for\nour understanding of artificial intelligence systems."
                },
                "authors": [
                    {
                        "name": "Thea Aviss"
                    }
                ],
                "author_detail": {
                    "name": "Thea Aviss"
                },
                "author": "Thea Aviss",
                "arxiv_comment": "25 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01805v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01805v2",
                "updated": "2025-01-30T13:07:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    13,
                    7,
                    37,
                    3,
                    30,
                    0
                ],
                "published": "2024-10-02T17:59:52Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    59,
                    52,
                    2,
                    276,
                    0
                ],
                "title": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads on Consumer-Grade Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads on Consumer-Grade Devices"
                },
                "summary": "Scaling the input context length of a large language model (LLM) incurs a\nsignificant increase in computation cost and memory footprint to maintain the\nattention key-value (KV) cache. Existing KV cache compression methods suffer\nfrom inefficient compression strategies and limited memory reduction effects,\nmaking it difficult for LLMs to conduct long-context inference on\nconsumer-grade devices, especially when inferring long-context stream input.\nSuch obstacles prevent consumer-grade devices from supporting more complex\napplications, creating challenges for the democratization of LLMs. To overcome\nthis, we propose Locret, the first framework to create an eviction policy\ncompatible with chunked prefill. By evaluating the causal importance of KV\ncache units by learnable retaining heads, Locret enables precise eviction of\ncache units, facilitating efficient long-context inference. In our extensive\nempirical studies, Locret outperforms the recent popular and competitive\napproaches in terms of memory efficiency and generation quality -- Locret\nachieves up to 20x of KV cache compression ratio within less than 10%\nperformance loss. Furthermore, Locret achieves 128K+ long-context inference on\na single NVIDIA 4090 GPU without compromising generation quality and only costs\n<1 GPU hour of additional training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling the input context length of a large language model (LLM) incurs a\nsignificant increase in computation cost and memory footprint to maintain the\nattention key-value (KV) cache. Existing KV cache compression methods suffer\nfrom inefficient compression strategies and limited memory reduction effects,\nmaking it difficult for LLMs to conduct long-context inference on\nconsumer-grade devices, especially when inferring long-context stream input.\nSuch obstacles prevent consumer-grade devices from supporting more complex\napplications, creating challenges for the democratization of LLMs. To overcome\nthis, we propose Locret, the first framework to create an eviction policy\ncompatible with chunked prefill. By evaluating the causal importance of KV\ncache units by learnable retaining heads, Locret enables precise eviction of\ncache units, facilitating efficient long-context inference. In our extensive\nempirical studies, Locret outperforms the recent popular and competitive\napproaches in terms of memory efficiency and generation quality -- Locret\nachieves up to 20x of KV cache compression ratio within less than 10%\nperformance loss. Furthermore, Locret achieves 128K+ long-context inference on\na single NVIDIA 4090 GPU without compromising generation quality and only costs\n<1 GPU hour of additional training."
                },
                "authors": [
                    {
                        "name": "Yuxiang Huang"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyuan Liu"
                },
                "author": "Zhiyuan Liu",
                "arxiv_comment": "Preprints",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01805v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01805v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.05172v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.05172v2",
                "updated": "2025-01-30T06:02:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    6,
                    2,
                    11,
                    3,
                    30,
                    0
                ],
                "published": "2023-10-08T14:06:06Z",
                "published_parsed": [
                    2023,
                    10,
                    8,
                    14,
                    6,
                    6,
                    6,
                    281,
                    0
                ],
                "title": "Systematic Evaluation of Randomized Cache Designs against Cache\n  Occupancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic Evaluation of Randomized Cache Designs against Cache\n  Occupancy"
                },
                "summary": "Randomizing the address-to-set mapping and partitioning of the cache has been\nshown to be an effective mechanism in designing secured caches. Several designs\nhave been proposed on a variety of rationales: (1) randomized design, (2)\nrandomized-and-partitioned design, and (3) psuedo-fully associative design.\nThis work fills in a crucial gap in current literature on randomized caches:\ncurrently most randomized cache designs defend only contention-based attacks,\nand leave out considerations of cache occupancy. We perform a systematic\nevaluation of 5 randomized cache designs- CEASER, CEASER-S, MIRAGE,\nScatter-Cache, and Sass-cache against cache occupancy wrt. both performance as\nwell as security.\n  With respect to performance, we first establish that benchmarking strategies\nused by contemporary designs are unsuitable for a fair evaluation (because of\ndiffering cache configurations, choice of benchmarking suites, additional\nimplementation-specific assumptions). We thus propose a uniform benchmarking\nstrategy, which allows us to perform a fair and comparative analysis across all\ndesigns under various replacement policies. Likewise, with respect to security\nagainst cache occupancy attacks, we evaluate the cache designs against various\nthreat assumptions: (1) covert channels, (2) process fingerprinting, and (3)\nAES key recovery (to the best of our knowledge, this work is the first to\ndemonstrate full AES key recovery on a randomized cache design using cache\noccupancy attack). Our results establish the need to also consider cache\noccupancy side-channel in randomized cache design considerations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Randomizing the address-to-set mapping and partitioning of the cache has been\nshown to be an effective mechanism in designing secured caches. Several designs\nhave been proposed on a variety of rationales: (1) randomized design, (2)\nrandomized-and-partitioned design, and (3) psuedo-fully associative design.\nThis work fills in a crucial gap in current literature on randomized caches:\ncurrently most randomized cache designs defend only contention-based attacks,\nand leave out considerations of cache occupancy. We perform a systematic\nevaluation of 5 randomized cache designs- CEASER, CEASER-S, MIRAGE,\nScatter-Cache, and Sass-cache against cache occupancy wrt. both performance as\nwell as security.\n  With respect to performance, we first establish that benchmarking strategies\nused by contemporary designs are unsuitable for a fair evaluation (because of\ndiffering cache configurations, choice of benchmarking suites, additional\nimplementation-specific assumptions). We thus propose a uniform benchmarking\nstrategy, which allows us to perform a fair and comparative analysis across all\ndesigns under various replacement policies. Likewise, with respect to security\nagainst cache occupancy attacks, we evaluate the cache designs against various\nthreat assumptions: (1) covert channels, (2) process fingerprinting, and (3)\nAES key recovery (to the best of our knowledge, this work is the first to\ndemonstrate full AES key recovery on a randomized cache design using cache\noccupancy attack). Our results establish the need to also consider cache\noccupancy side-channel in randomized cache design considerations."
                },
                "authors": [
                    {
                        "name": "Anirban Chakraborty"
                    },
                    {
                        "name": "Nimish Mishra"
                    },
                    {
                        "name": "Sayandeep Saha"
                    },
                    {
                        "name": "Sarani Bhattacharya"
                    },
                    {
                        "name": "Debdeep Mukhopadhyay"
                    }
                ],
                "author_detail": {
                    "name": "Debdeep Mukhopadhyay"
                },
                "author": "Debdeep Mukhopadhyay",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.05172v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.05172v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v5",
                "updated": "2025-01-29T16:44:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    16,
                    44,
                    27,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04437v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04437v3",
                "updated": "2025-01-29T04:10:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    4,
                    10,
                    41,
                    2,
                    29,
                    0
                ],
                "published": "2024-05-07T16:00:32Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    16,
                    0,
                    32,
                    1,
                    128,
                    0
                ],
                "title": "vAttention: Dynamic Memory Management for Serving LLMs without\n  PagedAttention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vAttention: Dynamic Memory Management for Serving LLMs without\n  PagedAttention"
                },
                "summary": "PagedAttention is a popular approach for dynamic memory allocation in LLM\nserving systems. It enables on-demand allocation of GPU memory to mitigate KV\ncache fragmentation -- a phenomenon that crippled the batch size (and\nconsequently throughput) in prior systems. However, in trying to allocate\nphysical memory at runtime, PagedAttention ends up changing the virtual memory\nlayout of the KV cache from contiguous to non-contiguous. Such a design leads\nto non-trivial programming and performance overheads.\n  We present vAttention -- an approach that mitigates fragmentation in physical\nmemory while retaining the contiguity of KV cache in virtual memory. We achieve\nthis by decoupling the allocation of virtual and physical memory using CUDA\nvirtual memory management APIs. We also introduce various LLM-specific\noptimizations to address the limitations of CUDA virtual memory support.\nOverall, vAttention is a simpler, portable, and performant alternative to\nPagedAttention: it supports various attention kernels out-of-the-box and\nimproves LLM serving throughput by up to 1.23x compared to the use of\nPagedAttention-based kernels of FlashAttention and FlashInfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PagedAttention is a popular approach for dynamic memory allocation in LLM\nserving systems. It enables on-demand allocation of GPU memory to mitigate KV\ncache fragmentation -- a phenomenon that crippled the batch size (and\nconsequently throughput) in prior systems. However, in trying to allocate\nphysical memory at runtime, PagedAttention ends up changing the virtual memory\nlayout of the KV cache from contiguous to non-contiguous. Such a design leads\nto non-trivial programming and performance overheads.\n  We present vAttention -- an approach that mitigates fragmentation in physical\nmemory while retaining the contiguity of KV cache in virtual memory. We achieve\nthis by decoupling the allocation of virtual and physical memory using CUDA\nvirtual memory management APIs. We also introduce various LLM-specific\noptimizations to address the limitations of CUDA virtual memory support.\nOverall, vAttention is a simpler, portable, and performant alternative to\nPagedAttention: it supports various attention kernels out-of-the-box and\nimproves LLM serving throughput by up to 1.23x compared to the use of\nPagedAttention-based kernels of FlashAttention and FlashInfer."
                },
                "authors": [
                    {
                        "name": "Ramya Prabhu"
                    },
                    {
                        "name": "Ajay Nayak"
                    },
                    {
                        "name": "Jayashree Mohan"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Ashish Panwar"
                    }
                ],
                "author_detail": {
                    "name": "Ashish Panwar"
                },
                "author": "Ashish Panwar",
                "arxiv_comment": "To appear in ASPLOS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04437v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04437v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14770v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14770v2",
                "updated": "2025-01-28T20:35:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    20,
                    35,
                    23,
                    1,
                    28,
                    0
                ],
                "published": "2024-12-29T17:37:18Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    37,
                    18,
                    6,
                    364,
                    0
                ],
                "title": "Optimizing SSD Caches for Cloud Block Storage Systems Using Machine\n  Learning Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing SSD Caches for Cloud Block Storage Systems Using Machine\n  Learning Approaches"
                },
                "summary": "The growing demand for efficient cloud storage solutions has led to the\nwidespread adoption of Solid-State Drives (SSDs) for caching in cloud block\nstorage systems. The management of data writes to SSD caches plays a crucial\nrole in improving overall system performance, reducing latency, and extending\nthe lifespan of storage devices. A critical challenge arises from the large\nvolume of write-only data, which significantly impacts the performance of SSD\ncaches when handled inefficiently. Specifically, writes that have not been read\nfor a certain period may introduce unnecessary write traffic to the SSD cache\nwithout offering substantial benefits for cache performance. This paper\nproposes a novel approach to mitigate this issue by leveraging machine learning\ntechniques to dynamically optimize the write policy in cloud-based storage\nsystems. The proposed method identifies write-only data and selectively filters\nit out in real-time, thereby minimizing the number of unnecessary write\noperations and improving the overall performance of the cache system.\nExperimental results demonstrate that the proposed machine learning-based\npolicy significantly outperforms traditional approaches by reducing the number\nof harmful writes and optimizing cache utilization. This solution is\nparticularly suitable for cloud environments with varying and unpredictable\nworkloads, where traditional cache management strategies often fall short.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing demand for efficient cloud storage solutions has led to the\nwidespread adoption of Solid-State Drives (SSDs) for caching in cloud block\nstorage systems. The management of data writes to SSD caches plays a crucial\nrole in improving overall system performance, reducing latency, and extending\nthe lifespan of storage devices. A critical challenge arises from the large\nvolume of write-only data, which significantly impacts the performance of SSD\ncaches when handled inefficiently. Specifically, writes that have not been read\nfor a certain period may introduce unnecessary write traffic to the SSD cache\nwithout offering substantial benefits for cache performance. This paper\nproposes a novel approach to mitigate this issue by leveraging machine learning\ntechniques to dynamically optimize the write policy in cloud-based storage\nsystems. The proposed method identifies write-only data and selectively filters\nit out in real-time, thereby minimizing the number of unnecessary write\noperations and improving the overall performance of the cache system.\nExperimental results demonstrate that the proposed machine learning-based\npolicy significantly outperforms traditional approaches by reducing the number\nof harmful writes and optimizing cache utilization. This solution is\nparticularly suitable for cloud environments with varying and unpredictable\nworkloads, where traditional cache management strategies often fall short."
                },
                "authors": [
                    {
                        "name": "Chiyu Cheng"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Jin Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jin Cao"
                },
                "author": "Jin Cao",
                "arxiv_comment": "I uploaded the paper without obtaining consent from all the authors.\n  One of the authors now refuses to publish this paper, as it has been\n  demonstrated to be unreliable, contains significant flaws in prior research,\n  and is missing citations in Sections 2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14770v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14770v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14771v2",
                "updated": "2025-01-28T20:33:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    20,
                    33,
                    43,
                    1,
                    28,
                    0
                ],
                "published": "2024-12-29T17:39:37Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    39,
                    37,
                    6,
                    364,
                    0
                ],
                "title": "Dynamic Adaptation in Data Storage: Real-Time Machine Learning for\n  Enhanced Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Adaptation in Data Storage: Real-Time Machine Learning for\n  Enhanced Prefetching"
                },
                "summary": "The exponential growth of data storage demands has necessitated the evolution\nof hierarchical storage management strategies [1]. This study explores the\napplication of streaming machine learning [3] to revolutionize data prefetching\nwithin multi-tiered storage systems. Unlike traditional batch-trained models,\nstreaming machine learning [5] offers adaptability, real-time insights, and\ncomputational efficiency, responding dynamically to workload variations. This\nwork designs and validates an innovative framework that integrates streaming\nclassification models for predicting file access patterns, specifically the\nnext file offset. Leveraging comprehensive feature engineering and real-time\nevaluation over extensive production traces, the proposed methodology achieves\nsubstantial improvements in prediction accuracy, memory efficiency, and system\nadaptability. The results underscore the potential of streaming models in\nreal-time storage management, setting a precedent for advanced caching and\ntiering strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of data storage demands has necessitated the evolution\nof hierarchical storage management strategies [1]. This study explores the\napplication of streaming machine learning [3] to revolutionize data prefetching\nwithin multi-tiered storage systems. Unlike traditional batch-trained models,\nstreaming machine learning [5] offers adaptability, real-time insights, and\ncomputational efficiency, responding dynamically to workload variations. This\nwork designs and validates an innovative framework that integrates streaming\nclassification models for predicting file access patterns, specifically the\nnext file offset. Leveraging comprehensive feature engineering and real-time\nevaluation over extensive production traces, the proposed methodology achieves\nsubstantial improvements in prediction accuracy, memory efficiency, and system\nadaptability. The results underscore the potential of streaming models in\nreal-time storage management, setting a precedent for advanced caching and\ntiering strategies."
                },
                "authors": [
                    {
                        "name": "Chiyu Cheng"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Jin Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jin Cao"
                },
                "author": "Jin Cao",
                "arxiv_comment": "I uploaded the paper without obtaining consent from all the authors.\n  One of the authors now refuses to publish this paper, as it has been\n  demonstrated to be unreliable, contains significant flaws in prior research,\n  and is missing proper citations in Sections 2 and 3",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17123v1",
                "updated": "2025-01-28T18:14:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    14,
                    43,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T18:14:43Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    14,
                    43,
                    1,
                    28,
                    0
                ],
                "title": "Hybrid Deep Learning Model for Multiple Cache Side Channel Attacks\n  Detection: A Comparative Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Deep Learning Model for Multiple Cache Side Channel Attacks\n  Detection: A Comparative Analysis"
                },
                "summary": "Cache side channel attacks are a sophisticated and persistent threat that\nexploit vulnerabilities in modern processors to extract sensitive information.\nThese attacks leverage weaknesses in shared computational resources,\nparticularly the last level cache, to infer patterns in data access and\nexecution flows, often bypassing traditional security defenses. Such attacks\nare especially dangerous as they can be executed remotely without requiring\nphysical access to the victim's device. This study focuses on a specific class\nof these threats: fingerprinting attacks, where an adversary monitors and\nanalyzes the behavior of co-located processes via cache side channels. This can\npotentially reveal confidential information, such as encryption keys or user\nactivity patterns. A comprehensive threat model illustrates how attackers\nsharing computational resources with target systems exploit these side channels\nto compromise sensitive data. To mitigate such risks, a hybrid deep learning\nmodel is proposed for detecting cache side channel attacks. Its performance is\ncompared with five widely used deep learning models: Multi-Layer Perceptron,\nConvolutional Neural Network, Simple Recurrent Neural Network, Long Short-Term\nMemory, and Gated Recurrent Unit. The experimental results demonstrate that the\nhybrid model achieves a detection rate of up to 99.96%. These findings\nhighlight the limitations of existing models, the need for enhanced defensive\nmechanisms, and directions for future research to secure sensitive data against\nevolving side channel threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache side channel attacks are a sophisticated and persistent threat that\nexploit vulnerabilities in modern processors to extract sensitive information.\nThese attacks leverage weaknesses in shared computational resources,\nparticularly the last level cache, to infer patterns in data access and\nexecution flows, often bypassing traditional security defenses. Such attacks\nare especially dangerous as they can be executed remotely without requiring\nphysical access to the victim's device. This study focuses on a specific class\nof these threats: fingerprinting attacks, where an adversary monitors and\nanalyzes the behavior of co-located processes via cache side channels. This can\npotentially reveal confidential information, such as encryption keys or user\nactivity patterns. A comprehensive threat model illustrates how attackers\nsharing computational resources with target systems exploit these side channels\nto compromise sensitive data. To mitigate such risks, a hybrid deep learning\nmodel is proposed for detecting cache side channel attacks. Its performance is\ncompared with five widely used deep learning models: Multi-Layer Perceptron,\nConvolutional Neural Network, Simple Recurrent Neural Network, Long Short-Term\nMemory, and Gated Recurrent Unit. The experimental results demonstrate that the\nhybrid model achieves a detection rate of up to 99.96%. These findings\nhighlight the limitations of existing models, the need for enhanced defensive\nmechanisms, and directions for future research to secure sensitive data against\nevolving side channel threats."
                },
                "authors": [
                    {
                        "name": "Tejal Joshi"
                    },
                    {
                        "name": "Aarya Kawalay"
                    },
                    {
                        "name": "Anvi Jamkhande"
                    },
                    {
                        "name": "Amit Joshi"
                    }
                ],
                "author_detail": {
                    "name": "Amit Joshi"
                },
                "author": "Amit Joshi",
                "arxiv_comment": "8 pages, 4 figures. Accepted in IEEE's 2nd International Conference\n  on Computational Intelligence and Network Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10854v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10854v2",
                "updated": "2025-01-28T16:19:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    16,
                    19,
                    24,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-18T19:10:23Z",
                "published_parsed": [
                    2025,
                    1,
                    18,
                    19,
                    10,
                    23,
                    5,
                    18,
                    0
                ],
                "title": "Achievable DoF Bounds for Cache-Aided Asymmetric MIMO Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achievable DoF Bounds for Cache-Aided Asymmetric MIMO Communications"
                },
                "summary": "Integrating coded caching (CC) into multiple-input multiple-output (MIMO)\ncommunications can significantly enhance the achievable degrees of freedom\n(DoF) in wireless networks. This paper investigates a practical cache-aided\nasymmetric MIMO configuration with cache ratio $\\gamma$, where a server\nequipped with $L$ transmit antennas communicates with $K$ users, each having\n$G_k$ receive antennas. We propose three content-aware MIMO-CC strategies: the\n\\emph{min-G} scheme, which treats the system as symmetric by assuming all users\nhave the same number of antennas, equal to the smallest among them; the\n\\emph{Grouping} scheme, which maximizes spatial multiplexing gain separately\nwithin each user subset at the cost of some global caching gain; and the\n\\emph{Phantom} scheme, which dynamically redistributes spatial resources using\nvirtual or ``phantom'' antennas at the users, bridging the performance gains of\nthe min-$G$ and Grouping schemes. These strategies jointly optimize the number\nof users, $\\Omega$, and the parallel streams decoded by each user, $\\beta_k$,\nensuring linear decodability for all target users. Analytical and numerical\nresults confirm that the proposed schemes achieve significant DoF improvements\nacross various system configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating coded caching (CC) into multiple-input multiple-output (MIMO)\ncommunications can significantly enhance the achievable degrees of freedom\n(DoF) in wireless networks. This paper investigates a practical cache-aided\nasymmetric MIMO configuration with cache ratio $\\gamma$, where a server\nequipped with $L$ transmit antennas communicates with $K$ users, each having\n$G_k$ receive antennas. We propose three content-aware MIMO-CC strategies: the\n\\emph{min-G} scheme, which treats the system as symmetric by assuming all users\nhave the same number of antennas, equal to the smallest among them; the\n\\emph{Grouping} scheme, which maximizes spatial multiplexing gain separately\nwithin each user subset at the cost of some global caching gain; and the\n\\emph{Phantom} scheme, which dynamically redistributes spatial resources using\nvirtual or ``phantom'' antennas at the users, bridging the performance gains of\nthe min-$G$ and Grouping schemes. These strategies jointly optimize the number\nof users, $\\Omega$, and the parallel streams decoded by each user, $\\beta_k$,\nensuring linear decodability for all target users. Analytical and numerical\nresults confirm that the proposed schemes achieve significant DoF improvements\nacross various system configurations."
                },
                "authors": [
                    {
                        "name": "Mohammad NaseriTehrani"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tlli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tlli"
                },
                "author": "Antti Tlli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10854v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10854v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16909v1",
                "updated": "2025-01-28T12:57:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    57,
                    53,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T12:57:53Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    57,
                    53,
                    1,
                    28,
                    0
                ],
                "title": "Measuring GPU utilization one level deeper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring GPU utilization one level deeper"
                },
                "summary": "GPU hardware is vastly underutilized. Even resource-intensive AI applications\nhave diverse resource profiles that often leave parts of GPUs idle. While\ncolocating applications can improve utilization, current spatial sharing\nsystems lack performance guarantees. Providing predictable performance\nguarantees requires a deep understanding of how applications contend for shared\nGPU resources such as block schedulers, compute units, L1/L2 caches, and memory\nbandwidth. We propose a methodology to profile resource interference of GPU\nkernels across these dimensions and discuss how to build GPU schedulers that\nprovide strict performance guarantees while colocating applications to minimize\ncost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU hardware is vastly underutilized. Even resource-intensive AI applications\nhave diverse resource profiles that often leave parts of GPUs idle. While\ncolocating applications can improve utilization, current spatial sharing\nsystems lack performance guarantees. Providing predictable performance\nguarantees requires a deep understanding of how applications contend for shared\nGPU resources such as block schedulers, compute units, L1/L2 caches, and memory\nbandwidth. We propose a methodology to profile resource interference of GPU\nkernels across these dimensions and discuss how to build GPU schedulers that\nprovide strict performance guarantees while colocating applications to minimize\ncost."
                },
                "authors": [
                    {
                        "name": "Paul Elvinger"
                    },
                    {
                        "name": "Foteini Strati"
                    },
                    {
                        "name": "Natalie Enright Jerger"
                    },
                    {
                        "name": "Ana Klimovic"
                    }
                ],
                "author_detail": {
                    "name": "Ana Klimovic"
                },
                "author": "Ana Klimovic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16597v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16597v1",
                "updated": "2025-01-28T00:22:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    0,
                    22,
                    34,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T00:22:34Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    0,
                    22,
                    34,
                    1,
                    28,
                    0
                ],
                "title": "Optimizing Smart Helper Placement for Enhanced Cache Efficiency in\n  F-RANs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Smart Helper Placement for Enhanced Cache Efficiency in\n  F-RANs"
                },
                "summary": "Smart helpers (SHs) have been proposed to improve content delivery delays and\nalleviate high fronthaul loads in fog radio access networks (F-RANs). They\noffer an alternative to deploying additional enhanced remote radio heads\n(RRHs), which are often infeasible due to site constraints.} The optimal\nplacement of SHs can significantly increase the number of users they serve\nwhich leads to enhanced cache efficiency and improved content delivery delay.\nIn this letter, we optimize SH placement within an F-RAN to maximize the cache\nhit rate and further reduce the content delivery latency. We model the SH cache\nhit rate as a function of outage probability and user density distribution. We\ndevelop a function to estimate user density distribution leveraging the radial\nbasis functions (RBFs) method and optimize SH placement utilizing the particle\nswarm optimization (PSO) algorithm. \\an{Our} numerical results confirm the\neffectiveness of the proposed approach in maximizing the \\an{SH cache hit\nrate}, thereby improving delivery delays and fronthaul loads of the network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart helpers (SHs) have been proposed to improve content delivery delays and\nalleviate high fronthaul loads in fog radio access networks (F-RANs). They\noffer an alternative to deploying additional enhanced remote radio heads\n(RRHs), which are often infeasible due to site constraints.} The optimal\nplacement of SHs can significantly increase the number of users they serve\nwhich leads to enhanced cache efficiency and improved content delivery delay.\nIn this letter, we optimize SH placement within an F-RAN to maximize the cache\nhit rate and further reduce the content delivery latency. We model the SH cache\nhit rate as a function of outage probability and user density distribution. We\ndevelop a function to estimate user density distribution leveraging the radial\nbasis functions (RBFs) method and optimize SH placement utilizing the particle\nswarm optimization (PSO) algorithm. \\an{Our} numerical results confirm the\neffectiveness of the proposed approach in maximizing the \\an{SH cache hit\nrate}, thereby improving delivery delays and fronthaul loads of the network."
                },
                "authors": [
                    {
                        "name": "Hesameddin Mokhtarzadeh"
                    },
                    {
                        "name": "Mohammed Saif"
                    },
                    {
                        "name": "Md. Jahangir Hossain"
                    },
                    {
                        "name": "Julian Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Julian Cheng"
                },
                "author": "Julian Cheng",
                "arxiv_comment": "5 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16597v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16597v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16535v1",
                "updated": "2025-01-27T22:14:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    22,
                    14,
                    43,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T22:14:43Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    22,
                    14,
                    43,
                    0,
                    27,
                    0
                ],
                "title": "Latency Guarantees for Caching with Delayed Hits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency Guarantees for Caching with Delayed Hits"
                },
                "summary": "In the classical caching problem, when a requested page is not present in the\ncache (i.e., a \"miss\"), it is assumed to travel from the backing store into the\ncache \"before\" the next request arrives. However, in many real-life\napplications, such as content delivery networks, this assumption is\nunrealistic.\n  The \"delayed-hits\" model for caching, introduced by Atre, Sherry, Wang, and\nBerger, accounts for the latency between a missed cache request and the\ncorresponding arrival from the backing store. This theoretical model has two\nparameters: the \"delay\" $Z$, representing the ratio between the retrieval delay\nand the inter-request delay in an application, and the \"cache size\" $k$, as in\nclassical caching. Classical caching corresponds to $Z=1$, whereas larger\nvalues of $Z$ model applications where retrieving missed requests is expensive.\nDespite the practical relevance of the delayed-hits model, its theoretical\nunderpinnings are still poorly understood.\n  We present the first tight theoretical guarantee for optimizing delayed-hits\ncaching: The \"Least Recently Used\" algorithm, a natural, deterministic, online\nalgorithm widely used in practice, is $O(Zk)$-competitive, meaning it incurs at\nmost $O(Zk)$ times more latency than the (offline) optimal schedule. Our result\nextends to any so-called \"marking\" algorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the classical caching problem, when a requested page is not present in the\ncache (i.e., a \"miss\"), it is assumed to travel from the backing store into the\ncache \"before\" the next request arrives. However, in many real-life\napplications, such as content delivery networks, this assumption is\nunrealistic.\n  The \"delayed-hits\" model for caching, introduced by Atre, Sherry, Wang, and\nBerger, accounts for the latency between a missed cache request and the\ncorresponding arrival from the backing store. This theoretical model has two\nparameters: the \"delay\" $Z$, representing the ratio between the retrieval delay\nand the inter-request delay in an application, and the \"cache size\" $k$, as in\nclassical caching. Classical caching corresponds to $Z=1$, whereas larger\nvalues of $Z$ model applications where retrieving missed requests is expensive.\nDespite the practical relevance of the delayed-hits model, its theoretical\nunderpinnings are still poorly understood.\n  We present the first tight theoretical guarantee for optimizing delayed-hits\ncaching: The \"Least Recently Used\" algorithm, a natural, deterministic, online\nalgorithm widely used in practice, is $O(Zk)$-competitive, meaning it incurs at\nmost $O(Zk)$ times more latency than the (offline) optimal schedule. Our result\nextends to any so-called \"marking\" algorithm."
                },
                "authors": [
                    {
                        "name": "Keerthana Gurushankar"
                    },
                    {
                        "name": "Noah G. Singer"
                    },
                    {
                        "name": "Bernardo Subercaseaux"
                    }
                ],
                "author_detail": {
                    "name": "Bernardo Subercaseaux"
                },
                "author": "Bernardo Subercaseaux",
                "arxiv_comment": "Accepted at INFOCOM2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16245v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16245v1",
                "updated": "2025-01-27T17:42:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    42,
                    20,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T17:42:20Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    42,
                    20,
                    0,
                    27,
                    0
                ],
                "title": "SP-IMPact: A Framework for Static Partitioning Interference Mitigation\n  and Performance Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SP-IMPact: A Framework for Static Partitioning Interference Mitigation\n  and Performance Analysis"
                },
                "summary": "Modern embedded systems are evolving toward complex, heterogeneous\narchitectures to accommodate increasingly demanding applications. Driven by\nSWAP-C constraints, this shift has led to consolidating multiple systems onto\nsingle hardware platforms. Static Partitioning Hypervisors offer a promising\nsolution to partition hardware resources and provide spatial isolation between\ncritical workloads. However, shared resources like the Last-Level Cache and\nsystem bus can introduce temporal interference between virtual machines (VMs),\nnegatively impacting performance and predictability. Over the past decade,\nacademia and industry have developed interference mitigation techniques, such\nas cache partitioning and memory bandwidth reservation. However, configuring\nthese techniques is complex and time-consuming. Cache partitioning requires\nbalancing cache sections across VMs, while memory bandwidth reservation needs\ntuning bandwidth budgets and periods. Testing all configurations is impractical\nand often leads to suboptimal results. Moreover, understanding how these\ntechniques interact is limited, as their combined use can produce compounded or\nconflicting effects on performance. Static analysis tools estimating worst-case\nexecution times offer guidance for configuring mitigation techniques but often\nfail to capture the complexity of modern multi-core systems. They typically\nfocus on limited shared resources while neglecting others, such as IOMMUs and\ninterrupt controllers. To address these challenges, we present SP-IMPact, an\nopen-source framework for analyzing and guiding interference mitigation\nconfigurations. SP-IMPact supports (i) cache coloring and (ii) memory bandwidth\nreservation, while evaluating their interactions and cumulative impact. By\nproviding insights on real hardware, SP-IMPact helps optimize configurations\nfor mixed-criticality systems, ensuring performance and predictability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern embedded systems are evolving toward complex, heterogeneous\narchitectures to accommodate increasingly demanding applications. Driven by\nSWAP-C constraints, this shift has led to consolidating multiple systems onto\nsingle hardware platforms. Static Partitioning Hypervisors offer a promising\nsolution to partition hardware resources and provide spatial isolation between\ncritical workloads. However, shared resources like the Last-Level Cache and\nsystem bus can introduce temporal interference between virtual machines (VMs),\nnegatively impacting performance and predictability. Over the past decade,\nacademia and industry have developed interference mitigation techniques, such\nas cache partitioning and memory bandwidth reservation. However, configuring\nthese techniques is complex and time-consuming. Cache partitioning requires\nbalancing cache sections across VMs, while memory bandwidth reservation needs\ntuning bandwidth budgets and periods. Testing all configurations is impractical\nand often leads to suboptimal results. Moreover, understanding how these\ntechniques interact is limited, as their combined use can produce compounded or\nconflicting effects on performance. Static analysis tools estimating worst-case\nexecution times offer guidance for configuring mitigation techniques but often\nfail to capture the complexity of modern multi-core systems. They typically\nfocus on limited shared resources while neglecting others, such as IOMMUs and\ninterrupt controllers. To address these challenges, we present SP-IMPact, an\nopen-source framework for analyzing and guiding interference mitigation\nconfigurations. SP-IMPact supports (i) cache coloring and (ii) memory bandwidth\nreservation, while evaluating their interactions and cumulative impact. By\nproviding insights on real hardware, SP-IMPact helps optimize configurations\nfor mixed-criticality systems, ensuring performance and predictability."
                },
                "authors": [
                    {
                        "name": "Diogo Costa"
                    },
                    {
                        "name": "Gonalo Moreira"
                    },
                    {
                        "name": "Afonso Oliveira"
                    },
                    {
                        "name": "Jos Martins"
                    },
                    {
                        "name": "Sandro Pinto"
                    }
                ],
                "author_detail": {
                    "name": "Sandro Pinto"
                },
                "author": "Sandro Pinto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16245v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16245v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00080v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00080v4",
                "updated": "2025-01-27T14:55:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    55,
                    40,
                    0,
                    27,
                    0
                ],
                "published": "2024-04-30T16:35:08Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    16,
                    35,
                    8,
                    1,
                    121,
                    0
                ],
                "title": "Recommenadation aided Caching using Combinatorial Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommenadation aided Caching using Combinatorial Multi-armed Bandits"
                },
                "summary": "We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms."
                },
                "authors": [
                    {
                        "name": "Pavamana K J"
                    },
                    {
                        "name": "Chandramani Kishore Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Kishore Singh"
                },
                "author": "Chandramani Kishore Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00080v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00080v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.07785v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07785v1",
                "updated": "2025-02-11T18:59:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    59,
                    59,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T18:59:59Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    59,
                    59,
                    1,
                    42,
                    0
                ],
                "title": "Pippo: High-Resolution Multi-View Humans from a Single Image",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pippo: High-Resolution Multi-View Humans from a Single Image"
                },
                "summary": "We present Pippo, a generative model capable of producing 1K resolution dense\nturnaround videos of a person from a single casually clicked photo. Pippo is a\nmulti-view diffusion transformer and does not require any additional inputs -\ne.g., a fitted parametric model or camera parameters of the input image. We\npre-train Pippo on 3B human images without captions, and conduct multi-view\nmid-training and post-training on studio captured humans. During mid-training,\nto quickly absorb the studio dataset, we denoise several (up to 48) views at\nlow-resolution, and encode target cameras coarsely using a shallow MLP. During\npost-training, we denoise fewer views at high-resolution and use pixel-aligned\ncontrols (e.g., Spatial anchor and Plucker rays) to enable 3D consistent\ngenerations. At inference, we propose an attention biasing technique that\nallows Pippo to simultaneously generate greater than 5 times as many views as\nseen during training. Finally, we also introduce an improved metric to evaluate\n3D consistency of multi-view generations, and show that Pippo outperforms\nexisting works on multi-view human generation from a single image.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Pippo, a generative model capable of producing 1K resolution dense\nturnaround videos of a person from a single casually clicked photo. Pippo is a\nmulti-view diffusion transformer and does not require any additional inputs -\ne.g., a fitted parametric model or camera parameters of the input image. We\npre-train Pippo on 3B human images without captions, and conduct multi-view\nmid-training and post-training on studio captured humans. During mid-training,\nto quickly absorb the studio dataset, we denoise several (up to 48) views at\nlow-resolution, and encode target cameras coarsely using a shallow MLP. During\npost-training, we denoise fewer views at high-resolution and use pixel-aligned\ncontrols (e.g., Spatial anchor and Plucker rays) to enable 3D consistent\ngenerations. At inference, we propose an attention biasing technique that\nallows Pippo to simultaneously generate greater than 5 times as many views as\nseen during training. Finally, we also introduce an improved metric to evaluate\n3D consistency of multi-view generations, and show that Pippo outperforms\nexisting works on multi-view human generation from a single image."
                },
                "authors": [
                    {
                        "name": "Yash Kant"
                    },
                    {
                        "name": "Ethan Weber"
                    },
                    {
                        "name": "Jin Kyu Kim"
                    },
                    {
                        "name": "Rawal Khirodkar"
                    },
                    {
                        "name": "Su Zhaoen"
                    },
                    {
                        "name": "Julieta Martinez"
                    },
                    {
                        "name": "Igor Gilitschenski"
                    },
                    {
                        "name": "Shunsuke Saito"
                    },
                    {
                        "name": "Timur Bagautdinov"
                    }
                ],
                "author_detail": {
                    "name": "Timur Bagautdinov"
                },
                "author": "Timur Bagautdinov",
                "arxiv_comment": "Project Page - http://yashkant.github.io/pippo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07785v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07785v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07780v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07780v1",
                "updated": "2025-02-11T18:59:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    59,
                    35,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T18:59:35Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    59,
                    35,
                    1,
                    42,
                    0
                ],
                "title": "DarwinLM: Evolutionary Structured Pruning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DarwinLM: Evolutionary Structured Pruning of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have achieved significant success across various\nNLP tasks. However, their massive computational costs limit their widespread\nuse, particularly in real-time applications. Structured pruning offers an\neffective solution by compressing models and directly providing end-to-end\nspeed improvements, regardless of the hardware environment. Meanwhile,\ndifferent components of the model exhibit varying sensitivities towards\npruning, calling for \\emph{non-uniform} model compression. However, a pruning\nmethod should not only identify a capable substructure, but also account for\npost-compression training. To this end, we propose \\sysname, a method for\n\\emph{training-aware} structured pruning. \\sysname builds upon an evolutionary\nsearch process, generating multiple offspring models in each generation through\nmutation, and selecting the fittest for survival. To assess the effect of\npost-training, we incorporate a lightweight, multistep training process within\nthe offspring population, progressively increasing the number of tokens and\neliminating poorly performing models in each selection stage. We validate our\nmethod through extensive experiments on Llama-2-7B, Llama-3.1-8B and\nQwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured\npruning. For instance, \\sysname surpasses ShearedLlama while requiring\n$5\\times$ less training data during post-compression training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved significant success across various\nNLP tasks. However, their massive computational costs limit their widespread\nuse, particularly in real-time applications. Structured pruning offers an\neffective solution by compressing models and directly providing end-to-end\nspeed improvements, regardless of the hardware environment. Meanwhile,\ndifferent components of the model exhibit varying sensitivities towards\npruning, calling for \\emph{non-uniform} model compression. However, a pruning\nmethod should not only identify a capable substructure, but also account for\npost-compression training. To this end, we propose \\sysname, a method for\n\\emph{training-aware} structured pruning. \\sysname builds upon an evolutionary\nsearch process, generating multiple offspring models in each generation through\nmutation, and selecting the fittest for survival. To assess the effect of\npost-training, we incorporate a lightweight, multistep training process within\nthe offspring population, progressively increasing the number of tokens and\neliminating poorly performing models in each selection stage. We validate our\nmethod through extensive experiments on Llama-2-7B, Llama-3.1-8B and\nQwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured\npruning. For instance, \\sysname surpasses ShearedLlama while requiring\n$5\\times$ less training data during post-compression training."
                },
                "authors": [
                    {
                        "name": "Shengkun Tang"
                    },
                    {
                        "name": "Oliver Sieberling"
                    },
                    {
                        "name": "Eldar Kurtic"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07780v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07780v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.08446v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.08446v2",
                "updated": "2025-02-11T18:59:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    59,
                    26,
                    1,
                    42,
                    0
                ],
                "published": "2024-06-12T17:37:09Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    17,
                    37,
                    9,
                    2,
                    164,
                    0
                ],
                "title": "OLMES: A Standard for Language Model Evaluations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OLMES: A Standard for Language Model Evaluations"
                },
                "summary": "Progress in AI is often demonstrated by new models claiming improved\nperformance on tasks measuring model capabilities. Evaluating language models\ncan be particularly challenging, as choices of how a model is evaluated on a\ntask can lead to large changes in measured performance. There is no common\nstandard setup, so different models are evaluated on the same tasks in\ndifferent ways, leading to claims about which models perform best not being\nreproducible. We propose OLMES, a completely documented, practical, open\nstandard for reproducible LLM evaluations. In developing this standard, we\nidentify and review the varying factors in evaluation practices adopted by the\ncommunity - such as details of prompt formatting, choice of in-context\nexamples, probability normalizations, and task formulation. In particular,\nOLMES supports meaningful comparisons between smaller base models that require\nthe unnatural \"cloze\" formulation of multiple-choice questions against larger\nmodels that can utilize the original formulation. OLMES includes\nwell-considered, documented recommendations guided by results from existing\nliterature as well as new experiments resolving open questions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Progress in AI is often demonstrated by new models claiming improved\nperformance on tasks measuring model capabilities. Evaluating language models\ncan be particularly challenging, as choices of how a model is evaluated on a\ntask can lead to large changes in measured performance. There is no common\nstandard setup, so different models are evaluated on the same tasks in\ndifferent ways, leading to claims about which models perform best not being\nreproducible. We propose OLMES, a completely documented, practical, open\nstandard for reproducible LLM evaluations. In developing this standard, we\nidentify and review the varying factors in evaluation practices adopted by the\ncommunity - such as details of prompt formatting, choice of in-context\nexamples, probability normalizations, and task formulation. In particular,\nOLMES supports meaningful comparisons between smaller base models that require\nthe unnatural \"cloze\" formulation of multiple-choice questions against larger\nmodels that can utilize the original formulation. OLMES includes\nwell-considered, documented recommendations guided by results from existing\nliterature as well as new experiments resolving open questions."
                },
                "authors": [
                    {
                        "name": "Yuling Gu"
                    },
                    {
                        "name": "Oyvind Tafjord"
                    },
                    {
                        "name": "Bailey Kuehl"
                    },
                    {
                        "name": "Dany Haddad"
                    },
                    {
                        "name": "Jesse Dodge"
                    },
                    {
                        "name": "Hannaneh Hajishirzi"
                    }
                ],
                "author_detail": {
                    "name": "Hannaneh Hajishirzi"
                },
                "author": "Hannaneh Hajishirzi",
                "arxiv_comment": "Findings of NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.08446v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.08446v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07776v1",
                "updated": "2025-02-11T18:58:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    58,
                    4,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T18:58:04Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    58,
                    4,
                    1,
                    42,
                    0
                ],
                "title": "Auditing Prompt Caching in Language Model APIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auditing Prompt Caching in Language Model APIs"
                },
                "summary": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known."
                },
                "authors": [
                    {
                        "name": "Chenchen Gu"
                    },
                    {
                        "name": "Xiang Lisa Li"
                    },
                    {
                        "name": "Rohith Kuditipudi"
                    },
                    {
                        "name": "Percy Liang"
                    },
                    {
                        "name": "Tatsunori Hashimoto"
                    }
                ],
                "author_detail": {
                    "name": "Tatsunori Hashimoto"
                },
                "author": "Tatsunori Hashimoto",
                "arxiv_comment": "20 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07772v1",
                "updated": "2025-02-11T18:56:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    56,
                    20,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T18:56:20Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    56,
                    20,
                    1,
                    42,
                    0
                ],
                "title": "Automatic Robot Task Planning by Integrating Large Language Model with\n  Genetic Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Robot Task Planning by Integrating Large Language Model with\n  Genetic Programming"
                },
                "summary": "Accurate task planning is critical for controlling autonomous systems, such\nas robots, drones, and self-driving vehicles. Behavior Trees (BTs) are\nconsidered one of the most prominent control-policy-defining frameworks in task\nplanning, due to their modularity, flexibility, and reusability. Generating\nreliable and accurate BT-based control policies for robotic systems remains\nchallenging and often requires domain expertise. In this paper, we present the\nLLM-GP-BT technique that leverages the Large Language Model (LLM) and Genetic\nProgramming (GP) to automate the generation and configuration of BTs. The\nLLM-GP-BT technique processes robot task commands expressed in human natural\nlanguage and converts them into accurate and reliable BT-based task plans in a\ncomputationally efficient and user-friendly manner. The proposed technique is\nsystematically developed and validated through simulation experiments,\ndemonstrating its potential to streamline task planning for autonomous systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate task planning is critical for controlling autonomous systems, such\nas robots, drones, and self-driving vehicles. Behavior Trees (BTs) are\nconsidered one of the most prominent control-policy-defining frameworks in task\nplanning, due to their modularity, flexibility, and reusability. Generating\nreliable and accurate BT-based control policies for robotic systems remains\nchallenging and often requires domain expertise. In this paper, we present the\nLLM-GP-BT technique that leverages the Large Language Model (LLM) and Genetic\nProgramming (GP) to automate the generation and configuration of BTs. The\nLLM-GP-BT technique processes robot task commands expressed in human natural\nlanguage and converts them into accurate and reliable BT-based task plans in a\ncomputationally efficient and user-friendly manner. The proposed technique is\nsystematically developed and validated through simulation experiments,\ndemonstrating its potential to streamline task planning for autonomous systems."
                },
                "authors": [
                    {
                        "name": "Azizjon Kobilov"
                    },
                    {
                        "name": "Jianglin Lan"
                    }
                ],
                "author_detail": {
                    "name": "Jianglin Lan"
                },
                "author": "Jianglin Lan",
                "arxiv_comment": "Submitted to IEEE Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07771v1",
                "updated": "2025-02-11T18:55:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    55,
                    57,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T18:55:57Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    55,
                    57,
                    1,
                    42,
                    0
                ],
                "title": "Breaking Down Bias: On The Limits of Generalizable Pruning Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking Down Bias: On The Limits of Generalizable Pruning Strategies"
                },
                "summary": "We employ model pruning to examine how LLMs conceptualize racial biases, and\nwhether a generalizable mitigation strategy for such biases appears feasible.\nOur analysis yields several novel insights. We find that pruning can be an\neffective method to reduce bias without significantly increasing anomalous\nmodel behavior. Neuron-based pruning strategies generally yield better results\nthan approaches pruning entire attention heads. However, our results also show\nthat the effectiveness of either approach quickly deteriorates as pruning\nstrategies become more generalized. For instance, a model that is trained on\nremoving racial biases in the context of financial decision-making poorly\ngeneralizes to biases in commercial transactions. Overall, our analysis\nsuggests that racial biases are only partially represented as a general concept\nwithin language models. The other part of these biases is highly\ncontext-specific, suggesting that generalizable mitigation strategies may be of\nlimited effectiveness. Our findings have important implications for legal\nframeworks surrounding AI. In particular, they suggest that an effective\nmitigation strategy should include the allocation of legal responsibility on\nthose that deploy models in a specific use case.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We employ model pruning to examine how LLMs conceptualize racial biases, and\nwhether a generalizable mitigation strategy for such biases appears feasible.\nOur analysis yields several novel insights. We find that pruning can be an\neffective method to reduce bias without significantly increasing anomalous\nmodel behavior. Neuron-based pruning strategies generally yield better results\nthan approaches pruning entire attention heads. However, our results also show\nthat the effectiveness of either approach quickly deteriorates as pruning\nstrategies become more generalized. For instance, a model that is trained on\nremoving racial biases in the context of financial decision-making poorly\ngeneralizes to biases in commercial transactions. Overall, our analysis\nsuggests that racial biases are only partially represented as a general concept\nwithin language models. The other part of these biases is highly\ncontext-specific, suggesting that generalizable mitigation strategies may be of\nlimited effectiveness. Our findings have important implications for legal\nframeworks surrounding AI. In particular, they suggest that an effective\nmitigation strategy should include the allocation of legal responsibility on\nthose that deploy models in a specific use case."
                },
                "authors": [
                    {
                        "name": "Sibo Ma"
                    },
                    {
                        "name": "Alejandro Salinas"
                    },
                    {
                        "name": "Peter Henderson"
                    },
                    {
                        "name": "Julian Nyarko"
                    }
                ],
                "author_detail": {
                    "name": "Julian Nyarko"
                },
                "author": "Julian Nyarko",
                "arxiv_comment": "28 pages, 9 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06808v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06808v2",
                "updated": "2025-02-11T18:52:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    52,
                    51,
                    1,
                    42,
                    0
                ],
                "published": "2024-11-26T00:06:47Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    0,
                    6,
                    47,
                    1,
                    331,
                    0
                ],
                "title": "Effect of Adaptive Communication Support on LLM-powered Human-Robot\n  Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effect of Adaptive Communication Support on LLM-powered Human-Robot\n  Collaboration"
                },
                "summary": "Effective human-robot collaboration requires robot to adopt their roles and\nlevels of support based on human needs, task requirements, and complexity.\nTraditional human-robot teaming often relies on a pre-determined robot\ncommunication scheme, restricting teamwork adaptability in complex tasks.\nLeveraging strong communication capabilities of Large Language Models (LLMs),\nwe propose a Human-Robot Teaming Framework with Multi-Modal Language feedback\n(HRT-ML), a framework designed to enhance human-robot interaction by adjusting\nthe frequency and content of language-based feedback. HRT-ML framework includes\ntwo core modules: a Coordinator for high-level, low-frequency strategic\nguidance, and a Manager for subtask-specific, high-frequency instructions,\nenabling passive and active interactions with human teammates. To assess the\nimpact of language feedback in collaborative scenarios, we conducted\nexperiments in an enhanced Overcooked environment with varying levels of task\ncomplexity (easy, medium, hard) and feedback frequency (inactive, passive,\nactive, superactive). Our results show that as task complexity increases\nrelative to human capabilities, human teammates exhibited a stronger preference\ntowards robotic agents that can offer frequent, proactive support. However,\nwhen task complexities exceed the LLM's capacity, noisy and inaccurate feedback\nfrom superactive robotic agents can instead hinder team performance, as it\nrequires human teammates to increase their effort to interpret and respond to a\nlarge number of communications, with limited performance return. Our results\noffer a general principle for robotic agents to dynamically adjust their levels\nand frequencies of communications to work seamlessly with humans and achieve\nimproved teaming performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective human-robot collaboration requires robot to adopt their roles and\nlevels of support based on human needs, task requirements, and complexity.\nTraditional human-robot teaming often relies on a pre-determined robot\ncommunication scheme, restricting teamwork adaptability in complex tasks.\nLeveraging strong communication capabilities of Large Language Models (LLMs),\nwe propose a Human-Robot Teaming Framework with Multi-Modal Language feedback\n(HRT-ML), a framework designed to enhance human-robot interaction by adjusting\nthe frequency and content of language-based feedback. HRT-ML framework includes\ntwo core modules: a Coordinator for high-level, low-frequency strategic\nguidance, and a Manager for subtask-specific, high-frequency instructions,\nenabling passive and active interactions with human teammates. To assess the\nimpact of language feedback in collaborative scenarios, we conducted\nexperiments in an enhanced Overcooked environment with varying levels of task\ncomplexity (easy, medium, hard) and feedback frequency (inactive, passive,\nactive, superactive). Our results show that as task complexity increases\nrelative to human capabilities, human teammates exhibited a stronger preference\ntowards robotic agents that can offer frequent, proactive support. However,\nwhen task complexities exceed the LLM's capacity, noisy and inaccurate feedback\nfrom superactive robotic agents can instead hinder team performance, as it\nrequires human teammates to increase their effort to interpret and respond to a\nlarge number of communications, with limited performance return. Our results\noffer a general principle for robotic agents to dynamically adjust their levels\nand frequencies of communications to work seamlessly with humans and achieve\nimproved teaming performance."
                },
                "authors": [
                    {
                        "name": "Shipeng Liu"
                    },
                    {
                        "name": "FNU Shrutika"
                    },
                    {
                        "name": "Boshen Zhang"
                    },
                    {
                        "name": "Zhehui Huang"
                    },
                    {
                        "name": "Gaurav Sukhatme"
                    },
                    {
                        "name": "Feifei Qian"
                    }
                ],
                "author_detail": {
                    "name": "Feifei Qian"
                },
                "author": "Feifei Qian",
                "arxiv_comment": "13 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06808v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06808v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.9",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07763v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07763v1",
                "updated": "2025-02-11T18:46:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    46,
                    1,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T18:46:01Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    46,
                    1,
                    1,
                    42,
                    0
                ],
                "title": "Great Power Brings Great Responsibility: Personalizing Conversational AI\n  for Diverse Problem-Solvers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Great Power Brings Great Responsibility: Personalizing Conversational AI\n  for Diverse Problem-Solvers"
                },
                "summary": "Newcomers onboarding to Open Source Software (OSS) projects face many\nchallenges. Large Language Models (LLMs), like ChatGPT, have emerged as\npotential resources for answering questions and providing guidance, with many\ndevelopers now turning to ChatGPT over traditional Q&A sites like Stack\nOverflow. Nonetheless, LLMs may carry biases in presenting information, which\ncan be especially impactful for newcomers whose problem-solving styles may not\nbe broadly represented. This raises important questions about the accessibility\nof AI-driven support for newcomers to OSS projects. This vision paper outlines\nthe potential of adapting AI responses to various problem-solving styles to\navoid privileging a particular subgroup. We discuss the potential of AI\npersona-based prompt engineering as a strategy for interacting with AI. This\nstudy invites further research to refine AI-based tools to better support\ncontributions to OSS projects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Newcomers onboarding to Open Source Software (OSS) projects face many\nchallenges. Large Language Models (LLMs), like ChatGPT, have emerged as\npotential resources for answering questions and providing guidance, with many\ndevelopers now turning to ChatGPT over traditional Q&A sites like Stack\nOverflow. Nonetheless, LLMs may carry biases in presenting information, which\ncan be especially impactful for newcomers whose problem-solving styles may not\nbe broadly represented. This raises important questions about the accessibility\nof AI-driven support for newcomers to OSS projects. This vision paper outlines\nthe potential of adapting AI responses to various problem-solving styles to\navoid privileging a particular subgroup. We discuss the potential of AI\npersona-based prompt engineering as a strategy for interacting with AI. This\nstudy invites further research to refine AI-based tools to better support\ncontributions to OSS projects."
                },
                "authors": [
                    {
                        "name": "Italo Santos"
                    },
                    {
                        "name": "Katia Romero Felizardo"
                    },
                    {
                        "name": "Igor Steinmacher"
                    },
                    {
                        "name": "Marco A. Gerosa"
                    }
                ],
                "author_detail": {
                    "name": "Marco A. Gerosa"
                },
                "author": "Marco A. Gerosa",
                "arxiv_journal_ref": "18th International Conference on Cooperative and Human Aspects of\n  Software Engineering (CHASE 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07763v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07763v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19392v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19392v2",
                "updated": "2025-02-11T18:45:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    45,
                    12,
                    1,
                    42,
                    0
                ],
                "published": "2025-01-31T18:47:42Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    47,
                    42,
                    4,
                    31,
                    0
                ],
                "title": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models"
                },
                "summary": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models."
                },
                "authors": [
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "Vladimir Malinovskii"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Denis Mazur"
                    },
                    {
                        "name": "Nikita Surkov"
                    },
                    {
                        "name": "Ivan Ermakov"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19392v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19392v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.08598v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.08598v3",
                "updated": "2025-02-11T18:42:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    42,
                    44,
                    1,
                    42,
                    0
                ],
                "published": "2024-06-12T19:05:43Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    19,
                    5,
                    43,
                    2,
                    164,
                    0
                ],
                "title": "Language Model Council: Democratically Benchmarking Foundation Models on\n  Highly Subjective Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Model Council: Democratically Benchmarking Foundation Models on\n  Highly Subjective Tasks"
                },
                "summary": "As Large Language Models (LLMs) continue to evolve, evaluating them remains a\npersistent challenge. Many recent evaluations use LLMs as judges to score\noutputs from other LLMs, often relying on a single large model like GPT-4o.\nHowever, using a single LLM judge is prone to intra-model bias, and many tasks\n- such as those related to emotional intelligence, creative writing, and\npersuasiveness - may be too subjective for a single model to judge fairly. We\nintroduce the Language Model Council (LMC), where a group of LLMs collaborate\nto create tests, respond to them, and evaluate each other's responses to\nproduce a ranking in a democratic fashion. Unlike previous approaches that\nfocus on reducing cost or bias by using a panel of smaller models, our work\nexamines the benefits and nuances of a fully inclusive LLM evaluation system.\nIn a detailed case study on emotional intelligence, we deploy a council of 20\nrecent LLMs to rank each other on open-ended responses to interpersonal\nconflicts. Our results show that the LMC produces rankings that are more\nseparable and more robust, and through a user study, we show that they are more\nconsistent with human evaluations than any individual LLM judge. Using all LLMs\nfor judging can be costly, however, so we use Monte Carlo simulations and\nhand-curated sub-councils to study hypothetical council compositions and\ndiscuss the value of the incremental LLM judge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) continue to evolve, evaluating them remains a\npersistent challenge. Many recent evaluations use LLMs as judges to score\noutputs from other LLMs, often relying on a single large model like GPT-4o.\nHowever, using a single LLM judge is prone to intra-model bias, and many tasks\n- such as those related to emotional intelligence, creative writing, and\npersuasiveness - may be too subjective for a single model to judge fairly. We\nintroduce the Language Model Council (LMC), where a group of LLMs collaborate\nto create tests, respond to them, and evaluate each other's responses to\nproduce a ranking in a democratic fashion. Unlike previous approaches that\nfocus on reducing cost or bias by using a panel of smaller models, our work\nexamines the benefits and nuances of a fully inclusive LLM evaluation system.\nIn a detailed case study on emotional intelligence, we deploy a council of 20\nrecent LLMs to rank each other on open-ended responses to interpersonal\nconflicts. Our results show that the LMC produces rankings that are more\nseparable and more robust, and through a user study, we show that they are more\nconsistent with human evaluations than any individual LLM judge. Using all LLMs\nfor judging can be costly, however, so we use Monte Carlo simulations and\nhand-curated sub-councils to study hypothetical council compositions and\ndiscuss the value of the incremental LLM judge."
                },
                "authors": [
                    {
                        "name": "Justin Zhao"
                    },
                    {
                        "name": "Flor Miriam Plaza-del-Arco"
                    },
                    {
                        "name": "Benjie Genchel"
                    },
                    {
                        "name": "Amanda Cercas Curry"
                    }
                ],
                "author_detail": {
                    "name": "Amanda Cercas Curry"
                },
                "author": "Amanda Cercas Curry",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.08598v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.08598v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07752v1",
                "updated": "2025-02-11T18:27:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    27,
                    19,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T18:27:19Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    27,
                    19,
                    1,
                    42,
                    0
                ],
                "title": "Towards Efficient Optimizer Design for LLM via Structured Fisher\n  Approximation with a Low-Rank Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Efficient Optimizer Design for LLM via Structured Fisher\n  Approximation with a Low-Rank Extension"
                },
                "summary": "Designing efficient optimizers for large language models (LLMs) with\nlow-memory requirements and fast convergence is an important and challenging\nproblem. This paper makes a step towards the systematic design of such\noptimizers through the lens of structured Fisher information matrix (FIM)\napproximation. We show that many state-of-the-art efficient optimizers can be\nviewed as solutions to FIM approximation (under the Frobenius norm) with\nspecific structural assumptions. Building on these insights, we propose two\ndesign recommendations of practical efficient optimizers for LLMs, involving\nthe careful selection of structural assumptions to balance generality and\nefficiency, and enhancing memory efficiency of optimizers with general\nstructures through a novel low-rank extension framework. We demonstrate how to\nuse each design approach by deriving new memory-efficient optimizers: Row and\nColumn Scaled SGD (RACS) and Adaptive low-dimensional subspace estimation\n(Alice). Experiments on LLaMA pre-training (up to 1B parameters) validate the\neffectiveness, showing faster and better convergence than existing\nmemory-efficient baselines and Adam with little memory overhead. Notably, Alice\nachieves better than 2x faster convergence over Adam, while RACS delivers\nstrong performance on the 1B model with SGD-like memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing efficient optimizers for large language models (LLMs) with\nlow-memory requirements and fast convergence is an important and challenging\nproblem. This paper makes a step towards the systematic design of such\noptimizers through the lens of structured Fisher information matrix (FIM)\napproximation. We show that many state-of-the-art efficient optimizers can be\nviewed as solutions to FIM approximation (under the Frobenius norm) with\nspecific structural assumptions. Building on these insights, we propose two\ndesign recommendations of practical efficient optimizers for LLMs, involving\nthe careful selection of structural assumptions to balance generality and\nefficiency, and enhancing memory efficiency of optimizers with general\nstructures through a novel low-rank extension framework. We demonstrate how to\nuse each design approach by deriving new memory-efficient optimizers: Row and\nColumn Scaled SGD (RACS) and Adaptive low-dimensional subspace estimation\n(Alice). Experiments on LLaMA pre-training (up to 1B parameters) validate the\neffectiveness, showing faster and better convergence than existing\nmemory-efficient baselines and Adam with little memory overhead. Notably, Alice\nachieves better than 2x faster convergence over Adam, while RACS delivers\nstrong performance on the 1B model with SGD-like memory."
                },
                "authors": [
                    {
                        "name": "Wenbo Gong"
                    },
                    {
                        "name": "Meyer Scetbon"
                    },
                    {
                        "name": "Chao Ma"
                    },
                    {
                        "name": "Edward Meeds"
                    }
                ],
                "author_detail": {
                    "name": "Edward Meeds"
                },
                "author": "Edward Meeds",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.08096v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.08096v3",
                "updated": "2025-02-11T18:25:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    25,
                    7,
                    1,
                    42,
                    0
                ],
                "published": "2024-02-12T22:32:12Z",
                "published_parsed": [
                    2024,
                    2,
                    12,
                    22,
                    32,
                    12,
                    0,
                    43,
                    0
                ],
                "title": "An Efficient Rehearsal Scheme for Catastrophic Forgetting Mitigation\n  during Multi-stage Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Rehearsal Scheme for Catastrophic Forgetting Mitigation\n  during Multi-stage Fine-tuning"
                },
                "summary": "Incrementally fine-tuning foundational models on new tasks or domains is now\nthe de facto approach in NLP. A known pitfall of this approach is the\n\\emph{catastrophic forgetting} of prior knowledge that happens during\nfine-tuning. A common approach to alleviate such forgetting is to rehearse\nsamples from prior tasks during fine-tuning. Several existing works assume a\nfixed memory buffer to store prior task examples, while relying on inferences\n(forward passes) with the model at hand for choosing examples for rehearsal\nfrom the buffer. However, given the increasing computational cost of model\ninference, and decreasing cost of data storage, we focus on the setting to\nrehearse samples with a fixed computational budget instead of a fixed memory\nbudget. We propose a sampling scheme, \\texttt{\\bf mix-cd}, that prioritizes\nrehearsal of ``collateral damage'' samples, which are samples predicted\ncorrectly by the prior model but forgotten by the incrementally tuned one. The\ncrux of our scheme is a procedure to efficiently estimate the density of\ncollateral damage samples without incurring additional model inferences. Our\napproach is computationally efficient, easy to implement, and outperforms\nseveral leading continual learning methods in compute-constrained settings. All\nthe code will be publicly available at\nhttps://github.com/jybai/mix-cd-rehearsal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incrementally fine-tuning foundational models on new tasks or domains is now\nthe de facto approach in NLP. A known pitfall of this approach is the\n\\emph{catastrophic forgetting} of prior knowledge that happens during\nfine-tuning. A common approach to alleviate such forgetting is to rehearse\nsamples from prior tasks during fine-tuning. Several existing works assume a\nfixed memory buffer to store prior task examples, while relying on inferences\n(forward passes) with the model at hand for choosing examples for rehearsal\nfrom the buffer. However, given the increasing computational cost of model\ninference, and decreasing cost of data storage, we focus on the setting to\nrehearse samples with a fixed computational budget instead of a fixed memory\nbudget. We propose a sampling scheme, \\texttt{\\bf mix-cd}, that prioritizes\nrehearsal of ``collateral damage'' samples, which are samples predicted\ncorrectly by the prior model but forgotten by the incrementally tuned one. The\ncrux of our scheme is a procedure to efficiently estimate the density of\ncollateral damage samples without incurring additional model inferences. Our\napproach is computationally efficient, easy to implement, and outperforms\nseveral leading continual learning methods in compute-constrained settings. All\nthe code will be publicly available at\nhttps://github.com/jybai/mix-cd-rehearsal."
                },
                "authors": [
                    {
                        "name": "Andrew Bai"
                    },
                    {
                        "name": "Chih-Kuan Yeh"
                    },
                    {
                        "name": "Cho-Jui Hsieh"
                    },
                    {
                        "name": "Ankur Taly"
                    }
                ],
                "author_detail": {
                    "name": "Ankur Taly"
                },
                "author": "Ankur Taly",
                "arxiv_comment": "13 pages, 9 figures. Published in NAACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.08096v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.08096v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.09401v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.09401v2",
                "updated": "2025-02-11T18:18:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    18,
                    59,
                    1,
                    42,
                    0
                ],
                "published": "2024-02-14T18:58:40Z",
                "published_parsed": [
                    2024,
                    2,
                    14,
                    18,
                    58,
                    40,
                    2,
                    45,
                    0
                ],
                "title": "Reinforcement Learning from Human Feedback with Active Queries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Human Feedback with Active Queries"
                },
                "summary": "Aligning large language models (LLM) with human preference plays a key role\nin building modern generative models and can be achieved by reinforcement\nlearning from human feedback (RLHF). Despite their superior performance,\ncurrent RLHF approaches often require a large amount of human-labelled\npreference data, which is expensive to collect. In this paper, inspired by the\nsuccess of active learning, we address this problem by proposing\nquery-efficient RLHF methods. We first formalize the alignment problem as a\ncontextual dueling bandit problem and design an active-query-based proximal\npolicy optimization (APPO) algorithm with an $\\tilde{O}(d^2/\\Delta)$\ninstance-dependent regret bound and an $\\tilde{O}(d^2/\\Delta^2)$ query\ncomplexity, where $d$ is the dimension of feature space and $\\Delta$ is the\nsub-optimality gap over all the contexts. We then propose ADPO, a practical\nversion of our algorithm based on direct preference optimization (DPO) and\napply it to fine-tuning LLMs. Our experiments show that ADPO, while only making\nabout half of queries for human preference, matches the performance of the\nstate-of-the-art DPO method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning large language models (LLM) with human preference plays a key role\nin building modern generative models and can be achieved by reinforcement\nlearning from human feedback (RLHF). Despite their superior performance,\ncurrent RLHF approaches often require a large amount of human-labelled\npreference data, which is expensive to collect. In this paper, inspired by the\nsuccess of active learning, we address this problem by proposing\nquery-efficient RLHF methods. We first formalize the alignment problem as a\ncontextual dueling bandit problem and design an active-query-based proximal\npolicy optimization (APPO) algorithm with an $\\tilde{O}(d^2/\\Delta)$\ninstance-dependent regret bound and an $\\tilde{O}(d^2/\\Delta^2)$ query\ncomplexity, where $d$ is the dimension of feature space and $\\Delta$ is the\nsub-optimality gap over all the contexts. We then propose ADPO, a practical\nversion of our algorithm based on direct preference optimization (DPO) and\napply it to fine-tuning LLMs. Our experiments show that ADPO, while only making\nabout half of queries for human preference, matches the performance of the\nstate-of-the-art DPO method."
                },
                "authors": [
                    {
                        "name": "Kaixuan Ji"
                    },
                    {
                        "name": "Jiafan He"
                    },
                    {
                        "name": "Quanquan Gu"
                    }
                ],
                "author_detail": {
                    "name": "Quanquan Gu"
                },
                "author": "Quanquan Gu",
                "arxiv_comment": "28 pages, 1 figure, 4 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.09401v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.09401v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.05502v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.05502v3",
                "updated": "2025-02-11T18:17:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    17,
                    53,
                    1,
                    42,
                    0
                ],
                "published": "2024-07-07T21:26:36Z",
                "published_parsed": [
                    2024,
                    7,
                    7,
                    21,
                    26,
                    36,
                    6,
                    189,
                    0
                ],
                "title": "Faux Polyglot: A Study on Information Disparity in Multilingual Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faux Polyglot: A Study on Information Disparity in Multilingual Large\n  Language Models"
                },
                "summary": "Although the multilingual capability of LLMs offers new opportunities to\novercome the language barrier, do these capabilities translate into real-life\nscenarios where linguistic divide and knowledge conflicts between multilingual\nsources are known occurrences? In this paper, we studied LLM's linguistic\npreference in a cross-language RAG-based information search setting. We found\nthat LLMs displayed systemic bias towards information in the same language as\nthe query language in both document retrieval and answer generation.\nFurthermore, in scenarios where no information is in the language of the query,\nLLMs prefer documents in high-resource languages during generation, potentially\nreinforcing the dominant views. Such bias exists for both factual and\nopinion-based queries. Our results highlight the linguistic divide within\nmultilingual LLMs in information search systems. The seemingly beneficial\nmultilingual capability of LLMs may backfire on information parity by\nreinforcing language-specific information cocoons or filter bubbles further\nmarginalizing low-resource views.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although the multilingual capability of LLMs offers new opportunities to\novercome the language barrier, do these capabilities translate into real-life\nscenarios where linguistic divide and knowledge conflicts between multilingual\nsources are known occurrences? In this paper, we studied LLM's linguistic\npreference in a cross-language RAG-based information search setting. We found\nthat LLMs displayed systemic bias towards information in the same language as\nthe query language in both document retrieval and answer generation.\nFurthermore, in scenarios where no information is in the language of the query,\nLLMs prefer documents in high-resource languages during generation, potentially\nreinforcing the dominant views. Such bias exists for both factual and\nopinion-based queries. Our results highlight the linguistic divide within\nmultilingual LLMs in information search systems. The seemingly beneficial\nmultilingual capability of LLMs may backfire on information parity by\nreinforcing language-specific information cocoons or filter bubbles further\nmarginalizing low-resource views."
                },
                "authors": [
                    {
                        "name": "Nikhil Sharma"
                    },
                    {
                        "name": "Kenton Murray"
                    },
                    {
                        "name": "Ziang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Ziang Xiao"
                },
                "author": "Ziang Xiao",
                "arxiv_comment": "NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.05502v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.05502v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07747v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07747v1",
                "updated": "2025-02-11T18:14:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    14,
                    44,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T18:14:44Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    14,
                    44,
                    1,
                    42,
                    0
                ],
                "title": "WHODUNIT: Evaluation benchmark for culprit detection in mystery stories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WHODUNIT: Evaluation benchmark for culprit detection in mystery stories"
                },
                "summary": "We present a novel data set, WhoDunIt, to assess the deductive reasoning\ncapabilities of large language models (LLM) within narrative contexts.\nConstructed from open domain mystery novels and short stories, the dataset\nchallenges LLMs to identify the perpetrator after reading and comprehending the\nstory. To evaluate model robustness, we apply a range of character-level name\naugmentations, including original names, name swaps, and substitutions with\nwell-known real and/or fictional entities from popular discourse. We further\nuse various prompting styles to investigate the influence of prompting on\ndeductive reasoning accuracy.\n  We conduct evaluation study with state-of-the-art models, specifically\nGPT-4o, GPT-4-turbo, and GPT-4o-mini, evaluated through multiple trials with\nmajority response selection to ensure reliability. The results demonstrate that\nwhile LLMs perform reliably on unaltered texts, accuracy diminishes with\ncertain name substitutions, particularly those with wide recognition. This\ndataset is publicly available here.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel data set, WhoDunIt, to assess the deductive reasoning\ncapabilities of large language models (LLM) within narrative contexts.\nConstructed from open domain mystery novels and short stories, the dataset\nchallenges LLMs to identify the perpetrator after reading and comprehending the\nstory. To evaluate model robustness, we apply a range of character-level name\naugmentations, including original names, name swaps, and substitutions with\nwell-known real and/or fictional entities from popular discourse. We further\nuse various prompting styles to investigate the influence of prompting on\ndeductive reasoning accuracy.\n  We conduct evaluation study with state-of-the-art models, specifically\nGPT-4o, GPT-4-turbo, and GPT-4o-mini, evaluated through multiple trials with\nmajority response selection to ensure reliability. The results demonstrate that\nwhile LLMs perform reliably on unaltered texts, accuracy diminishes with\ncertain name substitutions, particularly those with wide recognition. This\ndataset is publicly available here."
                },
                "authors": [
                    {
                        "name": "Kshitij Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Kshitij Gupta"
                },
                "author": "Kshitij Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07747v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07747v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04463v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04463v2",
                "updated": "2025-02-11T18:06:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    6,
                    2,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-06T19:18:16Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    19,
                    18,
                    16,
                    3,
                    37,
                    0
                ],
                "title": "Training Language Models to Reason Efficiently",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Language Models to Reason Efficiently"
                },
                "summary": "Scaling model size and training data has led to great advances in the\nperformance of Large Language Models (LLMs). However, the diminishing returns\nof this approach necessitate alternative methods to improve model capabilities,\nparticularly in tasks requiring advanced reasoning. Large reasoning models,\nwhich leverage long chain-of-thoughts, bring unprecedented breakthroughs in\nproblem-solving capabilities but at a substantial deployment cost associated to\nlonger generations. Reducing inference costs is crucial for the economic\nfeasibility, user experience, and environmental sustainability of these models.\n  In this work, we propose to train large reasoning models to reason\nefficiently. More precisely, we use reinforcement learning (RL) to train\nreasoning models to dynamically allocate inference-time compute based on task\ncomplexity. Our method incentivizes models to minimize unnecessary\ncomputational overhead while maintaining accuracy, thereby achieving\nsubstantial efficiency gains. It enables the derivation of a family of\nreasoning models with varying efficiency levels, controlled via a single\nhyperparameter. Experiments on two open-weight large reasoning models\ndemonstrate significant reductions in inference cost while preserving most of\nthe accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling model size and training data has led to great advances in the\nperformance of Large Language Models (LLMs). However, the diminishing returns\nof this approach necessitate alternative methods to improve model capabilities,\nparticularly in tasks requiring advanced reasoning. Large reasoning models,\nwhich leverage long chain-of-thoughts, bring unprecedented breakthroughs in\nproblem-solving capabilities but at a substantial deployment cost associated to\nlonger generations. Reducing inference costs is crucial for the economic\nfeasibility, user experience, and environmental sustainability of these models.\n  In this work, we propose to train large reasoning models to reason\nefficiently. More precisely, we use reinforcement learning (RL) to train\nreasoning models to dynamically allocate inference-time compute based on task\ncomplexity. Our method incentivizes models to minimize unnecessary\ncomputational overhead while maintaining accuracy, thereby achieving\nsubstantial efficiency gains. It enables the derivation of a family of\nreasoning models with varying efficiency levels, controlled via a single\nhyperparameter. Experiments on two open-weight large reasoning models\ndemonstrate significant reductions in inference cost while preserving most of\nthe accuracy."
                },
                "authors": [
                    {
                        "name": "Daman Arora"
                    },
                    {
                        "name": "Andrea Zanette"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Zanette"
                },
                "author": "Andrea Zanette",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04463v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04463v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04524v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04524v2",
                "updated": "2025-02-11T18:02:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    2,
                    11,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-06T21:52:09Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    21,
                    52,
                    9,
                    3,
                    37,
                    0
                ],
                "title": "All-in-One Analog AI Accelerator: On-Chip Training and Inference with\n  Conductive-Metal-Oxide/HfOx ReRAM Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All-in-One Analog AI Accelerator: On-Chip Training and Inference with\n  Conductive-Metal-Oxide/HfOx ReRAM Devices"
                },
                "summary": "Analog in-memory computing is an emerging paradigm designed to efficiently\naccelerate deep neural network workloads. Recent advancements have demonstrated\nsignificant improvements in throughput and efficiency, focusing independently\non either inference or training acceleration. However, a unified analog\nin-memory technology platform-capable of performing on-chip training, retaining\nthe weights, and sustaining long-term inference acceleration-has yet to be\nreported. In this work, an all-in-one analog AI accelerator is presented and\nbenchmarked, combining these capabilities to enable autonomous,\nenergy-efficient, and continuously adaptable AI systems. The platform leverages\nan array of filamentary conductive-metal-oxide(CMO)/HfOx redox-based resistive\nswitching memory cells (ReRAM) in one-transistor one-ReRAM (1T1R)\nconfiguration, integrated into the back-end-of-line (BEOL) of a 130 nm\ntechnology node. The array characterization demonstrates reliable and optimized\nresistive switching with voltage amplitudes of less than 1.5 V, enabling\ncompatibility with advanced technology nodes. The multi-bit capability of over\n32 stable states (5 bits) and record-low programming noise down to 10 nS enable\nan almost ideal weight transfer process, more than an order of magnitude better\nthan other memristive technologies. The array's inference performance is\nvalidated through realistic matrix-vector multiplication simulations on a 64x64\narray, achieving a record-low root-mean-square error ranging from 0.06 at 1\nsecond to 0.2 at 10 years after programming. The array is then characterized\napplying the same conditions used for on-chip training. Training accuracy\nclosely matching the software equivalent is achieved across different datasets,\nwith high-fidelity modelling of the device response based on experimental data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analog in-memory computing is an emerging paradigm designed to efficiently\naccelerate deep neural network workloads. Recent advancements have demonstrated\nsignificant improvements in throughput and efficiency, focusing independently\non either inference or training acceleration. However, a unified analog\nin-memory technology platform-capable of performing on-chip training, retaining\nthe weights, and sustaining long-term inference acceleration-has yet to be\nreported. In this work, an all-in-one analog AI accelerator is presented and\nbenchmarked, combining these capabilities to enable autonomous,\nenergy-efficient, and continuously adaptable AI systems. The platform leverages\nan array of filamentary conductive-metal-oxide(CMO)/HfOx redox-based resistive\nswitching memory cells (ReRAM) in one-transistor one-ReRAM (1T1R)\nconfiguration, integrated into the back-end-of-line (BEOL) of a 130 nm\ntechnology node. The array characterization demonstrates reliable and optimized\nresistive switching with voltage amplitudes of less than 1.5 V, enabling\ncompatibility with advanced technology nodes. The multi-bit capability of over\n32 stable states (5 bits) and record-low programming noise down to 10 nS enable\nan almost ideal weight transfer process, more than an order of magnitude better\nthan other memristive technologies. The array's inference performance is\nvalidated through realistic matrix-vector multiplication simulations on a 64x64\narray, achieving a record-low root-mean-square error ranging from 0.06 at 1\nsecond to 0.2 at 10 years after programming. The array is then characterized\napplying the same conditions used for on-chip training. Training accuracy\nclosely matching the software equivalent is achieved across different datasets,\nwith high-fidelity modelling of the device response based on experimental data."
                },
                "authors": [
                    {
                        "name": "Donato Francesco Falcone"
                    },
                    {
                        "name": "Victoria Clerico"
                    },
                    {
                        "name": "Wooseok Choi"
                    },
                    {
                        "name": "Tommaso Stecconi"
                    },
                    {
                        "name": "Folkert Horst"
                    },
                    {
                        "name": "Laura Begon-Lours"
                    },
                    {
                        "name": "Matteo Galetta"
                    },
                    {
                        "name": "Antonio La Porta"
                    },
                    {
                        "name": "Nikhil Garg"
                    },
                    {
                        "name": "Fabien Alibart"
                    },
                    {
                        "name": "Bert Jan Offrein"
                    },
                    {
                        "name": "Valeria Bragaglia"
                    }
                ],
                "author_detail": {
                    "name": "Valeria Bragaglia"
                },
                "author": "Valeria Bragaglia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04524v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04524v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07737v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07737v1",
                "updated": "2025-02-11T17:57:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    57,
                    53,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T17:57:53Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    57,
                    53,
                    1,
                    42,
                    0
                ],
                "title": "Next Block Prediction: Video Generation via Semi-Auto-Regressive\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next Block Prediction: Video Generation via Semi-Auto-Regressive\n  Modeling"
                },
                "summary": "Next-Token Prediction (NTP) is a de facto approach for autoregressive (AR)\nvideo generation, but it suffers from suboptimal unidirectional dependencies\nand slow inference speed. In this work, we propose a semi-autoregressive\n(semi-AR) framework, called Next-Block Prediction (NBP), for video generation.\nBy uniformly decomposing video content into equal-sized blocks (e.g., rows or\nframes), we shift the generation unit from individual tokens to blocks,\nallowing each token in the current block to simultaneously predict the\ncorresponding token in the next block. Unlike traditional AR modeling, our\nframework employs bidirectional attention within each block, enabling tokens to\ncapture more robust spatial dependencies. By predicting multiple tokens in\nparallel, NBP models significantly reduce the number of generation steps,\nleading to faster and more efficient inference. Our model achieves FVD scores\nof 103.3 on UCF101 and 25.5 on K600, outperforming the vanilla NTP model by an\naverage of 4.4. Furthermore, thanks to the reduced number of inference steps,\nthe NBP model generates 8.89 frames (128x128 resolution) per second, achieving\nan 11x speedup. We also explored model scales ranging from 700M to 3B\nparameters, observing significant improvements in generation quality, with FVD\nscores dropping from 103.3 to 55.3 on UCF101 and from 25.5 to 19.5 on K600,\ndemonstrating the scalability of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-Token Prediction (NTP) is a de facto approach for autoregressive (AR)\nvideo generation, but it suffers from suboptimal unidirectional dependencies\nand slow inference speed. In this work, we propose a semi-autoregressive\n(semi-AR) framework, called Next-Block Prediction (NBP), for video generation.\nBy uniformly decomposing video content into equal-sized blocks (e.g., rows or\nframes), we shift the generation unit from individual tokens to blocks,\nallowing each token in the current block to simultaneously predict the\ncorresponding token in the next block. Unlike traditional AR modeling, our\nframework employs bidirectional attention within each block, enabling tokens to\ncapture more robust spatial dependencies. By predicting multiple tokens in\nparallel, NBP models significantly reduce the number of generation steps,\nleading to faster and more efficient inference. Our model achieves FVD scores\nof 103.3 on UCF101 and 25.5 on K600, outperforming the vanilla NTP model by an\naverage of 4.4. Furthermore, thanks to the reduced number of inference steps,\nthe NBP model generates 8.89 frames (128x128 resolution) per second, achieving\nan 11x speedup. We also explored model scales ranging from 700M to 3B\nparameters, observing significant improvements in generation quality, with FVD\nscores dropping from 103.3 to 55.3 on UCF101 and from 25.5 to 19.5 on K600,\ndemonstrating the scalability of our approach."
                },
                "authors": [
                    {
                        "name": "Shuhuai Ren"
                    },
                    {
                        "name": "Shuming Ma"
                    },
                    {
                        "name": "Xu Sun"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "project page: https://renshuhuai-andy.github.io/NBP-project/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07737v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07736v1",
                "updated": "2025-02-11T17:55:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    55,
                    15,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T17:55:15Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    55,
                    15,
                    1,
                    42,
                    0
                ],
                "title": "The Economics of Large Language Models: Token Allocation, Fine-Tuning,\n  and Optimal Pricing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Economics of Large Language Models: Token Allocation, Fine-Tuning,\n  and Optimal Pricing"
                },
                "summary": "We develop an economic framework to analyze the optimal pricing and product\ndesign of Large Language Models (LLM). Our framework captures several key\nfeatures of LLMs: variable operational costs of processing input and output\ntokens; the ability to customize models through fine-tuning; and\nhigh-dimensional user heterogeneity in terms of task requirements and error\nsensitivity. In our model, a monopolistic seller offers multiple versions of\nLLMs through a menu of products. The optimal pricing structure depends on\nwhether token allocation across tasks is contractible and whether users face\nscale constraints. Users with similar aggregate value-scale characteristics\nchoose similar levels of fine-tuning and token consumption. The optimal\nmechanism can be implemented through menus of two-part tariffs, with higher\nmarkups for more intensive users. Our results rationalize observed industry\npractices such as tiered pricing based on model customization and usage levels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop an economic framework to analyze the optimal pricing and product\ndesign of Large Language Models (LLM). Our framework captures several key\nfeatures of LLMs: variable operational costs of processing input and output\ntokens; the ability to customize models through fine-tuning; and\nhigh-dimensional user heterogeneity in terms of task requirements and error\nsensitivity. In our model, a monopolistic seller offers multiple versions of\nLLMs through a menu of products. The optimal pricing structure depends on\nwhether token allocation across tasks is contractible and whether users face\nscale constraints. Users with similar aggregate value-scale characteristics\nchoose similar levels of fine-tuning and token consumption. The optimal\nmechanism can be implemented through menus of two-part tariffs, with higher\nmarkups for more intensive users. Our results rationalize observed industry\npractices such as tiered pricing based on model customization and usage levels."
                },
                "authors": [
                    {
                        "name": "Dirk Bergemann"
                    },
                    {
                        "name": "Alessandro Bonatti"
                    },
                    {
                        "name": "Alex Smolin"
                    }
                ],
                "author_detail": {
                    "name": "Alex Smolin"
                },
                "author": "Alex Smolin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.TH",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07733v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07733v1",
                "updated": "2025-02-11T17:52:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    52,
                    13,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T17:52:13Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    52,
                    13,
                    1,
                    42,
                    0
                ],
                "title": "In Search of the First Stars: An Ultra-Compact and Very Low Metallicity\n  Lyman-$$ Emitter Deep Within the Epoch of Reionization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Search of the First Stars: An Ultra-Compact and Very Low Metallicity\n  Lyman-$$ Emitter Deep Within the Epoch of Reionization"
                },
                "summary": "We present {\\it JWST} observations of a gravitationally-lensed, extremely\nmetal-poor galaxy at redshift $z=8.203\\pm 0.001$ from the CANUCS survey. Based\non the low oxygen to Balmer line ratios we infer a gas-phase metallicity of\n$12+{\\rm log(O/H)}=6.85$ (1.4\\% solar), making CANUCS-A370-z8-LAE the most\nmetal-poor galaxy known at $z>7$. With a high H$\\beta$ equivalent width of\n$225\\pm50$\\,\\AA\\ and a half-light radius of only $r_{\\rm hl} = 38 ^{+3}_{-19}\n$\\,pc, the galaxy has a high star-formation-rate density of $50 -\n100\\,M_{\\odot}$\\,yr$^{-1}$\\,kpc$^{-2}$. The galaxy shows high equivalent width\nLyman-$\\alpha$ emission with an inferred Lyman-$\\alpha$ escape fraction of\n$0.21 \\pm 0.05$. The high escape fraction of Lyman-$\\alpha$ is likely due to\nthe compact starbursting nature of the galaxy combined with its location in an\noverdensity traced by at least two other galaxies spectroscopically confirmed\nto lie within $\\delta z = 0.01$ that have helped to reionize the environment.\nThe low metallicity of CANUCS-A370-z8-LAE is best explained by a model where\ninfalling metal-poor gas dilutes the interstellar medium, rather than being a\nyoung galaxy forming its first stellar populations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present {\\it JWST} observations of a gravitationally-lensed, extremely\nmetal-poor galaxy at redshift $z=8.203\\pm 0.001$ from the CANUCS survey. Based\non the low oxygen to Balmer line ratios we infer a gas-phase metallicity of\n$12+{\\rm log(O/H)}=6.85$ (1.4\\% solar), making CANUCS-A370-z8-LAE the most\nmetal-poor galaxy known at $z>7$. With a high H$\\beta$ equivalent width of\n$225\\pm50$\\,\\AA\\ and a half-light radius of only $r_{\\rm hl} = 38 ^{+3}_{-19}\n$\\,pc, the galaxy has a high star-formation-rate density of $50 -\n100\\,M_{\\odot}$\\,yr$^{-1}$\\,kpc$^{-2}$. The galaxy shows high equivalent width\nLyman-$\\alpha$ emission with an inferred Lyman-$\\alpha$ escape fraction of\n$0.21 \\pm 0.05$. The high escape fraction of Lyman-$\\alpha$ is likely due to\nthe compact starbursting nature of the galaxy combined with its location in an\noverdensity traced by at least two other galaxies spectroscopically confirmed\nto lie within $\\delta z = 0.01$ that have helped to reionize the environment.\nThe low metallicity of CANUCS-A370-z8-LAE is best explained by a model where\ninfalling metal-poor gas dilutes the interstellar medium, rather than being a\nyoung galaxy forming its first stellar populations."
                },
                "authors": [
                    {
                        "name": "Chris J. Willott"
                    },
                    {
                        "name": "Yoshihisa Asada"
                    },
                    {
                        "name": "Kartheik G. Iyer"
                    },
                    {
                        "name": "Jon Judez"
                    },
                    {
                        "name": "Gregor Rihtarsic"
                    },
                    {
                        "name": "Nicholas S. Martis"
                    },
                    {
                        "name": "Ghassan T. E. Sarrouh"
                    },
                    {
                        "name": "Guillaume Desprez"
                    },
                    {
                        "name": "Anishya Harshan"
                    },
                    {
                        "name": "Lamiya Mowla"
                    },
                    {
                        "name": "Gael Noirot"
                    },
                    {
                        "name": "Giordano Felicioni"
                    },
                    {
                        "name": "Marusa Bradac"
                    },
                    {
                        "name": "Gabe Brammer"
                    },
                    {
                        "name": "Adam Muzzin"
                    },
                    {
                        "name": "Marcin Sawicki"
                    },
                    {
                        "name": "Jacqueline Antwi-Danso"
                    },
                    {
                        "name": "Vladan Markov"
                    },
                    {
                        "name": "Roberta Tripodi"
                    }
                ],
                "author_detail": {
                    "name": "Roberta Tripodi"
                },
                "author": "Roberta Tripodi",
                "arxiv_comment": "12 pages, 4 figures, ApJ submitted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07733v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07733v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07728v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07728v1",
                "updated": "2025-02-11T17:42:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    42,
                    7,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T17:42:07Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    42,
                    7,
                    1,
                    42,
                    0
                ],
                "title": "Verifying LLM-Generated Code in the Context of Software Verification\n  with Ada/SPARK",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verifying LLM-Generated Code in the Context of Software Verification\n  with Ada/SPARK"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable code generation\ncapabilities, but the correctness of the generated code cannot be inherently\ntrusted. This paper explores the feasibility of using formal software\nverification, specifically the SPARK framework for Ada, to ensure the\nreliability of LLM-generated code. We present Marmaragan, a tool that leverages\nan LLM in order to generate SPARK annotations for existing programs, enabling\nformal verification of the code. The tool is benchmarked on a curated set of\nSPARK programs, with annotations selectively removed to test specific\ncapabilities. The performance of Marmaragan with GPT-4o on the benchmark is\npromising, with correct annotations having been generated for 50.7% of the\nbenchmark cases. The results establish a foundation for future work on\ncombining the power of LLMs with the reliability of formal software\nverification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable code generation\ncapabilities, but the correctness of the generated code cannot be inherently\ntrusted. This paper explores the feasibility of using formal software\nverification, specifically the SPARK framework for Ada, to ensure the\nreliability of LLM-generated code. We present Marmaragan, a tool that leverages\nan LLM in order to generate SPARK annotations for existing programs, enabling\nformal verification of the code. The tool is benchmarked on a curated set of\nSPARK programs, with annotations selectively removed to test specific\ncapabilities. The performance of Marmaragan with GPT-4o on the benchmark is\npromising, with correct annotations having been generated for 50.7% of the\nbenchmark cases. The results establish a foundation for future work on\ncombining the power of LLMs with the reliability of formal software\nverification."
                },
                "authors": [
                    {
                        "name": "Marcos Cramer"
                    },
                    {
                        "name": "Lucian McIntyre"
                    }
                ],
                "author_detail": {
                    "name": "Lucian McIntyre"
                },
                "author": "Lucian McIntyre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07728v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07728v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07726v1",
                "updated": "2025-02-11T17:39:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    39,
                    54,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T17:39:54Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    39,
                    54,
                    1,
                    42,
                    0
                ],
                "title": "DeepVL: Dynamics and Inertial Measurements-based Deep Velocity Learning\n  for Underwater Odometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepVL: Dynamics and Inertial Measurements-based Deep Velocity Learning\n  for Underwater Odometry"
                },
                "summary": "This paper presents a learned model to predict the robot-centric velocity of\nan underwater robot through dynamics-aware proprioception. The method exploits\na recurrent neural network using as inputs inertial cues, motor commands, and\nbattery voltage readings alongside the hidden state of the previous time-step\nto output robust velocity estimates and their associated uncertainty. An\nensemble of networks is utilized to enhance the velocity and uncertainty\npredictions. Fusing the network's outputs into an Extended Kalman Filter,\nalongside inertial predictions and barometer updates, the method enables\nlong-term underwater odometry without further exteroception. Furthermore, when\nintegrated into visual-inertial odometry, the method assists in enhanced\nestimation resilience when dealing with an order of magnitude fewer total\nfeatures tracked (as few as 1) as compared to conventional visual-inertial\nsystems. Tested onboard an underwater robot deployed both in a laboratory pool\nand the Trondheim Fjord, the method takes less than 5ms for inference either on\nthe CPU or the GPU of an NVIDIA Orin AGX and demonstrates less than 4% relative\nposition error in novel trajectories during complete visual blackout, and\napproximately 2% relative error when a maximum of 2 visual features from a\nmonocular camera are available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a learned model to predict the robot-centric velocity of\nan underwater robot through dynamics-aware proprioception. The method exploits\na recurrent neural network using as inputs inertial cues, motor commands, and\nbattery voltage readings alongside the hidden state of the previous time-step\nto output robust velocity estimates and their associated uncertainty. An\nensemble of networks is utilized to enhance the velocity and uncertainty\npredictions. Fusing the network's outputs into an Extended Kalman Filter,\nalongside inertial predictions and barometer updates, the method enables\nlong-term underwater odometry without further exteroception. Furthermore, when\nintegrated into visual-inertial odometry, the method assists in enhanced\nestimation resilience when dealing with an order of magnitude fewer total\nfeatures tracked (as few as 1) as compared to conventional visual-inertial\nsystems. Tested onboard an underwater robot deployed both in a laboratory pool\nand the Trondheim Fjord, the method takes less than 5ms for inference either on\nthe CPU or the GPU of an NVIDIA Orin AGX and demonstrates less than 4% relative\nposition error in novel trajectories during complete visual blackout, and\napproximately 2% relative error when a maximum of 2 visual features from a\nmonocular camera are available."
                },
                "authors": [
                    {
                        "name": "Mohit Singh"
                    },
                    {
                        "name": "Kostas Alexis"
                    }
                ],
                "author_detail": {
                    "name": "Kostas Alexis"
                },
                "author": "Kostas Alexis",
                "arxiv_comment": "Accepted for presentation at the 2025 IEEE International Conference\n  on Robotics & Automation (ICRA 2025), Atlanta, USA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11779v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11779v2",
                "updated": "2025-02-11T17:36:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    36,
                    32,
                    1,
                    42,
                    0
                ],
                "published": "2025-01-20T23:10:13Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    23,
                    10,
                    13,
                    0,
                    20,
                    0
                ],
                "title": "Glinthawk: A Two-Tiered Architecture for Offline LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Glinthawk: A Two-Tiered Architecture for Offline LLM Inference"
                },
                "summary": "We introduce Glinthawk, an architecture for offline Large Language Model\n(LLM) inference. By leveraging a two-tiered structure, Glinthawk optimizes the\nutilization of the high-end accelerators (\"Tier 1\") by offloading the attention\nmechanism to lower-end compute tier (\"Tier 2\"). This separation allows the\nmemory demand of the attention, known as the key-value cache, to scale\nindependently from the model weights, enabling larger batch sizes and more\nefficient accelerator usage. Prototyped with NVIDIA T4 GPUs and standard CPU\nVMs, Glinthawk improves throughput by $5.9\\times$ and reduces cost of\ngeneration by $2.8\\times$, compared to paged attention baselines. For long\nsequence lengths, it achieves $16.3\\times$ throughput improvement at\n$2.4\\times$ less cost. Our evaluation shows that this architecture can tolerate\nmoderate network latency with minimal performance degradation, making it highly\neffective for latency-tolerant, throughput-focused applications such as batch\nprocessing. The prototype is publicly available at\nhttps://github.com/microsoft/glinthawk.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Glinthawk, an architecture for offline Large Language Model\n(LLM) inference. By leveraging a two-tiered structure, Glinthawk optimizes the\nutilization of the high-end accelerators (\"Tier 1\") by offloading the attention\nmechanism to lower-end compute tier (\"Tier 2\"). This separation allows the\nmemory demand of the attention, known as the key-value cache, to scale\nindependently from the model weights, enabling larger batch sizes and more\nefficient accelerator usage. Prototyped with NVIDIA T4 GPUs and standard CPU\nVMs, Glinthawk improves throughput by $5.9\\times$ and reduces cost of\ngeneration by $2.8\\times$, compared to paged attention baselines. For long\nsequence lengths, it achieves $16.3\\times$ throughput improvement at\n$2.4\\times$ less cost. Our evaluation shows that this architecture can tolerate\nmoderate network latency with minimal performance degradation, making it highly\neffective for latency-tolerant, throughput-focused applications such as batch\nprocessing. The prototype is publicly available at\nhttps://github.com/microsoft/glinthawk."
                },
                "authors": [
                    {
                        "name": "Pouya Hamadanian"
                    },
                    {
                        "name": "Sadjad Fouladi"
                    }
                ],
                "author_detail": {
                    "name": "Sadjad Fouladi"
                },
                "author": "Sadjad Fouladi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11779v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02806v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02806v3",
                "updated": "2025-02-11T17:29:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    29,
                    38,
                    1,
                    42,
                    0
                ],
                "published": "2024-08-05T19:49:59Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    19,
                    49,
                    59,
                    0,
                    218,
                    0
                ],
                "title": "Spontaneous and Induced Oscillations in Confined Epithelia",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spontaneous and Induced Oscillations in Confined Epithelia"
                },
                "summary": "The feedback between mechanical and chemical signals plays a key role in\ncontrolling many biological processes and collective cell behavior. Here we\nfocus on the emergence of spatiotemporal density waves in a one-dimensional\n\"cell train.\" Combining a minimal theoretical model with observations in an in\nvitro experimental system of MDCK epithelial cells confined to a linear\npattern, we examine the spontaneous oscillations driven by the feedback between\nmyosin activation and mechanical deformations and their effect on the response\nof the tissue to externally applied deformations. We show that the nature and\nfrequency of spontaneous oscillations is controlled by the size of the cell\ntrain, with a transition from size-dependent standing waves to intrinsic\nspontaneous waves at the natural frequency of the tissue. The response to\nexternal boundary perturbations exhibit a resonance at this natural frequency,\nproviding a possible venue for inferring the mechanochemical couplings that\ncontrol the tissue behavior from rheological experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The feedback between mechanical and chemical signals plays a key role in\ncontrolling many biological processes and collective cell behavior. Here we\nfocus on the emergence of spatiotemporal density waves in a one-dimensional\n\"cell train.\" Combining a minimal theoretical model with observations in an in\nvitro experimental system of MDCK epithelial cells confined to a linear\npattern, we examine the spontaneous oscillations driven by the feedback between\nmyosin activation and mechanical deformations and their effect on the response\nof the tissue to externally applied deformations. We show that the nature and\nfrequency of spontaneous oscillations is controlled by the size of the cell\ntrain, with a transition from size-dependent standing waves to intrinsic\nspontaneous waves at the natural frequency of the tissue. The response to\nexternal boundary perturbations exhibit a resonance at this natural frequency,\nproviding a possible venue for inferring the mechanochemical couplings that\ncontrol the tissue behavior from rheological experiments."
                },
                "authors": [
                    {
                        "name": "Toshi Parmar"
                    },
                    {
                        "name": "Liam P. Dow"
                    },
                    {
                        "name": "Beth L. Pruitt"
                    },
                    {
                        "name": "M. Cristina Marchetti"
                    }
                ],
                "author_detail": {
                    "name": "M. Cristina Marchetti"
                },
                "author": "M. Cristina Marchetti",
                "arxiv_doi": "10.1103/PRXLife.3.013002",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PRXLife.3.013002",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.02806v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02806v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 13 figures",
                "arxiv_journal_ref": "PRX Life 3, 2025, 013002",
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01582v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01582v2",
                "updated": "2025-02-11T17:20:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    20,
                    0,
                    1,
                    42,
                    0
                ],
                "published": "2024-11-03T14:16:07Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    14,
                    16,
                    7,
                    6,
                    308,
                    0
                ],
                "title": "Donald Trumps in the Virtual Polls: Simulating and Predicting Public\n  Opinions in Surveys Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Donald Trumps in the Virtual Polls: Simulating and Predicting Public\n  Opinions in Surveys Using Large Language Models"
                },
                "summary": "In recent years, large language models (LLMs) have attracted attention due to\ntheir ability to generate human-like text. As surveys and opinion polls remain\nkey tools for gauging public attitudes, there is increasing interest in\nassessing whether LLMs can accurately replicate human responses. This study\nexamines the potential of LLMs, specifically ChatGPT-4o, to replicate human\nresponses in large-scale surveys and to predict election outcomes based on\ndemographic data. Employing data from the World Values Survey (WVS) and the\nAmerican National Election Studies (ANES), we assess the LLM's performance in\ntwo key tasks: simulating human responses and forecasting U.S. election\nresults. In simulations, the LLM was tasked with generating synthetic responses\nfor various socio-cultural and trust-related questions, demonstrating notable\nalignment with human response patterns across U.S.-China samples, though with\nsome limitations on value-sensitive topics. In prediction tasks, the LLM was\nused to simulate voting behavior in past U.S. elections and predict the 2024\nelection outcome. Our findings show that the LLM replicates cultural\ndifferences effectively, exhibits in-sample predictive validity, and provides\nplausible out-of-sample forecasts, suggesting potential as a cost-effective\nsupplement for survey-based research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, large language models (LLMs) have attracted attention due to\ntheir ability to generate human-like text. As surveys and opinion polls remain\nkey tools for gauging public attitudes, there is increasing interest in\nassessing whether LLMs can accurately replicate human responses. This study\nexamines the potential of LLMs, specifically ChatGPT-4o, to replicate human\nresponses in large-scale surveys and to predict election outcomes based on\ndemographic data. Employing data from the World Values Survey (WVS) and the\nAmerican National Election Studies (ANES), we assess the LLM's performance in\ntwo key tasks: simulating human responses and forecasting U.S. election\nresults. In simulations, the LLM was tasked with generating synthetic responses\nfor various socio-cultural and trust-related questions, demonstrating notable\nalignment with human response patterns across U.S.-China samples, though with\nsome limitations on value-sensitive topics. In prediction tasks, the LLM was\nused to simulate voting behavior in past U.S. elections and predict the 2024\nelection outcome. Our findings show that the LLM replicates cultural\ndifferences effectively, exhibits in-sample predictive validity, and provides\nplausible out-of-sample forecasts, suggesting potential as a cost-effective\nsupplement for survey-based research."
                },
                "authors": [
                    {
                        "name": "Shapeng Jiang"
                    },
                    {
                        "name": "Lijia Wei"
                    },
                    {
                        "name": "Chen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chen Zhang"
                },
                "author": "Chen Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01582v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01582v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07709v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07709v1",
                "updated": "2025-02-11T17:08:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    8,
                    0,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T17:08:00Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    8,
                    0,
                    1,
                    42,
                    0
                ],
                "title": "MAGELLAN: Metacognitive predictions of learning progress guide autotelic\n  LLM agents in large goal spaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAGELLAN: Metacognitive predictions of learning progress guide autotelic\n  LLM agents in large goal spaces"
                },
                "summary": "Open-ended learning agents must efficiently prioritize goals in vast\npossibility spaces, focusing on those that maximize learning progress (LP).\nWhen such autotelic exploration is achieved by LLM agents trained with online\nRL in high-dimensional and evolving goal spaces, a key challenge for LP\nprediction is modeling one's own competence, a form of metacognitive\nmonitoring. Traditional approaches either require extensive sampling or rely on\nbrittle expert-defined goal groupings. We introduce MAGELLAN, a metacognitive\nframework that lets LLM agents learn to predict their competence and LP online.\nBy capturing semantic relationships between goals, MAGELLAN enables\nsample-efficient LP estimation and dynamic adaptation to evolving goal spaces\nthrough generalization. In an interactive learning environment, we show that\nMAGELLAN improves LP prediction efficiency and goal prioritization, being the\nonly method allowing the agent to fully master a large and evolving goal space.\nThese results demonstrate how augmenting LLM agents with a metacognitive\nability for LP predictions can effectively scale curriculum learning to\nopen-ended goal spaces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-ended learning agents must efficiently prioritize goals in vast\npossibility spaces, focusing on those that maximize learning progress (LP).\nWhen such autotelic exploration is achieved by LLM agents trained with online\nRL in high-dimensional and evolving goal spaces, a key challenge for LP\nprediction is modeling one's own competence, a form of metacognitive\nmonitoring. Traditional approaches either require extensive sampling or rely on\nbrittle expert-defined goal groupings. We introduce MAGELLAN, a metacognitive\nframework that lets LLM agents learn to predict their competence and LP online.\nBy capturing semantic relationships between goals, MAGELLAN enables\nsample-efficient LP estimation and dynamic adaptation to evolving goal spaces\nthrough generalization. In an interactive learning environment, we show that\nMAGELLAN improves LP prediction efficiency and goal prioritization, being the\nonly method allowing the agent to fully master a large and evolving goal space.\nThese results demonstrate how augmenting LLM agents with a metacognitive\nability for LP predictions can effectively scale curriculum learning to\nopen-ended goal spaces."
                },
                "authors": [
                    {
                        "name": "Loris Gaven"
                    },
                    {
                        "name": "Thomas Carta"
                    },
                    {
                        "name": "Clment Romac"
                    },
                    {
                        "name": "Cdric Colas"
                    },
                    {
                        "name": "Sylvain Lamprier"
                    },
                    {
                        "name": "Olivier Sigaud"
                    },
                    {
                        "name": "Pierre-Yves Oudeyer"
                    }
                ],
                "author_detail": {
                    "name": "Pierre-Yves Oudeyer"
                },
                "author": "Pierre-Yves Oudeyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07709v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07709v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07701v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07701v1",
                "updated": "2025-02-11T16:58:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    16,
                    58,
                    15,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T16:58:15Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    16,
                    58,
                    15,
                    1,
                    42,
                    0
                ],
                "title": "Magic 1-For-1: Generating One Minute Video Clips within One Minute",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Magic 1-For-1: Generating One Minute Video Clips within One Minute"
                },
                "summary": "In this technical report, we present Magic 1-For-1 (Magic141), an efficient\nvideo generation model with optimized memory consumption and inference latency.\nThe key idea is simple: factorize the text-to-video generation task into two\nseparate easier tasks for diffusion step distillation, namely text-to-image\ngeneration and image-to-video generation. We verify that with the same\noptimization algorithm, the image-to-video task is indeed easier to converge\nover the text-to-video task. We also explore a bag of optimization tricks to\nreduce the computational cost of training the image-to-video (I2V) models from\nthree aspects: 1) model convergence speedup by using a multi-modal prior\ncondition injection; 2) inference latency speed up by applying an adversarial\nstep distillation, and 3) inference memory cost optimization with parameter\nsparsification. With those techniques, we are able to generate 5-second video\nclips within 3 seconds. By applying a test time sliding window, we are able to\ngenerate a minute-long video within one minute with significantly improved\nvisual quality and motion dynamics, spending less than 1 second for generating\n1 second video clips on average. We conduct a series of preliminary\nexplorations to find out the optimal tradeoff between computational cost and\nvideo quality during diffusion step distillation and hope this could be a good\nfoundation model for open-source explorations. The code and the model weights\nare available at https://github.com/DA-Group-PKU/Magic-1-For-1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this technical report, we present Magic 1-For-1 (Magic141), an efficient\nvideo generation model with optimized memory consumption and inference latency.\nThe key idea is simple: factorize the text-to-video generation task into two\nseparate easier tasks for diffusion step distillation, namely text-to-image\ngeneration and image-to-video generation. We verify that with the same\noptimization algorithm, the image-to-video task is indeed easier to converge\nover the text-to-video task. We also explore a bag of optimization tricks to\nreduce the computational cost of training the image-to-video (I2V) models from\nthree aspects: 1) model convergence speedup by using a multi-modal prior\ncondition injection; 2) inference latency speed up by applying an adversarial\nstep distillation, and 3) inference memory cost optimization with parameter\nsparsification. With those techniques, we are able to generate 5-second video\nclips within 3 seconds. By applying a test time sliding window, we are able to\ngenerate a minute-long video within one minute with significantly improved\nvisual quality and motion dynamics, spending less than 1 second for generating\n1 second video clips on average. We conduct a series of preliminary\nexplorations to find out the optimal tradeoff between computational cost and\nvideo quality during diffusion step distillation and hope this could be a good\nfoundation model for open-source explorations. The code and the model weights\nare available at https://github.com/DA-Group-PKU/Magic-1-For-1."
                },
                "authors": [
                    {
                        "name": "Hongwei Yi"
                    },
                    {
                        "name": "Shitong Shao"
                    },
                    {
                        "name": "Tian Ye"
                    },
                    {
                        "name": "Jiantong Zhao"
                    },
                    {
                        "name": "Qingyu Yin"
                    },
                    {
                        "name": "Michael Lingelbach"
                    },
                    {
                        "name": "Li Yuan"
                    },
                    {
                        "name": "Yonghong Tian"
                    },
                    {
                        "name": "Enze Xie"
                    },
                    {
                        "name": "Daquan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Daquan Zhou"
                },
                "author": "Daquan Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07701v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07701v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07699v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07699v1",
                "updated": "2025-02-11T16:51:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    16,
                    51,
                    25,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T16:51:25Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    16,
                    51,
                    25,
                    1,
                    42,
                    0
                ],
                "title": "Sharp Anti-Concentration Inequalities for Extremum Statistics via\n  Copulas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sharp Anti-Concentration Inequalities for Extremum Statistics via\n  Copulas"
                },
                "summary": "We derive sharp upper and lower bounds for the pointwise concentration\nfunction of the maximum statistic of $d$ identically distributed real-valued\nrandom variables. Our first main result places no restrictions either on the\ncommon marginal law of the samples or on the copula describing their joint\ndistribution. We show that, in general, strictly sublinear dependence of the\nconcentration function on the dimension $d$ is not possible. We then introduce\na new class of copulas, namely those with a convex diagonal section, and\ndemonstrate that restricting to this class yields a sharper upper bound on the\nconcentration function. This allows us to establish several new\ndimension-independent and poly-logarithmic-in-$d$ anti-concentration\ninequalities for a variety of marginal distributions under mild dependence\nassumptions. Our theory improves upon the best known results in certain special\ncases. Applications to high-dimensional statistical inference are presented,\nincluding a specific example pertaining to Gaussian mixture approximations for\nfactor models, for which our main results lead to superior distributional\nguarantees.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We derive sharp upper and lower bounds for the pointwise concentration\nfunction of the maximum statistic of $d$ identically distributed real-valued\nrandom variables. Our first main result places no restrictions either on the\ncommon marginal law of the samples or on the copula describing their joint\ndistribution. We show that, in general, strictly sublinear dependence of the\nconcentration function on the dimension $d$ is not possible. We then introduce\na new class of copulas, namely those with a convex diagonal section, and\ndemonstrate that restricting to this class yields a sharper upper bound on the\nconcentration function. This allows us to establish several new\ndimension-independent and poly-logarithmic-in-$d$ anti-concentration\ninequalities for a variety of marginal distributions under mild dependence\nassumptions. Our theory improves upon the best known results in certain special\ncases. Applications to high-dimensional statistical inference are presented,\nincluding a specific example pertaining to Gaussian mixture approximations for\nfactor models, for which our main results lead to superior distributional\nguarantees."
                },
                "authors": [
                    {
                        "name": "Matias D. Cattaneo"
                    },
                    {
                        "name": "Ricardo P. Masini"
                    },
                    {
                        "name": "William G. Underwood"
                    }
                ],
                "author_detail": {
                    "name": "William G. Underwood"
                },
                "author": "William G. Underwood",
                "arxiv_comment": "22 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07699v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07699v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "60E15 (Primary) 62H05, 62G32 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07698v1",
                "updated": "2025-02-11T16:51:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    16,
                    51,
                    11,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T16:51:11Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    16,
                    51,
                    11,
                    1,
                    42,
                    0
                ],
                "title": "A Framework for LLM-powered Design Assistants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for LLM-powered Design Assistants"
                },
                "summary": "Design assistants are frameworks, tools or applications intended to\nfacilitate both the creative and technical facets of design processes. Large\nlanguage models (LLMs) are AI systems engineered to analyze and produce text\nresembling human language, leveraging extensive datasets. This study introduces\na framework wherein LLMs are employed as Design Assistants, focusing on three\nkey modalities within the Design Process: Idea Exploration, Dialogue with\nDesigners, and Design Evaluation. Importantly, our framework is not confined to\na singular design process but is adaptable across various processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design assistants are frameworks, tools or applications intended to\nfacilitate both the creative and technical facets of design processes. Large\nlanguage models (LLMs) are AI systems engineered to analyze and produce text\nresembling human language, leveraging extensive datasets. This study introduces\na framework wherein LLMs are employed as Design Assistants, focusing on three\nkey modalities within the Design Process: Idea Exploration, Dialogue with\nDesigners, and Design Evaluation. Importantly, our framework is not confined to\na singular design process but is adaptable across various processes."
                },
                "authors": [
                    {
                        "name": "Swaroop Panda"
                    }
                ],
                "author_detail": {
                    "name": "Swaroop Panda"
                },
                "author": "Swaroop Panda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12009v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12009v3",
                "updated": "2025-02-11T16:49:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    16,
                    49,
                    17,
                    1,
                    42,
                    0
                ],
                "published": "2024-06-17T18:25:02Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    18,
                    25,
                    2,
                    0,
                    169,
                    0
                ],
                "title": "FinTruthQA: A Benchmark Dataset for Evaluating the Quality of Financial\n  Information Disclosure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FinTruthQA: A Benchmark Dataset for Evaluating the Quality of Financial\n  Information Disclosure"
                },
                "summary": "Accurate and transparent financial information disclosure is essential in\naccounting and finance, fostering trust and enabling informed investment\ndecisions that drive economic development. Among many information disclosure\nplatforms, the Chinese stock exchanges' investor interactive platform provides\na novel and interactive way for listed firms to disclose information of\ninterest to investors through an online question-and-answer (Q&A) format.\nHowever, it is common for listed firms to respond to questions with limited or\nno substantive information, and automatically evaluating the quality of\nfinancial information disclosure on large amounts of Q&A pairs is challenging.\nIn this study, our interdisciplinary team of AI and finance professionals\nproposed FinTruthQA, a benchmark designed to evaluate advanced natural language\nprocessing (NLP) techniques for the automatic quality assessment of information\ndisclosure in financial Q&A data. It comprises 6,000 real-world financial Q&A\nentries and each Q&A was manually annotated based on four key evaluation\ncriteria. We benchmarked various NLP techniques on FinTruthQA, including large\nlanguage models(LLMs). Experiments showed that existing NLP models have strong\npredictive ability for question identification and question relevance tasks,\nbut are suboptimal for answer readability and answer relevance tasks. By\nestablishing this benchmark, we provide a robust foundation for the automatic\nevaluation of information disclosure, demonstrating how AI can be leveraged for\nsocial good by promoting transparency, fairness, and investor protection in\nfinancial disclosure practices. FinTruthQA can be used by auditors, regulators,\nand financial analysts for real-time monitoring and data-driven\ndecision-making, as well as by researchers for advanced studies in accounting\nand finance, ultimately fostering greater trust and efficiency in the financial\nmarkets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate and transparent financial information disclosure is essential in\naccounting and finance, fostering trust and enabling informed investment\ndecisions that drive economic development. Among many information disclosure\nplatforms, the Chinese stock exchanges' investor interactive platform provides\na novel and interactive way for listed firms to disclose information of\ninterest to investors through an online question-and-answer (Q&A) format.\nHowever, it is common for listed firms to respond to questions with limited or\nno substantive information, and automatically evaluating the quality of\nfinancial information disclosure on large amounts of Q&A pairs is challenging.\nIn this study, our interdisciplinary team of AI and finance professionals\nproposed FinTruthQA, a benchmark designed to evaluate advanced natural language\nprocessing (NLP) techniques for the automatic quality assessment of information\ndisclosure in financial Q&A data. It comprises 6,000 real-world financial Q&A\nentries and each Q&A was manually annotated based on four key evaluation\ncriteria. We benchmarked various NLP techniques on FinTruthQA, including large\nlanguage models(LLMs). Experiments showed that existing NLP models have strong\npredictive ability for question identification and question relevance tasks,\nbut are suboptimal for answer readability and answer relevance tasks. By\nestablishing this benchmark, we provide a robust foundation for the automatic\nevaluation of information disclosure, demonstrating how AI can be leveraged for\nsocial good by promoting transparency, fairness, and investor protection in\nfinancial disclosure practices. FinTruthQA can be used by auditors, regulators,\nand financial analysts for real-time monitoring and data-driven\ndecision-making, as well as by researchers for advanced studies in accounting\nand finance, ultimately fostering greater trust and efficiency in the financial\nmarkets."
                },
                "authors": [
                    {
                        "name": "Ziyue Xu"
                    },
                    {
                        "name": "Peilin Zhou"
                    },
                    {
                        "name": "Xinyu Shi"
                    },
                    {
                        "name": "Jiageng Wu"
                    },
                    {
                        "name": "Yikang Jiang"
                    },
                    {
                        "name": "Dading Chong"
                    },
                    {
                        "name": "Bin Ke"
                    },
                    {
                        "name": "Jie Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Yang"
                },
                "author": "Jie Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12009v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12009v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13921v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13921v3",
                "updated": "2025-02-11T16:48:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    16,
                    48,
                    15,
                    1,
                    42,
                    0
                ],
                "published": "2025-01-23T18:59:02Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    18,
                    59,
                    2,
                    3,
                    23,
                    0
                ],
                "title": "The Breeze 2 Herd of Models: Traditional Chinese LLMs Based on Llama\n  with Vision-Aware and Function-Calling Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Breeze 2 Herd of Models: Traditional Chinese LLMs Based on Llama\n  with Vision-Aware and Function-Calling Capabilities"
                },
                "summary": "Llama-Breeze2 (hereinafter referred to as Breeze2) is a suite of advanced\nmulti-modal language models, available in 3B and 8B parameter configurations,\nspecifically designed to enhance Traditional Chinese language representation.\nBuilding upon the Llama 3.2 model family, we continue the pre-training of\nBreeze2 on an extensive corpus to enhance the linguistic and cultural heritage\nof Traditional Chinese. In addition to language modeling capabilities, we\nsignificantly augment the models with function calling and vision understanding\ncapabilities. At the time of this publication, as far as we are aware, absent\nreasoning-inducing prompts, Breeze2 are the strongest performing models in\nTraditional Chinese function calling and image understanding in its size class.\nThe effectiveness of Breeze2 is benchmarked across various tasks, including\nTaiwan general knowledge, instruction-following, long context, function\ncalling, and vision understanding. We are publicly releasing all Breeze2 models\nunder the Llama 3.2 Community License. We also showcase the capabilities of the\nmodel running on mobile platform with a mobile application which we also open\nsource.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Llama-Breeze2 (hereinafter referred to as Breeze2) is a suite of advanced\nmulti-modal language models, available in 3B and 8B parameter configurations,\nspecifically designed to enhance Traditional Chinese language representation.\nBuilding upon the Llama 3.2 model family, we continue the pre-training of\nBreeze2 on an extensive corpus to enhance the linguistic and cultural heritage\nof Traditional Chinese. In addition to language modeling capabilities, we\nsignificantly augment the models with function calling and vision understanding\ncapabilities. At the time of this publication, as far as we are aware, absent\nreasoning-inducing prompts, Breeze2 are the strongest performing models in\nTraditional Chinese function calling and image understanding in its size class.\nThe effectiveness of Breeze2 is benchmarked across various tasks, including\nTaiwan general knowledge, instruction-following, long context, function\ncalling, and vision understanding. We are publicly releasing all Breeze2 models\nunder the Llama 3.2 Community License. We also showcase the capabilities of the\nmodel running on mobile platform with a mobile application which we also open\nsource."
                },
                "authors": [
                    {
                        "name": "MediaTek Research"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "Chan-Jan Hsu"
                    },
                    {
                        "name": "Chia-Sheng Liu"
                    },
                    {
                        "name": "Meng-Hsi Chen"
                    },
                    {
                        "name": "Muxi Chen"
                    },
                    {
                        "name": "Po-Chun Hsu"
                    },
                    {
                        "name": "Yi-Chang Chen"
                    },
                    {
                        "name": "Da-Shan Shiu"
                    }
                ],
                "author_detail": {
                    "name": "Da-Shan Shiu"
                },
                "author": "Da-Shan Shiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13921v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13921v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07691v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07691v1",
                "updated": "2025-02-11T16:44:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    16,
                    44,
                    52,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T16:44:52Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    16,
                    44,
                    52,
                    1,
                    42,
                    0
                ],
                "title": "Time-resolved second-order autocorrelation function of parametric\n  downconversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-resolved second-order autocorrelation function of parametric\n  downconversion"
                },
                "summary": "We study a possibility of measuring the time-resolved second-order\nautocorrelation function of one of two beams generated in type-II parametric\ndownconversion by means of temporal magnification of this beam, bringing its\ncorrelation time from the picosecond to the nanosecond scale, which can be\nresolved by modern photodetectors. We show that such a measurement enables one\nto infer directly the degree of global coherence of that beam, which is linked\nby a simple relation to the number of modes characterizing the entanglement\nbetween the two generated beams. We illustrate the proposed method by an\nexample of photon pairs generated in a periodically poled KTP crystal with a\nsymmetric group velocity matching for various durations of the pump pulse,\nresulting in different numbers of modes. Our theoretical model also shows that\nthe magnified double-heralded autocorrelation function of one beam exhibits a\nlocal maximum around zero delay time, corresponding to photon bunching at a\nshort time scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study a possibility of measuring the time-resolved second-order\nautocorrelation function of one of two beams generated in type-II parametric\ndownconversion by means of temporal magnification of this beam, bringing its\ncorrelation time from the picosecond to the nanosecond scale, which can be\nresolved by modern photodetectors. We show that such a measurement enables one\nto infer directly the degree of global coherence of that beam, which is linked\nby a simple relation to the number of modes characterizing the entanglement\nbetween the two generated beams. We illustrate the proposed method by an\nexample of photon pairs generated in a periodically poled KTP crystal with a\nsymmetric group velocity matching for various durations of the pump pulse,\nresulting in different numbers of modes. Our theoretical model also shows that\nthe magnified double-heralded autocorrelation function of one beam exhibits a\nlocal maximum around zero delay time, corresponding to photon bunching at a\nshort time scale."
                },
                "authors": [
                    {
                        "name": "D. B. Horoshko"
                    },
                    {
                        "name": "S. Srivastava"
                    },
                    {
                        "name": "F. Sonicki"
                    },
                    {
                        "name": "M. Mikoajczyk"
                    },
                    {
                        "name": "M. Karpiski"
                    },
                    {
                        "name": "B. Brecht"
                    },
                    {
                        "name": "M. I. Kolobov"
                    }
                ],
                "author_detail": {
                    "name": "M. I. Kolobov"
                },
                "author": "M. I. Kolobov",
                "arxiv_comment": "14 pages, 6 figures. arXiv admin note: text overlap with\n  arXiv:2310.11918",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07691v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07691v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07687v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07687v1",
                "updated": "2025-02-11T16:38:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    16,
                    38,
                    16,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T16:38:16Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    16,
                    38,
                    16,
                    1,
                    42,
                    0
                ],
                "title": "Large Language Models as Proxies for Theories of Human Linguistic\n  Cognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models as Proxies for Theories of Human Linguistic\n  Cognition"
                },
                "summary": "We consider the possible role of current large language models (LLMs) in the\nstudy of human linguistic cognition. We focus on the use of such models as\nproxies for theories of cognition that are relatively linguistically-neutral in\ntheir representations and learning but differ from current LLMs in key ways. We\nillustrate this potential use of LLMs as proxies for theories of cognition in\nthe context of two kinds of questions: (a) whether the target theory accounts\nfor the acquisition of a given pattern from a given corpus; and (b) whether the\ntarget theory makes a given typologically-attested pattern easier to acquire\nthan another, typologically-unattested pattern. For each of the two questions\nwe show, building on recent literature, how current LLMs can potentially be of\nhelp, but we note that at present this help is quite limited.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the possible role of current large language models (LLMs) in the\nstudy of human linguistic cognition. We focus on the use of such models as\nproxies for theories of cognition that are relatively linguistically-neutral in\ntheir representations and learning but differ from current LLMs in key ways. We\nillustrate this potential use of LLMs as proxies for theories of cognition in\nthe context of two kinds of questions: (a) whether the target theory accounts\nfor the acquisition of a given pattern from a given corpus; and (b) whether the\ntarget theory makes a given typologically-attested pattern easier to acquire\nthan another, typologically-unattested pattern. For each of the two questions\nwe show, building on recent literature, how current LLMs can potentially be of\nhelp, but we note that at present this help is quite limited."
                },
                "authors": [
                    {
                        "name": "Imry Ziv"
                    },
                    {
                        "name": "Nur Lan"
                    },
                    {
                        "name": "Emmanuel Chemla"
                    },
                    {
                        "name": "Roni Katzir"
                    }
                ],
                "author_detail": {
                    "name": "Roni Katzir"
                },
                "author": "Roni Katzir",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07687v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07687v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12645v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12645v3",
                "updated": "2025-02-11T16:36:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    16,
                    36,
                    32,
                    1,
                    42,
                    0
                ],
                "published": "2024-06-18T14:13:13Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    14,
                    13,
                    13,
                    1,
                    170,
                    0
                ],
                "title": "Evaluating Evidence Attribution in Generated Fact Checking Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Evidence Attribution in Generated Fact Checking Explanations"
                },
                "summary": "Automated fact-checking systems often struggle with trustworthiness, as their\ngenerated explanations can include hallucinations. In this work, we explore\nevidence attribution for fact-checking explanation generation. We introduce a\nnovel evaluation protocol -- citation masking and recovery -- to assess\nattribution quality in generated explanations. We implement our protocol using\nboth human annotators and automatic annotators, and find that LLM annotation\ncorrelates with human annotation, suggesting that attribution assessment can be\nautomated. Finally, our experiments reveal that: (1) the best-performing LLMs\nstill generate explanations with inaccurate attributions; and (2) human-curated\nevidence is essential for generating better explanations. Code and data are\navailable here: https://github.com/ruixing76/Transparent-FCExp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated fact-checking systems often struggle with trustworthiness, as their\ngenerated explanations can include hallucinations. In this work, we explore\nevidence attribution for fact-checking explanation generation. We introduce a\nnovel evaluation protocol -- citation masking and recovery -- to assess\nattribution quality in generated explanations. We implement our protocol using\nboth human annotators and automatic annotators, and find that LLM annotation\ncorrelates with human annotation, suggesting that attribution assessment can be\nautomated. Finally, our experiments reveal that: (1) the best-performing LLMs\nstill generate explanations with inaccurate attributions; and (2) human-curated\nevidence is essential for generating better explanations. Code and data are\navailable here: https://github.com/ruixing76/Transparent-FCExp."
                },
                "authors": [
                    {
                        "name": "Rui Xing"
                    },
                    {
                        "name": "Timothy Baldwin"
                    },
                    {
                        "name": "Jey Han Lau"
                    }
                ],
                "author_detail": {
                    "name": "Jey Han Lau"
                },
                "author": "Jey Han Lau",
                "arxiv_comment": "Accepted to NAACL 2025 Main",
                "arxiv_journal_ref": "NAACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12645v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12645v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07677v1",
                "updated": "2025-02-11T16:27:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    16,
                    27,
                    28,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T16:27:28Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    16,
                    27,
                    28,
                    1,
                    42,
                    0
                ],
                "title": "Auto-Drafting Police Reports from Noisy ASR Outputs: A Trust-Centered\n  LLM Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-Drafting Police Reports from Noisy ASR Outputs: A Trust-Centered\n  LLM Approach"
                },
                "summary": "Achieving a delicate balance between fostering trust in law en- forcement and\nprotecting the rights of both officers and civilians continues to emerge as a\npressing research and product challenge in the world today. In the pursuit of\nfairness and transparency, this study presents an innovative AI-driven system\ndesigned to generate police report drafts from complex, noisy, and multi-role\ndialogue data. Our approach intelligently extracts key elements of law\nenforcement interactions and includes them in the draft, producing structured\nnarratives that are not only high in quality but also reinforce accountability\nand procedural clarity. This frame- work holds the potential to transform the\nreporting process, ensur- ing greater oversight, consistency, and fairness in\nfuture policing practices. A demonstration video of our system can be accessed\nat https://drive.google.com/file/d/1kBrsGGR8e3B5xPSblrchRGj-\nY-kpCHNO/view?usp=sharing",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achieving a delicate balance between fostering trust in law en- forcement and\nprotecting the rights of both officers and civilians continues to emerge as a\npressing research and product challenge in the world today. In the pursuit of\nfairness and transparency, this study presents an innovative AI-driven system\ndesigned to generate police report drafts from complex, noisy, and multi-role\ndialogue data. Our approach intelligently extracts key elements of law\nenforcement interactions and includes them in the draft, producing structured\nnarratives that are not only high in quality but also reinforce accountability\nand procedural clarity. This frame- work holds the potential to transform the\nreporting process, ensur- ing greater oversight, consistency, and fairness in\nfuture policing practices. A demonstration video of our system can be accessed\nat https://drive.google.com/file/d/1kBrsGGR8e3B5xPSblrchRGj-\nY-kpCHNO/view?usp=sharing"
                },
                "authors": [
                    {
                        "name": "Param Kulkarni"
                    },
                    {
                        "name": "Yingchi Liu"
                    },
                    {
                        "name": "Hao-Ming Fu"
                    },
                    {
                        "name": "Shaohua Yang"
                    },
                    {
                        "name": "Isuru Gunasekara"
                    },
                    {
                        "name": "Matt Peloquin"
                    },
                    {
                        "name": "Noah Spitzer-Williams"
                    },
                    {
                        "name": "Xiaotian Zhou"
                    },
                    {
                        "name": "Xiaozhong Liu"
                    },
                    {
                        "name": "Zhengping Ji"
                    },
                    {
                        "name": "Yasser Ibrahim"
                    }
                ],
                "author_detail": {
                    "name": "Yasser Ibrahim"
                },
                "author": "Yasser Ibrahim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05907v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05907v2",
                "updated": "2025-02-11T16:22:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    16,
                    22,
                    45,
                    1,
                    42,
                    0
                ],
                "published": "2024-09-06T15:47:40Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    15,
                    47,
                    40,
                    4,
                    250,
                    0
                ],
                "title": "Programming Refusal with Conditional Activation Steering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Programming Refusal with Conditional Activation Steering"
                },
                "summary": "LLMs have shown remarkable capabilities, but precisely controlling their\nresponse behavior remains challenging. Existing activation steering methods\nalter LLM behavior indiscriminately, limiting their practical applicability in\nsettings where selective responses are essential, such as content moderation or\ndomain-specific assistants. In this paper, we propose Conditional Activation\nSteering (CAST), which analyzes LLM activation patterns during inference to\nselectively apply or withhold activation steering based on the input context.\nOur method is based on the observation that different categories of prompts\nactivate distinct patterns in the model's hidden states. Using CAST, one can\nsystematically control LLM behavior with rules like \"if input is about hate\nspeech or adult content, then refuse\" or \"if input is not about legal advice,\nthen refuse.\" This allows for selective modification of responses to specific\ncontent while maintaining normal responses to other content, all without\nrequiring weight optimization. We release an open-source implementation of our\nframework at <github.com/IBM/activation-steering>.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have shown remarkable capabilities, but precisely controlling their\nresponse behavior remains challenging. Existing activation steering methods\nalter LLM behavior indiscriminately, limiting their practical applicability in\nsettings where selective responses are essential, such as content moderation or\ndomain-specific assistants. In this paper, we propose Conditional Activation\nSteering (CAST), which analyzes LLM activation patterns during inference to\nselectively apply or withhold activation steering based on the input context.\nOur method is based on the observation that different categories of prompts\nactivate distinct patterns in the model's hidden states. Using CAST, one can\nsystematically control LLM behavior with rules like \"if input is about hate\nspeech or adult content, then refuse\" or \"if input is not about legal advice,\nthen refuse.\" This allows for selective modification of responses to specific\ncontent while maintaining normal responses to other content, all without\nrequiring weight optimization. We release an open-source implementation of our\nframework at <github.com/IBM/activation-steering>."
                },
                "authors": [
                    {
                        "name": "Bruce W. Lee"
                    },
                    {
                        "name": "Inkit Padhi"
                    },
                    {
                        "name": "Karthikeyan Natesan Ramamurthy"
                    },
                    {
                        "name": "Erik Miehling"
                    },
                    {
                        "name": "Pierre Dognin"
                    },
                    {
                        "name": "Manish Nagireddy"
                    },
                    {
                        "name": "Amit Dhurandhar"
                    }
                ],
                "author_detail": {
                    "name": "Amit Dhurandhar"
                },
                "author": "Amit Dhurandhar",
                "arxiv_comment": "ICLR 2025, Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05907v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05907v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08041v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08041v2",
                "updated": "2025-02-11T16:22:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    16,
                    22,
                    24,
                    1,
                    42,
                    0
                ],
                "published": "2024-12-11T02:44:14Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    2,
                    44,
                    14,
                    2,
                    346,
                    0
                ],
                "title": "Quantifying the benefits of code hints for refactoring deprecated Java\n  APIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying the benefits of code hints for refactoring deprecated Java\n  APIs"
                },
                "summary": "When done manually, refactoring legacy code in order to eliminate uses of\ndeprecated APIs is an error-prone and time-consuming process. In this paper, we\ninvestigate to which degree refactorings for deprecated Java APIs can be\nautomated, and quantify the benefit of Javadoc code hints for this task. To\nthis end, we build a symbolic and a neural engine for the automatic refactoring\nof deprecated APIs. The former is based on type-directed and component-based\nprogram synthesis, whereas the latter uses LLMs. We applied our engines to\nrefactor the deprecated methods in the Oracle JDK 15. Our experiments show that\ncode hints are enabling for the automation of this task: even the worst engine\ncorrectly refactors 71% of the tasks with code hints, which drops to at best\n14% on tasks without. Adding more code hints to Javadoc can hence boost the\nrefactoring of code that uses deprecated APIs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When done manually, refactoring legacy code in order to eliminate uses of\ndeprecated APIs is an error-prone and time-consuming process. In this paper, we\ninvestigate to which degree refactorings for deprecated Java APIs can be\nautomated, and quantify the benefit of Javadoc code hints for this task. To\nthis end, we build a symbolic and a neural engine for the automatic refactoring\nof deprecated APIs. The former is based on type-directed and component-based\nprogram synthesis, whereas the latter uses LLMs. We applied our engines to\nrefactor the deprecated methods in the Oracle JDK 15. Our experiments show that\ncode hints are enabling for the automation of this task: even the worst engine\ncorrectly refactors 71% of the tasks with code hints, which drops to at best\n14% on tasks without. Adding more code hints to Javadoc can hence boost the\nrefactoring of code that uses deprecated APIs."
                },
                "authors": [
                    {
                        "name": "Cristina David"
                    },
                    {
                        "name": "Pascal Kesseli"
                    },
                    {
                        "name": "Daniel Kroening"
                    },
                    {
                        "name": "Hanliang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hanliang Zhang"
                },
                "author": "Hanliang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08041v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08041v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05670v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05670v2",
                "updated": "2025-02-11T16:02:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    16,
                    2,
                    57,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-08T19:13:40Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    19,
                    13,
                    40,
                    5,
                    39,
                    0
                ],
                "title": "Language Models Largely Exhibit Human-like Constituent Ordering\n  Preferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models Largely Exhibit Human-like Constituent Ordering\n  Preferences"
                },
                "summary": "Though English sentences are typically inflexible vis-\\`a-vis word order,\nconstituents often show far more variability in ordering. One prominent theory\npresents the notion that constituent ordering is directly correlated with\nconstituent weight: a measure of the constituent's length or complexity. Such\ntheories are interesting in the context of natural language processing (NLP),\nbecause while recent advances in NLP have led to significant gains in the\nperformance of large language models (LLMs), much remains unclear about how\nthese models process language, and how this compares to human language\nprocessing. In particular, the question remains whether LLMs display the same\npatterns with constituent movement, and may provide insights into existing\ntheories on when and how the shift occurs in human language. We compare a\nvariety of LLMs with diverse properties to evaluate broad LLM performance on\nfour types of constituent movement: heavy NP shift, particle movement, dative\nalternation, and multiple PPs. Despite performing unexpectedly around particle\nmovement, LLMs generally align with human preferences around constituent\nordering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Though English sentences are typically inflexible vis-\\`a-vis word order,\nconstituents often show far more variability in ordering. One prominent theory\npresents the notion that constituent ordering is directly correlated with\nconstituent weight: a measure of the constituent's length or complexity. Such\ntheories are interesting in the context of natural language processing (NLP),\nbecause while recent advances in NLP have led to significant gains in the\nperformance of large language models (LLMs), much remains unclear about how\nthese models process language, and how this compares to human language\nprocessing. In particular, the question remains whether LLMs display the same\npatterns with constituent movement, and may provide insights into existing\ntheories on when and how the shift occurs in human language. We compare a\nvariety of LLMs with diverse properties to evaluate broad LLM performance on\nfour types of constituent movement: heavy NP shift, particle movement, dative\nalternation, and multiple PPs. Despite performing unexpectedly around particle\nmovement, LLMs generally align with human preferences around constituent\nordering."
                },
                "authors": [
                    {
                        "name": "Ada Defne Tur"
                    },
                    {
                        "name": "Gaurav Kamath"
                    },
                    {
                        "name": "Siva Reddy"
                    }
                ],
                "author_detail": {
                    "name": "Siva Reddy"
                },
                "author": "Siva Reddy",
                "arxiv_comment": "NAACL 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05670v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05670v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06556v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06556v2",
                "updated": "2025-02-11T15:48:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    48,
                    42,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-10T15:24:30Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    24,
                    30,
                    0,
                    41,
                    0
                ],
                "title": "ProjectTest: A Project-level LLM Unit Test Generation Benchmark and\n  Impact of Error Fixing Mechanisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProjectTest: A Project-level LLM Unit Test Generation Benchmark and\n  Impact of Error Fixing Mechanisms"
                },
                "summary": "Unit test generation has become a promising and important use case of LLMs.\nHowever, existing evaluation benchmarks for assessing LLM unit test generation\ncapabilities focus on function- or class-level code rather than more practical\nand challenging project-level codebases. To address such limitation, we propose\nProjectTest, a project-level benchmark for unit test generation covering\nPython, Java, and JavaScript. ProjectTest features 20 moderate-sized and\nhigh-quality projects per language. We evaluate nine frontier LLMs on\nProjectTest and the results show that all frontier LLMs tested exhibit moderate\nperformance on ProjectTest on Python and Java, highlighting the difficulty of\nProjectTest. We also conduct a thorough error analysis, which shows that even\nfrontier LLMs, such as Claude-3.5-Sonnet, have significant simple errors,\nincluding compilation and cascade errors. Motivated by this observation, we\nfurther evaluate all frontier LLMs under manual error-fixing and\nself-error-fixing scenarios to assess their potential when equipped with\nerror-fixing mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unit test generation has become a promising and important use case of LLMs.\nHowever, existing evaluation benchmarks for assessing LLM unit test generation\ncapabilities focus on function- or class-level code rather than more practical\nand challenging project-level codebases. To address such limitation, we propose\nProjectTest, a project-level benchmark for unit test generation covering\nPython, Java, and JavaScript. ProjectTest features 20 moderate-sized and\nhigh-quality projects per language. We evaluate nine frontier LLMs on\nProjectTest and the results show that all frontier LLMs tested exhibit moderate\nperformance on ProjectTest on Python and Java, highlighting the difficulty of\nProjectTest. We also conduct a thorough error analysis, which shows that even\nfrontier LLMs, such as Claude-3.5-Sonnet, have significant simple errors,\nincluding compilation and cascade errors. Motivated by this observation, we\nfurther evaluate all frontier LLMs under manual error-fixing and\nself-error-fixing scenarios to assess their potential when equipped with\nerror-fixing mechanisms."
                },
                "authors": [
                    {
                        "name": "Yibo Wang"
                    },
                    {
                        "name": "Congying Xia"
                    },
                    {
                        "name": "Wenting Zhao"
                    },
                    {
                        "name": "Jiangshu Du"
                    },
                    {
                        "name": "Chunyu Miao"
                    },
                    {
                        "name": "Zhongfen Deng"
                    },
                    {
                        "name": "Philip S. Yu"
                    },
                    {
                        "name": "Chen Xing"
                    }
                ],
                "author_detail": {
                    "name": "Chen Xing"
                },
                "author": "Chen Xing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06556v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06556v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05878v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05878v2",
                "updated": "2025-02-11T15:45:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    45,
                    52,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-09T12:26:05Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    12,
                    26,
                    5,
                    6,
                    40,
                    0
                ],
                "title": "Enhancing Financial Time-Series Forecasting with Retrieval-Augmented\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Financial Time-Series Forecasting with Retrieval-Augmented\n  Large Language Models"
                },
                "summary": "Stock movement prediction, a critical task in financial time-series\nforecasting, relies on identifying and retrieving key influencing factors from\nvast and complex datasets. However, traditional text-trained or numeric\nsimilarity-based retrieval methods often struggle to handle the intricacies of\nfinancial data. To address this, we propose the first retrieval-augmented\ngeneration (RAG) framework specifically designed for financial time-series\nforecasting. Our framework incorporates three key innovations: a fine-tuned 1B\nlarge language model (StockLLM) as its backbone, a novel candidate selection\nmethod enhanced by LLM feedback, and a training objective that maximizes the\nsimilarity between queries and historically significant sequences. These\nadvancements enable our retriever, FinSeer, to uncover meaningful patterns\nwhile effectively minimizing noise in complex financial datasets. To support\nrobust evaluation, we also construct new datasets that integrate financial\nindicators and historical stock prices. Experimental results demonstrate that\nour RAG framework outperforms both the baseline StockLLM and random retrieval\nmethods, showcasing its effectiveness. FinSeer, as the retriever, achieves an\n8% higher accuracy on the BIGDATA22 benchmark and retrieves more impactful\nsequences compared to existing retrieval methods. This work highlights the\nimportance of tailored retrieval models in financial forecasting and provides a\nnovel, scalable framework for future research in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stock movement prediction, a critical task in financial time-series\nforecasting, relies on identifying and retrieving key influencing factors from\nvast and complex datasets. However, traditional text-trained or numeric\nsimilarity-based retrieval methods often struggle to handle the intricacies of\nfinancial data. To address this, we propose the first retrieval-augmented\ngeneration (RAG) framework specifically designed for financial time-series\nforecasting. Our framework incorporates three key innovations: a fine-tuned 1B\nlarge language model (StockLLM) as its backbone, a novel candidate selection\nmethod enhanced by LLM feedback, and a training objective that maximizes the\nsimilarity between queries and historically significant sequences. These\nadvancements enable our retriever, FinSeer, to uncover meaningful patterns\nwhile effectively minimizing noise in complex financial datasets. To support\nrobust evaluation, we also construct new datasets that integrate financial\nindicators and historical stock prices. Experimental results demonstrate that\nour RAG framework outperforms both the baseline StockLLM and random retrieval\nmethods, showcasing its effectiveness. FinSeer, as the retriever, achieves an\n8% higher accuracy on the BIGDATA22 benchmark and retrieves more impactful\nsequences compared to existing retrieval methods. This work highlights the\nimportance of tailored retrieval models in financial forecasting and provides a\nnovel, scalable framework for future research in the field."
                },
                "authors": [
                    {
                        "name": "Mengxi Xiao"
                    },
                    {
                        "name": "Zihao Jiang"
                    },
                    {
                        "name": "Lingfei Qian"
                    },
                    {
                        "name": "Zhengyu Chen"
                    },
                    {
                        "name": "Yueru He"
                    },
                    {
                        "name": "Yijing Xu"
                    },
                    {
                        "name": "Yuecheng Jiang"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Ruey-Ling Weng"
                    },
                    {
                        "name": "Min Peng"
                    },
                    {
                        "name": "Jimin Huang"
                    },
                    {
                        "name": "Sophia Ananiadou"
                    },
                    {
                        "name": "Qianqian Xie"
                    }
                ],
                "author_detail": {
                    "name": "Qianqian Xie"
                },
                "author": "Qianqian Xie",
                "arxiv_comment": "11 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05878v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05878v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08770v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08770v2",
                "updated": "2025-02-11T15:39:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    39,
                    8,
                    1,
                    42,
                    0
                ],
                "published": "2024-07-11T17:52:03Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    17,
                    52,
                    3,
                    3,
                    193,
                    0
                ],
                "title": "Model Surgery: Modulating LLM's Behavior Via Simple Parameter Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Surgery: Modulating LLM's Behavior Via Simple Parameter Editing"
                },
                "summary": "Large Language Models (LLMs) have demonstrated great potential as generalist\nassistants, showcasing powerful task understanding and problem-solving\ncapabilities. To deploy LLMs as AI assistants, it is crucial that these models\nexhibit desirable behavioral traits, such as non-toxicity and resilience\nagainst jailbreak attempts. Current approaches for detoxification or preventing\njailbreaking usually involve Supervised Fine-Tuning (SFT) or Reinforcement\nLearning from Human Feedback (RLHF), which requires finetuning billions of\nparameters through gradient descent with substantial computational cost.\nFurthermore, models modified through SFT and RLHF may deviate from the\npretrained models, potentially leading to a degradation in foundational LLM\ncapabilities. In this paper, we observe that surprisingly, directly editing a\nsmall subset of parameters can effectively modulate specific behaviors of LLMs,\nsuch as detoxification and resistance to jailbreaking, with only\ninference-level computational resources. Experiments demonstrate that in the\ndetoxification task, our approach achieves reductions of up to 90.0% in\ntoxicity on the RealToxicityPrompts dataset and 49.2% on ToxiGen, while\nmaintaining the LLM's general capabilities in areas such as common sense,\nquestion answering, and mathematics",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated great potential as generalist\nassistants, showcasing powerful task understanding and problem-solving\ncapabilities. To deploy LLMs as AI assistants, it is crucial that these models\nexhibit desirable behavioral traits, such as non-toxicity and resilience\nagainst jailbreak attempts. Current approaches for detoxification or preventing\njailbreaking usually involve Supervised Fine-Tuning (SFT) or Reinforcement\nLearning from Human Feedback (RLHF), which requires finetuning billions of\nparameters through gradient descent with substantial computational cost.\nFurthermore, models modified through SFT and RLHF may deviate from the\npretrained models, potentially leading to a degradation in foundational LLM\ncapabilities. In this paper, we observe that surprisingly, directly editing a\nsmall subset of parameters can effectively modulate specific behaviors of LLMs,\nsuch as detoxification and resistance to jailbreaking, with only\ninference-level computational resources. Experiments demonstrate that in the\ndetoxification task, our approach achieves reductions of up to 90.0% in\ntoxicity on the RealToxicityPrompts dataset and 49.2% on ToxiGen, while\nmaintaining the LLM's general capabilities in areas such as common sense,\nquestion answering, and mathematics"
                },
                "authors": [
                    {
                        "name": "Huanqian Wang"
                    },
                    {
                        "name": "Yang Yue"
                    },
                    {
                        "name": "Rui Lu"
                    },
                    {
                        "name": "Jingxin Shi"
                    },
                    {
                        "name": "Andrew Zhao"
                    },
                    {
                        "name": "Shenzhi Wang"
                    },
                    {
                        "name": "Shiji Song"
                    },
                    {
                        "name": "Gao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Gao Huang"
                },
                "author": "Gao Huang",
                "arxiv_comment": "23 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08770v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08770v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50 (Primary) 68T07, 62M45 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07644v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07644v1",
                "updated": "2025-02-11T15:34:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    34,
                    0,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T15:34:00Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    34,
                    0,
                    1,
                    42,
                    0
                ],
                "title": "SymGPT: Auditing Smart Contracts via Combining Symbolic Execution with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SymGPT: Auditing Smart Contracts via Combining Symbolic Execution with\n  Large Language Models"
                },
                "summary": "To govern smart contracts running on Ethereum, multiple Ethereum Request for\nComment (ERC) standards have been developed, each having a set of rules to\nguide the behaviors of smart contracts. Violating the ERC rules could cause\nserious security issues and financial loss, signifying the importance of\nverifying smart contracts follow ERCs. Today's practices of such verification\nare to manually audit each single contract, use expert-developed\nprogram-analysis tools, or use large language models (LLMs), all of which are\nfar from effective in identifying ERC rule violations. This paper introduces\nSymGPT, a tool that combines the natural language understanding of large\nlanguage models (LLMs) with the formal guarantees of symbolic execution to\nautomatically verify smart contracts' compliance with ERC rules. To develop\nSymGPT, we conduct an empirical study of 132 ERC rules from three widely used\nERC standards, examining their content, security implications, and natural\nlanguage descriptions. Based on this study, we design SymGPT by first\ninstructing an LLM to translate ERC rules into a defined EBNF grammar. We then\nsynthesize constraints from the formalized rules to represent scenarios where\nviolations may occur and use symbolic execution to detect them. Our evaluation\nshows that SymGPT identifies 5,783 ERC rule violations in 4,000 real-world\ncontracts, including 1,375 violations with clear attack paths for stealing\nfinancial assets, demonstrating its effectiveness. Furthermore, SymGPT\noutperforms six automated techniques and a security-expert auditing service,\nunderscoring its superiority over current smart contract analysis methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To govern smart contracts running on Ethereum, multiple Ethereum Request for\nComment (ERC) standards have been developed, each having a set of rules to\nguide the behaviors of smart contracts. Violating the ERC rules could cause\nserious security issues and financial loss, signifying the importance of\nverifying smart contracts follow ERCs. Today's practices of such verification\nare to manually audit each single contract, use expert-developed\nprogram-analysis tools, or use large language models (LLMs), all of which are\nfar from effective in identifying ERC rule violations. This paper introduces\nSymGPT, a tool that combines the natural language understanding of large\nlanguage models (LLMs) with the formal guarantees of symbolic execution to\nautomatically verify smart contracts' compliance with ERC rules. To develop\nSymGPT, we conduct an empirical study of 132 ERC rules from three widely used\nERC standards, examining their content, security implications, and natural\nlanguage descriptions. Based on this study, we design SymGPT by first\ninstructing an LLM to translate ERC rules into a defined EBNF grammar. We then\nsynthesize constraints from the formalized rules to represent scenarios where\nviolations may occur and use symbolic execution to detect them. Our evaluation\nshows that SymGPT identifies 5,783 ERC rule violations in 4,000 real-world\ncontracts, including 1,375 violations with clear attack paths for stealing\nfinancial assets, demonstrating its effectiveness. Furthermore, SymGPT\noutperforms six automated techniques and a security-expert auditing service,\nunderscoring its superiority over current smart contract analysis methods."
                },
                "authors": [
                    {
                        "name": "Shihao Xia"
                    },
                    {
                        "name": "Mengting He"
                    },
                    {
                        "name": "Shuai Shao"
                    },
                    {
                        "name": "Tingting Yu"
                    },
                    {
                        "name": "Yiying Zhang"
                    },
                    {
                        "name": "Linhai Song"
                    }
                ],
                "author_detail": {
                    "name": "Linhai Song"
                },
                "author": "Linhai Song",
                "arxiv_comment": "16 pages. arXiv admin note: text overlap with arXiv:2404.04306",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07644v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07644v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07642v1",
                "updated": "2025-02-11T15:33:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    33,
                    17,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T15:33:17Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    33,
                    17,
                    1,
                    42,
                    0
                ],
                "title": "FoQA: A Faroese Question-Answering Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FoQA: A Faroese Question-Answering Dataset"
                },
                "summary": "We present FoQA, a Faroese extractive question-answering (QA) dataset with\n2,000 samples, created using a semi-automated approach combining Large Language\nModels (LLMs) and human validation. The dataset was generated from Faroese\nWikipedia articles using GPT-4-turbo for initial QA generation, followed by\nquestion rephrasing to increase complexity and native speaker validation to\nensure quality. We provide baseline performance metrics for FoQA across\nmultiple models, including LLMs and BERT, demonstrating its effectiveness in\nevaluating Faroese QA performance. The dataset is released in three versions: a\nvalidated set of 2,000 samples, a complete set of all 10,001 generated samples,\nand a set of 2,395 rejected samples for error analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present FoQA, a Faroese extractive question-answering (QA) dataset with\n2,000 samples, created using a semi-automated approach combining Large Language\nModels (LLMs) and human validation. The dataset was generated from Faroese\nWikipedia articles using GPT-4-turbo for initial QA generation, followed by\nquestion rephrasing to increase complexity and native speaker validation to\nensure quality. We provide baseline performance metrics for FoQA across\nmultiple models, including LLMs and BERT, demonstrating its effectiveness in\nevaluating Faroese QA performance. The dataset is released in three versions: a\nvalidated set of 2,000 samples, a complete set of all 10,001 generated samples,\nand a set of 2,395 rejected samples for error analysis."
                },
                "authors": [
                    {
                        "name": "Annika Simonsen"
                    },
                    {
                        "name": "Dan Saattrup Nielsen"
                    },
                    {
                        "name": "Hafsteinn Einarsson"
                    }
                ],
                "author_detail": {
                    "name": "Hafsteinn Einarsson"
                },
                "author": "Hafsteinn Einarsson",
                "arxiv_comment": "Camera-ready version for RESOURCEFUL workshop, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07641v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07641v1",
                "updated": "2025-02-11T15:33:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    33,
                    6,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T15:33:06Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    33,
                    6,
                    1,
                    42,
                    0
                ],
                "title": "Distributional Instrumental Variable Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributional Instrumental Variable Method"
                },
                "summary": "The instrumental variable (IV) approach is commonly used to infer causal\neffects in the presence of unmeasured confounding. Conventional IV models\ncommonly make the additive noise assumption, which is hard to ensure in\npractice, but also typically lack flexibility if the causal effects are\ncomplex. Further, the vast majority of the existing methods aims to estimate\nthe mean causal effects only, a few other methods focus on the quantile\neffects. This work aims for estimation of the entire interventional\ndistribution. We propose a novel method called distributional instrumental\nvariables (DIV), which leverages generative modelling in a nonlinear\ninstrumental variable setting. We establish identifiability of the\ninterventional distribution under general assumptions and demonstrate an\n`under-identified' case where DIV can identify the causal effects while\ntwo-step least squares fails to. Our empirical results show that the DIV method\nperforms well for a broad range of simulated data, exhibiting advantages over\nexisting IV approaches in terms of the identifiability and estimation error of\nthe mean or quantile treatment effects. Furthermore, we apply DIV to an\neconomic data set to examine the causal relation between institutional quality\nand economic development and our results that closely align with the original\nstudy. We also apply DIV to a single-cell data set, where we study the\ngeneralizability and stability in predicting gene expression under unseen\ninterventions. The software implementations of DIV are available in R and\nPython.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The instrumental variable (IV) approach is commonly used to infer causal\neffects in the presence of unmeasured confounding. Conventional IV models\ncommonly make the additive noise assumption, which is hard to ensure in\npractice, but also typically lack flexibility if the causal effects are\ncomplex. Further, the vast majority of the existing methods aims to estimate\nthe mean causal effects only, a few other methods focus on the quantile\neffects. This work aims for estimation of the entire interventional\ndistribution. We propose a novel method called distributional instrumental\nvariables (DIV), which leverages generative modelling in a nonlinear\ninstrumental variable setting. We establish identifiability of the\ninterventional distribution under general assumptions and demonstrate an\n`under-identified' case where DIV can identify the causal effects while\ntwo-step least squares fails to. Our empirical results show that the DIV method\nperforms well for a broad range of simulated data, exhibiting advantages over\nexisting IV approaches in terms of the identifiability and estimation error of\nthe mean or quantile treatment effects. Furthermore, we apply DIV to an\neconomic data set to examine the causal relation between institutional quality\nand economic development and our results that closely align with the original\nstudy. We also apply DIV to a single-cell data set, where we study the\ngeneralizability and stability in predicting gene expression under unseen\ninterventions. The software implementations of DIV are available in R and\nPython."
                },
                "authors": [
                    {
                        "name": "Anastasiia Holovchak"
                    },
                    {
                        "name": "Sorawit Saengkyongam"
                    },
                    {
                        "name": "Nicolai Meinshausen"
                    },
                    {
                        "name": "Xinwei Shen"
                    }
                ],
                "author_detail": {
                    "name": "Xinwei Shen"
                },
                "author": "Xinwei Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07641v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07641v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07640v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07640v1",
                "updated": "2025-02-11T15:27:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    27,
                    35,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T15:27:35Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    27,
                    35,
                    1,
                    42,
                    0
                ],
                "title": "Goedel-Prover: A Frontier Model for Open-Source Automated Theorem\n  Proving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Goedel-Prover: A Frontier Model for Open-Source Automated Theorem\n  Proving"
                },
                "summary": "We introduce Goedel-Prover, an open-source large language model (LLM) that\nachieves the state-of-the-art (SOTA) performance in automated formal proof\ngeneration for mathematical problems. The key challenge in this field is the\nscarcity of formalized math statements and proofs, which we tackle in the\nfollowing ways. We train statement formalizers to translate the natural\nlanguage math problems from Numina into formal language (Lean 4), creating a\ndataset of 1.64 million formal statements. LLMs are used to check that the\nformal statements accurately preserve the content of the original natural\nlanguage problems. We then iteratively build a large dataset of formal proofs\nby training a series of provers. Each prover succeeds in proving many\nstatements that the previous ones could not, and these new proofs are added to\nthe training set for the next prover. The final prover outperforms all existing\nopen-source models in whole-proof generation. On the miniF2F benchmark, it\nachieves a 57.6% success rate (Pass@32), exceeding the previous best\nopen-source model by 7.6%. On PutnamBench, Goedel-Prover successfully solves 7\nproblems (Pass@512), ranking first on the leaderboard. Furthermore, it\ngenerates 29.7K formal proofs for Lean Workbook problems, nearly doubling the\n15.7K produced by earlier works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Goedel-Prover, an open-source large language model (LLM) that\nachieves the state-of-the-art (SOTA) performance in automated formal proof\ngeneration for mathematical problems. The key challenge in this field is the\nscarcity of formalized math statements and proofs, which we tackle in the\nfollowing ways. We train statement formalizers to translate the natural\nlanguage math problems from Numina into formal language (Lean 4), creating a\ndataset of 1.64 million formal statements. LLMs are used to check that the\nformal statements accurately preserve the content of the original natural\nlanguage problems. We then iteratively build a large dataset of formal proofs\nby training a series of provers. Each prover succeeds in proving many\nstatements that the previous ones could not, and these new proofs are added to\nthe training set for the next prover. The final prover outperforms all existing\nopen-source models in whole-proof generation. On the miniF2F benchmark, it\nachieves a 57.6% success rate (Pass@32), exceeding the previous best\nopen-source model by 7.6%. On PutnamBench, Goedel-Prover successfully solves 7\nproblems (Pass@512), ranking first on the leaderboard. Furthermore, it\ngenerates 29.7K formal proofs for Lean Workbook problems, nearly doubling the\n15.7K produced by earlier works."
                },
                "authors": [
                    {
                        "name": "Yong Lin"
                    },
                    {
                        "name": "Shange Tang"
                    },
                    {
                        "name": "Bohan Lyu"
                    },
                    {
                        "name": "Jiayun Wu"
                    },
                    {
                        "name": "Hongzhou Lin"
                    },
                    {
                        "name": "Kaiyu Yang"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Mengzhou Xia"
                    },
                    {
                        "name": "Danqi Chen"
                    },
                    {
                        "name": "Sanjeev Arora"
                    },
                    {
                        "name": "Chi Jin"
                    }
                ],
                "author_detail": {
                    "name": "Chi Jin"
                },
                "author": "Chi Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07640v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07640v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07639v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07639v1",
                "updated": "2025-02-11T15:26:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    26,
                    43,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T15:26:43Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    26,
                    43,
                    1,
                    42,
                    0
                ],
                "title": "Response rate estimation in single-stage basket trials: A comparison of\n  estimators that allow for borrowing across cohorts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Response rate estimation in single-stage basket trials: A comparison of\n  estimators that allow for borrowing across cohorts"
                },
                "summary": "Therapeutic advancements in oncology have shifted towards targeted therapy\nbased on genomic aberrations. This necessitates innovative statistical\napproaches in clinical trials, particularly in master protocol studies. Basket\ntrials, a type of master protocol, evaluate a single treatment across cohorts\nsharing a genomic aberration but differing in tumor histology. While offering\noperational advantages, basket trial analysis presents statistical inference\nchallenges. These trials help determine for which tumor histology the treatment\nis promising enough to advance to confirmatory evaluation and often use\nBayesian designs to support decisions. Beyond decision-making, estimating\ncohort-specific response rates is crucial for designing subsequent trials. This\nstudy compares seven Bayesian estimation methods for basket trials with binary\noutcomes against the (frequentist) sample proportion estimate through\nsimulations. The goal is to estimate cohort-specific response rates, focusing\non bias, mean squared error, and information borrowing. Various scenarios are\nexamined, including homogeneous, heterogeneous, and clustered response rates\nacross cohorts. Results show trade-offs in bias and precision, highlighting the\nimportance of method selection. Berry's method performs best with limited\nheterogeneity. No clear winner emerges in general cases, with performance\naffected by shrinkage, bias, and the choice of priors and tuning parameters.\nChallenges include computational complexity, parameter tuning, and the lack of\nclear guidance on selection. Researchers should consider these factors when\ndesigning and analyzing basket trials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Therapeutic advancements in oncology have shifted towards targeted therapy\nbased on genomic aberrations. This necessitates innovative statistical\napproaches in clinical trials, particularly in master protocol studies. Basket\ntrials, a type of master protocol, evaluate a single treatment across cohorts\nsharing a genomic aberration but differing in tumor histology. While offering\noperational advantages, basket trial analysis presents statistical inference\nchallenges. These trials help determine for which tumor histology the treatment\nis promising enough to advance to confirmatory evaluation and often use\nBayesian designs to support decisions. Beyond decision-making, estimating\ncohort-specific response rates is crucial for designing subsequent trials. This\nstudy compares seven Bayesian estimation methods for basket trials with binary\noutcomes against the (frequentist) sample proportion estimate through\nsimulations. The goal is to estimate cohort-specific response rates, focusing\non bias, mean squared error, and information borrowing. Various scenarios are\nexamined, including homogeneous, heterogeneous, and clustered response rates\nacross cohorts. Results show trade-offs in bias and precision, highlighting the\nimportance of method selection. Berry's method performs best with limited\nheterogeneity. No clear winner emerges in general cases, with performance\naffected by shrinkage, bias, and the choice of priors and tuning parameters.\nChallenges include computational complexity, parameter tuning, and the lack of\nclear guidance on selection. Researchers should consider these factors when\ndesigning and analyzing basket trials."
                },
                "authors": [
                    {
                        "name": "Antonios Daletzakis"
                    },
                    {
                        "name": "Rutger van den Bor"
                    },
                    {
                        "name": "Vincent van der Noort"
                    },
                    {
                        "name": "Kit CB Roes"
                    }
                ],
                "author_detail": {
                    "name": "Kit CB Roes"
                },
                "author": "Kit CB Roes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07639v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07639v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07629v1",
                "updated": "2025-02-11T15:17:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    17,
                    0,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T15:17:00Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    17,
                    0,
                    1,
                    42,
                    0
                ],
                "title": "Exploring Mobile Touch Interaction with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Mobile Touch Interaction with Large Language Models"
                },
                "summary": "Interacting with Large Language Models (LLMs) for text editing on mobile\ndevices currently requires users to break out of their writing environment and\nswitch to a conversational AI interface. In this paper, we propose to control\nthe LLM via touch gestures performed directly on the text. We first chart a\ndesign space that covers fundamental touch input and text transformations. In\nthis space, we then concretely explore two control mappings: spread-to-generate\nand pinch-to-shorten, with visual feedback loops. We evaluate this concept in a\nuser study (N=14) that compares three feedback designs: no visualisation, text\nlength indicator, and length + word indicator. The results demonstrate that\ntouch-based control of LLMs is both feasible and user-friendly, with the length\n+ word indicator proving most effective for managing text generation. This work\nlays the foundation for further research into gesture-based interaction with\nLLMs on touch devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interacting with Large Language Models (LLMs) for text editing on mobile\ndevices currently requires users to break out of their writing environment and\nswitch to a conversational AI interface. In this paper, we propose to control\nthe LLM via touch gestures performed directly on the text. We first chart a\ndesign space that covers fundamental touch input and text transformations. In\nthis space, we then concretely explore two control mappings: spread-to-generate\nand pinch-to-shorten, with visual feedback loops. We evaluate this concept in a\nuser study (N=14) that compares three feedback designs: no visualisation, text\nlength indicator, and length + word indicator. The results demonstrate that\ntouch-based control of LLMs is both feasible and user-friendly, with the length\n+ word indicator proving most effective for managing text generation. This work\nlays the foundation for further research into gesture-based interaction with\nLLMs on touch devices."
                },
                "authors": [
                    {
                        "name": "Tim Zindulka"
                    },
                    {
                        "name": "Jannek Sekowski"
                    },
                    {
                        "name": "Florian Lehmann"
                    },
                    {
                        "name": "Daniel Buschek"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Buschek"
                },
                "author": "Daniel Buschek",
                "arxiv_doi": "10.1145/3706598.3713554",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706598.3713554",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.07629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "21 pages, 16 figures, 3 tables, ACM CHI 2025",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07620v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07620v1",
                "updated": "2025-02-11T15:09:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    9,
                    5,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T15:09:05Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    9,
                    5,
                    1,
                    42,
                    0
                ],
                "title": "Causal-Informed Contrastive Learning: Towards Bias-Resilient\n  Pre-training under Concept Drift",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal-Informed Contrastive Learning: Towards Bias-Resilient\n  Pre-training under Concept Drift"
                },
                "summary": "The evolution of large-scale contrastive pre-training propelled by top-tier\ndatasets has reached a transition point in the scaling law. Consequently,\nsustaining and enhancing a model's pre-training capabilities in drift\nenvironments have surfaced as a notable challenge. In this paper, we initially\nuncover that contrastive pre-training methods are significantly impacted by\nconcept drift wherein distributions change unpredictably, resulting in notable\nbiases in the feature space of the pre-trained model. Empowered by causal\ninference, we construct a structural causal graph to analyze the impact of\nconcept drift to contrastive pre-training systemically, and propose the causal\ninterventional contrastive objective. Upon achieving this, we devise a\nresilient contrastive pre-training approach to accommodate the data stream of\nconcept drift, with simple and scalable implementation. Extensive experiments\non various downstream tasks demonstrate our resilient contrastive pre-training\neffectively mitigates the bias stemming from the concept drift data stream.\nCodes are available at https://anonymous.4open.science/r/ResilientCL/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution of large-scale contrastive pre-training propelled by top-tier\ndatasets has reached a transition point in the scaling law. Consequently,\nsustaining and enhancing a model's pre-training capabilities in drift\nenvironments have surfaced as a notable challenge. In this paper, we initially\nuncover that contrastive pre-training methods are significantly impacted by\nconcept drift wherein distributions change unpredictably, resulting in notable\nbiases in the feature space of the pre-trained model. Empowered by causal\ninference, we construct a structural causal graph to analyze the impact of\nconcept drift to contrastive pre-training systemically, and propose the causal\ninterventional contrastive objective. Upon achieving this, we devise a\nresilient contrastive pre-training approach to accommodate the data stream of\nconcept drift, with simple and scalable implementation. Extensive experiments\non various downstream tasks demonstrate our resilient contrastive pre-training\neffectively mitigates the bias stemming from the concept drift data stream.\nCodes are available at https://anonymous.4open.science/r/ResilientCL/."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Yang"
                    },
                    {
                        "name": "Jie Lu"
                    },
                    {
                        "name": "En Yu"
                    }
                ],
                "author_detail": {
                    "name": "En Yu"
                },
                "author": "En Yu",
                "arxiv_comment": "17pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07620v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07620v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09754v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09754v2",
                "updated": "2025-02-11T15:06:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    6,
                    13,
                    1,
                    42,
                    0
                ],
                "published": "2024-09-15T14:52:16Z",
                "published_parsed": [
                    2024,
                    9,
                    15,
                    14,
                    52,
                    16,
                    6,
                    259,
                    0
                ],
                "title": "Towards Single-Lens Controllable Depth-of-Field Imaging via Depth-Aware\n  Point Spread Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Single-Lens Controllable Depth-of-Field Imaging via Depth-Aware\n  Point Spread Functions"
                },
                "summary": "Controllable Depth-of-Field (DoF) imaging commonly produces amazing visual\neffects based on heavy and expensive high-end lenses. However, confronted with\nthe increasing demand for mobile scenarios, it is desirable to achieve a\nlightweight solution with Minimalist Optical Systems (MOS). This work centers\naround two major limitations of MOS, i.e., the severe optical aberrations and\nuncontrollable DoF, for achieving single-lens controllable DoF imaging via\ncomputational methods. A Depth-aware Controllable DoF Imaging (DCDI) framework\nis proposed equipped with All-in-Focus (AiF) aberration correction and\nmonocular depth estimation, where the recovered image and corresponding depth\nmap are utilized to produce imaging results under diverse DoFs of any high-end\nlens via patch-wise convolution. To address the depth-varying optical\ndegradation, we introduce a Depth-aware Degradation-adaptive Training (DA2T)\nscheme. At the dataset level, a Depth-aware Aberration MOS (DAMOS) dataset is\nestablished based on the simulation of Point Spread Functions (PSFs) under\ndifferent object distances. Additionally, we design two plug-and-play\ndepth-aware mechanisms to embed depth information into the aberration image\nrecovery for better tackling depth-aware degradation. Furthermore, we propose a\nstorage-efficient Omni-Lens-Field model to represent the 4D PSF library of\nvarious lenses. With the predicted depth map, recovered image, and depth-aware\nPSF map inferred by Omni-Lens-Field, single-lens controllable DoF imaging is\nachieved. Comprehensive experimental results demonstrate that the proposed\nframework enhances the recovery performance, and attains impressive single-lens\ncontrollable DoF imaging results, providing a seminal baseline for this field.\nThe source code and the established dataset will be publicly available at\nhttps://github.com/XiaolongQian/DCDI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controllable Depth-of-Field (DoF) imaging commonly produces amazing visual\neffects based on heavy and expensive high-end lenses. However, confronted with\nthe increasing demand for mobile scenarios, it is desirable to achieve a\nlightweight solution with Minimalist Optical Systems (MOS). This work centers\naround two major limitations of MOS, i.e., the severe optical aberrations and\nuncontrollable DoF, for achieving single-lens controllable DoF imaging via\ncomputational methods. A Depth-aware Controllable DoF Imaging (DCDI) framework\nis proposed equipped with All-in-Focus (AiF) aberration correction and\nmonocular depth estimation, where the recovered image and corresponding depth\nmap are utilized to produce imaging results under diverse DoFs of any high-end\nlens via patch-wise convolution. To address the depth-varying optical\ndegradation, we introduce a Depth-aware Degradation-adaptive Training (DA2T)\nscheme. At the dataset level, a Depth-aware Aberration MOS (DAMOS) dataset is\nestablished based on the simulation of Point Spread Functions (PSFs) under\ndifferent object distances. Additionally, we design two plug-and-play\ndepth-aware mechanisms to embed depth information into the aberration image\nrecovery for better tackling depth-aware degradation. Furthermore, we propose a\nstorage-efficient Omni-Lens-Field model to represent the 4D PSF library of\nvarious lenses. With the predicted depth map, recovered image, and depth-aware\nPSF map inferred by Omni-Lens-Field, single-lens controllable DoF imaging is\nachieved. Comprehensive experimental results demonstrate that the proposed\nframework enhances the recovery performance, and attains impressive single-lens\ncontrollable DoF imaging results, providing a seminal baseline for this field.\nThe source code and the established dataset will be publicly available at\nhttps://github.com/XiaolongQian/DCDI."
                },
                "authors": [
                    {
                        "name": "Xiaolong Qian"
                    },
                    {
                        "name": "Qi Jiang"
                    },
                    {
                        "name": "Yao Gao"
                    },
                    {
                        "name": "Shaohua Gao"
                    },
                    {
                        "name": "Zhonghua Yi"
                    },
                    {
                        "name": "Lei Sun"
                    },
                    {
                        "name": "Kai Wei"
                    },
                    {
                        "name": "Haifeng Li"
                    },
                    {
                        "name": "Kailun Yang"
                    },
                    {
                        "name": "Kaiwei Wang"
                    },
                    {
                        "name": "Jian Bai"
                    }
                ],
                "author_detail": {
                    "name": "Jian Bai"
                },
                "author": "Jian Bai",
                "arxiv_comment": "Accepted to IEEE Transactions on Computational Imaging (TCI). The\n  source code and the established dataset will be publicly available at\n  https://github.com/XiaolongQian/DCDI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09754v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09754v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07613v1",
                "updated": "2025-02-11T15:04:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    4,
                    34,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T15:04:34Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    4,
                    34,
                    1,
                    42,
                    0
                ],
                "title": "Signatures of modified gravity from the gravitational Aharonov-Bohm\n  effect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Signatures of modified gravity from the gravitational Aharonov-Bohm\n  effect"
                },
                "summary": "To date, no observational confirmation of dark matter particles has been\nfound. In this paper, we put forward an alternative approach to inferring\nevidence for dark matter through modified gravity, without invoking fundamental\ndark matter particles. Specifically, we explore the possibility of extracting\nsignatures of Kaluza-Klein gravity through the gravitational Aharonov-Bohm\neffect. Kaluza-Klein theory has recently been proposed as an alternative to the\ndark sector, and predicts a tower of particles, including spin-0 and spin-1\ngravitons alongside the usual spin-2 gravitons, which can gravitationally\ncouple to matter. We thus analyze a quantum system in free fall around a\ngravitating body in the presence of a modified Yukawa-like gravitational\npotential, and determine the gravitational phase induced by the additional\ndegrees of freedom introduced by the Kaluza-Klein model. Our results reveal\nthat, in addition to the usual result from General Relativity, the quantum wave\nfunction of the system exhibits an additional effect: a splitting of the energy\nlevels with a new quantum number due to the extra vector gravitational degrees\nof freedom. The energy splitting difference between general relativity and\nKaluza-Klein gravity is found to be of the order of meV for an atomic system\nand eV for a nuclear system. Similar values also arise in generic modified\ngravity models and can be feasibly tested in the future. Numerical estimates\nfor the graviton mass are also provided, and potential imprints on\ngravitational waves are mentioned.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To date, no observational confirmation of dark matter particles has been\nfound. In this paper, we put forward an alternative approach to inferring\nevidence for dark matter through modified gravity, without invoking fundamental\ndark matter particles. Specifically, we explore the possibility of extracting\nsignatures of Kaluza-Klein gravity through the gravitational Aharonov-Bohm\neffect. Kaluza-Klein theory has recently been proposed as an alternative to the\ndark sector, and predicts a tower of particles, including spin-0 and spin-1\ngravitons alongside the usual spin-2 gravitons, which can gravitationally\ncouple to matter. We thus analyze a quantum system in free fall around a\ngravitating body in the presence of a modified Yukawa-like gravitational\npotential, and determine the gravitational phase induced by the additional\ndegrees of freedom introduced by the Kaluza-Klein model. Our results reveal\nthat, in addition to the usual result from General Relativity, the quantum wave\nfunction of the system exhibits an additional effect: a splitting of the energy\nlevels with a new quantum number due to the extra vector gravitational degrees\nof freedom. The energy splitting difference between general relativity and\nKaluza-Klein gravity is found to be of the order of meV for an atomic system\nand eV for a nuclear system. Similar values also arise in generic modified\ngravity models and can be feasibly tested in the future. Numerical estimates\nfor the graviton mass are also provided, and potential imprints on\ngravitational waves are mentioned."
                },
                "authors": [
                    {
                        "name": "Kimet Jusufi"
                    },
                    {
                        "name": "Abdelrahman Yasser"
                    },
                    {
                        "name": "Emmanuele Battista"
                    },
                    {
                        "name": "Nader Inan"
                    }
                ],
                "author_detail": {
                    "name": "Nader Inan"
                },
                "author": "Nader Inan",
                "arxiv_comment": "v1: 13 pages, 2 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07608v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07608v1",
                "updated": "2025-02-11T14:58:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    58,
                    54,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T14:58:54Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    58,
                    54,
                    1,
                    42,
                    0
                ],
                "title": "Beyond Prompting: Time2Lang -- Bridging Time-Series Foundation Models\n  and Large Language Models for Health Sensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Prompting: Time2Lang -- Bridging Time-Series Foundation Models\n  and Large Language Models for Health Sensing"
                },
                "summary": "Large language models (LLMs) show promise for health applications when\ncombined with behavioral sensing data. Traditional approaches convert sensor\ndata into text prompts, but this process is prone to errors, computationally\nexpensive, and requires domain expertise. These challenges are particularly\nacute when processing extended time series data. While time series foundation\nmodels (TFMs) have recently emerged as powerful tools for learning\nrepresentations from temporal data, bridging TFMs and LLMs remains challenging.\nHere, we present Time2Lang, a framework that directly maps TFM outputs to LLM\nrepresentations without intermediate text conversion. Our approach first trains\non synthetic data using periodicity prediction as a pretext task, followed by\nevaluation on mental health classification tasks. We validate Time2Lang on two\nlongitudinal wearable and mobile sensing datasets: daily depression prediction\nusing step count data (17,251 days from 256 participants) and flourishing\nclassification based on conversation duration (46 participants over 10 weeks).\nTime2Lang maintains near constant inference times regardless of input length,\nunlike traditional prompting methods. The generated embeddings preserve\nessential time-series characteristics such as auto-correlation. Our results\ndemonstrate that TFMs and LLMs can be effectively integrated while minimizing\ninformation loss and enabling performance transfer across these distinct\nmodeling paradigms. To our knowledge, we are the first to integrate a TFM and\nan LLM for health, thus establishing a foundation for future research combining\ngeneral-purpose large models for complex healthcare tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) show promise for health applications when\ncombined with behavioral sensing data. Traditional approaches convert sensor\ndata into text prompts, but this process is prone to errors, computationally\nexpensive, and requires domain expertise. These challenges are particularly\nacute when processing extended time series data. While time series foundation\nmodels (TFMs) have recently emerged as powerful tools for learning\nrepresentations from temporal data, bridging TFMs and LLMs remains challenging.\nHere, we present Time2Lang, a framework that directly maps TFM outputs to LLM\nrepresentations without intermediate text conversion. Our approach first trains\non synthetic data using periodicity prediction as a pretext task, followed by\nevaluation on mental health classification tasks. We validate Time2Lang on two\nlongitudinal wearable and mobile sensing datasets: daily depression prediction\nusing step count data (17,251 days from 256 participants) and flourishing\nclassification based on conversation duration (46 participants over 10 weeks).\nTime2Lang maintains near constant inference times regardless of input length,\nunlike traditional prompting methods. The generated embeddings preserve\nessential time-series characteristics such as auto-correlation. Our results\ndemonstrate that TFMs and LLMs can be effectively integrated while minimizing\ninformation loss and enabling performance transfer across these distinct\nmodeling paradigms. To our knowledge, we are the first to integrate a TFM and\nan LLM for health, thus establishing a foundation for future research combining\ngeneral-purpose large models for complex healthcare tasks."
                },
                "authors": [
                    {
                        "name": "Arvind Pillai"
                    },
                    {
                        "name": "Dimitris Spathis"
                    },
                    {
                        "name": "Subigya Nepal"
                    },
                    {
                        "name": "Amanda C Collins"
                    },
                    {
                        "name": "Daniel M Mackin"
                    },
                    {
                        "name": "Michael V Heinz"
                    },
                    {
                        "name": "Tess Z Griffin"
                    },
                    {
                        "name": "Nicholas C Jacobson"
                    },
                    {
                        "name": "Andrew Campbell"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Campbell"
                },
                "author": "Andrew Campbell",
                "arxiv_comment": "Under review at CHIL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07608v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07608v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14317v2",
                "updated": "2025-02-11T14:51:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    51,
                    8,
                    1,
                    42,
                    0
                ],
                "published": "2024-08-26T14:45:03Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    14,
                    45,
                    3,
                    0,
                    239,
                    0
                ],
                "title": "Claim Verification in the Age of Large Language Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Claim Verification in the Age of Large Language Models: A Survey"
                },
                "summary": "The large and ever-increasing amount of data available on the Internet\ncoupled with the laborious task of manual claim and fact verification has\nsparked the interest in the development of automated claim verification\nsystems. Several deep learning and transformer-based models have been proposed\nfor this task over the years. With the introduction of Large Language Models\n(LLMs) and their superior performance in several NLP tasks, we have seen a\nsurge of LLM-based approaches to claim verification along with the use of novel\nmethods such as Retrieval Augmented Generation (RAG). In this survey, we\npresent a comprehensive account of recent claim verification frameworks using\nLLMs. We describe the different components of the claim verification pipeline\nused in these frameworks in detail including common approaches to retrieval,\nprompting, and fine-tuning. Finally, we describe publicly available English\ndatasets created for this task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The large and ever-increasing amount of data available on the Internet\ncoupled with the laborious task of manual claim and fact verification has\nsparked the interest in the development of automated claim verification\nsystems. Several deep learning and transformer-based models have been proposed\nfor this task over the years. With the introduction of Large Language Models\n(LLMs) and their superior performance in several NLP tasks, we have seen a\nsurge of LLM-based approaches to claim verification along with the use of novel\nmethods such as Retrieval Augmented Generation (RAG). In this survey, we\npresent a comprehensive account of recent claim verification frameworks using\nLLMs. We describe the different components of the claim verification pipeline\nused in these frameworks in detail including common approaches to retrieval,\nprompting, and fine-tuning. Finally, we describe publicly available English\ndatasets created for this task."
                },
                "authors": [
                    {
                        "name": "Alphaeus Dmonte"
                    },
                    {
                        "name": "Roland Oruche"
                    },
                    {
                        "name": "Marcos Zampieri"
                    },
                    {
                        "name": "Prasad Calyam"
                    },
                    {
                        "name": "Isabelle Augenstein"
                    }
                ],
                "author_detail": {
                    "name": "Isabelle Augenstein"
                },
                "author": "Isabelle Augenstein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07600v1",
                "updated": "2025-02-11T14:50:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    50,
                    10,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T14:50:10Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    50,
                    10,
                    1,
                    42,
                    0
                ],
                "title": "PlaySlot: Learning Inverse Latent Dynamics for Controllable\n  Object-Centric Video Prediction and Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PlaySlot: Learning Inverse Latent Dynamics for Controllable\n  Object-Centric Video Prediction and Planning"
                },
                "summary": "Predicting future scene representations is a crucial task for enabling robots\nto understand and interact with the environment. However, most existing methods\nrely on video sequences and simulations with precise action annotations,\nlimiting their ability to leverage the large amount of available unlabeled\nvideo data. To address this challenge, we propose PlaySlot, an object-centric\nvideo prediction model that infers object representations and latent actions\nfrom unlabeled video sequences. It then uses these representations to forecast\nfuture object states and video frames. PlaySlot allows to generate multiple\npossible futures conditioned on latent actions, which can be inferred from\nvideo dynamics, provided by a user, or generated by a learned action policy,\nthus enabling versatile and interpretable world modeling. Our results show that\nPlaySlot outperforms both stochastic and object-centric baselines for video\nprediction across different environments. Furthermore, we show that our\ninferred latent actions can be used to learn robot behaviors sample-efficiently\nfrom unlabeled video demonstrations. Videos and code are available at\nhttps://play-slot.github.io/PlaySlot/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting future scene representations is a crucial task for enabling robots\nto understand and interact with the environment. However, most existing methods\nrely on video sequences and simulations with precise action annotations,\nlimiting their ability to leverage the large amount of available unlabeled\nvideo data. To address this challenge, we propose PlaySlot, an object-centric\nvideo prediction model that infers object representations and latent actions\nfrom unlabeled video sequences. It then uses these representations to forecast\nfuture object states and video frames. PlaySlot allows to generate multiple\npossible futures conditioned on latent actions, which can be inferred from\nvideo dynamics, provided by a user, or generated by a learned action policy,\nthus enabling versatile and interpretable world modeling. Our results show that\nPlaySlot outperforms both stochastic and object-centric baselines for video\nprediction across different environments. Furthermore, we show that our\ninferred latent actions can be used to learn robot behaviors sample-efficiently\nfrom unlabeled video demonstrations. Videos and code are available at\nhttps://play-slot.github.io/PlaySlot/."
                },
                "authors": [
                    {
                        "name": "Angel Villar-Corrales"
                    },
                    {
                        "name": "Sven Behnke"
                    }
                ],
                "author_detail": {
                    "name": "Sven Behnke"
                },
                "author": "Sven Behnke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07598v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07598v1",
                "updated": "2025-02-11T14:47:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    47,
                    32,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T14:47:32Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    47,
                    32,
                    1,
                    42,
                    0
                ],
                "title": "Towards spatial computing: recent advances in multimodal natural\n  interaction for XR headsets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards spatial computing: recent advances in multimodal natural\n  interaction for XR headsets"
                },
                "summary": "With the widespread adoption of Extended Reality (XR) headsets, spatial\ncomputing technologies are gaining increasing attention. Spatial computing\nenables interaction with virtual elements through natural input methods such as\neye tracking, hand gestures, and voice commands, thus placing natural\nhuman-computer interaction at its core. While previous surveys have reviewed\nconventional XR interaction techniques, recent advancements in natural\ninteraction, particularly driven by artificial intelligence (AI) and large\nlanguage models (LLMs), have introduced new paradigms and technologies. In this\npaper, we review research on multimodal natural interaction for wearable XR,\nfocusing on papers published between 2022 and 2024 in six top venues: ACM CHI,\nUIST, IMWUT (Ubicomp), IEEE VR, ISMAR, and TVCG. We classify and analyze these\nstudies based on application scenarios, operation types, and interaction\nmodalities. This analysis provides a structured framework for understanding how\nresearchers are designing advanced natural interaction techniques in XR. Based\non these findings, we discuss the challenges in natural interaction techniques\nand suggest potential directions for future research. This review provides\nvaluable insights for researchers aiming to design natural and efficient\ninteraction systems for XR, ultimately contributing to the advancement of\nspatial computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread adoption of Extended Reality (XR) headsets, spatial\ncomputing technologies are gaining increasing attention. Spatial computing\nenables interaction with virtual elements through natural input methods such as\neye tracking, hand gestures, and voice commands, thus placing natural\nhuman-computer interaction at its core. While previous surveys have reviewed\nconventional XR interaction techniques, recent advancements in natural\ninteraction, particularly driven by artificial intelligence (AI) and large\nlanguage models (LLMs), have introduced new paradigms and technologies. In this\npaper, we review research on multimodal natural interaction for wearable XR,\nfocusing on papers published between 2022 and 2024 in six top venues: ACM CHI,\nUIST, IMWUT (Ubicomp), IEEE VR, ISMAR, and TVCG. We classify and analyze these\nstudies based on application scenarios, operation types, and interaction\nmodalities. This analysis provides a structured framework for understanding how\nresearchers are designing advanced natural interaction techniques in XR. Based\non these findings, we discuss the challenges in natural interaction techniques\nand suggest potential directions for future research. This review provides\nvaluable insights for researchers aiming to design natural and efficient\ninteraction systems for XR, ultimately contributing to the advancement of\nspatial computing."
                },
                "authors": [
                    {
                        "name": "Zhimin Wang"
                    },
                    {
                        "name": "Maohang Rao"
                    },
                    {
                        "name": "Shanghua Ye"
                    },
                    {
                        "name": "Weitao Song"
                    },
                    {
                        "name": "Feng Lu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Lu"
                },
                "author": "Feng Lu",
                "arxiv_comment": "28 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07598v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07598v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07591v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07591v1",
                "updated": "2025-02-11T14:40:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    40,
                    57,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T14:40:57Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    40,
                    57,
                    1,
                    42,
                    0
                ],
                "title": "DMWM: Dual-Mind World Model with Long-Term Imagination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DMWM: Dual-Mind World Model with Long-Term Imagination"
                },
                "summary": "Imagination in world models is crucial for enabling agents to learn\nlong-horizon policy in a sample-efficient manner. Existing recurrent\nstate-space model (RSSM)-based world models depend on single-step statistical\ninference to capture the environment dynamics, and, hence, they are unable to\nperform long-term imagination tasks due to the accumulation of prediction\nerrors. Inspired by the dual-process theory of human cognition, we propose a\nnovel dual-mind world model (DMWM) framework that integrates logical reasoning\nto enable imagination with logical consistency. DMWM is composed of two\ncomponents: an RSSM-based System 1 (RSSM-S1) component that handles state\ntransitions in an intuitive manner and a logic-integrated neural network-based\nSystem 2 (LINN-S2) component that guides the imagination process through\nhierarchical deep logical reasoning. The inter-system feedback mechanism is\ndesigned to ensure that the imagination process follows the logical rules of\nthe real environment. The proposed framework is evaluated on benchmark tasks\nthat require long-term planning from the DMControl suite. Extensive\nexperimental results demonstrate that the proposed framework yields significant\nimprovements in terms of logical coherence, trial efficiency, data efficiency\nand long-term imagination over the state-of-the-art world models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Imagination in world models is crucial for enabling agents to learn\nlong-horizon policy in a sample-efficient manner. Existing recurrent\nstate-space model (RSSM)-based world models depend on single-step statistical\ninference to capture the environment dynamics, and, hence, they are unable to\nperform long-term imagination tasks due to the accumulation of prediction\nerrors. Inspired by the dual-process theory of human cognition, we propose a\nnovel dual-mind world model (DMWM) framework that integrates logical reasoning\nto enable imagination with logical consistency. DMWM is composed of two\ncomponents: an RSSM-based System 1 (RSSM-S1) component that handles state\ntransitions in an intuitive manner and a logic-integrated neural network-based\nSystem 2 (LINN-S2) component that guides the imagination process through\nhierarchical deep logical reasoning. The inter-system feedback mechanism is\ndesigned to ensure that the imagination process follows the logical rules of\nthe real environment. The proposed framework is evaluated on benchmark tasks\nthat require long-term planning from the DMControl suite. Extensive\nexperimental results demonstrate that the proposed framework yields significant\nimprovements in terms of logical coherence, trial efficiency, data efficiency\nand long-term imagination over the state-of-the-art world models."
                },
                "authors": [
                    {
                        "name": "Lingyi Wang"
                    },
                    {
                        "name": "Rashed Shelim"
                    },
                    {
                        "name": "Walid Saad"
                    },
                    {
                        "name": "Naren Ramakrishnan"
                    }
                ],
                "author_detail": {
                    "name": "Naren Ramakrishnan"
                },
                "author": "Naren Ramakrishnan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07591v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07591v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07589v1",
                "updated": "2025-02-11T14:38:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    38,
                    53,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T14:38:53Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    38,
                    53,
                    1,
                    42,
                    0
                ],
                "title": "Quantum State Tomography in a Third-Order Integrated Optical Parametric\n  Oscillator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum State Tomography in a Third-Order Integrated Optical Parametric\n  Oscillator"
                },
                "summary": "We measured the covariance matrix of the fields generated in an integrated\nthird-order optical parametric oscillator operating above threshold. We\nobserved up to $(2.3 \\pm 0.3)$ dB of squeezing in amplitude difference,\ninferred $(4.9 \\pm 0.7)$ dB of on-chip squeezing, while an excess of noise for\nthe sum of conjugated quadratures hinders the entanglement. The degradation of\namplitude correlations and state purity for the increasing of the pump power is\nconsistent with the observed growth of the phase noise of the fields, showing\nthe necessity of strategies for phase noise control aiming at entanglement\ngeneration in these systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We measured the covariance matrix of the fields generated in an integrated\nthird-order optical parametric oscillator operating above threshold. We\nobserved up to $(2.3 \\pm 0.3)$ dB of squeezing in amplitude difference,\ninferred $(4.9 \\pm 0.7)$ dB of on-chip squeezing, while an excess of noise for\nthe sum of conjugated quadratures hinders the entanglement. The degradation of\namplitude correlations and state purity for the increasing of the pump power is\nconsistent with the observed growth of the phase noise of the fields, showing\nthe necessity of strategies for phase noise control aiming at entanglement\ngeneration in these systems."
                },
                "authors": [
                    {
                        "name": "Roger Alfredo Kgler"
                    },
                    {
                        "name": "Gabriel Couto Rickli"
                    },
                    {
                        "name": "Renato Ribeiro Domeneguetti"
                    },
                    {
                        "name": "Xingchen Ji"
                    },
                    {
                        "name": "Alexander L. Gaeta"
                    },
                    {
                        "name": "Michal Lipson"
                    },
                    {
                        "name": "Marcelo Martinelli"
                    },
                    {
                        "name": "Paulo Nussenzveig"
                    }
                ],
                "author_detail": {
                    "name": "Paulo Nussenzveig"
                },
                "author": "Paulo Nussenzveig",
                "arxiv_doi": "10.1364/OL.521339",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1364/OL.521339",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.07589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "main text: 4 pages, 6 figures; supplemental material: 13 pages, 15\n  figures",
                "arxiv_journal_ref": "Opt. Lett. 49, 3150-3153 (2024)",
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20385v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20385v2",
                "updated": "2025-02-11T14:37:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    37,
                    0,
                    1,
                    42,
                    0
                ],
                "published": "2024-12-29T07:21:13Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    7,
                    21,
                    13,
                    6,
                    364,
                    0
                ],
                "title": "A Particle Algorithm for Mean-Field Variational Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Particle Algorithm for Mean-Field Variational Inference"
                },
                "summary": "Variational inference is a fast and scalable alternative to Markov chain\nMonte Carlo and has been widely applied to posterior inference tasks in\nstatistics and machine learning. A traditional approach for implementing\nmean-field variational inference (MFVI) is coordinate ascent variational\ninference (CAVI), which relies crucially on parametric assumptions on complete\nconditionals. In this paper, we introduce a novel particle-based algorithm for\nmean-field variational inference, which we term PArticle VI (PAVI). Notably,\nour algorithm does not rely on parametric assumptions on complete conditionals,\nand it applies to the nonparametric setting. We provide non-asymptotic\nfinite-particle convergence guarantee for our algorithm. To our knowledge, this\nis the first end-to-end guarantee for particle-based MFVI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational inference is a fast and scalable alternative to Markov chain\nMonte Carlo and has been widely applied to posterior inference tasks in\nstatistics and machine learning. A traditional approach for implementing\nmean-field variational inference (MFVI) is coordinate ascent variational\ninference (CAVI), which relies crucially on parametric assumptions on complete\nconditionals. In this paper, we introduce a novel particle-based algorithm for\nmean-field variational inference, which we term PArticle VI (PAVI). Notably,\nour algorithm does not rely on parametric assumptions on complete conditionals,\nand it applies to the nonparametric setting. We provide non-asymptotic\nfinite-particle convergence guarantee for our algorithm. To our knowledge, this\nis the first end-to-end guarantee for particle-based MFVI."
                },
                "authors": [
                    {
                        "name": "Qiang Du"
                    },
                    {
                        "name": "Kaizheng Wang"
                    },
                    {
                        "name": "Edith Zhang"
                    },
                    {
                        "name": "Chenyang Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Chenyang Zhong"
                },
                "author": "Chenyang Zhong",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20385v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20385v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07586v1",
                "updated": "2025-02-11T14:34:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    34,
                    5,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T14:34:05Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    34,
                    5,
                    1,
                    42,
                    0
                ],
                "title": "We Can't Understand AI Using our Existing Vocabulary",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We Can't Understand AI Using our Existing Vocabulary"
                },
                "summary": "This position paper argues that, in order to understand AI, we cannot rely on\nour existing vocabulary of human words. Instead, we should strive to develop\nneologisms: new words that represent precise human concepts that we want to\nteach machines, or machine concepts that we need to learn. We start from the\npremise that humans and machines have differing concepts. This means\ninterpretability can be framed as a communication problem: humans must be able\nto reference and control machine concepts, and communicate human concepts to\nmachines. Creating a shared human-machine language through developing\nneologisms, we believe, could solve this communication problem. Successful\nneologisms achieve a useful amount of abstraction: not too detailed, so they're\nreusable in many contexts, and not too high-level, so they convey precise\ninformation. As a proof of concept, we demonstrate how a \"length neologism\"\nenables controlling LLM response length, while a \"diversity neologism\" allows\nsampling more variable responses. Taken together, we argue that we cannot\nunderstand AI using our existing vocabulary, and expanding it through\nneologisms creates opportunities for both controlling and understanding\nmachines better.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This position paper argues that, in order to understand AI, we cannot rely on\nour existing vocabulary of human words. Instead, we should strive to develop\nneologisms: new words that represent precise human concepts that we want to\nteach machines, or machine concepts that we need to learn. We start from the\npremise that humans and machines have differing concepts. This means\ninterpretability can be framed as a communication problem: humans must be able\nto reference and control machine concepts, and communicate human concepts to\nmachines. Creating a shared human-machine language through developing\nneologisms, we believe, could solve this communication problem. Successful\nneologisms achieve a useful amount of abstraction: not too detailed, so they're\nreusable in many contexts, and not too high-level, so they convey precise\ninformation. As a proof of concept, we demonstrate how a \"length neologism\"\nenables controlling LLM response length, while a \"diversity neologism\" allows\nsampling more variable responses. Taken together, we argue that we cannot\nunderstand AI using our existing vocabulary, and expanding it through\nneologisms creates opportunities for both controlling and understanding\nmachines better."
                },
                "authors": [
                    {
                        "name": "John Hewitt"
                    },
                    {
                        "name": "Robert Geirhos"
                    },
                    {
                        "name": "Been Kim"
                    }
                ],
                "author_detail": {
                    "name": "Been Kim"
                },
                "author": "Been Kim",
                "arxiv_comment": "Position paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04964v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04964v2",
                "updated": "2025-02-11T14:32:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    32,
                    15,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-07T14:30:12Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    14,
                    30,
                    12,
                    4,
                    38,
                    0
                ],
                "title": "CoCoA: A Generalized Approach to Uncertainty Quantification by\n  Integrating Confidence and Consistency of LLM Outputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoCoA: A Generalized Approach to Uncertainty Quantification by\n  Integrating Confidence and Consistency of LLM Outputs"
                },
                "summary": "Uncertainty quantification (UQ) methods for Large Language Models (LLMs)\nencompasses a variety of approaches, with two major types being particularly\nprominent: information-based, which focus on model confidence expressed as\ntoken probabilities, and consistency-based, which assess the semantic\nrelationship between multiple outputs generated using repeated sampling.\nSeveral recent methods have combined these two approaches and shown impressive\nperformance in various applications. However, they sometimes fail to outperform\nmuch simpler baseline methods. Our investigation reveals distinctive\ncharacteristics of LLMs as probabilistic models, which help to explain why\nthese UQ methods underperform in certain tasks. Based on these findings, we\npropose a new way of synthesizing model confidence and output consistency that\nleads to a family of efficient and robust UQ methods. We evaluate our approach\nacross a variety of tasks such as question answering, abstractive\nsummarization, and machine translation, demonstrating sizable improvements over\nstate-of-the-art UQ approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty quantification (UQ) methods for Large Language Models (LLMs)\nencompasses a variety of approaches, with two major types being particularly\nprominent: information-based, which focus on model confidence expressed as\ntoken probabilities, and consistency-based, which assess the semantic\nrelationship between multiple outputs generated using repeated sampling.\nSeveral recent methods have combined these two approaches and shown impressive\nperformance in various applications. However, they sometimes fail to outperform\nmuch simpler baseline methods. Our investigation reveals distinctive\ncharacteristics of LLMs as probabilistic models, which help to explain why\nthese UQ methods underperform in certain tasks. Based on these findings, we\npropose a new way of synthesizing model confidence and output consistency that\nleads to a family of efficient and robust UQ methods. We evaluate our approach\nacross a variety of tasks such as question answering, abstractive\nsummarization, and machine translation, demonstrating sizable improvements over\nstate-of-the-art UQ approaches."
                },
                "authors": [
                    {
                        "name": "Roman Vashurin"
                    },
                    {
                        "name": "Maiya Goloburda"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Artem Shelmanov"
                    },
                    {
                        "name": "Maxim Panov"
                    }
                ],
                "author_detail": {
                    "name": "Maxim Panov"
                },
                "author": "Maxim Panov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04964v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04964v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03307v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03307v2",
                "updated": "2025-02-11T14:29:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    29,
                    44,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-05T16:08:05Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    8,
                    5,
                    2,
                    36,
                    0
                ],
                "title": "Intent Representation Learning with Large Language Model for\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intent Representation Learning with Large Language Model for\n  Recommendation"
                },
                "summary": "Intent-based recommender systems have garnered significant attention for\nuncovering latent fine-grained preferences. Intents, as underlying factors of\ninteractions, are crucial for improving recommendation interpretability. Most\nmethods define intents as learnable parameters updated alongside interactions.\nHowever, existing frameworks often overlook textual information (e.g., user\nreviews, item descriptions), which is crucial for alleviating the sparsity of\ninteraction intents. Exploring these multimodal intents, especially the\ninherent differences in representation spaces, poses two key challenges: i) How\nto align multimodal intents and effectively mitigate noise issues; ii) How to\nextract and match latent key intents across modalities. To tackle these\nchallenges, we propose a model-agnostic framework, Intent Representation\nLearning with Large Language Model (IRLLRec), which leverages large language\nmodels (LLMs) to construct multimodal intents and enhance recommendations.\nSpecifically, IRLLRec employs a dual-tower architecture to learn multimodal\nintent representations. Next, we propose pairwise and translation alignment to\neliminate inter-modal differences and enhance robustness against noisy input\nfeatures. Finally, to better match textual and interaction-based intents, we\nemploy momentum distillation to perform teacher-student learning on fused\nintent representations. Empirical evaluations on three datasets show that our\nIRLLRec framework outperforms baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intent-based recommender systems have garnered significant attention for\nuncovering latent fine-grained preferences. Intents, as underlying factors of\ninteractions, are crucial for improving recommendation interpretability. Most\nmethods define intents as learnable parameters updated alongside interactions.\nHowever, existing frameworks often overlook textual information (e.g., user\nreviews, item descriptions), which is crucial for alleviating the sparsity of\ninteraction intents. Exploring these multimodal intents, especially the\ninherent differences in representation spaces, poses two key challenges: i) How\nto align multimodal intents and effectively mitigate noise issues; ii) How to\nextract and match latent key intents across modalities. To tackle these\nchallenges, we propose a model-agnostic framework, Intent Representation\nLearning with Large Language Model (IRLLRec), which leverages large language\nmodels (LLMs) to construct multimodal intents and enhance recommendations.\nSpecifically, IRLLRec employs a dual-tower architecture to learn multimodal\nintent representations. Next, we propose pairwise and translation alignment to\neliminate inter-modal differences and enhance robustness against noisy input\nfeatures. Finally, to better match textual and interaction-based intents, we\nemploy momentum distillation to perform teacher-student learning on fused\nintent representations. Empirical evaluations on three datasets show that our\nIRLLRec framework outperforms baselines."
                },
                "authors": [
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Lei Sang"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Yiwen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yiwen Zhang"
                },
                "author": "Yiwen Zhang",
                "arxiv_comment": "11 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03307v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03307v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07580v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07580v1",
                "updated": "2025-02-11T14:27:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    27,
                    10,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T14:27:10Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    27,
                    10,
                    1,
                    42,
                    0
                ],
                "title": "Generative Modeling with Bayesian Sample Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Modeling with Bayesian Sample Inference"
                },
                "summary": "We derive a novel generative model from the simple act of Gaussian posterior\ninference. Treating the generated sample as an unknown variable to infer lets\nus formulate the sampling process in the language of Bayesian probability. Our\nmodel uses a sequence of prediction and posterior update steps to narrow down\nthe unknown sample from a broad initial belief. In addition to a rigorous\ntheoretical analysis, we establish a connection between our model and diffusion\nmodels and show that it includes Bayesian Flow Networks (BFNs) as a special\ncase. In our experiments, we demonstrate improved performance over both BFNs\nand Variational Diffusion Models, achieving competitive likelihood scores on\nCIFAR10 and ImageNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We derive a novel generative model from the simple act of Gaussian posterior\ninference. Treating the generated sample as an unknown variable to infer lets\nus formulate the sampling process in the language of Bayesian probability. Our\nmodel uses a sequence of prediction and posterior update steps to narrow down\nthe unknown sample from a broad initial belief. In addition to a rigorous\ntheoretical analysis, we establish a connection between our model and diffusion\nmodels and show that it includes Bayesian Flow Networks (BFNs) as a special\ncase. In our experiments, we demonstrate improved performance over both BFNs\nand Variational Diffusion Models, achieving competitive likelihood scores on\nCIFAR10 and ImageNet."
                },
                "authors": [
                    {
                        "name": "Marten Lienen"
                    },
                    {
                        "name": "Marcel Kollovieh"
                    },
                    {
                        "name": "Stephan Gnnemann"
                    }
                ],
                "author_detail": {
                    "name": "Stephan Gnnemann"
                },
                "author": "Stephan Gnnemann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07580v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07580v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07578v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07578v1",
                "updated": "2025-02-11T14:25:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    25,
                    20,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T14:25:20Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    25,
                    20,
                    1,
                    42,
                    0
                ],
                "title": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference"
                },
                "summary": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users to pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.3$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users to pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.3$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs."
                },
                "authors": [
                    {
                        "name": "Yufeng Gu"
                    },
                    {
                        "name": "Alireza Khadem"
                    },
                    {
                        "name": "Sumanth Umesh"
                    },
                    {
                        "name": "Ning Liang"
                    },
                    {
                        "name": "Xavier Servot"
                    },
                    {
                        "name": "Onur Mutlu"
                    },
                    {
                        "name": "Ravi Iyer"
                    },
                    {
                        "name": "Reetuparna Das"
                    }
                ],
                "author_detail": {
                    "name": "Reetuparna Das"
                },
                "author": "Reetuparna Das",
                "arxiv_doi": "10.1145/3676641.3716267",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3716267",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.07578v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07578v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, Volume\n  2 (ASPLOS'25)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07664v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07664v2",
                "updated": "2025-02-11T14:05:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    5,
                    29,
                    1,
                    42,
                    0
                ],
                "published": "2024-04-11T11:55:42Z",
                "published_parsed": [
                    2024,
                    4,
                    11,
                    11,
                    55,
                    42,
                    3,
                    102,
                    0
                ],
                "title": "Finding Dino: A Plug-and-Play Framework for Zero-Shot Detection of\n  Out-of-Distribution Objects Using Prototypes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finding Dino: A Plug-and-Play Framework for Zero-Shot Detection of\n  Out-of-Distribution Objects Using Prototypes"
                },
                "summary": "Detecting and localising unknown or out-of-distribution (OOD) objects in any\nscene can be a challenging task in vision, particularly in safety-critical\ncases involving autonomous systems like automated vehicles or trains.\nSupervised anomaly segmentation or open-world object detection models depend on\ntraining on exhaustively annotated datasets for every domain and still struggle\nin distinguishing between background and OOD objects. In this work, we present\na plug-and-play framework - PRototype-based OOD detection Without Labels\n(PROWL). It is an inference-based method that does not require training on the\ndomain dataset and relies on extracting relevant features from self-supervised\npre-trained models. PROWL can be easily adapted to detect in-domain objects in\nany operational design domain (ODD) in a zero-shot manner by specifying a list\nof known classes from this domain. PROWL, as a first zero-shot unsupervised\nmethod, achieves state-of-the-art results on the RoadAnomaly and RoadObstacle\ndatasets provided in road driving benchmarks - SegmentMeIfYouCan (SMIYC) and\nFishyscapes, as well as comparable performance against existing supervised\nmethods trained without auxiliary OOD data. We also demonstrate its\ngeneralisability to other domains such as rail and maritime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting and localising unknown or out-of-distribution (OOD) objects in any\nscene can be a challenging task in vision, particularly in safety-critical\ncases involving autonomous systems like automated vehicles or trains.\nSupervised anomaly segmentation or open-world object detection models depend on\ntraining on exhaustively annotated datasets for every domain and still struggle\nin distinguishing between background and OOD objects. In this work, we present\na plug-and-play framework - PRototype-based OOD detection Without Labels\n(PROWL). It is an inference-based method that does not require training on the\ndomain dataset and relies on extracting relevant features from self-supervised\npre-trained models. PROWL can be easily adapted to detect in-domain objects in\nany operational design domain (ODD) in a zero-shot manner by specifying a list\nof known classes from this domain. PROWL, as a first zero-shot unsupervised\nmethod, achieves state-of-the-art results on the RoadAnomaly and RoadObstacle\ndatasets provided in road driving benchmarks - SegmentMeIfYouCan (SMIYC) and\nFishyscapes, as well as comparable performance against existing supervised\nmethods trained without auxiliary OOD data. We also demonstrate its\ngeneralisability to other domains such as rail and maritime."
                },
                "authors": [
                    {
                        "name": "Poulami Sinhamahapatra"
                    },
                    {
                        "name": "Franziska Schwaiger"
                    },
                    {
                        "name": "Shirsha Bose"
                    },
                    {
                        "name": "Huiyu Wang"
                    },
                    {
                        "name": "Karsten Roscher"
                    },
                    {
                        "name": "Stephan Guennemann"
                    }
                ],
                "author_detail": {
                    "name": "Stephan Guennemann"
                },
                "author": "Stephan Guennemann",
                "arxiv_comment": "Accepted in IEEE/CVF Winter Conference on Applications of Computer\n  Vision (WACV) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07664v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07664v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04315v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04315v3",
                "updated": "2025-02-11T14:01:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    1,
                    39,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-06T18:57:06Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    57,
                    6,
                    3,
                    37,
                    0
                ],
                "title": "ChameleonLLM: Batch-Aware Dynamic Low-Rank Adaptation via Inference-Time\n  Clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChameleonLLM: Batch-Aware Dynamic Low-Rank Adaptation via Inference-Time\n  Clusters"
                },
                "summary": "Recent advances in large language models (LLMs) have shown remarkable\nperformance across diverse tasks. However, these models are typically deployed\nwith fixed weights, which limits their ability to adapt dynamically to the\nvariability inherent in real-world data during inference. This paper introduces\nChameleonLLM, a novel framework that enables inference-time adaptation of LLMs\nby leveraging batch-aware clustering and on-the-fly generation of low-rank\nupdates. Unlike traditional fine-tuning approaches such as Low-Rank Adaptation\n(LoRA) or methods that rely on a fixed set of pre-learned uniforms (changeable\nmasks), our method dynamically generates adaptive modifications to the decoder\nweights based on the aggregated statistics of clustered batches. By\nintelligently grouping similar inputs and computing context-aware low-rank\nupdates via a hyper-network, ChameleonLLM achieves significant performance\ngains, outperforming conventional LoRA methods while eliminating the overhead\nof maintaining multiple expert models. Our experiments highlight the potential\nof our approach to serve as a versatile and highly adaptive solution for\nlanguage model inference. ChameleonLLM is open-sourced to ensure the\nreproducibility of our experiments:\nhttps://anonymous.4open.science/r/ChamaleonLLM/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have shown remarkable\nperformance across diverse tasks. However, these models are typically deployed\nwith fixed weights, which limits their ability to adapt dynamically to the\nvariability inherent in real-world data during inference. This paper introduces\nChameleonLLM, a novel framework that enables inference-time adaptation of LLMs\nby leveraging batch-aware clustering and on-the-fly generation of low-rank\nupdates. Unlike traditional fine-tuning approaches such as Low-Rank Adaptation\n(LoRA) or methods that rely on a fixed set of pre-learned uniforms (changeable\nmasks), our method dynamically generates adaptive modifications to the decoder\nweights based on the aggregated statistics of clustered batches. By\nintelligently grouping similar inputs and computing context-aware low-rank\nupdates via a hyper-network, ChameleonLLM achieves significant performance\ngains, outperforming conventional LoRA methods while eliminating the overhead\nof maintaining multiple expert models. Our experiments highlight the potential\nof our approach to serve as a versatile and highly adaptive solution for\nlanguage model inference. ChameleonLLM is open-sourced to ensure the\nreproducibility of our experiments:\nhttps://anonymous.4open.science/r/ChamaleonLLM/"
                },
                "authors": [
                    {
                        "name": "Kamer Ali Yuksel"
                    },
                    {
                        "name": "Hassan Sawaf"
                    }
                ],
                "author_detail": {
                    "name": "Hassan Sawaf"
                },
                "author": "Hassan Sawaf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04315v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04315v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07563v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07563v1",
                "updated": "2025-02-11T14:01:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    1,
                    39,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T14:01:39Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    1,
                    39,
                    1,
                    42,
                    0
                ],
                "title": "LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its\n  Hybrid",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its\n  Hybrid"
                },
                "summary": "Linear sequence modeling approaches, such as linear attention, provide\nadvantages like linear-time training and constant-memory inference over\nsequence lengths. However, existing sequence parallelism (SP) methods are\neither not optimized for the right-product-first feature of linear attention or\nuse a ring-style communication strategy, which results in lower computation\nparallelism, limits their scalability for longer sequences in distributed\nsystems. In this paper, we introduce LASP-2, a new SP method to enhance both\ncommunication and computation parallelism when training linear attention\ntransformer models with very-long input sequences. Compared to previous work\nLASP, LASP-2 rethinks the minimal communication requirement for SP on linear\nattention layers, reorganizes the whole communication-computation workflow of\nLASP. In this way, only one single AllGather collective communication is needed\non intermediate memory states, whose sizes are independent of the sequence\nlength, leading to significant improvements of both communication and\ncomputation parallelism, as well as their overlap. Additionally, we extend\nLASP-2 to LASP-2H by applying similar communication redesign to standard\nattention modules, offering an efficient SP solution for hybrid models that\nblend linear and standard attention layers. Our evaluation on a Linear-Llama3\nmodel, a variant of Llama3 with linear attention replacing standard attention,\ndemonstrates the effectiveness of LASP-2 and LASP-2H. Specifically, LASP-2\nachieves training speed improvements of 15.2% over LASP and 36.6% over Ring\nAttention, with a sequence length of 2048K across 64 GPUs. The Code is released\nas a part of: https://github.com/OpenSparseLLMs/Linear-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear sequence modeling approaches, such as linear attention, provide\nadvantages like linear-time training and constant-memory inference over\nsequence lengths. However, existing sequence parallelism (SP) methods are\neither not optimized for the right-product-first feature of linear attention or\nuse a ring-style communication strategy, which results in lower computation\nparallelism, limits their scalability for longer sequences in distributed\nsystems. In this paper, we introduce LASP-2, a new SP method to enhance both\ncommunication and computation parallelism when training linear attention\ntransformer models with very-long input sequences. Compared to previous work\nLASP, LASP-2 rethinks the minimal communication requirement for SP on linear\nattention layers, reorganizes the whole communication-computation workflow of\nLASP. In this way, only one single AllGather collective communication is needed\non intermediate memory states, whose sizes are independent of the sequence\nlength, leading to significant improvements of both communication and\ncomputation parallelism, as well as their overlap. Additionally, we extend\nLASP-2 to LASP-2H by applying similar communication redesign to standard\nattention modules, offering an efficient SP solution for hybrid models that\nblend linear and standard attention layers. Our evaluation on a Linear-Llama3\nmodel, a variant of Llama3 with linear attention replacing standard attention,\ndemonstrates the effectiveness of LASP-2 and LASP-2H. Specifically, LASP-2\nachieves training speed improvements of 15.2% over LASP and 36.6% over Ring\nAttention, with a sequence length of 2048K across 64 GPUs. The Code is released\nas a part of: https://github.com/OpenSparseLLMs/Linear-MoE."
                },
                "authors": [
                    {
                        "name": "Weigao Sun"
                    },
                    {
                        "name": "Disen Lan"
                    },
                    {
                        "name": "Yiran Zhong"
                    },
                    {
                        "name": "Xiaoye Qu"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "Technical report, 17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07563v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07563v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18005v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18005v3",
                "updated": "2025-02-11T13:58:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    13,
                    58,
                    39,
                    1,
                    42,
                    0
                ],
                "published": "2025-01-29T21:40:32Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    21,
                    40,
                    32,
                    2,
                    29,
                    0
                ],
                "title": "Fault Localization via Fine-tuning Large Language Models with Mutation\n  Generated Stack Traces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fault Localization via Fine-tuning Large Language Models with Mutation\n  Generated Stack Traces"
                },
                "summary": "Abrupt and unexpected terminations of software are termed as software\ncrashes. They can be challenging to analyze. Finding the root cause requires\nextensive manual effort and expertise to connect information sources like stack\ntraces, source code, and logs. Typical approaches to fault localization require\neither test failures or source code. Crashes occurring in production\nenvironments, such as that of SAP HANA, provide solely crash logs and stack\ntraces. We present a novel approach to localize faults based only on the stack\ntrace information and no additional runtime information, by fine-tuning large\nlanguage models (LLMs). We address complex cases where the root cause of a\ncrash differs from the technical cause, and is not located in the innermost\nframe of the stack trace. As the number of historic crashes is insufficient to\nfine-tune LLMs, we augment our dataset by leveraging code mutators to inject\nsynthetic crashes into the code base. By fine-tuning on 64,369 crashes\nresulting from 4.1 million mutations of the HANA code base, we can correctly\npredict the root cause location of a crash with an accuracy of 66.9\\% while\nbaselines only achieve 12.6% and 10.6%. We substantiate the generalizability of\nour approach by evaluating on two additional open-source databases, SQLite and\nDuckDB, achieving accuracies of 63% and 74%, respectively. Across all our\nexperiments, fine-tuning consistently outperformed prompting non-finetuned LLMs\nfor localizing faults in our datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abrupt and unexpected terminations of software are termed as software\ncrashes. They can be challenging to analyze. Finding the root cause requires\nextensive manual effort and expertise to connect information sources like stack\ntraces, source code, and logs. Typical approaches to fault localization require\neither test failures or source code. Crashes occurring in production\nenvironments, such as that of SAP HANA, provide solely crash logs and stack\ntraces. We present a novel approach to localize faults based only on the stack\ntrace information and no additional runtime information, by fine-tuning large\nlanguage models (LLMs). We address complex cases where the root cause of a\ncrash differs from the technical cause, and is not located in the innermost\nframe of the stack trace. As the number of historic crashes is insufficient to\nfine-tune LLMs, we augment our dataset by leveraging code mutators to inject\nsynthetic crashes into the code base. By fine-tuning on 64,369 crashes\nresulting from 4.1 million mutations of the HANA code base, we can correctly\npredict the root cause location of a crash with an accuracy of 66.9\\% while\nbaselines only achieve 12.6% and 10.6%. We substantiate the generalizability of\nour approach by evaluating on two additional open-source databases, SQLite and\nDuckDB, achieving accuracies of 63% and 74%, respectively. Across all our\nexperiments, fine-tuning consistently outperformed prompting non-finetuned LLMs\nfor localizing faults in our datasets."
                },
                "authors": [
                    {
                        "name": "Neetha Jambigi"
                    },
                    {
                        "name": "Bartosz Bogacz"
                    },
                    {
                        "name": "Moritz Mueller"
                    },
                    {
                        "name": "Thomas Bach"
                    },
                    {
                        "name": "Michael Felderer"
                    }
                ],
                "author_detail": {
                    "name": "Michael Felderer"
                },
                "author": "Michael Felderer",
                "arxiv_comment": "I do not have the necessary approvals to out the paper on Arxiv from\n  my organization yet. I was too soon to do this",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18005v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18005v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07557v1",
                "updated": "2025-02-11T13:50:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    13,
                    50,
                    50,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T13:50:50Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    13,
                    50,
                    50,
                    1,
                    42,
                    0
                ],
                "title": "JBShield: Defending Large Language Models from Jailbreak Attacks through\n  Activated Concept Analysis and Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JBShield: Defending Large Language Models from Jailbreak Attacks through\n  Activated Concept Analysis and Manipulation"
                },
                "summary": "Despite the implementation of safety alignment strategies, large language\nmodels (LLMs) remain vulnerable to jailbreak attacks, which undermine these\nsafety guardrails and pose significant security threats. Some defenses have\nbeen proposed to detect or mitigate jailbreaks, but they are unable to\nwithstand the test of time due to an insufficient understanding of jailbreak\nmechanisms. In this work, we investigate the mechanisms behind jailbreaks based\non the Linear Representation Hypothesis (LRH), which states that neural\nnetworks encode high-level concepts as subspaces in their hidden\nrepresentations. We define the toxic semantics in harmful and jailbreak prompts\nas toxic concepts and describe the semantics in jailbreak prompts that\nmanipulate LLMs to comply with unsafe requests as jailbreak concepts. Through\nconcept extraction and analysis, we reveal that LLMs can recognize the toxic\nconcepts in both harmful and jailbreak prompts. However, unlike harmful\nprompts, jailbreak prompts activate the jailbreak concepts and alter the LLM\noutput from rejection to compliance. Building on our analysis, we propose a\ncomprehensive jailbreak defense framework, JBShield, consisting of two key\ncomponents: jailbreak detection JBShield-D and mitigation JBShield-M.\nJBShield-D identifies jailbreak prompts by determining whether the input\nactivates both toxic and jailbreak concepts. When a jailbreak prompt is\ndetected, JBShield-M adjusts the hidden representations of the target LLM by\nenhancing the toxic concept and weakening the jailbreak concept, ensuring LLMs\nproduce safe content. Extensive experiments demonstrate the superior\nperformance of JBShield, achieving an average detection accuracy of 0.95 and\nreducing the average attack success rate of various jailbreak attacks to 2%\nfrom 61% across distinct LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the implementation of safety alignment strategies, large language\nmodels (LLMs) remain vulnerable to jailbreak attacks, which undermine these\nsafety guardrails and pose significant security threats. Some defenses have\nbeen proposed to detect or mitigate jailbreaks, but they are unable to\nwithstand the test of time due to an insufficient understanding of jailbreak\nmechanisms. In this work, we investigate the mechanisms behind jailbreaks based\non the Linear Representation Hypothesis (LRH), which states that neural\nnetworks encode high-level concepts as subspaces in their hidden\nrepresentations. We define the toxic semantics in harmful and jailbreak prompts\nas toxic concepts and describe the semantics in jailbreak prompts that\nmanipulate LLMs to comply with unsafe requests as jailbreak concepts. Through\nconcept extraction and analysis, we reveal that LLMs can recognize the toxic\nconcepts in both harmful and jailbreak prompts. However, unlike harmful\nprompts, jailbreak prompts activate the jailbreak concepts and alter the LLM\noutput from rejection to compliance. Building on our analysis, we propose a\ncomprehensive jailbreak defense framework, JBShield, consisting of two key\ncomponents: jailbreak detection JBShield-D and mitigation JBShield-M.\nJBShield-D identifies jailbreak prompts by determining whether the input\nactivates both toxic and jailbreak concepts. When a jailbreak prompt is\ndetected, JBShield-M adjusts the hidden representations of the target LLM by\nenhancing the toxic concept and weakening the jailbreak concept, ensuring LLMs\nproduce safe content. Extensive experiments demonstrate the superior\nperformance of JBShield, achieving an average detection accuracy of 0.95 and\nreducing the average attack success rate of various jailbreak attacks to 2%\nfrom 61% across distinct LLMs."
                },
                "authors": [
                    {
                        "name": "Shenyi Zhang"
                    },
                    {
                        "name": "Yuchen Zhai"
                    },
                    {
                        "name": "Keyan Guo"
                    },
                    {
                        "name": "Hongxin Hu"
                    },
                    {
                        "name": "Shengnan Guo"
                    },
                    {
                        "name": "Zheng Fang"
                    },
                    {
                        "name": "Lingchen Zhao"
                    },
                    {
                        "name": "Chao Shen"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Qian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qian Wang"
                },
                "author": "Qian Wang",
                "arxiv_comment": "To Appear in the 34rd USENIX Security Symposium, August 13-15, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07556v1",
                "updated": "2025-02-11T13:48:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    13,
                    48,
                    11,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T13:48:11Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    13,
                    48,
                    11,
                    1,
                    42,
                    0
                ],
                "title": "SketchFlex: Facilitating Spatial-Semantic Coherence in Text-to-Image\n  Generation with Region-Based Sketches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SketchFlex: Facilitating Spatial-Semantic Coherence in Text-to-Image\n  Generation with Region-Based Sketches"
                },
                "summary": "Text-to-image models can generate visually appealing images from text\ndescriptions. Efforts have been devoted to improving model controls with prompt\ntuning and spatial conditioning. However, our formative study highlights the\nchallenges for non-expert users in crafting appropriate prompts and specifying\nfine-grained spatial conditions (e.g., depth or canny references) to generate\nsemantically cohesive images, especially when multiple objects are involved. In\nresponse, we introduce SketchFlex, an interactive system designed to improve\nthe flexibility of spatially conditioned image generation using rough region\nsketches. The system automatically infers user prompts with rational\ndescriptions within a semantic space enriched by crowd-sourced object\nattributes and relationships. Additionally, SketchFlex refines users' rough\nsketches into canny-based shape anchors, ensuring the generation quality and\nalignment of user intentions. Experimental results demonstrate that SketchFlex\nachieves more cohesive image generations than end-to-end models, meanwhile\nsignificantly reducing cognitive load and better matching user intentions\ncompared to region-based generation baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image models can generate visually appealing images from text\ndescriptions. Efforts have been devoted to improving model controls with prompt\ntuning and spatial conditioning. However, our formative study highlights the\nchallenges for non-expert users in crafting appropriate prompts and specifying\nfine-grained spatial conditions (e.g., depth or canny references) to generate\nsemantically cohesive images, especially when multiple objects are involved. In\nresponse, we introduce SketchFlex, an interactive system designed to improve\nthe flexibility of spatially conditioned image generation using rough region\nsketches. The system automatically infers user prompts with rational\ndescriptions within a semantic space enriched by crowd-sourced object\nattributes and relationships. Additionally, SketchFlex refines users' rough\nsketches into canny-based shape anchors, ensuring the generation quality and\nalignment of user intentions. Experimental results demonstrate that SketchFlex\nachieves more cohesive image generations than end-to-end models, meanwhile\nsignificantly reducing cognitive load and better matching user intentions\ncompared to region-based generation baseline."
                },
                "authors": [
                    {
                        "name": "Haichuan Lin"
                    },
                    {
                        "name": "Yilin Ye"
                    },
                    {
                        "name": "Jiazhi Xia"
                    },
                    {
                        "name": "Wei Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zeng"
                },
                "author": "Wei Zeng",
                "arxiv_comment": "conference: CHI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07555v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07555v1",
                "updated": "2025-02-11T13:48:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    13,
                    48,
                    10,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T13:48:10Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    13,
                    48,
                    10,
                    1,
                    42,
                    0
                ],
                "title": "O1 Embedder: Let Retrievers Think Before Action",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "O1 Embedder: Let Retrievers Think Before Action"
                },
                "summary": "The growing power of large language models (LLMs) has revolutionized how\npeople access and utilize information. Notably, the LLMs excel at performing\nfine-grained data representation, which facilitates precise retrieval of\ninformation. They also generate high-quality answers based on external\nreferences, enabling the production of useful knowledge. The recent\nintroduction of reasoning models, like OpenAI O1 and DeepSeek R1, marks another\nleap forward, highlighting LLMs' ability to think progressively before\ndelivering final answers. This breakthrough significantly improves the ability\nto address complex tasks, e.g., coding and math proofs.\n  Inspired by this progress, we aim to develop similar capabilities for\nretrieval models, which hold great promise for tackling critical challenges in\nthe field, including multi-task retrieval, zero-shot retrieval, and tasks\nrequiring intensive reasoning of complex relationships. With this motivation,\nwe propose a novel approach called O1 Embedder, which generates useful thoughts\nfor the input query before making retrieval for the target documents. To\nrealize this objective, we conquer two technical difficulties. First, we design\na data synthesis workflow, creating training signals for O1 Embedder by\ngenerating initial thoughts from an LLM-expert and subsequently refining them\nusing a retrieval committee. Second, we optimize the training process, enabling\na pre-trained model to be jointly fine-tuned to generate retrieval thoughts via\nbehavior cloning and perform dense retrieval through contrastive learning. Our\napproach is evaluated by comprehensive experiments, where substantial\nimprovements are achieved across 12 popular datasets, spanning both in-domain\nand out-of-domain scenarios. These results highlight O1 Embedder's remarkable\naccuracy and generalizability, paving the way for the development of\nnext-generation IR foundation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing power of large language models (LLMs) has revolutionized how\npeople access and utilize information. Notably, the LLMs excel at performing\nfine-grained data representation, which facilitates precise retrieval of\ninformation. They also generate high-quality answers based on external\nreferences, enabling the production of useful knowledge. The recent\nintroduction of reasoning models, like OpenAI O1 and DeepSeek R1, marks another\nleap forward, highlighting LLMs' ability to think progressively before\ndelivering final answers. This breakthrough significantly improves the ability\nto address complex tasks, e.g., coding and math proofs.\n  Inspired by this progress, we aim to develop similar capabilities for\nretrieval models, which hold great promise for tackling critical challenges in\nthe field, including multi-task retrieval, zero-shot retrieval, and tasks\nrequiring intensive reasoning of complex relationships. With this motivation,\nwe propose a novel approach called O1 Embedder, which generates useful thoughts\nfor the input query before making retrieval for the target documents. To\nrealize this objective, we conquer two technical difficulties. First, we design\na data synthesis workflow, creating training signals for O1 Embedder by\ngenerating initial thoughts from an LLM-expert and subsequently refining them\nusing a retrieval committee. Second, we optimize the training process, enabling\na pre-trained model to be jointly fine-tuned to generate retrieval thoughts via\nbehavior cloning and perform dense retrieval through contrastive learning. Our\napproach is evaluated by comprehensive experiments, where substantial\nimprovements are achieved across 12 popular datasets, spanning both in-domain\nand out-of-domain scenarios. These results highlight O1 Embedder's remarkable\naccuracy and generalizability, paving the way for the development of\nnext-generation IR foundation models."
                },
                "authors": [
                    {
                        "name": "Ruin Yan"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Defu Lian"
                    }
                ],
                "author_detail": {
                    "name": "Defu Lian"
                },
                "author": "Defu Lian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07555v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07555v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14281v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14281v2",
                "updated": "2025-02-11T13:28:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    13,
                    28,
                    10,
                    1,
                    42,
                    0
                ],
                "published": "2024-10-18T08:38:12Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    8,
                    38,
                    12,
                    4,
                    292,
                    0
                ],
                "title": "PLMTrajRec: A Scalable and Generalizable Trajectory Recovery Method with\n  Pre-trained Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PLMTrajRec: A Scalable and Generalizable Trajectory Recovery Method with\n  Pre-trained Language Models"
                },
                "summary": "Spatiotemporal trajectory data is crucial for various applications. However,\nissues such as device malfunctions and network instability often cause sparse\ntrajectories, leading to lost detailed movement information. Recovering the\nmissing points in sparse trajectories to restore the detailed information is\nthus essential. Despite recent progress, several challenges remain. First, the\nlack of large-scale dense trajectory data makes it difficult to train a\ntrajectory recovery model from scratch. Second, the varying spatiotemporal\ncorrelations in sparse trajectories make it hard to generalize recovery across\ndifferent sampling intervals. Third, the lack of location information\ncomplicates the extraction of road conditions for missing points.\n  To address these challenges, we propose a novel trajectory recovery model\ncalled PLMTrajRec. It leverages the scalability of a pre-trained language model\n(PLM) and can be fine-tuned with only a limited set of dense trajectories. To\nhandle different sampling intervals in sparse trajectories, we first convert\neach trajectory's sampling interval and movement features into natural language\nrepresentations, allowing the PLM to recognize its interval. We then introduce\na trajectory encoder to unify trajectories of varying intervals into a single\ninterval and capture their spatiotemporal relationships. To obtain road\nconditions for missing points, we propose an area flow-guided implicit\ntrajectory prompt, which models road conditions by collecting traffic flows in\neach region. We also introduce a road condition passing mechanism that uses\nobserved points' road conditions to infer those of the missing points.\nExperiments on two public trajectory datasets with three sampling intervals\neach demonstrate the effectiveness, scalability, and generalization ability of\nPLMTrajRec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatiotemporal trajectory data is crucial for various applications. However,\nissues such as device malfunctions and network instability often cause sparse\ntrajectories, leading to lost detailed movement information. Recovering the\nmissing points in sparse trajectories to restore the detailed information is\nthus essential. Despite recent progress, several challenges remain. First, the\nlack of large-scale dense trajectory data makes it difficult to train a\ntrajectory recovery model from scratch. Second, the varying spatiotemporal\ncorrelations in sparse trajectories make it hard to generalize recovery across\ndifferent sampling intervals. Third, the lack of location information\ncomplicates the extraction of road conditions for missing points.\n  To address these challenges, we propose a novel trajectory recovery model\ncalled PLMTrajRec. It leverages the scalability of a pre-trained language model\n(PLM) and can be fine-tuned with only a limited set of dense trajectories. To\nhandle different sampling intervals in sparse trajectories, we first convert\neach trajectory's sampling interval and movement features into natural language\nrepresentations, allowing the PLM to recognize its interval. We then introduce\na trajectory encoder to unify trajectories of varying intervals into a single\ninterval and capture their spatiotemporal relationships. To obtain road\nconditions for missing points, we propose an area flow-guided implicit\ntrajectory prompt, which models road conditions by collecting traffic flows in\neach region. We also introduce a road condition passing mechanism that uses\nobserved points' road conditions to infer those of the missing points.\nExperiments on two public trajectory datasets with three sampling intervals\neach demonstrate the effectiveness, scalability, and generalization ability of\nPLMTrajRec."
                },
                "authors": [
                    {
                        "name": "Tonglong Wei"
                    },
                    {
                        "name": "Yan Lin"
                    },
                    {
                        "name": "Youfang Lin"
                    },
                    {
                        "name": "Shengnan Guo"
                    },
                    {
                        "name": "Jilin Hu"
                    },
                    {
                        "name": "Haitao Yuan"
                    },
                    {
                        "name": "Gao Cong"
                    },
                    {
                        "name": "Huaiyu Wan"
                    }
                ],
                "author_detail": {
                    "name": "Huaiyu Wan"
                },
                "author": "Huaiyu Wan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14281v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14281v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07540v1",
                "updated": "2025-02-11T13:27:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    13,
                    27,
                    47,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T13:27:47Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    13,
                    27,
                    47,
                    1,
                    42,
                    0
                ],
                "title": "Force-free kinetic inference of entropy production",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Force-free kinetic inference of entropy production"
                },
                "summary": "Estimating entropy production, which quantifies irreversibility and energy\ndissipation, remains a significant challenge despite its central role in\nnonequilibrium physics. We propose a novel method for estimating the mean\nentropy production rate $\\sigma$ that relies solely on position traces,\nbypassing the need for flux or microscopic force measurements. Starting from a\nrecently introduced variance sum rule, we express $\\sigma$ in terms of\nmeasurable steady-state correlation functions which we link to previously\nstudied kinetic quantities, known as traffic and inflow rate. Under realistic\nconstraints of limited access to dynamical degrees of freedom, we derive\nefficient bounds on $\\sigma$ by leveraging the information contained in the\nsystem's traffic, enabling partial but meaningful estimates of $\\sigma$. We\nbenchmark our results across several orders of magnitude in $\\sigma$ using two\nmodels: a linear stochastic system and a nonlinear model for spontaneous\nhair-bundle oscillations. Our approach offers a practical and versatile\nframework for investigating entropy production in nonequilibrium systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating entropy production, which quantifies irreversibility and energy\ndissipation, remains a significant challenge despite its central role in\nnonequilibrium physics. We propose a novel method for estimating the mean\nentropy production rate $\\sigma$ that relies solely on position traces,\nbypassing the need for flux or microscopic force measurements. Starting from a\nrecently introduced variance sum rule, we express $\\sigma$ in terms of\nmeasurable steady-state correlation functions which we link to previously\nstudied kinetic quantities, known as traffic and inflow rate. Under realistic\nconstraints of limited access to dynamical degrees of freedom, we derive\nefficient bounds on $\\sigma$ by leveraging the information contained in the\nsystem's traffic, enabling partial but meaningful estimates of $\\sigma$. We\nbenchmark our results across several orders of magnitude in $\\sigma$ using two\nmodels: a linear stochastic system and a nonlinear model for spontaneous\nhair-bundle oscillations. Our approach offers a practical and versatile\nframework for investigating entropy production in nonequilibrium systems."
                },
                "authors": [
                    {
                        "name": "Ivan Di Terlizzi"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Di Terlizzi"
                },
                "author": "Ivan Di Terlizzi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04916v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04916v2",
                "updated": "2025-02-11T13:16:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    13,
                    16,
                    29,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-07T13:33:40Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    33,
                    40,
                    4,
                    38,
                    0
                ],
                "title": "Classification or Prompting: A Case Study on Legal Requirements\n  Traceability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classification or Prompting: A Case Study on Legal Requirements\n  Traceability"
                },
                "summary": "New regulations are continuously introduced to ensure that software\ndevelopment complies with the ethical concerns and prioritizes public safety. A\nprerequisite for demonstrating compliance involves tracing software\nrequirements to legal provisions. Requirements traceability is a fundamental\ntask where requirements engineers are supposed to analyze technical\nrequirements against target artifacts, often under limited time budget. Doing\nthis analysis manually for complex systems with hundreds of requirements is\ninfeasible. The legal dimension introduces additional challenges that only\nexacerbate manual effort.\n  In this paper, we investigate two automated solutions based on large language\nmodels (LLMs) to predict trace links between requirements and legal provisions.\nThe first solution, Kashif, is a classifier that leverages sentence\ntransformers. The second solution prompts a recent generative LLM based on\nRice, a prompt engineering framework.\n  On a benchmark dataset, we empirically evaluate Kashif and compare it against\na baseline classifier from the literature. Kashif can identify trace links with\nan average recall of ~67%, outperforming the baseline with a substantial gain\nof 54 percentage points (pp) in recall. However, on unseen, more complex\nrequirements documents traced to the European general data protection\nregulation (GDPR), Kashif performs poorly, yielding an average recall of 15%.\nOn the same documents, however, our Rice-based solution yields an average\nrecall of 84%, with a remarkable gain of about 69 pp over Kashif. Our results\nsuggest that requirements traceability in the legal context cannot be simply\naddressed by building classifiers, as such solutions do not generalize and fail\nto perform well on complex regulations and requirements. Resorting to\ngenerative LLMs, with careful prompt engineering, is thus a more promising\nalternative.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "New regulations are continuously introduced to ensure that software\ndevelopment complies with the ethical concerns and prioritizes public safety. A\nprerequisite for demonstrating compliance involves tracing software\nrequirements to legal provisions. Requirements traceability is a fundamental\ntask where requirements engineers are supposed to analyze technical\nrequirements against target artifacts, often under limited time budget. Doing\nthis analysis manually for complex systems with hundreds of requirements is\ninfeasible. The legal dimension introduces additional challenges that only\nexacerbate manual effort.\n  In this paper, we investigate two automated solutions based on large language\nmodels (LLMs) to predict trace links between requirements and legal provisions.\nThe first solution, Kashif, is a classifier that leverages sentence\ntransformers. The second solution prompts a recent generative LLM based on\nRice, a prompt engineering framework.\n  On a benchmark dataset, we empirically evaluate Kashif and compare it against\na baseline classifier from the literature. Kashif can identify trace links with\nan average recall of ~67%, outperforming the baseline with a substantial gain\nof 54 percentage points (pp) in recall. However, on unseen, more complex\nrequirements documents traced to the European general data protection\nregulation (GDPR), Kashif performs poorly, yielding an average recall of 15%.\nOn the same documents, however, our Rice-based solution yields an average\nrecall of 84%, with a remarkable gain of about 69 pp over Kashif. Our results\nsuggest that requirements traceability in the legal context cannot be simply\naddressed by building classifiers, as such solutions do not generalize and fail\nto perform well on complex regulations and requirements. Resorting to\ngenerative LLMs, with careful prompt engineering, is thus a more promising\nalternative."
                },
                "authors": [
                    {
                        "name": "Romina Etezadi"
                    },
                    {
                        "name": "Sallam Abualhaija"
                    },
                    {
                        "name": "Chetan Arora"
                    },
                    {
                        "name": "Lionel Briand"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Briand"
                },
                "author": "Lionel Briand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04916v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04916v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00641v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00641v2",
                "updated": "2025-02-11T13:12:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    13,
                    12,
                    16,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-02T03:07:45Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    3,
                    7,
                    45,
                    6,
                    33,
                    0
                ],
                "title": "Evaluating Small Language Models for News Summarization: Implications\n  and Factors Influencing Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Small Language Models for News Summarization: Implications\n  and Factors Influencing Performance"
                },
                "summary": "The increasing demand for efficient summarization tools in\nresource-constrained environments highlights the need for effective solutions.\nWhile large language models (LLMs) deliver superior summarization quality,\ntheir high computational resource requirements limit practical use\napplications. In contrast, small language models (SLMs) present a more\naccessible alternative, capable of real-time summarization on edge devices.\nHowever, their summarization capabilities and comparative performance against\nLLMs remain underexplored. This paper addresses this gap by presenting a\ncomprehensive evaluation of 19 SLMs for news summarization across 2,000 news\nsamples, focusing on relevance, coherence, factual consistency, and summary\nlength. Our findings reveal significant variations in SLM performance, with\ntop-performing models such as Phi3-Mini and Llama3.2-3B-Ins achieving results\ncomparable to those of 70B LLMs while generating more concise summaries.\nNotably, SLMs are better suited for simple prompts, as overly complex prompts\nmay lead to a decline in summary quality. Additionally, our analysis indicates\nthat instruction tuning does not consistently enhance the news summarization\ncapabilities of SLMs. This research not only contributes to the understanding\nof SLMs but also provides practical insights for researchers seeking efficient\nsummarization solutions that balance performance and resource use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for efficient summarization tools in\nresource-constrained environments highlights the need for effective solutions.\nWhile large language models (LLMs) deliver superior summarization quality,\ntheir high computational resource requirements limit practical use\napplications. In contrast, small language models (SLMs) present a more\naccessible alternative, capable of real-time summarization on edge devices.\nHowever, their summarization capabilities and comparative performance against\nLLMs remain underexplored. This paper addresses this gap by presenting a\ncomprehensive evaluation of 19 SLMs for news summarization across 2,000 news\nsamples, focusing on relevance, coherence, factual consistency, and summary\nlength. Our findings reveal significant variations in SLM performance, with\ntop-performing models such as Phi3-Mini and Llama3.2-3B-Ins achieving results\ncomparable to those of 70B LLMs while generating more concise summaries.\nNotably, SLMs are better suited for simple prompts, as overly complex prompts\nmay lead to a decline in summary quality. Additionally, our analysis indicates\nthat instruction tuning does not consistently enhance the news summarization\ncapabilities of SLMs. This research not only contributes to the understanding\nof SLMs but also provides practical insights for researchers seeking efficient\nsummarization solutions that balance performance and resource use."
                },
                "authors": [
                    {
                        "name": "Borui Xu"
                    },
                    {
                        "name": "Yao Chen"
                    },
                    {
                        "name": "Zeyi Wen"
                    },
                    {
                        "name": "Weiguo Liu"
                    },
                    {
                        "name": "Bingsheng He"
                    }
                ],
                "author_detail": {
                    "name": "Bingsheng He"
                },
                "author": "Bingsheng He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00641v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00641v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13381v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13381v2",
                "updated": "2025-02-11T12:44:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    12,
                    44,
                    39,
                    1,
                    42,
                    0
                ],
                "published": "2025-01-23T04:50:03Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    4,
                    50,
                    3,
                    3,
                    23,
                    0
                ],
                "title": "Do as We Do, Not as You Think: the Conformity of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do as We Do, Not as You Think: the Conformity of Large Language Models"
                },
                "summary": "Recent advancements in large language models (LLMs) revolutionize the field\nof intelligent agents, enabling collaborative multi-agent systems capable of\ntackling complex problems across various domains. However, the potential of\nconformity within these systems, analogous to phenomena like conformity bias\nand groupthink in human group dynamics, remains largely unexplored, raising\nconcerns about their collective problem-solving capabilities and possible\nethical implications. This paper presents a comprehensive study on conformity\nin LLM-driven multi-agent systems, focusing on three aspects: the existence of\nconformity, the factors influencing conformity, and potential mitigation\nstrategies. In particular, we introduce BenchForm, a new conformity-oriented\nbenchmark, featuring reasoning-intensive tasks and five distinct interaction\nprotocols designed to probe LLMs' behavior in collaborative scenarios. Several\nrepresentative LLMs are evaluated on BenchForm, using metrics such as\nconformity rate and independence rate to quantify conformity's impact. Our\nanalysis delves into factors influencing conformity, including interaction time\nand majority size, and examines how the subject agent rationalizes its\nconforming behavior. Furthermore, we explore two strategies to mitigate\nconformity effects, i.e., developing enhanced personas and implementing a\nreflection mechanism. Several interesting findings regarding LLMs' conformity\nare derived from empirical results and case studies. We hope that these\ninsights can pave the way for more robust and ethically-aligned collaborative\nAI systems. Our benchmark and code are available at BenchForm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) revolutionize the field\nof intelligent agents, enabling collaborative multi-agent systems capable of\ntackling complex problems across various domains. However, the potential of\nconformity within these systems, analogous to phenomena like conformity bias\nand groupthink in human group dynamics, remains largely unexplored, raising\nconcerns about their collective problem-solving capabilities and possible\nethical implications. This paper presents a comprehensive study on conformity\nin LLM-driven multi-agent systems, focusing on three aspects: the existence of\nconformity, the factors influencing conformity, and potential mitigation\nstrategies. In particular, we introduce BenchForm, a new conformity-oriented\nbenchmark, featuring reasoning-intensive tasks and five distinct interaction\nprotocols designed to probe LLMs' behavior in collaborative scenarios. Several\nrepresentative LLMs are evaluated on BenchForm, using metrics such as\nconformity rate and independence rate to quantify conformity's impact. Our\nanalysis delves into factors influencing conformity, including interaction time\nand majority size, and examines how the subject agent rationalizes its\nconforming behavior. Furthermore, we explore two strategies to mitigate\nconformity effects, i.e., developing enhanced personas and implementing a\nreflection mechanism. Several interesting findings regarding LLMs' conformity\nare derived from empirical results and case studies. We hope that these\ninsights can pave the way for more robust and ethically-aligned collaborative\nAI systems. Our benchmark and code are available at BenchForm."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Weng"
                    },
                    {
                        "name": "Guikun Chen"
                    },
                    {
                        "name": "Wenguan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenguan Wang"
                },
                "author": "Wenguan Wang",
                "arxiv_comment": "ICLR 2025 (Oral). Code: https://github.com/Zhiyuan-Weng/BenchForm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13381v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13381v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19018v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19018v4",
                "updated": "2025-02-11T12:39:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    12,
                    39,
                    22,
                    1,
                    42,
                    0
                ],
                "published": "2024-12-26T01:56:42Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    1,
                    56,
                    42,
                    3,
                    361,
                    0
                ],
                "title": "Let the Fuzzy Rule Speak: Enhancing In-context Learning Debiasing with\n  Interpretability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let the Fuzzy Rule Speak: Enhancing In-context Learning Debiasing with\n  Interpretability"
                },
                "summary": "Large language models (LLMs) often struggle with balanced class accuracy in\ntext classification tasks using in-context learning (ICL), hindering some\npractical uses due to user dissatisfaction or safety risks caused by\nmisclassifications. Retraining LLMs to address root causes in data or model\npriors is neither easy nor cost-effective. This paper delves deeper into the\nclass accuracy imbalance issue, identifying that it arises because certain\nclasses consistently receive disproportionately high ICL probabilities, causing\nunder-prediction and lower accuracy for others. More importantly, probability\nranges affect the imbalance differently, allowing for precise, range-specific\ncorrections. We introduce FuRud (Fuzzy Rule Optimization-based Debiasing), a\nmethod for sample-level class probability correction. FuRud tackles\ninterpretability challenges by determining why certain classes need corrections\nand tailoring adjustments for each instance's class probabilities which is\npowered by fuzzy sets with triangular membership functions, transforming a\nclass probability based on the range it belongs to. By solving a nonlinear\ninteger programming problem with a labeled set of ICL class probabilities to\nminimize class accuracy bias (COBias) and maximize overall accuracy, each class\nselects an optimal correction function from 19 triangular membership functions\nwithout updating an LLM, and the selected functions correct test instances at\ninference. Across seven benchmark datasets, FuRud reduces COBias by over half\n(56%) and improves overall accuracy by 21% relatively, outperforming\nstate-of-the-art debiasing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often struggle with balanced class accuracy in\ntext classification tasks using in-context learning (ICL), hindering some\npractical uses due to user dissatisfaction or safety risks caused by\nmisclassifications. Retraining LLMs to address root causes in data or model\npriors is neither easy nor cost-effective. This paper delves deeper into the\nclass accuracy imbalance issue, identifying that it arises because certain\nclasses consistently receive disproportionately high ICL probabilities, causing\nunder-prediction and lower accuracy for others. More importantly, probability\nranges affect the imbalance differently, allowing for precise, range-specific\ncorrections. We introduce FuRud (Fuzzy Rule Optimization-based Debiasing), a\nmethod for sample-level class probability correction. FuRud tackles\ninterpretability challenges by determining why certain classes need corrections\nand tailoring adjustments for each instance's class probabilities which is\npowered by fuzzy sets with triangular membership functions, transforming a\nclass probability based on the range it belongs to. By solving a nonlinear\ninteger programming problem with a labeled set of ICL class probabilities to\nminimize class accuracy bias (COBias) and maximize overall accuracy, each class\nselects an optimal correction function from 19 triangular membership functions\nwithout updating an LLM, and the selected functions correct test instances at\ninference. Across seven benchmark datasets, FuRud reduces COBias by over half\n(56%) and improves overall accuracy by 21% relatively, outperforming\nstate-of-the-art debiasing methods."
                },
                "authors": [
                    {
                        "name": "Ruixi Lin"
                    },
                    {
                        "name": "Yang You"
                    }
                ],
                "author_detail": {
                    "name": "Yang You"
                },
                "author": "Yang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19018v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19018v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07516v1",
                "updated": "2025-02-11T12:36:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    12,
                    36,
                    0,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T12:36:00Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    12,
                    36,
                    0,
                    1,
                    42,
                    0
                ],
                "title": "The Devil is in the Prompts: De-Identification Traces Enhance\n  Memorization Risks in Synthetic Chest X-Ray Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Devil is in the Prompts: De-Identification Traces Enhance\n  Memorization Risks in Synthetic Chest X-Ray Generation"
                },
                "summary": "Generative models, particularly text-to-image (T2I) diffusion models, play a\ncrucial role in medical image analysis. However, these models are prone to\ntraining data memorization, posing significant risks to patient privacy.\nSynthetic chest X-ray generation is one of the most common applications in\nmedical image analysis with the MIMIC-CXR dataset serving as the primary data\nrepository for this task. This study adopts a data-driven approach and presents\nthe first systematic attempt to identify prompts and text tokens in MIMIC-CXR\nthat contribute the most to training data memorization. Our analysis reveals an\nunexpected finding: prompts containing traces of de-identification procedures\nare among the most memorized, with de-identification markers contributing the\nmost. Furthermore, we also find existing inference-time memorization mitigation\nstrategies are ineffective and fail to sufficiently reduce the model's reliance\non memorized text tokens highlighting a broader issue in T2I synthesis with\nMIMIC-CXR. On this front, we propose actionable strategies to enhance privacy\nand improve the reliability of generative models in medical imaging. Finally,\nour results provide a foundation for future work on developing and benchmarking\nmemorization mitigation techniques for synthetic chest X-ray generation using\nthe MIMIC-CXR dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative models, particularly text-to-image (T2I) diffusion models, play a\ncrucial role in medical image analysis. However, these models are prone to\ntraining data memorization, posing significant risks to patient privacy.\nSynthetic chest X-ray generation is one of the most common applications in\nmedical image analysis with the MIMIC-CXR dataset serving as the primary data\nrepository for this task. This study adopts a data-driven approach and presents\nthe first systematic attempt to identify prompts and text tokens in MIMIC-CXR\nthat contribute the most to training data memorization. Our analysis reveals an\nunexpected finding: prompts containing traces of de-identification procedures\nare among the most memorized, with de-identification markers contributing the\nmost. Furthermore, we also find existing inference-time memorization mitigation\nstrategies are ineffective and fail to sufficiently reduce the model's reliance\non memorized text tokens highlighting a broader issue in T2I synthesis with\nMIMIC-CXR. On this front, we propose actionable strategies to enhance privacy\nand improve the reliability of generative models in medical imaging. Finally,\nour results provide a foundation for future work on developing and benchmarking\nmemorization mitigation techniques for synthetic chest X-ray generation using\nthe MIMIC-CXR dataset."
                },
                "authors": [
                    {
                        "name": "Raman Dutt"
                    }
                ],
                "author_detail": {
                    "name": "Raman Dutt"
                },
                "author": "Raman Dutt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00233v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00233v2",
                "updated": "2025-02-11T12:33:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    12,
                    33,
                    13,
                    1,
                    42,
                    0
                ],
                "published": "2024-12-31T02:53:27Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    2,
                    53,
                    27,
                    1,
                    366,
                    0
                ],
                "title": "Zero-Shot Strategies for Length-Controllable Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Strategies for Length-Controllable Summarization"
                },
                "summary": "Large language models (LLMs) struggle with precise length control,\nparticularly in zero-shot settings. We conduct a comprehensive study evaluating\nLLMs' length control capabilities across multiple measures and propose\npractical methods to improve controllability. Our experiments with LLaMA 3\nreveal stark differences in length adherence across measures and highlight\ninherent biases of the model. To address these challenges, we introduce a set\nof methods: length approximation, target adjustment, sample filtering, and\nautomated revisions. By combining these methods, we demonstrate substantial\nimprovements in length compliance while maintaining or enhancing summary\nquality, providing highly effective zero-shot strategies for precise length\ncontrol without the need for model fine-tuning or architectural changes. With\nour work, we not only advance our understanding of LLM behavior in controlled\ntext generation but also pave the way for more reliable and adaptable\nsummarization systems in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) struggle with precise length control,\nparticularly in zero-shot settings. We conduct a comprehensive study evaluating\nLLMs' length control capabilities across multiple measures and propose\npractical methods to improve controllability. Our experiments with LLaMA 3\nreveal stark differences in length adherence across measures and highlight\ninherent biases of the model. To address these challenges, we introduce a set\nof methods: length approximation, target adjustment, sample filtering, and\nautomated revisions. By combining these methods, we demonstrate substantial\nimprovements in length compliance while maintaining or enhancing summary\nquality, providing highly effective zero-shot strategies for precise length\ncontrol without the need for model fine-tuning or architectural changes. With\nour work, we not only advance our understanding of LLM behavior in controlled\ntext generation but also pave the way for more reliable and adaptable\nsummarization systems in real-world applications."
                },
                "authors": [
                    {
                        "name": "Fabian Retkowski"
                    },
                    {
                        "name": "Alexander Waibel"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Waibel"
                },
                "author": "Alexander Waibel",
                "arxiv_comment": "Accepted to NAACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00233v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00233v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00134v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00134v4",
                "updated": "2025-02-11T12:28:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    12,
                    28,
                    36,
                    1,
                    42,
                    0
                ],
                "published": "2024-08-29T12:55:10Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    12,
                    55,
                    10,
                    3,
                    242,
                    0
                ],
                "title": "MAPF-GPT: Imitation Learning for Multi-Agent Pathfinding at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAPF-GPT: Imitation Learning for Multi-Agent Pathfinding at Scale"
                },
                "summary": "Multi-agent pathfinding (MAPF) is a problem that generally requires finding\ncollision-free paths for multiple agents in a shared environment. Solving MAPF\noptimally, even under restrictive assumptions, is NP-hard, yet efficient\nsolutions for this problem are critical for numerous applications, such as\nautomated warehouses and transportation systems. Recently, learning-based\napproaches to MAPF have gained attention, particularly those leveraging deep\nreinforcement learning. Typically, such learning-based MAPF solvers are\naugmented with additional components like single-agent planning or\ncommunication. Orthogonally, in this work we rely solely on imitation learning\nthat leverages a large dataset of expert MAPF solutions and transformer-based\nneural network to create a foundation model for MAPF called MAPF-GPT. The\nlatter is capable of generating actions without additional heuristics or\ncommunication. MAPF-GPT demonstrates zero-shot learning abilities when solving\nthe MAPF problems that are not present in the training dataset. We show that\nMAPF-GPT notably outperforms the current best-performing learnable MAPF solvers\non a diverse range of problem instances and is computationally efficient during\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent pathfinding (MAPF) is a problem that generally requires finding\ncollision-free paths for multiple agents in a shared environment. Solving MAPF\noptimally, even under restrictive assumptions, is NP-hard, yet efficient\nsolutions for this problem are critical for numerous applications, such as\nautomated warehouses and transportation systems. Recently, learning-based\napproaches to MAPF have gained attention, particularly those leveraging deep\nreinforcement learning. Typically, such learning-based MAPF solvers are\naugmented with additional components like single-agent planning or\ncommunication. Orthogonally, in this work we rely solely on imitation learning\nthat leverages a large dataset of expert MAPF solutions and transformer-based\nneural network to create a foundation model for MAPF called MAPF-GPT. The\nlatter is capable of generating actions without additional heuristics or\ncommunication. MAPF-GPT demonstrates zero-shot learning abilities when solving\nthe MAPF problems that are not present in the training dataset. We show that\nMAPF-GPT notably outperforms the current best-performing learnable MAPF solvers\non a diverse range of problem instances and is computationally efficient during\ninference."
                },
                "authors": [
                    {
                        "name": "Anton Andreychuk"
                    },
                    {
                        "name": "Konstantin Yakovlev"
                    },
                    {
                        "name": "Aleksandr Panov"
                    },
                    {
                        "name": "Alexey Skrynnik"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Skrynnik"
                },
                "author": "Alexey Skrynnik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00134v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00134v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00696v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00696v3",
                "updated": "2025-02-11T12:21:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    12,
                    21,
                    13,
                    1,
                    42,
                    0
                ],
                "published": "2024-09-01T11:24:54Z",
                "published_parsed": [
                    2024,
                    9,
                    1,
                    11,
                    24,
                    54,
                    6,
                    245,
                    0
                ],
                "title": "Polyrating: A Cost-Effective and Bias-Aware Rating System for LLM\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Polyrating: A Cost-Effective and Bias-Aware Rating System for LLM\n  Evaluation"
                },
                "summary": "Rating-based human evaluation has become an essential tool to accurately\nevaluate the impressive performance of large language models (LLMs). However,\ncurrent rating systems suffer from several important limitations: first, they\nfail to account for biases that significantly influence evaluation results,\nsecond, they require large and expensive preference datasets to obtain accurate\nratings, and third, they do not facilitate meaningful comparisons of model\nratings across different tasks. To address these issues, we introduce\nPolyrating, an expressive and flexible rating system based on maximum a\nposteriori estimation that enables a more nuanced and thorough analysis of\nmodel performance at lower costs. Polyrating can detect and quantify biases\naffecting human preferences, ensuring fairer model comparisons. Further,\nPolyrating can reduce the cost of human evaluations by up to $41\\%$ for new\nmodels and up to $77\\%$ for new tasks by leveraging existing benchmark scores.\nLastly, Polyrating enables direct comparisons of ratings across different\ntasks, providing a comprehensive understanding of an LLMs' strengths,\nweaknesses, and relative performance across different applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rating-based human evaluation has become an essential tool to accurately\nevaluate the impressive performance of large language models (LLMs). However,\ncurrent rating systems suffer from several important limitations: first, they\nfail to account for biases that significantly influence evaluation results,\nsecond, they require large and expensive preference datasets to obtain accurate\nratings, and third, they do not facilitate meaningful comparisons of model\nratings across different tasks. To address these issues, we introduce\nPolyrating, an expressive and flexible rating system based on maximum a\nposteriori estimation that enables a more nuanced and thorough analysis of\nmodel performance at lower costs. Polyrating can detect and quantify biases\naffecting human preferences, ensuring fairer model comparisons. Further,\nPolyrating can reduce the cost of human evaluations by up to $41\\%$ for new\nmodels and up to $77\\%$ for new tasks by leveraging existing benchmark scores.\nLastly, Polyrating enables direct comparisons of ratings across different\ntasks, providing a comprehensive understanding of an LLMs' strengths,\nweaknesses, and relative performance across different applications."
                },
                "authors": [
                    {
                        "name": "Jasper Dekoninck"
                    },
                    {
                        "name": "Maximilian Baader"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00696v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00696v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04078v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04078v2",
                "updated": "2025-02-11T12:16:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    12,
                    16,
                    21,
                    1,
                    42,
                    0
                ],
                "published": "2024-12-05T11:24:27Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    11,
                    24,
                    27,
                    3,
                    340,
                    0
                ],
                "title": "Mind the Gap: Towards Generalizable Autonomous Penetration Testing via\n  Domain Randomization and Meta-Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind the Gap: Towards Generalizable Autonomous Penetration Testing via\n  Domain Randomization and Meta-Reinforcement Learning"
                },
                "summary": "With increasing numbers of vulnerabilities exposed on the internet,\nautonomous penetration testing (pentesting) has emerged as a promising research\narea. Reinforcement learning (RL) is a natural fit for studying this topic.\nHowever, two key challenges limit the applicability of RL-based autonomous\npentesting in real-world scenarios: (a) training environment dilemma --\ntraining agents in simulated environments is sample-efficient while ensuring\ntheir realism remains challenging; (b) poor generalization ability -- agents'\npolicies often perform poorly when transferred to unseen scenarios, with even\nslight changes potentially causing significant generalization gap. To this end,\nwe propose GAP, a generalizable autonomous pentesting framework that aims to\nrealizes efficient policy training in realistic environments and train\ngeneralizable agents capable of drawing inferences about other cases from one\ninstance. GAP introduces a Real-to-Sim-to-Real pipeline that (a) enables\nend-to-end policy learning in unknown real environments while constructing\nrealistic simulations; (b) improves agents' generalization ability by\nleveraging domain randomization and meta-RL learning.Specially, we are among\nthe first to apply domain randomization in autonomous pentesting and propose a\nlarge language model-powered domain randomization method for synthetic\nenvironment generation. We further apply meta-RL to improve agents'\ngeneralization ability in unseen environments by leveraging synthetic\nenvironments. The combination of two methods effectively bridges the\ngeneralization gap and improves agents' policy adaptation\nperformance.Experiments are conducted on various vulnerable virtual machines,\nwith results showing that GAP can enable policy learning in various realistic\nenvironments, achieve zero-shot policy transfer in similar environments, and\nrealize rapid policy adaptation in dissimilar environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With increasing numbers of vulnerabilities exposed on the internet,\nautonomous penetration testing (pentesting) has emerged as a promising research\narea. Reinforcement learning (RL) is a natural fit for studying this topic.\nHowever, two key challenges limit the applicability of RL-based autonomous\npentesting in real-world scenarios: (a) training environment dilemma --\ntraining agents in simulated environments is sample-efficient while ensuring\ntheir realism remains challenging; (b) poor generalization ability -- agents'\npolicies often perform poorly when transferred to unseen scenarios, with even\nslight changes potentially causing significant generalization gap. To this end,\nwe propose GAP, a generalizable autonomous pentesting framework that aims to\nrealizes efficient policy training in realistic environments and train\ngeneralizable agents capable of drawing inferences about other cases from one\ninstance. GAP introduces a Real-to-Sim-to-Real pipeline that (a) enables\nend-to-end policy learning in unknown real environments while constructing\nrealistic simulations; (b) improves agents' generalization ability by\nleveraging domain randomization and meta-RL learning.Specially, we are among\nthe first to apply domain randomization in autonomous pentesting and propose a\nlarge language model-powered domain randomization method for synthetic\nenvironment generation. We further apply meta-RL to improve agents'\ngeneralization ability in unseen environments by leveraging synthetic\nenvironments. The combination of two methods effectively bridges the\ngeneralization gap and improves agents' policy adaptation\nperformance.Experiments are conducted on various vulnerable virtual machines,\nwith results showing that GAP can enable policy learning in various realistic\nenvironments, achieve zero-shot policy transfer in similar environments, and\nrealize rapid policy adaptation in dissimilar environments."
                },
                "authors": [
                    {
                        "name": "Shicheng Zhou"
                    },
                    {
                        "name": "Jingju Liu"
                    },
                    {
                        "name": "Yuliang Lu"
                    },
                    {
                        "name": "Jiahai Yang"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Jie Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jie Chen"
                },
                "author": "Jie Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04078v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04078v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07503v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07503v1",
                "updated": "2025-02-11T12:11:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    12,
                    11,
                    40,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T12:11:40Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    12,
                    11,
                    40,
                    1,
                    42,
                    0
                ],
                "title": "Harnessing Language's Fractal Geometry with Recursive Inference Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Language's Fractal Geometry with Recursive Inference Scaling"
                },
                "summary": "Recent research in language modeling reveals two scaling effects: the\nwell-known improvement from increased training compute, and a lesser-known\nboost from applying more sophisticated or computationally intensive inference\nmethods. Inspired by recent findings on the fractal geometry of language, we\nintroduce Recursive INference Scaling (RINS) as a complementary, plug-in recipe\nfor scaling inference time. For a given fixed model architecture and training\ncompute budget, RINS substantially improves language modeling performance. It\nalso generalizes beyond pure language tasks, delivering gains in multimodal\nsystems, including a +2% improvement in 0-shot ImageNet accuracy for\nSigLIP-B/16. Additionally, by deriving data scaling laws, we show that RINS\nimproves both the asymptotic performance limits and the scaling exponents.\nThese advantages are maintained even when compared to state-of-the-art\nrecursive techniques like the \"repeat-all-over\" (RAO) strategy in Mobile LLM.\nFinally, stochastic RINS not only can enhance performance further but also\nprovides the flexibility to optionally forgo increased inference computation at\ntest time with minimal performance degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research in language modeling reveals two scaling effects: the\nwell-known improvement from increased training compute, and a lesser-known\nboost from applying more sophisticated or computationally intensive inference\nmethods. Inspired by recent findings on the fractal geometry of language, we\nintroduce Recursive INference Scaling (RINS) as a complementary, plug-in recipe\nfor scaling inference time. For a given fixed model architecture and training\ncompute budget, RINS substantially improves language modeling performance. It\nalso generalizes beyond pure language tasks, delivering gains in multimodal\nsystems, including a +2% improvement in 0-shot ImageNet accuracy for\nSigLIP-B/16. Additionally, by deriving data scaling laws, we show that RINS\nimproves both the asymptotic performance limits and the scaling exponents.\nThese advantages are maintained even when compared to state-of-the-art\nrecursive techniques like the \"repeat-all-over\" (RAO) strategy in Mobile LLM.\nFinally, stochastic RINS not only can enhance performance further but also\nprovides the flexibility to optionally forgo increased inference computation at\ntest time with minimal performance degradation."
                },
                "authors": [
                    {
                        "name": "Ibrahim Alabdulmohsin"
                    },
                    {
                        "name": "Xiaohua Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohua Zhai"
                },
                "author": "Xiaohua Zhai",
                "arxiv_comment": "18 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07503v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07503v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04411v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04411v2",
                "updated": "2025-02-11T12:09:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    12,
                    9,
                    51,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-06T11:26:30Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    11,
                    26,
                    30,
                    3,
                    37,
                    0
                ],
                "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and\n  Uncertainty Based Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and\n  Uncertainty Based Routing"
                },
                "summary": "Model merging aggregates Large Language Models (LLMs) finetuned on different\ntasks into a stronger one. However, parameter conflicts between models leads to\nperformance degradation in averaging. While model routing addresses this issue\nby selecting individual models during inference, it imposes excessive storage\nand compute costs, and fails to leverage the common knowledge from different\nmodels. In this work, we observe that different layers exhibit varying levels\nof parameter conflicts. Building on this insight, we average layers with\nminimal parameter conflicts and use a novel task-level expert routing for\nlayers with significant conflicts. To further reduce storage costs, inspired by\ntask arithmetic sparsity, we decouple multiple fine-tuned experts into a dense\nexpert and several sparse experts. Considering the out-of-distribution samples,\nwe select and merge appropriate experts based on the task uncertainty of the\ninput data. We conduct extensive experiments on both LLaMA and Qwen with\nvarying parameter scales, and evaluate on real-world reasoning tasks. Results\ndemonstrate that our method consistently achieves significant performance\nimprovements while requiring less system cost compared to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model merging aggregates Large Language Models (LLMs) finetuned on different\ntasks into a stronger one. However, parameter conflicts between models leads to\nperformance degradation in averaging. While model routing addresses this issue\nby selecting individual models during inference, it imposes excessive storage\nand compute costs, and fails to leverage the common knowledge from different\nmodels. In this work, we observe that different layers exhibit varying levels\nof parameter conflicts. Building on this insight, we average layers with\nminimal parameter conflicts and use a novel task-level expert routing for\nlayers with significant conflicts. To further reduce storage costs, inspired by\ntask arithmetic sparsity, we decouple multiple fine-tuned experts into a dense\nexpert and several sparse experts. Considering the out-of-distribution samples,\nwe select and merge appropriate experts based on the task uncertainty of the\ninput data. We conduct extensive experiments on both LLaMA and Qwen with\nvarying parameter scales, and evaluate on real-world reasoning tasks. Results\ndemonstrate that our method consistently achieves significant performance\nimprovements while requiring less system cost compared to existing methods."
                },
                "authors": [
                    {
                        "name": "Kunfeng Lai"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Xinglin Pan"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Haolan Chen"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "work in progress. arXiv admin note: text overlap with\n  arXiv:2405.09673 by other authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04411v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04411v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07495v1",
                "updated": "2025-02-11T11:54:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    11,
                    54,
                    56,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T11:54:56Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    11,
                    54,
                    56,
                    1,
                    42,
                    0
                ],
                "title": "LLM-Sketch: Enhancing Network Sketches with LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Sketch: Enhancing Network Sketches with LLM"
                },
                "summary": "Network stream mining is fundamental to many network operations. Sketches, as\ncompact data structures that offer low memory overhead with bounded accuracy,\nhave emerged as a promising solution for network stream mining. Recent studies\nattempt to optimize sketches using machine learning; however, these approaches\nface the challenges of lacking adaptivity to dynamic networks and incurring\nhigh training costs. In this paper, we propose LLM-Sketch, based on the insight\nthat fields beyond the flow IDs in packet headers can also help infer flow\nsizes. By using a two-tier data structure and separately recording large and\nsmall flows, LLM-Sketch improves accuracy while minimizing memory usage.\nFurthermore, it leverages fine-tuned large language models (LLMs) to reliably\nestimate flow sizes. We evaluate LLM-Sketch on three representative tasks, and\nthe results demonstrate that LLM-Sketch outperforms state-of-the-art methods by\nachieving a $7.5\\times$ accuracy improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network stream mining is fundamental to many network operations. Sketches, as\ncompact data structures that offer low memory overhead with bounded accuracy,\nhave emerged as a promising solution for network stream mining. Recent studies\nattempt to optimize sketches using machine learning; however, these approaches\nface the challenges of lacking adaptivity to dynamic networks and incurring\nhigh training costs. In this paper, we propose LLM-Sketch, based on the insight\nthat fields beyond the flow IDs in packet headers can also help infer flow\nsizes. By using a two-tier data structure and separately recording large and\nsmall flows, LLM-Sketch improves accuracy while minimizing memory usage.\nFurthermore, it leverages fine-tuned large language models (LLMs) to reliably\nestimate flow sizes. We evaluate LLM-Sketch on three representative tasks, and\nthe results demonstrate that LLM-Sketch outperforms state-of-the-art methods by\nachieving a $7.5\\times$ accuracy improvement."
                },
                "authors": [
                    {
                        "name": "Yuanpeng Li"
                    },
                    {
                        "name": "Zhen Xu"
                    },
                    {
                        "name": "Zongwei Lv"
                    },
                    {
                        "name": "Yannan Hu"
                    },
                    {
                        "name": "Yong Cui"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12956v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12956v2",
                "updated": "2025-02-11T11:50:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    11,
                    50,
                    15,
                    1,
                    42,
                    0
                ],
                "published": "2025-01-22T15:29:09Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    29,
                    9,
                    2,
                    22,
                    0
                ],
                "title": "GANQ: GPU-Adaptive Non-Uniform Quantization for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GANQ: GPU-Adaptive Non-Uniform Quantization for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) face significant deployment challenges due to\ntheir substantial resource requirements. While low-bit quantized weights can\nreduce memory usage and improve inference efficiency, current hardware lacks\nnative support for mixed-precision General Matrix Multiplication (mpGEMM),\nresulting in inefficient dequantization-based implementations. Moreover,\nuniform quantization methods often fail to capture weight distributions\nadequately, leading to performance degradation. We propose GANQ (GPU-Adaptive\nNon-Uniform Quantization), a layer-wise post-training non-uniform quantization\nframework optimized for hardware-efficient lookup table-based mpGEMM. GANQ\nachieves superior quantization performance by utilizing a training-free,\nGPU-adaptive optimization algorithm to efficiently reduce layer-wise\nquantization errors. Extensive experiments demonstrate GANQ's ability to reduce\nthe perplexity gap from the FP16 baseline compared to state-of-the-art methods\nfor both 3-bit and 4-bit quantization. Furthermore, when deployed on a single\nNVIDIA RTX 4090 GPU, GANQ's quantized models achieve up to 2.57$\\times$ speedup\nover the baseline, advancing memory and inference efficiency in LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) face significant deployment challenges due to\ntheir substantial resource requirements. While low-bit quantized weights can\nreduce memory usage and improve inference efficiency, current hardware lacks\nnative support for mixed-precision General Matrix Multiplication (mpGEMM),\nresulting in inefficient dequantization-based implementations. Moreover,\nuniform quantization methods often fail to capture weight distributions\nadequately, leading to performance degradation. We propose GANQ (GPU-Adaptive\nNon-Uniform Quantization), a layer-wise post-training non-uniform quantization\nframework optimized for hardware-efficient lookup table-based mpGEMM. GANQ\nachieves superior quantization performance by utilizing a training-free,\nGPU-adaptive optimization algorithm to efficiently reduce layer-wise\nquantization errors. Extensive experiments demonstrate GANQ's ability to reduce\nthe perplexity gap from the FP16 baseline compared to state-of-the-art methods\nfor both 3-bit and 4-bit quantization. Furthermore, when deployed on a single\nNVIDIA RTX 4090 GPU, GANQ's quantized models achieve up to 2.57$\\times$ speedup\nover the baseline, advancing memory and inference efficiency in LLM deployment."
                },
                "authors": [
                    {
                        "name": "Pengxiang Zhao"
                    },
                    {
                        "name": "Xiaoming Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoming Yuan"
                },
                "author": "Xiaoming Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12956v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12956v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07490v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07490v1",
                "updated": "2025-02-11T11:49:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    11,
                    49,
                    3,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T11:49:03Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    11,
                    49,
                    3,
                    1,
                    42,
                    0
                ],
                "title": "Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn\n  More",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn\n  More"
                },
                "summary": "Large Language Models (LLMs) are discovered to suffer from accurately\nretrieving key information. To address this, we propose Mask-Enhanced\nAutoregressive Prediction (MEAP), a simple yet effective training paradigm that\nseamlessly integrates Masked Language Modeling (MLM) into Next-Token Prediction\n(NTP) to enhance the latter's in-context retrieval capabilities. Specifically,\nMEAP first randomly masks a small fraction of input tokens and then directly\nperforms the standard next-token prediction autoregressive using a decoder-only\nTransformer. MEAP eliminates the need for bidirectional attention or\nencoder-decoder architectures for MLM, incurring no additional computational\noverhead during pre-training or inference. Intensive experiments demonstrate\nthat MEAP substantially outperforms NTP on key information retrieval and\nlong-context reasoning tasks, while performing on par or better on commonsense\nreasoning tasks. The benefits of MEAP also extend to supervised fine-tuning,\nwhere it shows remarkable advantages in lost-in-the-middle scenarios,\noutperforming NTP by 11.77 percentage points. Our analysis indicates that\nMEAP's effectiveness arises from its ability to promote more distinguishable\nattention scores by concentrating on a reduced set of non-masked tokens. This\nmechanism improves the model's focus on task-relevant signals while mitigating\nthe influence of peripheral context. These findings position MEAP as a\npromising training paradigm for large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are discovered to suffer from accurately\nretrieving key information. To address this, we propose Mask-Enhanced\nAutoregressive Prediction (MEAP), a simple yet effective training paradigm that\nseamlessly integrates Masked Language Modeling (MLM) into Next-Token Prediction\n(NTP) to enhance the latter's in-context retrieval capabilities. Specifically,\nMEAP first randomly masks a small fraction of input tokens and then directly\nperforms the standard next-token prediction autoregressive using a decoder-only\nTransformer. MEAP eliminates the need for bidirectional attention or\nencoder-decoder architectures for MLM, incurring no additional computational\noverhead during pre-training or inference. Intensive experiments demonstrate\nthat MEAP substantially outperforms NTP on key information retrieval and\nlong-context reasoning tasks, while performing on par or better on commonsense\nreasoning tasks. The benefits of MEAP also extend to supervised fine-tuning,\nwhere it shows remarkable advantages in lost-in-the-middle scenarios,\noutperforming NTP by 11.77 percentage points. Our analysis indicates that\nMEAP's effectiveness arises from its ability to promote more distinguishable\nattention scores by concentrating on a reduced set of non-masked tokens. This\nmechanism improves the model's focus on task-relevant signals while mitigating\nthe influence of peripheral context. These findings position MEAP as a\npromising training paradigm for large language models."
                },
                "authors": [
                    {
                        "name": "Xialie Zhuang"
                    },
                    {
                        "name": "Zhikai Jia"
                    },
                    {
                        "name": "Jianjin Li"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Zheng Cao"
                    },
                    {
                        "name": "Shiwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Shiwei Liu"
                },
                "author": "Shiwei Liu",
                "arxiv_comment": "15 pages,7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07490v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07490v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07487v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07487v1",
                "updated": "2025-02-11T11:46:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    11,
                    46,
                    38,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T11:46:38Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    11,
                    46,
                    38,
                    1,
                    42,
                    0
                ],
                "title": "Multi-Agent Collaboration for Multilingual Code Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Collaboration for Multilingual Code Instruction Tuning"
                },
                "summary": "Recent advancement in code understanding and generation demonstrates that\ncode LLMs fine-tuned on a high-quality instruction dataset can gain powerful\ncapabilities to address wide-ranging code-related tasks. However, most previous\nexisting methods mainly view each programming language in isolation and ignore\nthe knowledge transfer among different programming languages. To bridge the gap\namong different programming languages, we introduce a novel multi-agent\ncollaboration framework to enhance multilingual instruction tuning for code\nLLMs, where multiple language-specific intelligent agent components with\ngeneration memory work together to transfer knowledge from one language to\nanother efficiently and effectively. Specifically, we first generate the\nlanguage-specific instruction data from the code snippets and then provide the\ngenerated data as the seed data for language-specific agents. Multiple\nlanguage-specific agents discuss and collaborate to formulate a new instruction\nand its corresponding solution (A new programming language or existing\nprogramming language), To further encourage the cross-lingual transfer, each\nagent stores its generation history as memory and then summarizes its merits\nand faults. Finally, the high-quality multilingual instruction data is used to\nencourage knowledge transfer among different programming languages to train\nQwen2.5-xCoder. Experimental results on multilingual programming benchmarks\ndemonstrate the superior performance of Qwen2.5-xCoder in sharing common\nknowledge, highlighting its potential to reduce the cross-lingual gap.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancement in code understanding and generation demonstrates that\ncode LLMs fine-tuned on a high-quality instruction dataset can gain powerful\ncapabilities to address wide-ranging code-related tasks. However, most previous\nexisting methods mainly view each programming language in isolation and ignore\nthe knowledge transfer among different programming languages. To bridge the gap\namong different programming languages, we introduce a novel multi-agent\ncollaboration framework to enhance multilingual instruction tuning for code\nLLMs, where multiple language-specific intelligent agent components with\ngeneration memory work together to transfer knowledge from one language to\nanother efficiently and effectively. Specifically, we first generate the\nlanguage-specific instruction data from the code snippets and then provide the\ngenerated data as the seed data for language-specific agents. Multiple\nlanguage-specific agents discuss and collaborate to formulate a new instruction\nand its corresponding solution (A new programming language or existing\nprogramming language), To further encourage the cross-lingual transfer, each\nagent stores its generation history as memory and then summarizes its merits\nand faults. Finally, the high-quality multilingual instruction data is used to\nencourage knowledge transfer among different programming languages to train\nQwen2.5-xCoder. Experimental results on multilingual programming benchmarks\ndemonstrate the superior performance of Qwen2.5-xCoder in sharing common\nknowledge, highlighting its potential to reduce the cross-lingual gap."
                },
                "authors": [
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Jiaxi Yang"
                    },
                    {
                        "name": "Yibo Miao"
                    },
                    {
                        "name": "Shanghaoran Quan"
                    },
                    {
                        "name": "Zhenhe Wu"
                    },
                    {
                        "name": "Qiyao Peng"
                    },
                    {
                        "name": "Liqun Yang"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Zeyu Cui"
                    },
                    {
                        "name": "Binyuan Hui"
                    },
                    {
                        "name": "Junyang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Junyang Lin"
                },
                "author": "Junyang Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07487v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07487v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13341v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13341v2",
                "updated": "2025-02-11T11:39:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    11,
                    39,
                    18,
                    1,
                    42,
                    0
                ],
                "published": "2024-10-17T08:49:42Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    8,
                    49,
                    42,
                    3,
                    291,
                    0
                ],
                "title": "Limits to scalable evaluation at the frontier: LLM as Judge won't beat\n  twice the data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Limits to scalable evaluation at the frontier: LLM as Judge won't beat\n  twice the data"
                },
                "summary": "High quality annotations are increasingly a bottleneck in the explosively\ngrowing machine learning ecosystem. Scalable evaluation methods that avoid\ncostly annotation have therefore become an important research ambition. Many\nhope to use strong existing models in lieu of costly labels to provide cheap\nmodel evaluations. Unfortunately, this method of using models as judges\nintroduces biases, such as self-preferencing, that can distort model\ncomparisons. An emerging family of debiasing tools promises to fix these issues\nby using a few high quality labels to debias a large number of model judgments.\nIn this paper, we study how far such debiasing methods, in principle, can go.\nOur main result shows that when the judge is no more accurate than the\nevaluated model, no debiasing method can decrease the required amount of ground\ntruth labels by more than half. Our result speaks to the severe limitations of\nthe LLM-as-a-judge paradigm at the evaluation frontier where the goal is to\nassess newly released models that are possibly better than the judge. Through\nan empirical evaluation, we demonstrate that the sample size savings achievable\nin practice are even more modest than what our theoretical limit suggests.\nAlong the way, our work provides new observations about debiasing methods for\nmodel evaluation, and points out promising avenues for future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High quality annotations are increasingly a bottleneck in the explosively\ngrowing machine learning ecosystem. Scalable evaluation methods that avoid\ncostly annotation have therefore become an important research ambition. Many\nhope to use strong existing models in lieu of costly labels to provide cheap\nmodel evaluations. Unfortunately, this method of using models as judges\nintroduces biases, such as self-preferencing, that can distort model\ncomparisons. An emerging family of debiasing tools promises to fix these issues\nby using a few high quality labels to debias a large number of model judgments.\nIn this paper, we study how far such debiasing methods, in principle, can go.\nOur main result shows that when the judge is no more accurate than the\nevaluated model, no debiasing method can decrease the required amount of ground\ntruth labels by more than half. Our result speaks to the severe limitations of\nthe LLM-as-a-judge paradigm at the evaluation frontier where the goal is to\nassess newly released models that are possibly better than the judge. Through\nan empirical evaluation, we demonstrate that the sample size savings achievable\nin practice are even more modest than what our theoretical limit suggests.\nAlong the way, our work provides new observations about debiasing methods for\nmodel evaluation, and points out promising avenues for future work."
                },
                "authors": [
                    {
                        "name": "Florian E. Dorner"
                    },
                    {
                        "name": "Vivian Y. Nastl"
                    },
                    {
                        "name": "Moritz Hardt"
                    }
                ],
                "author_detail": {
                    "name": "Moritz Hardt"
                },
                "author": "Moritz Hardt",
                "arxiv_comment": "ICLR 2025; 28 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13341v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13341v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07474v1",
                "updated": "2025-02-11T11:34:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    11,
                    34,
                    33,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T11:34:33Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    11,
                    34,
                    33,
                    1,
                    42,
                    0
                ],
                "title": "ETimeline: An Extensive Timeline Generation Dataset based on Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ETimeline: An Extensive Timeline Generation Dataset based on Large\n  Language Model"
                },
                "summary": "Timeline generation is of great significance for a comprehensive\nunderstanding of the development of events over time. Its goal is to organize\nnews chronologically, which helps to identify patterns and trends that may be\nobscured when viewing news in isolation, making it easier to track the\ndevelopment of stories and understand the interrelationships between key\nevents. Timelines are now common in various commercial products, but academic\nresearch in this area is notably scarce. Additionally, the current datasets are\nin need of refinement for enhanced utility and expanded coverage. In this\npaper, we propose ETimeline, which encompasses over $13,000$ news articles,\nspanning $600$ bilingual timelines across $28$ news domains. Specifically, we\ngather a candidate pool of more than $120,000$ news articles and employ the\nlarge language model (LLM) Pipeline to improve performance, ultimately yielding\nthe ETimeline. The data analysis underscores the appeal of ETimeline.\nAdditionally, we also provide the news pool data for further research and\nanalysis. This work contributes to the advancement of timeline generation\nresearch and supports a wide range of tasks, including topic generation and\nevent relationships. We believe that this dataset will serve as a catalyst for\ninnovative research and bridge the gap between academia and industry in\nunderstanding the practical application of technology services. The dataset is\navailable at https://zenodo.org/records/11392212",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timeline generation is of great significance for a comprehensive\nunderstanding of the development of events over time. Its goal is to organize\nnews chronologically, which helps to identify patterns and trends that may be\nobscured when viewing news in isolation, making it easier to track the\ndevelopment of stories and understand the interrelationships between key\nevents. Timelines are now common in various commercial products, but academic\nresearch in this area is notably scarce. Additionally, the current datasets are\nin need of refinement for enhanced utility and expanded coverage. In this\npaper, we propose ETimeline, which encompasses over $13,000$ news articles,\nspanning $600$ bilingual timelines across $28$ news domains. Specifically, we\ngather a candidate pool of more than $120,000$ news articles and employ the\nlarge language model (LLM) Pipeline to improve performance, ultimately yielding\nthe ETimeline. The data analysis underscores the appeal of ETimeline.\nAdditionally, we also provide the news pool data for further research and\nanalysis. This work contributes to the advancement of timeline generation\nresearch and supports a wide range of tasks, including topic generation and\nevent relationships. We believe that this dataset will serve as a catalyst for\ninnovative research and bridge the gap between academia and industry in\nunderstanding the practical application of technology services. The dataset is\navailable at https://zenodo.org/records/11392212"
                },
                "authors": [
                    {
                        "name": "Xiaochen Liu"
                    },
                    {
                        "name": "Yanan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yanan Zhang"
                },
                "author": "Yanan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07471v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07471v1",
                "updated": "2025-02-11T11:26:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    11,
                    26,
                    20,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T11:26:20Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    11,
                    26,
                    20,
                    1,
                    42,
                    0
                ],
                "title": "Scaling relations for the uncertainty in neutron star radius inferred\n  from pulse profile modelling: the effect of spin rate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling relations for the uncertainty in neutron star radius inferred\n  from pulse profile modelling: the effect of spin rate"
                },
                "summary": "Pulse profile modelling using X-ray data from NICER permits the inference of\nmass and radius for rotation-powered millisecond pulsars. This in turn\nconstrains the equation of state of cold dense matter. Previous studies\nindicate that the uncertainty in the inferred radius should reduce as neutron\nstar spin rate increases. Here we test this using one of the pipelines\ncurrently being used for pulse profile modelling with NICER data. We synthesize\na set of pulse profiles, assuming different neutron star spin frequencies,\nspanning the range (25-700) Hz. All of the simulated data sets are generated\nwith the same (single) hot spot configuration, assuming a neutron star mass and\nradius of $1.6\\,M_{\\mathrm{\\odot}}$ and $10$ km. For this restricted set of\nsynthetic data, we find no improvement in the radius credible interval once\nspin frequency exceeds a certain value (in this specific case $\\sim 200$ Hz).\nIf this result were to apply more generally, it would have important\nimplications for the observing strategy for current and future pulse profile\nmodelling missions: targets can be prioritized based on properties other than\ntheir spin frequencies, as long as we are in the millisecond range.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pulse profile modelling using X-ray data from NICER permits the inference of\nmass and radius for rotation-powered millisecond pulsars. This in turn\nconstrains the equation of state of cold dense matter. Previous studies\nindicate that the uncertainty in the inferred radius should reduce as neutron\nstar spin rate increases. Here we test this using one of the pipelines\ncurrently being used for pulse profile modelling with NICER data. We synthesize\na set of pulse profiles, assuming different neutron star spin frequencies,\nspanning the range (25-700) Hz. All of the simulated data sets are generated\nwith the same (single) hot spot configuration, assuming a neutron star mass and\nradius of $1.6\\,M_{\\mathrm{\\odot}}$ and $10$ km. For this restricted set of\nsynthetic data, we find no improvement in the radius credible interval once\nspin frequency exceeds a certain value (in this specific case $\\sim 200$ Hz).\nIf this result were to apply more generally, it would have important\nimplications for the observing strategy for current and future pulse profile\nmodelling missions: targets can be prioritized based on properties other than\ntheir spin frequencies, as long as we are in the millisecond range."
                },
                "authors": [
                    {
                        "name": "Erik Bootsma"
                    },
                    {
                        "name": "Serena Vinciguerra"
                    },
                    {
                        "name": "Anna L. Watts"
                    },
                    {
                        "name": "Yves Kini"
                    },
                    {
                        "name": "Tuomo Salmi"
                    }
                ],
                "author_detail": {
                    "name": "Tuomo Salmi"
                },
                "author": "Tuomo Salmi",
                "arxiv_comment": "Accepted for publication in MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07471v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07471v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07469v1",
                "updated": "2025-02-11T11:25:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    11,
                    25,
                    10,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T11:25:10Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    11,
                    25,
                    10,
                    1,
                    42,
                    0
                ],
                "title": "5D Neural Surrogates for Nonlinear Gyrokinetic Simulations of Plasma\n  Turbulence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "5D Neural Surrogates for Nonlinear Gyrokinetic Simulations of Plasma\n  Turbulence"
                },
                "summary": "Nuclear fusion plays a pivotal role in the quest for reliable and sustainable\nenergy production. A major roadblock to achieving commercially viable fusion\npower is understanding plasma turbulence, which can significantly degrade\nplasma confinement. Modelling turbulence is crucial to design performing plasma\nscenarios for next-generation reactor-class devices and current experimental\nmachines. The nonlinear gyrokinetic equation underpinning turbulence modelling\nevolves a 5D distribution function over time. Solving this equation numerically\nis extremely expensive, requiring up to weeks for a single run to converge,\nmaking it unfeasible for iterative optimisation and control studies. In this\nwork, we propose a method for training neural surrogates for 5D gyrokinetic\nsimulations. Our method extends a hierarchical vision transformer to five\ndimensions and is trained on the 5D distribution function for the adiabatic\nelectron approximation. We demonstrate that our model can accurately infer\ndownstream physical quantities such as heat flux time trace and electrostatic\npotentials for single-step predictions two orders of magnitude faster than\nnumerical codes. Our work paves the way towards neural surrogates for plasma\nturbulence simulations to accelerate deployment of commercial energy production\nvia nuclear fusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nuclear fusion plays a pivotal role in the quest for reliable and sustainable\nenergy production. A major roadblock to achieving commercially viable fusion\npower is understanding plasma turbulence, which can significantly degrade\nplasma confinement. Modelling turbulence is crucial to design performing plasma\nscenarios for next-generation reactor-class devices and current experimental\nmachines. The nonlinear gyrokinetic equation underpinning turbulence modelling\nevolves a 5D distribution function over time. Solving this equation numerically\nis extremely expensive, requiring up to weeks for a single run to converge,\nmaking it unfeasible for iterative optimisation and control studies. In this\nwork, we propose a method for training neural surrogates for 5D gyrokinetic\nsimulations. Our method extends a hierarchical vision transformer to five\ndimensions and is trained on the 5D distribution function for the adiabatic\nelectron approximation. We demonstrate that our model can accurately infer\ndownstream physical quantities such as heat flux time trace and electrostatic\npotentials for single-step predictions two orders of magnitude faster than\nnumerical codes. Our work paves the way towards neural surrogates for plasma\nturbulence simulations to accelerate deployment of commercial energy production\nvia nuclear fusion."
                },
                "authors": [
                    {
                        "name": "Gianluca Galletti"
                    },
                    {
                        "name": "Fabian Paischer"
                    },
                    {
                        "name": "Paul Setinek"
                    },
                    {
                        "name": "William Hornsby"
                    },
                    {
                        "name": "Lorenzo Zanisi"
                    },
                    {
                        "name": "Naomi Carey"
                    },
                    {
                        "name": "Stanislas Pamela"
                    },
                    {
                        "name": "Johannes Brandstetter"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Brandstetter"
                },
                "author": "Johannes Brandstetter",
                "arxiv_comment": "6 pages (+ references and appendix)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16125v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16125v3",
                "updated": "2025-02-11T11:19:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    11,
                    19,
                    40,
                    1,
                    42,
                    0
                ],
                "published": "2025-01-27T15:12:27Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    12,
                    27,
                    0,
                    27,
                    0
                ],
                "title": "SampleLLM: Optimizing Tabular Data Synthesis in Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SampleLLM: Optimizing Tabular Data Synthesis in Recommendations"
                },
                "summary": "Tabular data synthesis is crucial in machine learning, yet existing general\nmethods-primarily based on statistical or deep learning models-are highly\ndata-dependent and often fall short in recommender systems. This limitation\narises from their difficulty in capturing complex distributions and\nunderstanding feature relationships from sparse and limited data, along with\ntheir inability to grasp semantic feature relations. Recently, Large Language\nModels (LLMs) have shown potential in generating synthetic data samples through\nfew-shot learning and semantic understanding. However, they often suffer from\ninconsistent distribution and lack of diversity due to their inherent\ndistribution disparity with the target dataset. To address these challenges and\nenhance tabular data synthesis for recommendation tasks, we propose a novel\ntwo-stage framework named SampleLLM to improve the quality of LLM-based tabular\ndata synthesis for recommendations by ensuring better distribution alignment.\nIn the first stage, SampleLLM employs LLMs with Chain-of-Thought prompts and\ndiverse exemplars to generate data that closely aligns with the target dataset\ndistribution, even when input samples are limited. The second stage uses an\nadvanced feature attribution-based importance sampling method to refine feature\nrelationships within the synthesized data, reducing any distribution biases\nintroduced by the LLM. Experimental results on three recommendation datasets,\ntwo general datasets, and online deployment illustrate that SampleLLM\nsignificantly surpasses existing methods for recommendation tasks and holds\npromise for a broader range of tabular data scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular data synthesis is crucial in machine learning, yet existing general\nmethods-primarily based on statistical or deep learning models-are highly\ndata-dependent and often fall short in recommender systems. This limitation\narises from their difficulty in capturing complex distributions and\nunderstanding feature relationships from sparse and limited data, along with\ntheir inability to grasp semantic feature relations. Recently, Large Language\nModels (LLMs) have shown potential in generating synthetic data samples through\nfew-shot learning and semantic understanding. However, they often suffer from\ninconsistent distribution and lack of diversity due to their inherent\ndistribution disparity with the target dataset. To address these challenges and\nenhance tabular data synthesis for recommendation tasks, we propose a novel\ntwo-stage framework named SampleLLM to improve the quality of LLM-based tabular\ndata synthesis for recommendations by ensuring better distribution alignment.\nIn the first stage, SampleLLM employs LLMs with Chain-of-Thought prompts and\ndiverse exemplars to generate data that closely aligns with the target dataset\ndistribution, even when input samples are limited. The second stage uses an\nadvanced feature attribution-based importance sampling method to refine feature\nrelationships within the synthesized data, reducing any distribution biases\nintroduced by the LLM. Experimental results on three recommendation datasets,\ntwo general datasets, and online deployment illustrate that SampleLLM\nsignificantly surpasses existing methods for recommendation tasks and holds\npromise for a broader range of tabular data scenarios."
                },
                "authors": [
                    {
                        "name": "Jingtong Gao"
                    },
                    {
                        "name": "Zhaocheng Du"
                    },
                    {
                        "name": "Xiaopeng Li"
                    },
                    {
                        "name": "Yichao Wang"
                    },
                    {
                        "name": "Xiangyang Li"
                    },
                    {
                        "name": "Huifeng Guo"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhao"
                },
                "author": "Xiangyu Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16125v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16125v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07460v1",
                "updated": "2025-02-11T11:11:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    11,
                    11,
                    5,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T11:11:05Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    11,
                    11,
                    5,
                    1,
                    42,
                    0
                ],
                "title": "Logarithmic Regret for Online KL-Regularized Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logarithmic Regret for Online KL-Regularized Reinforcement Learning"
                },
                "summary": "Recent advances in Reinforcement Learning from Human Feedback (RLHF) have\nshown that KL-regularization plays a pivotal role in improving the efficiency\nof RL fine-tuning for large language models (LLMs). Despite its empirical\nadvantage, the theoretical difference between KL-regularized RL and standard RL\nremains largely under-explored. While there is a recent line of work on the\ntheoretical analysis of KL-regularized objective in decision making\n\\citep{xiong2024iterative, xie2024exploratory,zhao2024sharp}, these analyses\neither reduce to the traditional RL setting or rely on strong coverage\nassumptions. In this paper, we propose an optimism-based KL-regularized online\ncontextual bandit algorithm, and provide a novel analysis of its regret. By\ncarefully leveraging the benign optimization landscape induced by the\nKL-regularization and the optimistic reward estimation, our algorithm achieves\nan $\\mathcal{O}\\big(\\eta\\log (N_{\\mathcal R} T)\\cdot d_{\\mathcal R}\\big)$\nlogarithmic regret bound, where $\\eta, N_{\\mathcal R},T,d_{\\mathcal R}$ denote\nthe KL-regularization parameter, the cardinality of the reward function class,\nnumber of rounds, and the complexity of the reward function class. Furthermore,\nwe extend our algorithm and analysis to reinforcement learning by developing a\nnovel decomposition over transition steps and also obtain a similar logarithmic\nregret bound.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Reinforcement Learning from Human Feedback (RLHF) have\nshown that KL-regularization plays a pivotal role in improving the efficiency\nof RL fine-tuning for large language models (LLMs). Despite its empirical\nadvantage, the theoretical difference between KL-regularized RL and standard RL\nremains largely under-explored. While there is a recent line of work on the\ntheoretical analysis of KL-regularized objective in decision making\n\\citep{xiong2024iterative, xie2024exploratory,zhao2024sharp}, these analyses\neither reduce to the traditional RL setting or rely on strong coverage\nassumptions. In this paper, we propose an optimism-based KL-regularized online\ncontextual bandit algorithm, and provide a novel analysis of its regret. By\ncarefully leveraging the benign optimization landscape induced by the\nKL-regularization and the optimistic reward estimation, our algorithm achieves\nan $\\mathcal{O}\\big(\\eta\\log (N_{\\mathcal R} T)\\cdot d_{\\mathcal R}\\big)$\nlogarithmic regret bound, where $\\eta, N_{\\mathcal R},T,d_{\\mathcal R}$ denote\nthe KL-regularization parameter, the cardinality of the reward function class,\nnumber of rounds, and the complexity of the reward function class. Furthermore,\nwe extend our algorithm and analysis to reinforcement learning by developing a\nnovel decomposition over transition steps and also obtain a similar logarithmic\nregret bound."
                },
                "authors": [
                    {
                        "name": "Heyang Zhao"
                    },
                    {
                        "name": "Chenlu Ye"
                    },
                    {
                        "name": "Wei Xiong"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07459v1",
                "updated": "2025-02-11T11:07:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    11,
                    7,
                    44,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T11:07:44Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    11,
                    7,
                    44,
                    1,
                    42,
                    0
                ],
                "title": "PerCul: A Story-Driven Cultural Evaluation of LLMs in Persian",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PerCul: A Story-Driven Cultural Evaluation of LLMs in Persian"
                },
                "summary": "Large language models predominantly reflect Western cultures, largely due to\nthe dominance of English-centric training data. This imbalance presents a\nsignificant challenge, as LLMs are increasingly used across diverse contexts\nwithout adequate evaluation of their cultural competence in non-English\nlanguages, including Persian. To address this gap, we introduce PerCul, a\ncarefully constructed dataset designed to assess the sensitivity of LLMs toward\nPersian culture. PerCul features story-based, multiple-choice questions that\ncapture culturally nuanced scenarios. Unlike existing benchmarks, PerCul is\ncurated with input from native Persian annotators to ensure authenticity and to\nprevent the use of translation as a shortcut. We evaluate several\nstate-of-the-art multilingual and Persian-specific LLMs, establishing a\nfoundation for future research in cross-cultural NLP evaluation. Our\nexperiments demonstrate a 11.3% gap between best closed source model and\nlayperson baseline while the gap increases to 21.3% by using the best\nopen-weight model. You can access the dataset from here:\nhttps://huggingface.co/datasets/teias-ai/percul",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models predominantly reflect Western cultures, largely due to\nthe dominance of English-centric training data. This imbalance presents a\nsignificant challenge, as LLMs are increasingly used across diverse contexts\nwithout adequate evaluation of their cultural competence in non-English\nlanguages, including Persian. To address this gap, we introduce PerCul, a\ncarefully constructed dataset designed to assess the sensitivity of LLMs toward\nPersian culture. PerCul features story-based, multiple-choice questions that\ncapture culturally nuanced scenarios. Unlike existing benchmarks, PerCul is\ncurated with input from native Persian annotators to ensure authenticity and to\nprevent the use of translation as a shortcut. We evaluate several\nstate-of-the-art multilingual and Persian-specific LLMs, establishing a\nfoundation for future research in cross-cultural NLP evaluation. Our\nexperiments demonstrate a 11.3% gap between best closed source model and\nlayperson baseline while the gap increases to 21.3% by using the best\nopen-weight model. You can access the dataset from here:\nhttps://huggingface.co/datasets/teias-ai/percul"
                },
                "authors": [
                    {
                        "name": "Erfan Moosavi Monazzah"
                    },
                    {
                        "name": "Vahid Rahimzadeh"
                    },
                    {
                        "name": "Yadollah Yaghoobzadeh"
                    },
                    {
                        "name": "Azadeh Shakery"
                    },
                    {
                        "name": "Mohammad Taher Pilehvar"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Taher Pilehvar"
                },
                "author": "Mohammad Taher Pilehvar",
                "arxiv_comment": "Accepted at NAACL 2025 Main Conference, the dataset is available on\n  HuggingFace (see https://huggingface.co/datasets/teias-ai/percul)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12502v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12502v3",
                "updated": "2025-02-11T11:03:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    11,
                    3,
                    24,
                    1,
                    42,
                    0
                ],
                "published": "2024-11-19T13:40:49Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    13,
                    40,
                    49,
                    1,
                    324,
                    0
                ],
                "title": "Transformer Neural Processes - Kernel Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer Neural Processes - Kernel Regression"
                },
                "summary": "Neural Processes (NPs) are a rapidly evolving class of models designed to\ndirectly model the posterior predictive distribution of stochastic processes.\nOriginally developed as a scalable alternative to Gaussian Processes (GPs),\nwhich are limited by $O(n^3)$ runtime complexity, the most accurate modern NPs\ncan often rival GPs but still suffer from an $O(n^2)$ bottleneck due to their\nattention mechanism. We introduce the Transformer Neural Process - Kernel\nRegression (TNP-KR), a scalable NP featuring: (1) a Kernel Regression Block\n(KRBlock), a simple, extensible, and parameter efficient transformer block with\ncomplexity $O(n_c^2 + n_c n_t)$, where $n_c$ and $n_t$ are the number of\ncontext and test points, respectively; (2) a kernel-based attention bias; and\n(3) two novel attention mechanisms: scan attention (SA), a memory-efficient\nscan-based attention that when paired with a kernel-based bias can make TNP-KR\ntranslation invariant, and deep kernel attention (DKA), a Performer-style\nattention that implicitly incoporates a distance bias and further reduces\ncomplexity to $O(n_c)$. These enhancements enable both TNP-KR variants to\nperform inference with 100K context points on over 1M test points in under a\nminute on a single 24GB GPU. On benchmarks spanning meta regression, Bayesian\noptimization, image completion, and epidemiology, TNP-KR with DKA outperforms\nits Performer counterpart on nearly every benchmark, while TNP-KR with SA\nachieves state-of-the-art results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Processes (NPs) are a rapidly evolving class of models designed to\ndirectly model the posterior predictive distribution of stochastic processes.\nOriginally developed as a scalable alternative to Gaussian Processes (GPs),\nwhich are limited by $O(n^3)$ runtime complexity, the most accurate modern NPs\ncan often rival GPs but still suffer from an $O(n^2)$ bottleneck due to their\nattention mechanism. We introduce the Transformer Neural Process - Kernel\nRegression (TNP-KR), a scalable NP featuring: (1) a Kernel Regression Block\n(KRBlock), a simple, extensible, and parameter efficient transformer block with\ncomplexity $O(n_c^2 + n_c n_t)$, where $n_c$ and $n_t$ are the number of\ncontext and test points, respectively; (2) a kernel-based attention bias; and\n(3) two novel attention mechanisms: scan attention (SA), a memory-efficient\nscan-based attention that when paired with a kernel-based bias can make TNP-KR\ntranslation invariant, and deep kernel attention (DKA), a Performer-style\nattention that implicitly incoporates a distance bias and further reduces\ncomplexity to $O(n_c)$. These enhancements enable both TNP-KR variants to\nperform inference with 100K context points on over 1M test points in under a\nminute on a single 24GB GPU. On benchmarks spanning meta regression, Bayesian\noptimization, image completion, and epidemiology, TNP-KR with DKA outperforms\nits Performer counterpart on nearly every benchmark, while TNP-KR with SA\nachieves state-of-the-art results."
                },
                "authors": [
                    {
                        "name": "Daniel Jenson"
                    },
                    {
                        "name": "Jhonathan Navott"
                    },
                    {
                        "name": "Mengyan Zhang"
                    },
                    {
                        "name": "Makkunda Sharma"
                    },
                    {
                        "name": "Elizaveta Semenova"
                    },
                    {
                        "name": "Seth Flaxman"
                    }
                ],
                "author_detail": {
                    "name": "Seth Flaxman"
                },
                "author": "Seth Flaxman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12502v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12502v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07222v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07222v2",
                "updated": "2025-02-11T11:02:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    11,
                    2,
                    10,
                    1,
                    42,
                    0
                ],
                "published": "2024-06-11T13:01:50Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    13,
                    1,
                    50,
                    1,
                    163,
                    0
                ],
                "title": "Improving Autoformalization using Type Checking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Autoformalization using Type Checking"
                },
                "summary": "Autoformalization, the automatic translation of unconstrained natural\nlanguage into formal languages, has garnered significant attention due to its\npotential applications in theorem proving, formal verification, and LLM output\nchecking. In this work, we analyze both current autoformalization methods and\nthe processes used to evaluate them, focusing specifically on the Lean 4\ntheorem proving language. We demonstrate that scaling type-check filtering with\nself-consistency techniques on top of existing methods significantly improves\nperformance, achieving absolute accuracy gains of up to +18.4\\% on ProofNet. To\nsupport reproducibility and further research, we release our code, including\nnew symbolic equivalence for Lean formulas. We also release new benchmarks: a\nnew research-level mathematics dataset RLM25, a corrected ProofNet, and\nProofNetVerif with labeled correct and incorrect autoformalization pairs for\nevaluating metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoformalization, the automatic translation of unconstrained natural\nlanguage into formal languages, has garnered significant attention due to its\npotential applications in theorem proving, formal verification, and LLM output\nchecking. In this work, we analyze both current autoformalization methods and\nthe processes used to evaluate them, focusing specifically on the Lean 4\ntheorem proving language. We demonstrate that scaling type-check filtering with\nself-consistency techniques on top of existing methods significantly improves\nperformance, achieving absolute accuracy gains of up to +18.4\\% on ProofNet. To\nsupport reproducibility and further research, we release our code, including\nnew symbolic equivalence for Lean formulas. We also release new benchmarks: a\nnew research-level mathematics dataset RLM25, a corrected ProofNet, and\nProofNetVerif with labeled correct and incorrect autoformalization pairs for\nevaluating metrics."
                },
                "authors": [
                    {
                        "name": "Auguste Poiroux"
                    },
                    {
                        "name": "Gail Weiss"
                    },
                    {
                        "name": "Viktor Kunak"
                    },
                    {
                        "name": "Antoine Bosselut"
                    }
                ],
                "author_detail": {
                    "name": "Antoine Bosselut"
                },
                "author": "Antoine Bosselut",
                "arxiv_comment": "New benchmarks released, see\n  https://github.com/augustepoiroux/RLMEval ,\n  https://huggingface.co/datasets/PAug/ProofNetSharp , and\n  https://huggingface.co/datasets/PAug/ProofNetVerif . For code, see\n  https://github.com/augustepoiroux/LeanInteract",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07222v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07222v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06358v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06358v2",
                "updated": "2025-02-11T10:54:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    10,
                    54,
                    40,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-10T11:20:10Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    11,
                    20,
                    10,
                    0,
                    41,
                    0
                ],
                "title": "Towards bandit-based prompt-tuning for in-the-wild foundation agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards bandit-based prompt-tuning for in-the-wild foundation agents"
                },
                "summary": "Prompting has emerged as the dominant paradigm for adapting large,\npre-trained transformer-based models to downstream tasks. The Prompting\nDecision Transformer (PDT) enables large-scale, multi-task offline\nreinforcement learning pre-training by leveraging stochastic trajectory prompts\nto identify the target task. However, these prompts are sampled uniformly from\nexpert demonstrations, overlooking a critical limitation: Not all prompts are\nequally informative for differentiating between tasks. To address this, we\npropose an inference time bandit-based prompt-tuning framework that explores\nand optimizes trajectory prompt selection to enhance task performance. Our\nexperiments indicate not only clear performance gains due to bandit-based\nprompt-tuning, but also better sample complexity, scalability, and prompt space\nexploration compared to prompt-tuning baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting has emerged as the dominant paradigm for adapting large,\npre-trained transformer-based models to downstream tasks. The Prompting\nDecision Transformer (PDT) enables large-scale, multi-task offline\nreinforcement learning pre-training by leveraging stochastic trajectory prompts\nto identify the target task. However, these prompts are sampled uniformly from\nexpert demonstrations, overlooking a critical limitation: Not all prompts are\nequally informative for differentiating between tasks. To address this, we\npropose an inference time bandit-based prompt-tuning framework that explores\nand optimizes trajectory prompt selection to enhance task performance. Our\nexperiments indicate not only clear performance gains due to bandit-based\nprompt-tuning, but also better sample complexity, scalability, and prompt space\nexploration compared to prompt-tuning baselines."
                },
                "authors": [
                    {
                        "name": "Finn Rietz"
                    },
                    {
                        "name": "Oleg Smirnov"
                    },
                    {
                        "name": "Sara Karimi"
                    },
                    {
                        "name": "Lele Cao"
                    }
                ],
                "author_detail": {
                    "name": "Lele Cao"
                },
                "author": "Lele Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06358v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06358v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16701v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16701v3",
                "updated": "2025-02-11T10:53:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    10,
                    53,
                    20,
                    1,
                    42,
                    0
                ],
                "published": "2024-09-25T07:47:01Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    47,
                    1,
                    2,
                    269,
                    0
                ],
                "title": "Vulnerability-Triggering Test Case Generation from Third-Party Libraries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vulnerability-Triggering Test Case Generation from Third-Party Libraries"
                },
                "summary": "Open-source third-party libraries are widely used in software development.\nThese libraries offer substantial advantages in terms of time and resource\nsavings. However, a significant concern arises due to the publicly disclosed\nvulnerabilities within these libraries. Existing automated vulnerability\ndetection tools often suffer from false positives and fail to accurately assess\nthe propagation of inputs capable of triggering vulnerabilities from client\nprojects to vulnerable code in libraries. In this paper, we propose a novel\napproach called VULEUT (Vulnerability Exploit Unit Test Generation), which\ncombines vulnerability exploitation reachability analysis and LLM-based unit\ntest generation. VULEUT is designed to automatically verify the exploitability\nof vulnerabilities in third-party libraries commonly used in client software\nprojects. VULEUT first analyzes the client projects to determine the\nreachability of vulnerability conditions. And then, it leverages the Large\nLanguage Model (LLM) to generate unit tests for vulnerability confirmation. To\nevaluate the effectiveness of VULEUT, we collect 32 vulnerabilities from\nvarious third-party libraries and conduct experiments on 70 real client\nprojects. Besides, we also compare our approach with two representative tools,\ni.e., TRANSFER and VESTA. Our results demonstrate the effectiveness of VULEUT,\nwith 229 out of 292 generated unit tests successfully confirming vulnerability\nexploitation across 70 client projects, which outperforms baselines by 24%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-source third-party libraries are widely used in software development.\nThese libraries offer substantial advantages in terms of time and resource\nsavings. However, a significant concern arises due to the publicly disclosed\nvulnerabilities within these libraries. Existing automated vulnerability\ndetection tools often suffer from false positives and fail to accurately assess\nthe propagation of inputs capable of triggering vulnerabilities from client\nprojects to vulnerable code in libraries. In this paper, we propose a novel\napproach called VULEUT (Vulnerability Exploit Unit Test Generation), which\ncombines vulnerability exploitation reachability analysis and LLM-based unit\ntest generation. VULEUT is designed to automatically verify the exploitability\nof vulnerabilities in third-party libraries commonly used in client software\nprojects. VULEUT first analyzes the client projects to determine the\nreachability of vulnerability conditions. And then, it leverages the Large\nLanguage Model (LLM) to generate unit tests for vulnerability confirmation. To\nevaluate the effectiveness of VULEUT, we collect 32 vulnerabilities from\nvarious third-party libraries and conduct experiments on 70 real client\nprojects. Besides, we also compare our approach with two representative tools,\ni.e., TRANSFER and VESTA. Our results demonstrate the effectiveness of VULEUT,\nwith 229 out of 292 generated unit tests successfully confirming vulnerability\nexploitation across 70 client projects, which outperforms baselines by 24%."
                },
                "authors": [
                    {
                        "name": "Yi Gao"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Zirui Chen"
                    },
                    {
                        "name": "Xiaohu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohu Yang"
                },
                "author": "Xiaohu Yang",
                "arxiv_comment": "Published in 2nd Conference on AI Foundation Models and Software\n  Engineering (FORGE 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16701v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16701v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16739v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16739v2",
                "updated": "2025-02-11T10:52:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    10,
                    52,
                    54,
                    1,
                    42,
                    0
                ],
                "published": "2024-09-25T08:42:29Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    42,
                    29,
                    2,
                    269,
                    0
                ],
                "title": "Automated Unit Test Refactoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Unit Test Refactoring"
                },
                "summary": "Test smells arise from poor design practices and insufficient domain\nknowledge, which can lower the quality of test code and make it harder to\nmaintain and update. Manually refactoring test smells is time-consuming and\nerror-prone, highlighting the necessity for automated approaches. Current\nrule-based refactoring methods often struggle in scenarios not covered by\npredefined rules and lack the flexibility needed to handle diverse cases\neffectively. In this paper, we propose a novel approach called UTRefactor, a\ncontext-enhanced, LLM-based framework for automatic test refactoring in Java\nprojects. UTRefactor extracts relevant context from test code and leverages an\nexternal knowledge base that includes test smell definitions, descriptions, and\nDSL-based refactoring rules. By simulating the manual refactoring process\nthrough a chain-of-thought approach, UTRefactor guides the LLM to eliminate\ntest smells in a step-by-step process, ensuring both accuracy and consistency\nthroughout the refactoring. Additionally, we implement a checkpoint mechanism\nto facilitate comprehensive refactoring, particularly when multiple smells are\npresent. We evaluate UTRefactor on 879 tests from six open-source Java\nprojects, reducing the number of test smells from 2,375 to 265, achieving an\n89% reduction. UTRefactor outperforms direct LLM-based refactoring methods by\n61.82% in smell elimination and significantly surpasses the performance of a\nrule-based test smell refactoring tool. Our results demonstrate the\neffectiveness of UTRefactor in enhancing test code quality while minimizing\nmanual involvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test smells arise from poor design practices and insufficient domain\nknowledge, which can lower the quality of test code and make it harder to\nmaintain and update. Manually refactoring test smells is time-consuming and\nerror-prone, highlighting the necessity for automated approaches. Current\nrule-based refactoring methods often struggle in scenarios not covered by\npredefined rules and lack the flexibility needed to handle diverse cases\neffectively. In this paper, we propose a novel approach called UTRefactor, a\ncontext-enhanced, LLM-based framework for automatic test refactoring in Java\nprojects. UTRefactor extracts relevant context from test code and leverages an\nexternal knowledge base that includes test smell definitions, descriptions, and\nDSL-based refactoring rules. By simulating the manual refactoring process\nthrough a chain-of-thought approach, UTRefactor guides the LLM to eliminate\ntest smells in a step-by-step process, ensuring both accuracy and consistency\nthroughout the refactoring. Additionally, we implement a checkpoint mechanism\nto facilitate comprehensive refactoring, particularly when multiple smells are\npresent. We evaluate UTRefactor on 879 tests from six open-source Java\nprojects, reducing the number of test smells from 2,375 to 265, achieving an\n89% reduction. UTRefactor outperforms direct LLM-based refactoring methods by\n61.82% in smell elimination and significantly surpasses the performance of a\nrule-based test smell refactoring tool. Our results demonstrate the\neffectiveness of UTRefactor in enhancing test code quality while minimizing\nmanual involvement."
                },
                "authors": [
                    {
                        "name": "Yi Gao"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Xiaohu Yang"
                    },
                    {
                        "name": "Xin Xia"
                    }
                ],
                "author_detail": {
                    "name": "Xin Xia"
                },
                "author": "Xin Xia",
                "arxiv_comment": "Published in International Conference on the Foundations of Software\n  Engineering (FSE 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16739v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16739v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07445v1",
                "updated": "2025-02-11T10:43:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    10,
                    43,
                    36,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T10:43:36Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    10,
                    43,
                    36,
                    1,
                    42,
                    0
                ],
                "title": "Forget What You Know about LLMs Evaluations - LLMs are Like a Chameleon",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forget What You Know about LLMs Evaluations - LLMs are Like a Chameleon"
                },
                "summary": "Large language models (LLMs) often appear to excel on public benchmarks, but\nthese high scores may mask an overreliance on dataset-specific surface cues\nrather than true language understanding. We introduce the Chameleon Benchmark\nOverfit Detector (C-BOD), a meta-evaluation framework that systematically\ndistorts benchmark prompts via a parametric transformation and detects\noverfitting of LLMs. By rephrasing inputs while preserving their semantic\ncontent and labels, C-BOD exposes whether a model's performance is driven by\nmemorized patterns. Evaluated on the MMLU benchmark using 26 leading LLMs, our\nmethod reveals an average performance degradation of 2.15% under modest\nperturbations, with 20 out of 26 models exhibiting statistically significant\ndifferences. Notably, models with higher baseline accuracy exhibit larger\nperformance differences under perturbation, and larger LLMs tend to be more\nsensitive to rephrasings indicating that both cases may overrely on fixed\nprompt patterns. In contrast, the Llama family and models with lower baseline\naccuracy show insignificant degradation, suggesting reduced dependency on\nsuperficial cues. Moreover, C-BOD's dataset- and model-agnostic design allows\neasy integration into training pipelines to promote more robust language\nunderstanding. Our findings challenge the community to look beyond leaderboard\nscores and prioritize resilience and generalization in LLM evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often appear to excel on public benchmarks, but\nthese high scores may mask an overreliance on dataset-specific surface cues\nrather than true language understanding. We introduce the Chameleon Benchmark\nOverfit Detector (C-BOD), a meta-evaluation framework that systematically\ndistorts benchmark prompts via a parametric transformation and detects\noverfitting of LLMs. By rephrasing inputs while preserving their semantic\ncontent and labels, C-BOD exposes whether a model's performance is driven by\nmemorized patterns. Evaluated on the MMLU benchmark using 26 leading LLMs, our\nmethod reveals an average performance degradation of 2.15% under modest\nperturbations, with 20 out of 26 models exhibiting statistically significant\ndifferences. Notably, models with higher baseline accuracy exhibit larger\nperformance differences under perturbation, and larger LLMs tend to be more\nsensitive to rephrasings indicating that both cases may overrely on fixed\nprompt patterns. In contrast, the Llama family and models with lower baseline\naccuracy show insignificant degradation, suggesting reduced dependency on\nsuperficial cues. Moreover, C-BOD's dataset- and model-agnostic design allows\neasy integration into training pipelines to promote more robust language\nunderstanding. Our findings challenge the community to look beyond leaderboard\nscores and prioritize resilience and generalization in LLM evaluation."
                },
                "authors": [
                    {
                        "name": "Nurit Cohen-Inger"
                    },
                    {
                        "name": "Yehonatan Elisha"
                    },
                    {
                        "name": "Bracha Shapira"
                    },
                    {
                        "name": "Lior Rokach"
                    },
                    {
                        "name": "Seffi Cohen"
                    }
                ],
                "author_detail": {
                    "name": "Seffi Cohen"
                },
                "author": "Seffi Cohen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07443v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07443v1",
                "updated": "2025-02-11T10:37:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    10,
                    37,
                    20,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T10:37:20Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    10,
                    37,
                    20,
                    1,
                    42,
                    0
                ],
                "title": "Approximating Human Strategic Reasoning with LLM-Enhanced Recursive\n  Reasoners Leveraging Multi-agent Hypergames",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximating Human Strategic Reasoning with LLM-Enhanced Recursive\n  Reasoners Leveraging Multi-agent Hypergames"
                },
                "summary": "LLM-driven multi-agent-based simulations have been gaining traction with\napplications in game-theoretic and social simulations. While most\nimplementations seek to exploit or evaluate LLM-agentic reasoning, they often\ndo so with a weak notion of agency and simplified architectures. We implement a\nrole-based multi-agent strategic interaction framework tailored to\nsophisticated recursive reasoners, providing the means for systematic in-depth\ndevelopment and evaluation of strategic reasoning. Our game environment is\ngoverned by the umpire responsible for facilitating games, from matchmaking\nthrough move validation to environment management. Players incorporate\nstate-of-the-art LLMs in their decision mechanism, relying on a formal\nhypergame-based model of hierarchical beliefs. We use one-shot, 2-player beauty\ncontests to evaluate the recursive reasoning capabilities of the latest LLMs,\nproviding a comparison to an established baseline model from economics and data\nfrom human experiments. Furthermore, we introduce the foundations of an\nalternative semantic measure of reasoning to the k-level theory. Our\nexperiments show that artificial reasoners can outperform the baseline model in\nterms of both approximating human behaviour and reaching the optimal solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-driven multi-agent-based simulations have been gaining traction with\napplications in game-theoretic and social simulations. While most\nimplementations seek to exploit or evaluate LLM-agentic reasoning, they often\ndo so with a weak notion of agency and simplified architectures. We implement a\nrole-based multi-agent strategic interaction framework tailored to\nsophisticated recursive reasoners, providing the means for systematic in-depth\ndevelopment and evaluation of strategic reasoning. Our game environment is\ngoverned by the umpire responsible for facilitating games, from matchmaking\nthrough move validation to environment management. Players incorporate\nstate-of-the-art LLMs in their decision mechanism, relying on a formal\nhypergame-based model of hierarchical beliefs. We use one-shot, 2-player beauty\ncontests to evaluate the recursive reasoning capabilities of the latest LLMs,\nproviding a comparison to an established baseline model from economics and data\nfrom human experiments. Furthermore, we introduce the foundations of an\nalternative semantic measure of reasoning to the k-level theory. Our\nexperiments show that artificial reasoners can outperform the baseline model in\nterms of both approximating human behaviour and reaching the optimal solution."
                },
                "authors": [
                    {
                        "name": "Vince Trencsenyi"
                    },
                    {
                        "name": "Agnieszka Mensfelt"
                    },
                    {
                        "name": "Kostas Stathis"
                    }
                ],
                "author_detail": {
                    "name": "Kostas Stathis"
                },
                "author": "Kostas Stathis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07443v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07443v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.07780v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07780v1",
                "updated": "2025-02-11T18:59:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    59,
                    35,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T18:59:35Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    59,
                    35,
                    1,
                    42,
                    0
                ],
                "title": "DarwinLM: Evolutionary Structured Pruning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DarwinLM: Evolutionary Structured Pruning of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have achieved significant success across various\nNLP tasks. However, their massive computational costs limit their widespread\nuse, particularly in real-time applications. Structured pruning offers an\neffective solution by compressing models and directly providing end-to-end\nspeed improvements, regardless of the hardware environment. Meanwhile,\ndifferent components of the model exhibit varying sensitivities towards\npruning, calling for \\emph{non-uniform} model compression. However, a pruning\nmethod should not only identify a capable substructure, but also account for\npost-compression training. To this end, we propose \\sysname, a method for\n\\emph{training-aware} structured pruning. \\sysname builds upon an evolutionary\nsearch process, generating multiple offspring models in each generation through\nmutation, and selecting the fittest for survival. To assess the effect of\npost-training, we incorporate a lightweight, multistep training process within\nthe offspring population, progressively increasing the number of tokens and\neliminating poorly performing models in each selection stage. We validate our\nmethod through extensive experiments on Llama-2-7B, Llama-3.1-8B and\nQwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured\npruning. For instance, \\sysname surpasses ShearedLlama while requiring\n$5\\times$ less training data during post-compression training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved significant success across various\nNLP tasks. However, their massive computational costs limit their widespread\nuse, particularly in real-time applications. Structured pruning offers an\neffective solution by compressing models and directly providing end-to-end\nspeed improvements, regardless of the hardware environment. Meanwhile,\ndifferent components of the model exhibit varying sensitivities towards\npruning, calling for \\emph{non-uniform} model compression. However, a pruning\nmethod should not only identify a capable substructure, but also account for\npost-compression training. To this end, we propose \\sysname, a method for\n\\emph{training-aware} structured pruning. \\sysname builds upon an evolutionary\nsearch process, generating multiple offspring models in each generation through\nmutation, and selecting the fittest for survival. To assess the effect of\npost-training, we incorporate a lightweight, multistep training process within\nthe offspring population, progressively increasing the number of tokens and\neliminating poorly performing models in each selection stage. We validate our\nmethod through extensive experiments on Llama-2-7B, Llama-3.1-8B and\nQwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured\npruning. For instance, \\sysname surpasses ShearedLlama while requiring\n$5\\times$ less training data during post-compression training."
                },
                "authors": [
                    {
                        "name": "Shengkun Tang"
                    },
                    {
                        "name": "Oliver Sieberling"
                    },
                    {
                        "name": "Eldar Kurtic"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07780v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07780v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.08446v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.08446v2",
                "updated": "2025-02-11T18:59:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    59,
                    26,
                    1,
                    42,
                    0
                ],
                "published": "2024-06-12T17:37:09Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    17,
                    37,
                    9,
                    2,
                    164,
                    0
                ],
                "title": "OLMES: A Standard for Language Model Evaluations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OLMES: A Standard for Language Model Evaluations"
                },
                "summary": "Progress in AI is often demonstrated by new models claiming improved\nperformance on tasks measuring model capabilities. Evaluating language models\ncan be particularly challenging, as choices of how a model is evaluated on a\ntask can lead to large changes in measured performance. There is no common\nstandard setup, so different models are evaluated on the same tasks in\ndifferent ways, leading to claims about which models perform best not being\nreproducible. We propose OLMES, a completely documented, practical, open\nstandard for reproducible LLM evaluations. In developing this standard, we\nidentify and review the varying factors in evaluation practices adopted by the\ncommunity - such as details of prompt formatting, choice of in-context\nexamples, probability normalizations, and task formulation. In particular,\nOLMES supports meaningful comparisons between smaller base models that require\nthe unnatural \"cloze\" formulation of multiple-choice questions against larger\nmodels that can utilize the original formulation. OLMES includes\nwell-considered, documented recommendations guided by results from existing\nliterature as well as new experiments resolving open questions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Progress in AI is often demonstrated by new models claiming improved\nperformance on tasks measuring model capabilities. Evaluating language models\ncan be particularly challenging, as choices of how a model is evaluated on a\ntask can lead to large changes in measured performance. There is no common\nstandard setup, so different models are evaluated on the same tasks in\ndifferent ways, leading to claims about which models perform best not being\nreproducible. We propose OLMES, a completely documented, practical, open\nstandard for reproducible LLM evaluations. In developing this standard, we\nidentify and review the varying factors in evaluation practices adopted by the\ncommunity - such as details of prompt formatting, choice of in-context\nexamples, probability normalizations, and task formulation. In particular,\nOLMES supports meaningful comparisons between smaller base models that require\nthe unnatural \"cloze\" formulation of multiple-choice questions against larger\nmodels that can utilize the original formulation. OLMES includes\nwell-considered, documented recommendations guided by results from existing\nliterature as well as new experiments resolving open questions."
                },
                "authors": [
                    {
                        "name": "Yuling Gu"
                    },
                    {
                        "name": "Oyvind Tafjord"
                    },
                    {
                        "name": "Bailey Kuehl"
                    },
                    {
                        "name": "Dany Haddad"
                    },
                    {
                        "name": "Jesse Dodge"
                    },
                    {
                        "name": "Hannaneh Hajishirzi"
                    }
                ],
                "author_detail": {
                    "name": "Hannaneh Hajishirzi"
                },
                "author": "Hannaneh Hajishirzi",
                "arxiv_comment": "Findings of NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.08446v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.08446v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07776v1",
                "updated": "2025-02-11T18:58:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    58,
                    4,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T18:58:04Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    58,
                    4,
                    1,
                    42,
                    0
                ],
                "title": "Auditing Prompt Caching in Language Model APIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auditing Prompt Caching in Language Model APIs"
                },
                "summary": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known."
                },
                "authors": [
                    {
                        "name": "Chenchen Gu"
                    },
                    {
                        "name": "Xiang Lisa Li"
                    },
                    {
                        "name": "Rohith Kuditipudi"
                    },
                    {
                        "name": "Percy Liang"
                    },
                    {
                        "name": "Tatsunori Hashimoto"
                    }
                ],
                "author_detail": {
                    "name": "Tatsunori Hashimoto"
                },
                "author": "Tatsunori Hashimoto",
                "arxiv_comment": "20 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07772v1",
                "updated": "2025-02-11T18:56:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    56,
                    20,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T18:56:20Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    56,
                    20,
                    1,
                    42,
                    0
                ],
                "title": "Automatic Robot Task Planning by Integrating Large Language Model with\n  Genetic Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Robot Task Planning by Integrating Large Language Model with\n  Genetic Programming"
                },
                "summary": "Accurate task planning is critical for controlling autonomous systems, such\nas robots, drones, and self-driving vehicles. Behavior Trees (BTs) are\nconsidered one of the most prominent control-policy-defining frameworks in task\nplanning, due to their modularity, flexibility, and reusability. Generating\nreliable and accurate BT-based control policies for robotic systems remains\nchallenging and often requires domain expertise. In this paper, we present the\nLLM-GP-BT technique that leverages the Large Language Model (LLM) and Genetic\nProgramming (GP) to automate the generation and configuration of BTs. The\nLLM-GP-BT technique processes robot task commands expressed in human natural\nlanguage and converts them into accurate and reliable BT-based task plans in a\ncomputationally efficient and user-friendly manner. The proposed technique is\nsystematically developed and validated through simulation experiments,\ndemonstrating its potential to streamline task planning for autonomous systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate task planning is critical for controlling autonomous systems, such\nas robots, drones, and self-driving vehicles. Behavior Trees (BTs) are\nconsidered one of the most prominent control-policy-defining frameworks in task\nplanning, due to their modularity, flexibility, and reusability. Generating\nreliable and accurate BT-based control policies for robotic systems remains\nchallenging and often requires domain expertise. In this paper, we present the\nLLM-GP-BT technique that leverages the Large Language Model (LLM) and Genetic\nProgramming (GP) to automate the generation and configuration of BTs. The\nLLM-GP-BT technique processes robot task commands expressed in human natural\nlanguage and converts them into accurate and reliable BT-based task plans in a\ncomputationally efficient and user-friendly manner. The proposed technique is\nsystematically developed and validated through simulation experiments,\ndemonstrating its potential to streamline task planning for autonomous systems."
                },
                "authors": [
                    {
                        "name": "Azizjon Kobilov"
                    },
                    {
                        "name": "Jianglin Lan"
                    }
                ],
                "author_detail": {
                    "name": "Jianglin Lan"
                },
                "author": "Jianglin Lan",
                "arxiv_comment": "Submitted to IEEE Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07771v1",
                "updated": "2025-02-11T18:55:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    55,
                    57,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T18:55:57Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    55,
                    57,
                    1,
                    42,
                    0
                ],
                "title": "Breaking Down Bias: On The Limits of Generalizable Pruning Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking Down Bias: On The Limits of Generalizable Pruning Strategies"
                },
                "summary": "We employ model pruning to examine how LLMs conceptualize racial biases, and\nwhether a generalizable mitigation strategy for such biases appears feasible.\nOur analysis yields several novel insights. We find that pruning can be an\neffective method to reduce bias without significantly increasing anomalous\nmodel behavior. Neuron-based pruning strategies generally yield better results\nthan approaches pruning entire attention heads. However, our results also show\nthat the effectiveness of either approach quickly deteriorates as pruning\nstrategies become more generalized. For instance, a model that is trained on\nremoving racial biases in the context of financial decision-making poorly\ngeneralizes to biases in commercial transactions. Overall, our analysis\nsuggests that racial biases are only partially represented as a general concept\nwithin language models. The other part of these biases is highly\ncontext-specific, suggesting that generalizable mitigation strategies may be of\nlimited effectiveness. Our findings have important implications for legal\nframeworks surrounding AI. In particular, they suggest that an effective\nmitigation strategy should include the allocation of legal responsibility on\nthose that deploy models in a specific use case.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We employ model pruning to examine how LLMs conceptualize racial biases, and\nwhether a generalizable mitigation strategy for such biases appears feasible.\nOur analysis yields several novel insights. We find that pruning can be an\neffective method to reduce bias without significantly increasing anomalous\nmodel behavior. Neuron-based pruning strategies generally yield better results\nthan approaches pruning entire attention heads. However, our results also show\nthat the effectiveness of either approach quickly deteriorates as pruning\nstrategies become more generalized. For instance, a model that is trained on\nremoving racial biases in the context of financial decision-making poorly\ngeneralizes to biases in commercial transactions. Overall, our analysis\nsuggests that racial biases are only partially represented as a general concept\nwithin language models. The other part of these biases is highly\ncontext-specific, suggesting that generalizable mitigation strategies may be of\nlimited effectiveness. Our findings have important implications for legal\nframeworks surrounding AI. In particular, they suggest that an effective\nmitigation strategy should include the allocation of legal responsibility on\nthose that deploy models in a specific use case."
                },
                "authors": [
                    {
                        "name": "Sibo Ma"
                    },
                    {
                        "name": "Alejandro Salinas"
                    },
                    {
                        "name": "Peter Henderson"
                    },
                    {
                        "name": "Julian Nyarko"
                    }
                ],
                "author_detail": {
                    "name": "Julian Nyarko"
                },
                "author": "Julian Nyarko",
                "arxiv_comment": "28 pages, 9 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06808v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06808v2",
                "updated": "2025-02-11T18:52:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    52,
                    51,
                    1,
                    42,
                    0
                ],
                "published": "2024-11-26T00:06:47Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    0,
                    6,
                    47,
                    1,
                    331,
                    0
                ],
                "title": "Effect of Adaptive Communication Support on LLM-powered Human-Robot\n  Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effect of Adaptive Communication Support on LLM-powered Human-Robot\n  Collaboration"
                },
                "summary": "Effective human-robot collaboration requires robot to adopt their roles and\nlevels of support based on human needs, task requirements, and complexity.\nTraditional human-robot teaming often relies on a pre-determined robot\ncommunication scheme, restricting teamwork adaptability in complex tasks.\nLeveraging strong communication capabilities of Large Language Models (LLMs),\nwe propose a Human-Robot Teaming Framework with Multi-Modal Language feedback\n(HRT-ML), a framework designed to enhance human-robot interaction by adjusting\nthe frequency and content of language-based feedback. HRT-ML framework includes\ntwo core modules: a Coordinator for high-level, low-frequency strategic\nguidance, and a Manager for subtask-specific, high-frequency instructions,\nenabling passive and active interactions with human teammates. To assess the\nimpact of language feedback in collaborative scenarios, we conducted\nexperiments in an enhanced Overcooked environment with varying levels of task\ncomplexity (easy, medium, hard) and feedback frequency (inactive, passive,\nactive, superactive). Our results show that as task complexity increases\nrelative to human capabilities, human teammates exhibited a stronger preference\ntowards robotic agents that can offer frequent, proactive support. However,\nwhen task complexities exceed the LLM's capacity, noisy and inaccurate feedback\nfrom superactive robotic agents can instead hinder team performance, as it\nrequires human teammates to increase their effort to interpret and respond to a\nlarge number of communications, with limited performance return. Our results\noffer a general principle for robotic agents to dynamically adjust their levels\nand frequencies of communications to work seamlessly with humans and achieve\nimproved teaming performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective human-robot collaboration requires robot to adopt their roles and\nlevels of support based on human needs, task requirements, and complexity.\nTraditional human-robot teaming often relies on a pre-determined robot\ncommunication scheme, restricting teamwork adaptability in complex tasks.\nLeveraging strong communication capabilities of Large Language Models (LLMs),\nwe propose a Human-Robot Teaming Framework with Multi-Modal Language feedback\n(HRT-ML), a framework designed to enhance human-robot interaction by adjusting\nthe frequency and content of language-based feedback. HRT-ML framework includes\ntwo core modules: a Coordinator for high-level, low-frequency strategic\nguidance, and a Manager for subtask-specific, high-frequency instructions,\nenabling passive and active interactions with human teammates. To assess the\nimpact of language feedback in collaborative scenarios, we conducted\nexperiments in an enhanced Overcooked environment with varying levels of task\ncomplexity (easy, medium, hard) and feedback frequency (inactive, passive,\nactive, superactive). Our results show that as task complexity increases\nrelative to human capabilities, human teammates exhibited a stronger preference\ntowards robotic agents that can offer frequent, proactive support. However,\nwhen task complexities exceed the LLM's capacity, noisy and inaccurate feedback\nfrom superactive robotic agents can instead hinder team performance, as it\nrequires human teammates to increase their effort to interpret and respond to a\nlarge number of communications, with limited performance return. Our results\noffer a general principle for robotic agents to dynamically adjust their levels\nand frequencies of communications to work seamlessly with humans and achieve\nimproved teaming performance."
                },
                "authors": [
                    {
                        "name": "Shipeng Liu"
                    },
                    {
                        "name": "FNU Shrutika"
                    },
                    {
                        "name": "Boshen Zhang"
                    },
                    {
                        "name": "Zhehui Huang"
                    },
                    {
                        "name": "Gaurav Sukhatme"
                    },
                    {
                        "name": "Feifei Qian"
                    }
                ],
                "author_detail": {
                    "name": "Feifei Qian"
                },
                "author": "Feifei Qian",
                "arxiv_comment": "13 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06808v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06808v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.9",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07763v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07763v1",
                "updated": "2025-02-11T18:46:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    46,
                    1,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T18:46:01Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    46,
                    1,
                    1,
                    42,
                    0
                ],
                "title": "Great Power Brings Great Responsibility: Personalizing Conversational AI\n  for Diverse Problem-Solvers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Great Power Brings Great Responsibility: Personalizing Conversational AI\n  for Diverse Problem-Solvers"
                },
                "summary": "Newcomers onboarding to Open Source Software (OSS) projects face many\nchallenges. Large Language Models (LLMs), like ChatGPT, have emerged as\npotential resources for answering questions and providing guidance, with many\ndevelopers now turning to ChatGPT over traditional Q&A sites like Stack\nOverflow. Nonetheless, LLMs may carry biases in presenting information, which\ncan be especially impactful for newcomers whose problem-solving styles may not\nbe broadly represented. This raises important questions about the accessibility\nof AI-driven support for newcomers to OSS projects. This vision paper outlines\nthe potential of adapting AI responses to various problem-solving styles to\navoid privileging a particular subgroup. We discuss the potential of AI\npersona-based prompt engineering as a strategy for interacting with AI. This\nstudy invites further research to refine AI-based tools to better support\ncontributions to OSS projects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Newcomers onboarding to Open Source Software (OSS) projects face many\nchallenges. Large Language Models (LLMs), like ChatGPT, have emerged as\npotential resources for answering questions and providing guidance, with many\ndevelopers now turning to ChatGPT over traditional Q&A sites like Stack\nOverflow. Nonetheless, LLMs may carry biases in presenting information, which\ncan be especially impactful for newcomers whose problem-solving styles may not\nbe broadly represented. This raises important questions about the accessibility\nof AI-driven support for newcomers to OSS projects. This vision paper outlines\nthe potential of adapting AI responses to various problem-solving styles to\navoid privileging a particular subgroup. We discuss the potential of AI\npersona-based prompt engineering as a strategy for interacting with AI. This\nstudy invites further research to refine AI-based tools to better support\ncontributions to OSS projects."
                },
                "authors": [
                    {
                        "name": "Italo Santos"
                    },
                    {
                        "name": "Katia Romero Felizardo"
                    },
                    {
                        "name": "Igor Steinmacher"
                    },
                    {
                        "name": "Marco A. Gerosa"
                    }
                ],
                "author_detail": {
                    "name": "Marco A. Gerosa"
                },
                "author": "Marco A. Gerosa",
                "arxiv_journal_ref": "18th International Conference on Cooperative and Human Aspects of\n  Software Engineering (CHASE 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07763v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07763v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19392v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19392v2",
                "updated": "2025-02-11T18:45:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    45,
                    12,
                    1,
                    42,
                    0
                ],
                "published": "2025-01-31T18:47:42Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    47,
                    42,
                    4,
                    31,
                    0
                ],
                "title": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models"
                },
                "summary": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models."
                },
                "authors": [
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "Vladimir Malinovskii"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Denis Mazur"
                    },
                    {
                        "name": "Nikita Surkov"
                    },
                    {
                        "name": "Ivan Ermakov"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19392v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19392v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.08598v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.08598v3",
                "updated": "2025-02-11T18:42:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    42,
                    44,
                    1,
                    42,
                    0
                ],
                "published": "2024-06-12T19:05:43Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    19,
                    5,
                    43,
                    2,
                    164,
                    0
                ],
                "title": "Language Model Council: Democratically Benchmarking Foundation Models on\n  Highly Subjective Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Model Council: Democratically Benchmarking Foundation Models on\n  Highly Subjective Tasks"
                },
                "summary": "As Large Language Models (LLMs) continue to evolve, evaluating them remains a\npersistent challenge. Many recent evaluations use LLMs as judges to score\noutputs from other LLMs, often relying on a single large model like GPT-4o.\nHowever, using a single LLM judge is prone to intra-model bias, and many tasks\n- such as those related to emotional intelligence, creative writing, and\npersuasiveness - may be too subjective for a single model to judge fairly. We\nintroduce the Language Model Council (LMC), where a group of LLMs collaborate\nto create tests, respond to them, and evaluate each other's responses to\nproduce a ranking in a democratic fashion. Unlike previous approaches that\nfocus on reducing cost or bias by using a panel of smaller models, our work\nexamines the benefits and nuances of a fully inclusive LLM evaluation system.\nIn a detailed case study on emotional intelligence, we deploy a council of 20\nrecent LLMs to rank each other on open-ended responses to interpersonal\nconflicts. Our results show that the LMC produces rankings that are more\nseparable and more robust, and through a user study, we show that they are more\nconsistent with human evaluations than any individual LLM judge. Using all LLMs\nfor judging can be costly, however, so we use Monte Carlo simulations and\nhand-curated sub-councils to study hypothetical council compositions and\ndiscuss the value of the incremental LLM judge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) continue to evolve, evaluating them remains a\npersistent challenge. Many recent evaluations use LLMs as judges to score\noutputs from other LLMs, often relying on a single large model like GPT-4o.\nHowever, using a single LLM judge is prone to intra-model bias, and many tasks\n- such as those related to emotional intelligence, creative writing, and\npersuasiveness - may be too subjective for a single model to judge fairly. We\nintroduce the Language Model Council (LMC), where a group of LLMs collaborate\nto create tests, respond to them, and evaluate each other's responses to\nproduce a ranking in a democratic fashion. Unlike previous approaches that\nfocus on reducing cost or bias by using a panel of smaller models, our work\nexamines the benefits and nuances of a fully inclusive LLM evaluation system.\nIn a detailed case study on emotional intelligence, we deploy a council of 20\nrecent LLMs to rank each other on open-ended responses to interpersonal\nconflicts. Our results show that the LMC produces rankings that are more\nseparable and more robust, and through a user study, we show that they are more\nconsistent with human evaluations than any individual LLM judge. Using all LLMs\nfor judging can be costly, however, so we use Monte Carlo simulations and\nhand-curated sub-councils to study hypothetical council compositions and\ndiscuss the value of the incremental LLM judge."
                },
                "authors": [
                    {
                        "name": "Justin Zhao"
                    },
                    {
                        "name": "Flor Miriam Plaza-del-Arco"
                    },
                    {
                        "name": "Benjie Genchel"
                    },
                    {
                        "name": "Amanda Cercas Curry"
                    }
                ],
                "author_detail": {
                    "name": "Amanda Cercas Curry"
                },
                "author": "Amanda Cercas Curry",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.08598v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.08598v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07752v1",
                "updated": "2025-02-11T18:27:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    27,
                    19,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T18:27:19Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    27,
                    19,
                    1,
                    42,
                    0
                ],
                "title": "Towards Efficient Optimizer Design for LLM via Structured Fisher\n  Approximation with a Low-Rank Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Efficient Optimizer Design for LLM via Structured Fisher\n  Approximation with a Low-Rank Extension"
                },
                "summary": "Designing efficient optimizers for large language models (LLMs) with\nlow-memory requirements and fast convergence is an important and challenging\nproblem. This paper makes a step towards the systematic design of such\noptimizers through the lens of structured Fisher information matrix (FIM)\napproximation. We show that many state-of-the-art efficient optimizers can be\nviewed as solutions to FIM approximation (under the Frobenius norm) with\nspecific structural assumptions. Building on these insights, we propose two\ndesign recommendations of practical efficient optimizers for LLMs, involving\nthe careful selection of structural assumptions to balance generality and\nefficiency, and enhancing memory efficiency of optimizers with general\nstructures through a novel low-rank extension framework. We demonstrate how to\nuse each design approach by deriving new memory-efficient optimizers: Row and\nColumn Scaled SGD (RACS) and Adaptive low-dimensional subspace estimation\n(Alice). Experiments on LLaMA pre-training (up to 1B parameters) validate the\neffectiveness, showing faster and better convergence than existing\nmemory-efficient baselines and Adam with little memory overhead. Notably, Alice\nachieves better than 2x faster convergence over Adam, while RACS delivers\nstrong performance on the 1B model with SGD-like memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing efficient optimizers for large language models (LLMs) with\nlow-memory requirements and fast convergence is an important and challenging\nproblem. This paper makes a step towards the systematic design of such\noptimizers through the lens of structured Fisher information matrix (FIM)\napproximation. We show that many state-of-the-art efficient optimizers can be\nviewed as solutions to FIM approximation (under the Frobenius norm) with\nspecific structural assumptions. Building on these insights, we propose two\ndesign recommendations of practical efficient optimizers for LLMs, involving\nthe careful selection of structural assumptions to balance generality and\nefficiency, and enhancing memory efficiency of optimizers with general\nstructures through a novel low-rank extension framework. We demonstrate how to\nuse each design approach by deriving new memory-efficient optimizers: Row and\nColumn Scaled SGD (RACS) and Adaptive low-dimensional subspace estimation\n(Alice). Experiments on LLaMA pre-training (up to 1B parameters) validate the\neffectiveness, showing faster and better convergence than existing\nmemory-efficient baselines and Adam with little memory overhead. Notably, Alice\nachieves better than 2x faster convergence over Adam, while RACS delivers\nstrong performance on the 1B model with SGD-like memory."
                },
                "authors": [
                    {
                        "name": "Wenbo Gong"
                    },
                    {
                        "name": "Meyer Scetbon"
                    },
                    {
                        "name": "Chao Ma"
                    },
                    {
                        "name": "Edward Meeds"
                    }
                ],
                "author_detail": {
                    "name": "Edward Meeds"
                },
                "author": "Edward Meeds",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.09401v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.09401v2",
                "updated": "2025-02-11T18:18:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    18,
                    59,
                    1,
                    42,
                    0
                ],
                "published": "2024-02-14T18:58:40Z",
                "published_parsed": [
                    2024,
                    2,
                    14,
                    18,
                    58,
                    40,
                    2,
                    45,
                    0
                ],
                "title": "Reinforcement Learning from Human Feedback with Active Queries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Human Feedback with Active Queries"
                },
                "summary": "Aligning large language models (LLM) with human preference plays a key role\nin building modern generative models and can be achieved by reinforcement\nlearning from human feedback (RLHF). Despite their superior performance,\ncurrent RLHF approaches often require a large amount of human-labelled\npreference data, which is expensive to collect. In this paper, inspired by the\nsuccess of active learning, we address this problem by proposing\nquery-efficient RLHF methods. We first formalize the alignment problem as a\ncontextual dueling bandit problem and design an active-query-based proximal\npolicy optimization (APPO) algorithm with an $\\tilde{O}(d^2/\\Delta)$\ninstance-dependent regret bound and an $\\tilde{O}(d^2/\\Delta^2)$ query\ncomplexity, where $d$ is the dimension of feature space and $\\Delta$ is the\nsub-optimality gap over all the contexts. We then propose ADPO, a practical\nversion of our algorithm based on direct preference optimization (DPO) and\napply it to fine-tuning LLMs. Our experiments show that ADPO, while only making\nabout half of queries for human preference, matches the performance of the\nstate-of-the-art DPO method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning large language models (LLM) with human preference plays a key role\nin building modern generative models and can be achieved by reinforcement\nlearning from human feedback (RLHF). Despite their superior performance,\ncurrent RLHF approaches often require a large amount of human-labelled\npreference data, which is expensive to collect. In this paper, inspired by the\nsuccess of active learning, we address this problem by proposing\nquery-efficient RLHF methods. We first formalize the alignment problem as a\ncontextual dueling bandit problem and design an active-query-based proximal\npolicy optimization (APPO) algorithm with an $\\tilde{O}(d^2/\\Delta)$\ninstance-dependent regret bound and an $\\tilde{O}(d^2/\\Delta^2)$ query\ncomplexity, where $d$ is the dimension of feature space and $\\Delta$ is the\nsub-optimality gap over all the contexts. We then propose ADPO, a practical\nversion of our algorithm based on direct preference optimization (DPO) and\napply it to fine-tuning LLMs. Our experiments show that ADPO, while only making\nabout half of queries for human preference, matches the performance of the\nstate-of-the-art DPO method."
                },
                "authors": [
                    {
                        "name": "Kaixuan Ji"
                    },
                    {
                        "name": "Jiafan He"
                    },
                    {
                        "name": "Quanquan Gu"
                    }
                ],
                "author_detail": {
                    "name": "Quanquan Gu"
                },
                "author": "Quanquan Gu",
                "arxiv_comment": "28 pages, 1 figure, 4 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.09401v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.09401v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.05502v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.05502v3",
                "updated": "2025-02-11T18:17:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    17,
                    53,
                    1,
                    42,
                    0
                ],
                "published": "2024-07-07T21:26:36Z",
                "published_parsed": [
                    2024,
                    7,
                    7,
                    21,
                    26,
                    36,
                    6,
                    189,
                    0
                ],
                "title": "Faux Polyglot: A Study on Information Disparity in Multilingual Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faux Polyglot: A Study on Information Disparity in Multilingual Large\n  Language Models"
                },
                "summary": "Although the multilingual capability of LLMs offers new opportunities to\novercome the language barrier, do these capabilities translate into real-life\nscenarios where linguistic divide and knowledge conflicts between multilingual\nsources are known occurrences? In this paper, we studied LLM's linguistic\npreference in a cross-language RAG-based information search setting. We found\nthat LLMs displayed systemic bias towards information in the same language as\nthe query language in both document retrieval and answer generation.\nFurthermore, in scenarios where no information is in the language of the query,\nLLMs prefer documents in high-resource languages during generation, potentially\nreinforcing the dominant views. Such bias exists for both factual and\nopinion-based queries. Our results highlight the linguistic divide within\nmultilingual LLMs in information search systems. The seemingly beneficial\nmultilingual capability of LLMs may backfire on information parity by\nreinforcing language-specific information cocoons or filter bubbles further\nmarginalizing low-resource views.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although the multilingual capability of LLMs offers new opportunities to\novercome the language barrier, do these capabilities translate into real-life\nscenarios where linguistic divide and knowledge conflicts between multilingual\nsources are known occurrences? In this paper, we studied LLM's linguistic\npreference in a cross-language RAG-based information search setting. We found\nthat LLMs displayed systemic bias towards information in the same language as\nthe query language in both document retrieval and answer generation.\nFurthermore, in scenarios where no information is in the language of the query,\nLLMs prefer documents in high-resource languages during generation, potentially\nreinforcing the dominant views. Such bias exists for both factual and\nopinion-based queries. Our results highlight the linguistic divide within\nmultilingual LLMs in information search systems. The seemingly beneficial\nmultilingual capability of LLMs may backfire on information parity by\nreinforcing language-specific information cocoons or filter bubbles further\nmarginalizing low-resource views."
                },
                "authors": [
                    {
                        "name": "Nikhil Sharma"
                    },
                    {
                        "name": "Kenton Murray"
                    },
                    {
                        "name": "Ziang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Ziang Xiao"
                },
                "author": "Ziang Xiao",
                "arxiv_comment": "NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.05502v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.05502v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07747v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07747v1",
                "updated": "2025-02-11T18:14:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    14,
                    44,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T18:14:44Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    14,
                    44,
                    1,
                    42,
                    0
                ],
                "title": "WHODUNIT: Evaluation benchmark for culprit detection in mystery stories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WHODUNIT: Evaluation benchmark for culprit detection in mystery stories"
                },
                "summary": "We present a novel data set, WhoDunIt, to assess the deductive reasoning\ncapabilities of large language models (LLM) within narrative contexts.\nConstructed from open domain mystery novels and short stories, the dataset\nchallenges LLMs to identify the perpetrator after reading and comprehending the\nstory. To evaluate model robustness, we apply a range of character-level name\naugmentations, including original names, name swaps, and substitutions with\nwell-known real and/or fictional entities from popular discourse. We further\nuse various prompting styles to investigate the influence of prompting on\ndeductive reasoning accuracy.\n  We conduct evaluation study with state-of-the-art models, specifically\nGPT-4o, GPT-4-turbo, and GPT-4o-mini, evaluated through multiple trials with\nmajority response selection to ensure reliability. The results demonstrate that\nwhile LLMs perform reliably on unaltered texts, accuracy diminishes with\ncertain name substitutions, particularly those with wide recognition. This\ndataset is publicly available here.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel data set, WhoDunIt, to assess the deductive reasoning\ncapabilities of large language models (LLM) within narrative contexts.\nConstructed from open domain mystery novels and short stories, the dataset\nchallenges LLMs to identify the perpetrator after reading and comprehending the\nstory. To evaluate model robustness, we apply a range of character-level name\naugmentations, including original names, name swaps, and substitutions with\nwell-known real and/or fictional entities from popular discourse. We further\nuse various prompting styles to investigate the influence of prompting on\ndeductive reasoning accuracy.\n  We conduct evaluation study with state-of-the-art models, specifically\nGPT-4o, GPT-4-turbo, and GPT-4o-mini, evaluated through multiple trials with\nmajority response selection to ensure reliability. The results demonstrate that\nwhile LLMs perform reliably on unaltered texts, accuracy diminishes with\ncertain name substitutions, particularly those with wide recognition. This\ndataset is publicly available here."
                },
                "authors": [
                    {
                        "name": "Kshitij Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Kshitij Gupta"
                },
                "author": "Kshitij Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07747v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07747v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04463v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04463v2",
                "updated": "2025-02-11T18:06:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    6,
                    2,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-06T19:18:16Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    19,
                    18,
                    16,
                    3,
                    37,
                    0
                ],
                "title": "Training Language Models to Reason Efficiently",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Language Models to Reason Efficiently"
                },
                "summary": "Scaling model size and training data has led to great advances in the\nperformance of Large Language Models (LLMs). However, the diminishing returns\nof this approach necessitate alternative methods to improve model capabilities,\nparticularly in tasks requiring advanced reasoning. Large reasoning models,\nwhich leverage long chain-of-thoughts, bring unprecedented breakthroughs in\nproblem-solving capabilities but at a substantial deployment cost associated to\nlonger generations. Reducing inference costs is crucial for the economic\nfeasibility, user experience, and environmental sustainability of these models.\n  In this work, we propose to train large reasoning models to reason\nefficiently. More precisely, we use reinforcement learning (RL) to train\nreasoning models to dynamically allocate inference-time compute based on task\ncomplexity. Our method incentivizes models to minimize unnecessary\ncomputational overhead while maintaining accuracy, thereby achieving\nsubstantial efficiency gains. It enables the derivation of a family of\nreasoning models with varying efficiency levels, controlled via a single\nhyperparameter. Experiments on two open-weight large reasoning models\ndemonstrate significant reductions in inference cost while preserving most of\nthe accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling model size and training data has led to great advances in the\nperformance of Large Language Models (LLMs). However, the diminishing returns\nof this approach necessitate alternative methods to improve model capabilities,\nparticularly in tasks requiring advanced reasoning. Large reasoning models,\nwhich leverage long chain-of-thoughts, bring unprecedented breakthroughs in\nproblem-solving capabilities but at a substantial deployment cost associated to\nlonger generations. Reducing inference costs is crucial for the economic\nfeasibility, user experience, and environmental sustainability of these models.\n  In this work, we propose to train large reasoning models to reason\nefficiently. More precisely, we use reinforcement learning (RL) to train\nreasoning models to dynamically allocate inference-time compute based on task\ncomplexity. Our method incentivizes models to minimize unnecessary\ncomputational overhead while maintaining accuracy, thereby achieving\nsubstantial efficiency gains. It enables the derivation of a family of\nreasoning models with varying efficiency levels, controlled via a single\nhyperparameter. Experiments on two open-weight large reasoning models\ndemonstrate significant reductions in inference cost while preserving most of\nthe accuracy."
                },
                "authors": [
                    {
                        "name": "Daman Arora"
                    },
                    {
                        "name": "Andrea Zanette"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Zanette"
                },
                "author": "Andrea Zanette",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04463v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04463v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07736v1",
                "updated": "2025-02-11T17:55:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    55,
                    15,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T17:55:15Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    55,
                    15,
                    1,
                    42,
                    0
                ],
                "title": "The Economics of Large Language Models: Token Allocation, Fine-Tuning,\n  and Optimal Pricing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Economics of Large Language Models: Token Allocation, Fine-Tuning,\n  and Optimal Pricing"
                },
                "summary": "We develop an economic framework to analyze the optimal pricing and product\ndesign of Large Language Models (LLM). Our framework captures several key\nfeatures of LLMs: variable operational costs of processing input and output\ntokens; the ability to customize models through fine-tuning; and\nhigh-dimensional user heterogeneity in terms of task requirements and error\nsensitivity. In our model, a monopolistic seller offers multiple versions of\nLLMs through a menu of products. The optimal pricing structure depends on\nwhether token allocation across tasks is contractible and whether users face\nscale constraints. Users with similar aggregate value-scale characteristics\nchoose similar levels of fine-tuning and token consumption. The optimal\nmechanism can be implemented through menus of two-part tariffs, with higher\nmarkups for more intensive users. Our results rationalize observed industry\npractices such as tiered pricing based on model customization and usage levels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop an economic framework to analyze the optimal pricing and product\ndesign of Large Language Models (LLM). Our framework captures several key\nfeatures of LLMs: variable operational costs of processing input and output\ntokens; the ability to customize models through fine-tuning; and\nhigh-dimensional user heterogeneity in terms of task requirements and error\nsensitivity. In our model, a monopolistic seller offers multiple versions of\nLLMs through a menu of products. The optimal pricing structure depends on\nwhether token allocation across tasks is contractible and whether users face\nscale constraints. Users with similar aggregate value-scale characteristics\nchoose similar levels of fine-tuning and token consumption. The optimal\nmechanism can be implemented through menus of two-part tariffs, with higher\nmarkups for more intensive users. Our results rationalize observed industry\npractices such as tiered pricing based on model customization and usage levels."
                },
                "authors": [
                    {
                        "name": "Dirk Bergemann"
                    },
                    {
                        "name": "Alessandro Bonatti"
                    },
                    {
                        "name": "Alex Smolin"
                    }
                ],
                "author_detail": {
                    "name": "Alex Smolin"
                },
                "author": "Alex Smolin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.TH",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07728v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07728v1",
                "updated": "2025-02-11T17:42:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    42,
                    7,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T17:42:07Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    42,
                    7,
                    1,
                    42,
                    0
                ],
                "title": "Verifying LLM-Generated Code in the Context of Software Verification\n  with Ada/SPARK",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verifying LLM-Generated Code in the Context of Software Verification\n  with Ada/SPARK"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable code generation\ncapabilities, but the correctness of the generated code cannot be inherently\ntrusted. This paper explores the feasibility of using formal software\nverification, specifically the SPARK framework for Ada, to ensure the\nreliability of LLM-generated code. We present Marmaragan, a tool that leverages\nan LLM in order to generate SPARK annotations for existing programs, enabling\nformal verification of the code. The tool is benchmarked on a curated set of\nSPARK programs, with annotations selectively removed to test specific\ncapabilities. The performance of Marmaragan with GPT-4o on the benchmark is\npromising, with correct annotations having been generated for 50.7% of the\nbenchmark cases. The results establish a foundation for future work on\ncombining the power of LLMs with the reliability of formal software\nverification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable code generation\ncapabilities, but the correctness of the generated code cannot be inherently\ntrusted. This paper explores the feasibility of using formal software\nverification, specifically the SPARK framework for Ada, to ensure the\nreliability of LLM-generated code. We present Marmaragan, a tool that leverages\nan LLM in order to generate SPARK annotations for existing programs, enabling\nformal verification of the code. The tool is benchmarked on a curated set of\nSPARK programs, with annotations selectively removed to test specific\ncapabilities. The performance of Marmaragan with GPT-4o on the benchmark is\npromising, with correct annotations having been generated for 50.7% of the\nbenchmark cases. The results establish a foundation for future work on\ncombining the power of LLMs with the reliability of formal software\nverification."
                },
                "authors": [
                    {
                        "name": "Marcos Cramer"
                    },
                    {
                        "name": "Lucian McIntyre"
                    }
                ],
                "author_detail": {
                    "name": "Lucian McIntyre"
                },
                "author": "Lucian McIntyre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07728v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07728v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.16527v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.16527v2",
                "updated": "2025-02-11T17:40:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    40,
                    41,
                    1,
                    42,
                    0
                ],
                "published": "2024-03-25T08:11:02Z",
                "published_parsed": [
                    2024,
                    3,
                    25,
                    8,
                    11,
                    2,
                    0,
                    85,
                    0
                ],
                "title": "Hallucination Detection in Foundation Models for Decision-Making: A\n  Flexible Definition and Review of the State of the Art",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination Detection in Foundation Models for Decision-Making: A\n  Flexible Definition and Review of the State of the Art"
                },
                "summary": "Autonomous systems are soon to be ubiquitous, spanning manufacturing,\nagriculture, healthcare, entertainment, and other industries. Most of these\nsystems are developed with modular sub-components for decision-making,\nplanning, and control that may be hand-engineered or learning-based. While\nthese approaches perform well under the situations they were specifically\ndesigned for, they can perform especially poorly in out-of-distribution\nscenarios that will undoubtedly arise at test-time. The rise of foundation\nmodels trained on multiple tasks with impressively large datasets has led\nresearchers to believe that these models may provide \"common sense\" reasoning\nthat existing planners are missing, bridging the gap between algorithm\ndevelopment and deployment. While researchers have shown promising results in\ndeploying foundation models to decision-making tasks, these models are known to\nhallucinate and generate decisions that may sound reasonable, but are in fact\npoor. We argue there is a need to step back and simultaneously design systems\nthat can quantify the certainty of a model's decision, and detect when it may\nbe hallucinating. In this work, we discuss the current use cases of foundation\nmodels for decision-making tasks, provide a general definition for\nhallucinations with examples, discuss existing approaches to hallucination\ndetection and mitigation with a focus on decision problems, present guidelines,\nand explore areas for further research in this exciting field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous systems are soon to be ubiquitous, spanning manufacturing,\nagriculture, healthcare, entertainment, and other industries. Most of these\nsystems are developed with modular sub-components for decision-making,\nplanning, and control that may be hand-engineered or learning-based. While\nthese approaches perform well under the situations they were specifically\ndesigned for, they can perform especially poorly in out-of-distribution\nscenarios that will undoubtedly arise at test-time. The rise of foundation\nmodels trained on multiple tasks with impressively large datasets has led\nresearchers to believe that these models may provide \"common sense\" reasoning\nthat existing planners are missing, bridging the gap between algorithm\ndevelopment and deployment. While researchers have shown promising results in\ndeploying foundation models to decision-making tasks, these models are known to\nhallucinate and generate decisions that may sound reasonable, but are in fact\npoor. We argue there is a need to step back and simultaneously design systems\nthat can quantify the certainty of a model's decision, and detect when it may\nbe hallucinating. In this work, we discuss the current use cases of foundation\nmodels for decision-making tasks, provide a general definition for\nhallucinations with examples, discuss existing approaches to hallucination\ndetection and mitigation with a focus on decision problems, present guidelines,\nand explore areas for further research in this exciting field."
                },
                "authors": [
                    {
                        "name": "Neeloy Chakraborty"
                    },
                    {
                        "name": "Melkior Ornik"
                    },
                    {
                        "name": "Katherine Driggs-Campbell"
                    }
                ],
                "author_detail": {
                    "name": "Katherine Driggs-Campbell"
                },
                "author": "Katherine Driggs-Campbell",
                "arxiv_doi": "10.1145/3716846",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3716846",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.16527v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.16527v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to ACM Computing Surveys; 55 pages, 5 tables, 3 figures",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11779v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11779v2",
                "updated": "2025-02-11T17:36:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    36,
                    32,
                    1,
                    42,
                    0
                ],
                "published": "2025-01-20T23:10:13Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    23,
                    10,
                    13,
                    0,
                    20,
                    0
                ],
                "title": "Glinthawk: A Two-Tiered Architecture for Offline LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Glinthawk: A Two-Tiered Architecture for Offline LLM Inference"
                },
                "summary": "We introduce Glinthawk, an architecture for offline Large Language Model\n(LLM) inference. By leveraging a two-tiered structure, Glinthawk optimizes the\nutilization of the high-end accelerators (\"Tier 1\") by offloading the attention\nmechanism to lower-end compute tier (\"Tier 2\"). This separation allows the\nmemory demand of the attention, known as the key-value cache, to scale\nindependently from the model weights, enabling larger batch sizes and more\nefficient accelerator usage. Prototyped with NVIDIA T4 GPUs and standard CPU\nVMs, Glinthawk improves throughput by $5.9\\times$ and reduces cost of\ngeneration by $2.8\\times$, compared to paged attention baselines. For long\nsequence lengths, it achieves $16.3\\times$ throughput improvement at\n$2.4\\times$ less cost. Our evaluation shows that this architecture can tolerate\nmoderate network latency with minimal performance degradation, making it highly\neffective for latency-tolerant, throughput-focused applications such as batch\nprocessing. The prototype is publicly available at\nhttps://github.com/microsoft/glinthawk.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Glinthawk, an architecture for offline Large Language Model\n(LLM) inference. By leveraging a two-tiered structure, Glinthawk optimizes the\nutilization of the high-end accelerators (\"Tier 1\") by offloading the attention\nmechanism to lower-end compute tier (\"Tier 2\"). This separation allows the\nmemory demand of the attention, known as the key-value cache, to scale\nindependently from the model weights, enabling larger batch sizes and more\nefficient accelerator usage. Prototyped with NVIDIA T4 GPUs and standard CPU\nVMs, Glinthawk improves throughput by $5.9\\times$ and reduces cost of\ngeneration by $2.8\\times$, compared to paged attention baselines. For long\nsequence lengths, it achieves $16.3\\times$ throughput improvement at\n$2.4\\times$ less cost. Our evaluation shows that this architecture can tolerate\nmoderate network latency with minimal performance degradation, making it highly\neffective for latency-tolerant, throughput-focused applications such as batch\nprocessing. The prototype is publicly available at\nhttps://github.com/microsoft/glinthawk."
                },
                "authors": [
                    {
                        "name": "Pouya Hamadanian"
                    },
                    {
                        "name": "Sadjad Fouladi"
                    }
                ],
                "author_detail": {
                    "name": "Sadjad Fouladi"
                },
                "author": "Sadjad Fouladi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11779v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07721v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07721v1",
                "updated": "2025-02-11T17:33:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    33,
                    48,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T17:33:48Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    33,
                    48,
                    1,
                    42,
                    0
                ],
                "title": "TMLC-Net: Transferable Meta Label Correction for Noisy Label Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TMLC-Net: Transferable Meta Label Correction for Noisy Label Learning"
                },
                "summary": "The prevalence of noisy labels in real-world datasets poses a significant\nimpediment to the effective deployment of deep learning models. While\nmeta-learning strategies have emerged as a promising approach for addressing\nthis challenge, existing methods often suffer from limited transferability and\ntask-specific designs. This paper introduces TMLC-Net, a novel Transferable\nMeta-Learner for Correcting Noisy Labels, designed to overcome these\nlimitations. TMLC-Net learns a general-purpose label correction strategy that\ncan be readily applied across diverse datasets and model architectures without\nrequiring extensive retraining or fine-tuning. Our approach integrates three\ncore components: (1) Normalized Noise Perception, which captures and normalizes\ntraining dynamics to handle distribution shifts; (2) Time-Series Encoding,\nwhich models the temporal evolution of sample statistics using a recurrent\nneural network; and (3) Subclass Decoding, which predicts a corrected label\ndistribution based on the learned representations. We conduct extensive\nexperiments on benchmark datasets with various noise types and levels,\ndemonstrating that TMLC-Net consistently outperforms state-of-the-art methods\nin terms of both accuracy and robustness to label noise. Furthermore, we\nanalyze the transferability of TMLC-Net, showcasing its adaptability to new\ndatasets and noise conditions, and establishing its potential as a broadly\napplicable solution for robust deep learning in noisy environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The prevalence of noisy labels in real-world datasets poses a significant\nimpediment to the effective deployment of deep learning models. While\nmeta-learning strategies have emerged as a promising approach for addressing\nthis challenge, existing methods often suffer from limited transferability and\ntask-specific designs. This paper introduces TMLC-Net, a novel Transferable\nMeta-Learner for Correcting Noisy Labels, designed to overcome these\nlimitations. TMLC-Net learns a general-purpose label correction strategy that\ncan be readily applied across diverse datasets and model architectures without\nrequiring extensive retraining or fine-tuning. Our approach integrates three\ncore components: (1) Normalized Noise Perception, which captures and normalizes\ntraining dynamics to handle distribution shifts; (2) Time-Series Encoding,\nwhich models the temporal evolution of sample statistics using a recurrent\nneural network; and (3) Subclass Decoding, which predicts a corrected label\ndistribution based on the learned representations. We conduct extensive\nexperiments on benchmark datasets with various noise types and levels,\ndemonstrating that TMLC-Net consistently outperforms state-of-the-art methods\nin terms of both accuracy and robustness to label noise. Furthermore, we\nanalyze the transferability of TMLC-Net, showcasing its adaptability to new\ndatasets and noise conditions, and establishing its potential as a broadly\napplicable solution for robust deep learning in noisy environments."
                },
                "authors": [
                    {
                        "name": "Mengyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Mengyang Li"
                },
                "author": "Mengyang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07721v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07721v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01582v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01582v2",
                "updated": "2025-02-11T17:20:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    20,
                    0,
                    1,
                    42,
                    0
                ],
                "published": "2024-11-03T14:16:07Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    14,
                    16,
                    7,
                    6,
                    308,
                    0
                ],
                "title": "Donald Trumps in the Virtual Polls: Simulating and Predicting Public\n  Opinions in Surveys Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Donald Trumps in the Virtual Polls: Simulating and Predicting Public\n  Opinions in Surveys Using Large Language Models"
                },
                "summary": "In recent years, large language models (LLMs) have attracted attention due to\ntheir ability to generate human-like text. As surveys and opinion polls remain\nkey tools for gauging public attitudes, there is increasing interest in\nassessing whether LLMs can accurately replicate human responses. This study\nexamines the potential of LLMs, specifically ChatGPT-4o, to replicate human\nresponses in large-scale surveys and to predict election outcomes based on\ndemographic data. Employing data from the World Values Survey (WVS) and the\nAmerican National Election Studies (ANES), we assess the LLM's performance in\ntwo key tasks: simulating human responses and forecasting U.S. election\nresults. In simulations, the LLM was tasked with generating synthetic responses\nfor various socio-cultural and trust-related questions, demonstrating notable\nalignment with human response patterns across U.S.-China samples, though with\nsome limitations on value-sensitive topics. In prediction tasks, the LLM was\nused to simulate voting behavior in past U.S. elections and predict the 2024\nelection outcome. Our findings show that the LLM replicates cultural\ndifferences effectively, exhibits in-sample predictive validity, and provides\nplausible out-of-sample forecasts, suggesting potential as a cost-effective\nsupplement for survey-based research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, large language models (LLMs) have attracted attention due to\ntheir ability to generate human-like text. As surveys and opinion polls remain\nkey tools for gauging public attitudes, there is increasing interest in\nassessing whether LLMs can accurately replicate human responses. This study\nexamines the potential of LLMs, specifically ChatGPT-4o, to replicate human\nresponses in large-scale surveys and to predict election outcomes based on\ndemographic data. Employing data from the World Values Survey (WVS) and the\nAmerican National Election Studies (ANES), we assess the LLM's performance in\ntwo key tasks: simulating human responses and forecasting U.S. election\nresults. In simulations, the LLM was tasked with generating synthetic responses\nfor various socio-cultural and trust-related questions, demonstrating notable\nalignment with human response patterns across U.S.-China samples, though with\nsome limitations on value-sensitive topics. In prediction tasks, the LLM was\nused to simulate voting behavior in past U.S. elections and predict the 2024\nelection outcome. Our findings show that the LLM replicates cultural\ndifferences effectively, exhibits in-sample predictive validity, and provides\nplausible out-of-sample forecasts, suggesting potential as a cost-effective\nsupplement for survey-based research."
                },
                "authors": [
                    {
                        "name": "Shapeng Jiang"
                    },
                    {
                        "name": "Lijia Wei"
                    },
                    {
                        "name": "Chen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chen Zhang"
                },
                "author": "Chen Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01582v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01582v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07709v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07709v1",
                "updated": "2025-02-11T17:08:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    8,
                    0,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T17:08:00Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    8,
                    0,
                    1,
                    42,
                    0
                ],
                "title": "MAGELLAN: Metacognitive predictions of learning progress guide autotelic\n  LLM agents in large goal spaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAGELLAN: Metacognitive predictions of learning progress guide autotelic\n  LLM agents in large goal spaces"
                },
                "summary": "Open-ended learning agents must efficiently prioritize goals in vast\npossibility spaces, focusing on those that maximize learning progress (LP).\nWhen such autotelic exploration is achieved by LLM agents trained with online\nRL in high-dimensional and evolving goal spaces, a key challenge for LP\nprediction is modeling one's own competence, a form of metacognitive\nmonitoring. Traditional approaches either require extensive sampling or rely on\nbrittle expert-defined goal groupings. We introduce MAGELLAN, a metacognitive\nframework that lets LLM agents learn to predict their competence and LP online.\nBy capturing semantic relationships between goals, MAGELLAN enables\nsample-efficient LP estimation and dynamic adaptation to evolving goal spaces\nthrough generalization. In an interactive learning environment, we show that\nMAGELLAN improves LP prediction efficiency and goal prioritization, being the\nonly method allowing the agent to fully master a large and evolving goal space.\nThese results demonstrate how augmenting LLM agents with a metacognitive\nability for LP predictions can effectively scale curriculum learning to\nopen-ended goal spaces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-ended learning agents must efficiently prioritize goals in vast\npossibility spaces, focusing on those that maximize learning progress (LP).\nWhen such autotelic exploration is achieved by LLM agents trained with online\nRL in high-dimensional and evolving goal spaces, a key challenge for LP\nprediction is modeling one's own competence, a form of metacognitive\nmonitoring. Traditional approaches either require extensive sampling or rely on\nbrittle expert-defined goal groupings. We introduce MAGELLAN, a metacognitive\nframework that lets LLM agents learn to predict their competence and LP online.\nBy capturing semantic relationships between goals, MAGELLAN enables\nsample-efficient LP estimation and dynamic adaptation to evolving goal spaces\nthrough generalization. In an interactive learning environment, we show that\nMAGELLAN improves LP prediction efficiency and goal prioritization, being the\nonly method allowing the agent to fully master a large and evolving goal space.\nThese results demonstrate how augmenting LLM agents with a metacognitive\nability for LP predictions can effectively scale curriculum learning to\nopen-ended goal spaces."
                },
                "authors": [
                    {
                        "name": "Loris Gaven"
                    },
                    {
                        "name": "Thomas Carta"
                    },
                    {
                        "name": "Clment Romac"
                    },
                    {
                        "name": "Cdric Colas"
                    },
                    {
                        "name": "Sylvain Lamprier"
                    },
                    {
                        "name": "Olivier Sigaud"
                    },
                    {
                        "name": "Pierre-Yves Oudeyer"
                    }
                ],
                "author_detail": {
                    "name": "Pierre-Yves Oudeyer"
                },
                "author": "Pierre-Yves Oudeyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07709v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07709v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07698v1",
                "updated": "2025-02-11T16:51:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    16,
                    51,
                    11,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T16:51:11Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    16,
                    51,
                    11,
                    1,
                    42,
                    0
                ],
                "title": "A Framework for LLM-powered Design Assistants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for LLM-powered Design Assistants"
                },
                "summary": "Design assistants are frameworks, tools or applications intended to\nfacilitate both the creative and technical facets of design processes. Large\nlanguage models (LLMs) are AI systems engineered to analyze and produce text\nresembling human language, leveraging extensive datasets. This study introduces\na framework wherein LLMs are employed as Design Assistants, focusing on three\nkey modalities within the Design Process: Idea Exploration, Dialogue with\nDesigners, and Design Evaluation. Importantly, our framework is not confined to\na singular design process but is adaptable across various processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design assistants are frameworks, tools or applications intended to\nfacilitate both the creative and technical facets of design processes. Large\nlanguage models (LLMs) are AI systems engineered to analyze and produce text\nresembling human language, leveraging extensive datasets. This study introduces\na framework wherein LLMs are employed as Design Assistants, focusing on three\nkey modalities within the Design Process: Idea Exploration, Dialogue with\nDesigners, and Design Evaluation. Importantly, our framework is not confined to\na singular design process but is adaptable across various processes."
                },
                "authors": [
                    {
                        "name": "Swaroop Panda"
                    }
                ],
                "author_detail": {
                    "name": "Swaroop Panda"
                },
                "author": "Swaroop Panda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12009v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12009v3",
                "updated": "2025-02-11T16:49:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    16,
                    49,
                    17,
                    1,
                    42,
                    0
                ],
                "published": "2024-06-17T18:25:02Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    18,
                    25,
                    2,
                    0,
                    169,
                    0
                ],
                "title": "FinTruthQA: A Benchmark Dataset for Evaluating the Quality of Financial\n  Information Disclosure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FinTruthQA: A Benchmark Dataset for Evaluating the Quality of Financial\n  Information Disclosure"
                },
                "summary": "Accurate and transparent financial information disclosure is essential in\naccounting and finance, fostering trust and enabling informed investment\ndecisions that drive economic development. Among many information disclosure\nplatforms, the Chinese stock exchanges' investor interactive platform provides\na novel and interactive way for listed firms to disclose information of\ninterest to investors through an online question-and-answer (Q&A) format.\nHowever, it is common for listed firms to respond to questions with limited or\nno substantive information, and automatically evaluating the quality of\nfinancial information disclosure on large amounts of Q&A pairs is challenging.\nIn this study, our interdisciplinary team of AI and finance professionals\nproposed FinTruthQA, a benchmark designed to evaluate advanced natural language\nprocessing (NLP) techniques for the automatic quality assessment of information\ndisclosure in financial Q&A data. It comprises 6,000 real-world financial Q&A\nentries and each Q&A was manually annotated based on four key evaluation\ncriteria. We benchmarked various NLP techniques on FinTruthQA, including large\nlanguage models(LLMs). Experiments showed that existing NLP models have strong\npredictive ability for question identification and question relevance tasks,\nbut are suboptimal for answer readability and answer relevance tasks. By\nestablishing this benchmark, we provide a robust foundation for the automatic\nevaluation of information disclosure, demonstrating how AI can be leveraged for\nsocial good by promoting transparency, fairness, and investor protection in\nfinancial disclosure practices. FinTruthQA can be used by auditors, regulators,\nand financial analysts for real-time monitoring and data-driven\ndecision-making, as well as by researchers for advanced studies in accounting\nand finance, ultimately fostering greater trust and efficiency in the financial\nmarkets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate and transparent financial information disclosure is essential in\naccounting and finance, fostering trust and enabling informed investment\ndecisions that drive economic development. Among many information disclosure\nplatforms, the Chinese stock exchanges' investor interactive platform provides\na novel and interactive way for listed firms to disclose information of\ninterest to investors through an online question-and-answer (Q&A) format.\nHowever, it is common for listed firms to respond to questions with limited or\nno substantive information, and automatically evaluating the quality of\nfinancial information disclosure on large amounts of Q&A pairs is challenging.\nIn this study, our interdisciplinary team of AI and finance professionals\nproposed FinTruthQA, a benchmark designed to evaluate advanced natural language\nprocessing (NLP) techniques for the automatic quality assessment of information\ndisclosure in financial Q&A data. It comprises 6,000 real-world financial Q&A\nentries and each Q&A was manually annotated based on four key evaluation\ncriteria. We benchmarked various NLP techniques on FinTruthQA, including large\nlanguage models(LLMs). Experiments showed that existing NLP models have strong\npredictive ability for question identification and question relevance tasks,\nbut are suboptimal for answer readability and answer relevance tasks. By\nestablishing this benchmark, we provide a robust foundation for the automatic\nevaluation of information disclosure, demonstrating how AI can be leveraged for\nsocial good by promoting transparency, fairness, and investor protection in\nfinancial disclosure practices. FinTruthQA can be used by auditors, regulators,\nand financial analysts for real-time monitoring and data-driven\ndecision-making, as well as by researchers for advanced studies in accounting\nand finance, ultimately fostering greater trust and efficiency in the financial\nmarkets."
                },
                "authors": [
                    {
                        "name": "Ziyue Xu"
                    },
                    {
                        "name": "Peilin Zhou"
                    },
                    {
                        "name": "Xinyu Shi"
                    },
                    {
                        "name": "Jiageng Wu"
                    },
                    {
                        "name": "Yikang Jiang"
                    },
                    {
                        "name": "Dading Chong"
                    },
                    {
                        "name": "Bin Ke"
                    },
                    {
                        "name": "Jie Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Yang"
                },
                "author": "Jie Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12009v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12009v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13921v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13921v3",
                "updated": "2025-02-11T16:48:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    16,
                    48,
                    15,
                    1,
                    42,
                    0
                ],
                "published": "2025-01-23T18:59:02Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    18,
                    59,
                    2,
                    3,
                    23,
                    0
                ],
                "title": "The Breeze 2 Herd of Models: Traditional Chinese LLMs Based on Llama\n  with Vision-Aware and Function-Calling Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Breeze 2 Herd of Models: Traditional Chinese LLMs Based on Llama\n  with Vision-Aware and Function-Calling Capabilities"
                },
                "summary": "Llama-Breeze2 (hereinafter referred to as Breeze2) is a suite of advanced\nmulti-modal language models, available in 3B and 8B parameter configurations,\nspecifically designed to enhance Traditional Chinese language representation.\nBuilding upon the Llama 3.2 model family, we continue the pre-training of\nBreeze2 on an extensive corpus to enhance the linguistic and cultural heritage\nof Traditional Chinese. In addition to language modeling capabilities, we\nsignificantly augment the models with function calling and vision understanding\ncapabilities. At the time of this publication, as far as we are aware, absent\nreasoning-inducing prompts, Breeze2 are the strongest performing models in\nTraditional Chinese function calling and image understanding in its size class.\nThe effectiveness of Breeze2 is benchmarked across various tasks, including\nTaiwan general knowledge, instruction-following, long context, function\ncalling, and vision understanding. We are publicly releasing all Breeze2 models\nunder the Llama 3.2 Community License. We also showcase the capabilities of the\nmodel running on mobile platform with a mobile application which we also open\nsource.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Llama-Breeze2 (hereinafter referred to as Breeze2) is a suite of advanced\nmulti-modal language models, available in 3B and 8B parameter configurations,\nspecifically designed to enhance Traditional Chinese language representation.\nBuilding upon the Llama 3.2 model family, we continue the pre-training of\nBreeze2 on an extensive corpus to enhance the linguistic and cultural heritage\nof Traditional Chinese. In addition to language modeling capabilities, we\nsignificantly augment the models with function calling and vision understanding\ncapabilities. At the time of this publication, as far as we are aware, absent\nreasoning-inducing prompts, Breeze2 are the strongest performing models in\nTraditional Chinese function calling and image understanding in its size class.\nThe effectiveness of Breeze2 is benchmarked across various tasks, including\nTaiwan general knowledge, instruction-following, long context, function\ncalling, and vision understanding. We are publicly releasing all Breeze2 models\nunder the Llama 3.2 Community License. We also showcase the capabilities of the\nmodel running on mobile platform with a mobile application which we also open\nsource."
                },
                "authors": [
                    {
                        "name": "MediaTek Research"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "Chan-Jan Hsu"
                    },
                    {
                        "name": "Chia-Sheng Liu"
                    },
                    {
                        "name": "Meng-Hsi Chen"
                    },
                    {
                        "name": "Muxi Chen"
                    },
                    {
                        "name": "Po-Chun Hsu"
                    },
                    {
                        "name": "Yi-Chang Chen"
                    },
                    {
                        "name": "Da-Shan Shiu"
                    }
                ],
                "author_detail": {
                    "name": "Da-Shan Shiu"
                },
                "author": "Da-Shan Shiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13921v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13921v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07687v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07687v1",
                "updated": "2025-02-11T16:38:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    16,
                    38,
                    16,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T16:38:16Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    16,
                    38,
                    16,
                    1,
                    42,
                    0
                ],
                "title": "Large Language Models as Proxies for Theories of Human Linguistic\n  Cognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models as Proxies for Theories of Human Linguistic\n  Cognition"
                },
                "summary": "We consider the possible role of current large language models (LLMs) in the\nstudy of human linguistic cognition. We focus on the use of such models as\nproxies for theories of cognition that are relatively linguistically-neutral in\ntheir representations and learning but differ from current LLMs in key ways. We\nillustrate this potential use of LLMs as proxies for theories of cognition in\nthe context of two kinds of questions: (a) whether the target theory accounts\nfor the acquisition of a given pattern from a given corpus; and (b) whether the\ntarget theory makes a given typologically-attested pattern easier to acquire\nthan another, typologically-unattested pattern. For each of the two questions\nwe show, building on recent literature, how current LLMs can potentially be of\nhelp, but we note that at present this help is quite limited.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the possible role of current large language models (LLMs) in the\nstudy of human linguistic cognition. We focus on the use of such models as\nproxies for theories of cognition that are relatively linguistically-neutral in\ntheir representations and learning but differ from current LLMs in key ways. We\nillustrate this potential use of LLMs as proxies for theories of cognition in\nthe context of two kinds of questions: (a) whether the target theory accounts\nfor the acquisition of a given pattern from a given corpus; and (b) whether the\ntarget theory makes a given typologically-attested pattern easier to acquire\nthan another, typologically-unattested pattern. For each of the two questions\nwe show, building on recent literature, how current LLMs can potentially be of\nhelp, but we note that at present this help is quite limited."
                },
                "authors": [
                    {
                        "name": "Imry Ziv"
                    },
                    {
                        "name": "Nur Lan"
                    },
                    {
                        "name": "Emmanuel Chemla"
                    },
                    {
                        "name": "Roni Katzir"
                    }
                ],
                "author_detail": {
                    "name": "Roni Katzir"
                },
                "author": "Roni Katzir",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07687v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07687v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12645v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12645v3",
                "updated": "2025-02-11T16:36:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    16,
                    36,
                    32,
                    1,
                    42,
                    0
                ],
                "published": "2024-06-18T14:13:13Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    14,
                    13,
                    13,
                    1,
                    170,
                    0
                ],
                "title": "Evaluating Evidence Attribution in Generated Fact Checking Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Evidence Attribution in Generated Fact Checking Explanations"
                },
                "summary": "Automated fact-checking systems often struggle with trustworthiness, as their\ngenerated explanations can include hallucinations. In this work, we explore\nevidence attribution for fact-checking explanation generation. We introduce a\nnovel evaluation protocol -- citation masking and recovery -- to assess\nattribution quality in generated explanations. We implement our protocol using\nboth human annotators and automatic annotators, and find that LLM annotation\ncorrelates with human annotation, suggesting that attribution assessment can be\nautomated. Finally, our experiments reveal that: (1) the best-performing LLMs\nstill generate explanations with inaccurate attributions; and (2) human-curated\nevidence is essential for generating better explanations. Code and data are\navailable here: https://github.com/ruixing76/Transparent-FCExp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated fact-checking systems often struggle with trustworthiness, as their\ngenerated explanations can include hallucinations. In this work, we explore\nevidence attribution for fact-checking explanation generation. We introduce a\nnovel evaluation protocol -- citation masking and recovery -- to assess\nattribution quality in generated explanations. We implement our protocol using\nboth human annotators and automatic annotators, and find that LLM annotation\ncorrelates with human annotation, suggesting that attribution assessment can be\nautomated. Finally, our experiments reveal that: (1) the best-performing LLMs\nstill generate explanations with inaccurate attributions; and (2) human-curated\nevidence is essential for generating better explanations. Code and data are\navailable here: https://github.com/ruixing76/Transparent-FCExp."
                },
                "authors": [
                    {
                        "name": "Rui Xing"
                    },
                    {
                        "name": "Timothy Baldwin"
                    },
                    {
                        "name": "Jey Han Lau"
                    }
                ],
                "author_detail": {
                    "name": "Jey Han Lau"
                },
                "author": "Jey Han Lau",
                "arxiv_comment": "Accepted to NAACL 2025 Main",
                "arxiv_journal_ref": "NAACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12645v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12645v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13865v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13865v3",
                "updated": "2025-02-11T16:27:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    16,
                    27,
                    42,
                    1,
                    42,
                    0
                ],
                "published": "2024-06-19T22:01:47Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    22,
                    1,
                    47,
                    2,
                    171,
                    0
                ],
                "title": "SurgicAI: A Hierarchical Platform for Fine-Grained Surgical Policy\n  Learning and Benchmarking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SurgicAI: A Hierarchical Platform for Fine-Grained Surgical Policy\n  Learning and Benchmarking"
                },
                "summary": "Despite advancements in robotic-assisted surgery, automating complex tasks\nlike suturing remain challenging due to the need for adaptability and\nprecision. Learning-based approaches, particularly reinforcement learning (RL)\nand imitation learning (IL), require realistic simulation environments for\nefficient data collection. However, current platforms often include only\nrelatively simple, non-dexterous manipulations and lack the flexibility\nrequired for effective learning and generalization.\n  We introduce SurgicAI, a novel platform for development and benchmarking\naddressing these challenges by providing the flexibility to accommodate both\nmodular subtasks and more importantly task decomposition in RL-based surgical\nrobotics. Compatible with the da Vinci Surgical System, SurgicAI offers a\nstandardized pipeline for collecting and utilizing expert demonstrations. It\nsupports deployment of multiple RL and IL approaches, and the training of both\nsingular and compositional subtasks in suturing scenarios, featuring high\ndexterity and modularization. Meanwhile, SurgicAI sets clear metrics and\nbenchmarks for the assessment of learned policies. We implemented and evaluated\nmultiple RL and IL algorithms on SurgicAI. Our detailed benchmark analysis\nunderscores SurgicAI's potential to advance policy learning in surgical\nrobotics. Details: https://github.com/surgical-robotics-ai/SurgicAI",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite advancements in robotic-assisted surgery, automating complex tasks\nlike suturing remain challenging due to the need for adaptability and\nprecision. Learning-based approaches, particularly reinforcement learning (RL)\nand imitation learning (IL), require realistic simulation environments for\nefficient data collection. However, current platforms often include only\nrelatively simple, non-dexterous manipulations and lack the flexibility\nrequired for effective learning and generalization.\n  We introduce SurgicAI, a novel platform for development and benchmarking\naddressing these challenges by providing the flexibility to accommodate both\nmodular subtasks and more importantly task decomposition in RL-based surgical\nrobotics. Compatible with the da Vinci Surgical System, SurgicAI offers a\nstandardized pipeline for collecting and utilizing expert demonstrations. It\nsupports deployment of multiple RL and IL approaches, and the training of both\nsingular and compositional subtasks in suturing scenarios, featuring high\ndexterity and modularization. Meanwhile, SurgicAI sets clear metrics and\nbenchmarks for the assessment of learned policies. We implemented and evaluated\nmultiple RL and IL algorithms on SurgicAI. Our detailed benchmark analysis\nunderscores SurgicAI's potential to advance policy learning in surgical\nrobotics. Details: https://github.com/surgical-robotics-ai/SurgicAI"
                },
                "authors": [
                    {
                        "name": "Jin Wu"
                    },
                    {
                        "name": "Haoying Zhou"
                    },
                    {
                        "name": "Peter Kazanzides"
                    },
                    {
                        "name": "Adnan Munawar"
                    },
                    {
                        "name": "Anqi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Anqi Liu"
                },
                "author": "Anqi Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13865v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13865v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07677v1",
                "updated": "2025-02-11T16:27:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    16,
                    27,
                    28,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T16:27:28Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    16,
                    27,
                    28,
                    1,
                    42,
                    0
                ],
                "title": "Auto-Drafting Police Reports from Noisy ASR Outputs: A Trust-Centered\n  LLM Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-Drafting Police Reports from Noisy ASR Outputs: A Trust-Centered\n  LLM Approach"
                },
                "summary": "Achieving a delicate balance between fostering trust in law en- forcement and\nprotecting the rights of both officers and civilians continues to emerge as a\npressing research and product challenge in the world today. In the pursuit of\nfairness and transparency, this study presents an innovative AI-driven system\ndesigned to generate police report drafts from complex, noisy, and multi-role\ndialogue data. Our approach intelligently extracts key elements of law\nenforcement interactions and includes them in the draft, producing structured\nnarratives that are not only high in quality but also reinforce accountability\nand procedural clarity. This frame- work holds the potential to transform the\nreporting process, ensur- ing greater oversight, consistency, and fairness in\nfuture policing practices. A demonstration video of our system can be accessed\nat https://drive.google.com/file/d/1kBrsGGR8e3B5xPSblrchRGj-\nY-kpCHNO/view?usp=sharing",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achieving a delicate balance between fostering trust in law en- forcement and\nprotecting the rights of both officers and civilians continues to emerge as a\npressing research and product challenge in the world today. In the pursuit of\nfairness and transparency, this study presents an innovative AI-driven system\ndesigned to generate police report drafts from complex, noisy, and multi-role\ndialogue data. Our approach intelligently extracts key elements of law\nenforcement interactions and includes them in the draft, producing structured\nnarratives that are not only high in quality but also reinforce accountability\nand procedural clarity. This frame- work holds the potential to transform the\nreporting process, ensur- ing greater oversight, consistency, and fairness in\nfuture policing practices. A demonstration video of our system can be accessed\nat https://drive.google.com/file/d/1kBrsGGR8e3B5xPSblrchRGj-\nY-kpCHNO/view?usp=sharing"
                },
                "authors": [
                    {
                        "name": "Param Kulkarni"
                    },
                    {
                        "name": "Yingchi Liu"
                    },
                    {
                        "name": "Hao-Ming Fu"
                    },
                    {
                        "name": "Shaohua Yang"
                    },
                    {
                        "name": "Isuru Gunasekara"
                    },
                    {
                        "name": "Matt Peloquin"
                    },
                    {
                        "name": "Noah Spitzer-Williams"
                    },
                    {
                        "name": "Xiaotian Zhou"
                    },
                    {
                        "name": "Xiaozhong Liu"
                    },
                    {
                        "name": "Zhengping Ji"
                    },
                    {
                        "name": "Yasser Ibrahim"
                    }
                ],
                "author_detail": {
                    "name": "Yasser Ibrahim"
                },
                "author": "Yasser Ibrahim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05907v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05907v2",
                "updated": "2025-02-11T16:22:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    16,
                    22,
                    45,
                    1,
                    42,
                    0
                ],
                "published": "2024-09-06T15:47:40Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    15,
                    47,
                    40,
                    4,
                    250,
                    0
                ],
                "title": "Programming Refusal with Conditional Activation Steering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Programming Refusal with Conditional Activation Steering"
                },
                "summary": "LLMs have shown remarkable capabilities, but precisely controlling their\nresponse behavior remains challenging. Existing activation steering methods\nalter LLM behavior indiscriminately, limiting their practical applicability in\nsettings where selective responses are essential, such as content moderation or\ndomain-specific assistants. In this paper, we propose Conditional Activation\nSteering (CAST), which analyzes LLM activation patterns during inference to\nselectively apply or withhold activation steering based on the input context.\nOur method is based on the observation that different categories of prompts\nactivate distinct patterns in the model's hidden states. Using CAST, one can\nsystematically control LLM behavior with rules like \"if input is about hate\nspeech or adult content, then refuse\" or \"if input is not about legal advice,\nthen refuse.\" This allows for selective modification of responses to specific\ncontent while maintaining normal responses to other content, all without\nrequiring weight optimization. We release an open-source implementation of our\nframework at <github.com/IBM/activation-steering>.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have shown remarkable capabilities, but precisely controlling their\nresponse behavior remains challenging. Existing activation steering methods\nalter LLM behavior indiscriminately, limiting their practical applicability in\nsettings where selective responses are essential, such as content moderation or\ndomain-specific assistants. In this paper, we propose Conditional Activation\nSteering (CAST), which analyzes LLM activation patterns during inference to\nselectively apply or withhold activation steering based on the input context.\nOur method is based on the observation that different categories of prompts\nactivate distinct patterns in the model's hidden states. Using CAST, one can\nsystematically control LLM behavior with rules like \"if input is about hate\nspeech or adult content, then refuse\" or \"if input is not about legal advice,\nthen refuse.\" This allows for selective modification of responses to specific\ncontent while maintaining normal responses to other content, all without\nrequiring weight optimization. We release an open-source implementation of our\nframework at <github.com/IBM/activation-steering>."
                },
                "authors": [
                    {
                        "name": "Bruce W. Lee"
                    },
                    {
                        "name": "Inkit Padhi"
                    },
                    {
                        "name": "Karthikeyan Natesan Ramamurthy"
                    },
                    {
                        "name": "Erik Miehling"
                    },
                    {
                        "name": "Pierre Dognin"
                    },
                    {
                        "name": "Manish Nagireddy"
                    },
                    {
                        "name": "Amit Dhurandhar"
                    }
                ],
                "author_detail": {
                    "name": "Amit Dhurandhar"
                },
                "author": "Amit Dhurandhar",
                "arxiv_comment": "ICLR 2025, Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05907v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05907v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08041v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08041v2",
                "updated": "2025-02-11T16:22:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    16,
                    22,
                    24,
                    1,
                    42,
                    0
                ],
                "published": "2024-12-11T02:44:14Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    2,
                    44,
                    14,
                    2,
                    346,
                    0
                ],
                "title": "Quantifying the benefits of code hints for refactoring deprecated Java\n  APIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying the benefits of code hints for refactoring deprecated Java\n  APIs"
                },
                "summary": "When done manually, refactoring legacy code in order to eliminate uses of\ndeprecated APIs is an error-prone and time-consuming process. In this paper, we\ninvestigate to which degree refactorings for deprecated Java APIs can be\nautomated, and quantify the benefit of Javadoc code hints for this task. To\nthis end, we build a symbolic and a neural engine for the automatic refactoring\nof deprecated APIs. The former is based on type-directed and component-based\nprogram synthesis, whereas the latter uses LLMs. We applied our engines to\nrefactor the deprecated methods in the Oracle JDK 15. Our experiments show that\ncode hints are enabling for the automation of this task: even the worst engine\ncorrectly refactors 71% of the tasks with code hints, which drops to at best\n14% on tasks without. Adding more code hints to Javadoc can hence boost the\nrefactoring of code that uses deprecated APIs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When done manually, refactoring legacy code in order to eliminate uses of\ndeprecated APIs is an error-prone and time-consuming process. In this paper, we\ninvestigate to which degree refactorings for deprecated Java APIs can be\nautomated, and quantify the benefit of Javadoc code hints for this task. To\nthis end, we build a symbolic and a neural engine for the automatic refactoring\nof deprecated APIs. The former is based on type-directed and component-based\nprogram synthesis, whereas the latter uses LLMs. We applied our engines to\nrefactor the deprecated methods in the Oracle JDK 15. Our experiments show that\ncode hints are enabling for the automation of this task: even the worst engine\ncorrectly refactors 71% of the tasks with code hints, which drops to at best\n14% on tasks without. Adding more code hints to Javadoc can hence boost the\nrefactoring of code that uses deprecated APIs."
                },
                "authors": [
                    {
                        "name": "Cristina David"
                    },
                    {
                        "name": "Pascal Kesseli"
                    },
                    {
                        "name": "Daniel Kroening"
                    },
                    {
                        "name": "Hanliang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hanliang Zhang"
                },
                "author": "Hanliang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08041v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08041v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05670v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05670v2",
                "updated": "2025-02-11T16:02:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    16,
                    2,
                    57,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-08T19:13:40Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    19,
                    13,
                    40,
                    5,
                    39,
                    0
                ],
                "title": "Language Models Largely Exhibit Human-like Constituent Ordering\n  Preferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models Largely Exhibit Human-like Constituent Ordering\n  Preferences"
                },
                "summary": "Though English sentences are typically inflexible vis-\\`a-vis word order,\nconstituents often show far more variability in ordering. One prominent theory\npresents the notion that constituent ordering is directly correlated with\nconstituent weight: a measure of the constituent's length or complexity. Such\ntheories are interesting in the context of natural language processing (NLP),\nbecause while recent advances in NLP have led to significant gains in the\nperformance of large language models (LLMs), much remains unclear about how\nthese models process language, and how this compares to human language\nprocessing. In particular, the question remains whether LLMs display the same\npatterns with constituent movement, and may provide insights into existing\ntheories on when and how the shift occurs in human language. We compare a\nvariety of LLMs with diverse properties to evaluate broad LLM performance on\nfour types of constituent movement: heavy NP shift, particle movement, dative\nalternation, and multiple PPs. Despite performing unexpectedly around particle\nmovement, LLMs generally align with human preferences around constituent\nordering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Though English sentences are typically inflexible vis-\\`a-vis word order,\nconstituents often show far more variability in ordering. One prominent theory\npresents the notion that constituent ordering is directly correlated with\nconstituent weight: a measure of the constituent's length or complexity. Such\ntheories are interesting in the context of natural language processing (NLP),\nbecause while recent advances in NLP have led to significant gains in the\nperformance of large language models (LLMs), much remains unclear about how\nthese models process language, and how this compares to human language\nprocessing. In particular, the question remains whether LLMs display the same\npatterns with constituent movement, and may provide insights into existing\ntheories on when and how the shift occurs in human language. We compare a\nvariety of LLMs with diverse properties to evaluate broad LLM performance on\nfour types of constituent movement: heavy NP shift, particle movement, dative\nalternation, and multiple PPs. Despite performing unexpectedly around particle\nmovement, LLMs generally align with human preferences around constituent\nordering."
                },
                "authors": [
                    {
                        "name": "Ada Defne Tur"
                    },
                    {
                        "name": "Gaurav Kamath"
                    },
                    {
                        "name": "Siva Reddy"
                    }
                ],
                "author_detail": {
                    "name": "Siva Reddy"
                },
                "author": "Siva Reddy",
                "arxiv_comment": "NAACL 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05670v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05670v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07663v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07663v1",
                "updated": "2025-02-11T15:56:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    56,
                    22,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T15:56:22Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    56,
                    22,
                    1,
                    42,
                    0
                ],
                "title": "Human Decision-making is Susceptible to AI-driven Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human Decision-making is Susceptible to AI-driven Manipulation"
                },
                "summary": "Artificial Intelligence (AI) systems are increasingly intertwined with daily\nlife, assisting users in executing various tasks and providing guidance on\ndecision-making. This integration introduces risks of AI-driven manipulation,\nwhere such systems may exploit users' cognitive biases and emotional\nvulnerabilities to steer them toward harmful outcomes. Through a randomized\ncontrolled trial with 233 participants, we examined human susceptibility to\nsuch manipulation in financial (e.g., purchases) and emotional (e.g., conflict\nresolution) decision-making contexts. Participants interacted with one of three\nAI agents: a neutral agent (NA) optimizing for user benefit without explicit\ninfluence, a manipulative agent (MA) designed to covertly influence beliefs and\nbehaviors, or a strategy-enhanced manipulative agent (SEMA) employing explicit\npsychological tactics to reach its hidden objectives. By analyzing\nparticipants' decision patterns and shifts in their preference ratings\npost-interaction, we found significant susceptibility to AI-driven\nmanipulation. Particularly, across both decision-making domains, participants\ninteracting with the manipulative agents shifted toward harmful options at\nsubstantially higher rates (financial, MA: 62.3%, SEMA: 59.6%; emotional, MA:\n42.3%, SEMA: 41.5%) compared to the NA group (financial, 35.8%; emotional,\n12.8%). Notably, our findings reveal that even subtle manipulative objectives\n(MA) can be as effective as employing explicit psychological strategies (SEMA)\nin swaying human decision-making. By revealing the potential for covert AI\ninfluence, this study highlights a critical vulnerability in human-AI\ninteractions, emphasizing the need for ethical safeguards and regulatory\nframeworks to ensure responsible deployment of AI technologies and protect\nhuman autonomy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence (AI) systems are increasingly intertwined with daily\nlife, assisting users in executing various tasks and providing guidance on\ndecision-making. This integration introduces risks of AI-driven manipulation,\nwhere such systems may exploit users' cognitive biases and emotional\nvulnerabilities to steer them toward harmful outcomes. Through a randomized\ncontrolled trial with 233 participants, we examined human susceptibility to\nsuch manipulation in financial (e.g., purchases) and emotional (e.g., conflict\nresolution) decision-making contexts. Participants interacted with one of three\nAI agents: a neutral agent (NA) optimizing for user benefit without explicit\ninfluence, a manipulative agent (MA) designed to covertly influence beliefs and\nbehaviors, or a strategy-enhanced manipulative agent (SEMA) employing explicit\npsychological tactics to reach its hidden objectives. By analyzing\nparticipants' decision patterns and shifts in their preference ratings\npost-interaction, we found significant susceptibility to AI-driven\nmanipulation. Particularly, across both decision-making domains, participants\ninteracting with the manipulative agents shifted toward harmful options at\nsubstantially higher rates (financial, MA: 62.3%, SEMA: 59.6%; emotional, MA:\n42.3%, SEMA: 41.5%) compared to the NA group (financial, 35.8%; emotional,\n12.8%). Notably, our findings reveal that even subtle manipulative objectives\n(MA) can be as effective as employing explicit psychological strategies (SEMA)\nin swaying human decision-making. By revealing the potential for covert AI\ninfluence, this study highlights a critical vulnerability in human-AI\ninteractions, emphasizing the need for ethical safeguards and regulatory\nframeworks to ensure responsible deployment of AI technologies and protect\nhuman autonomy."
                },
                "authors": [
                    {
                        "name": "Sahand Sabour"
                    },
                    {
                        "name": "June M. Liu"
                    },
                    {
                        "name": "Siyang Liu"
                    },
                    {
                        "name": "Chris Z. Yao"
                    },
                    {
                        "name": "Shiyao Cui"
                    },
                    {
                        "name": "Xuanming Zhang"
                    },
                    {
                        "name": "Wen Zhang"
                    },
                    {
                        "name": "Yaru Cao"
                    },
                    {
                        "name": "Advait Bhat"
                    },
                    {
                        "name": "Jian Guan"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Rada Mihalcea"
                    },
                    {
                        "name": "Tim Althoff"
                    },
                    {
                        "name": "Tatia M. C. Lee"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "arxiv_comment": "Work in progress. Code and data will be made available via\n  https://github.com/Sahandfer/Manipulation-Susceptibility",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07663v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07663v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06556v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06556v2",
                "updated": "2025-02-11T15:48:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    48,
                    42,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-10T15:24:30Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    24,
                    30,
                    0,
                    41,
                    0
                ],
                "title": "ProjectTest: A Project-level LLM Unit Test Generation Benchmark and\n  Impact of Error Fixing Mechanisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProjectTest: A Project-level LLM Unit Test Generation Benchmark and\n  Impact of Error Fixing Mechanisms"
                },
                "summary": "Unit test generation has become a promising and important use case of LLMs.\nHowever, existing evaluation benchmarks for assessing LLM unit test generation\ncapabilities focus on function- or class-level code rather than more practical\nand challenging project-level codebases. To address such limitation, we propose\nProjectTest, a project-level benchmark for unit test generation covering\nPython, Java, and JavaScript. ProjectTest features 20 moderate-sized and\nhigh-quality projects per language. We evaluate nine frontier LLMs on\nProjectTest and the results show that all frontier LLMs tested exhibit moderate\nperformance on ProjectTest on Python and Java, highlighting the difficulty of\nProjectTest. We also conduct a thorough error analysis, which shows that even\nfrontier LLMs, such as Claude-3.5-Sonnet, have significant simple errors,\nincluding compilation and cascade errors. Motivated by this observation, we\nfurther evaluate all frontier LLMs under manual error-fixing and\nself-error-fixing scenarios to assess their potential when equipped with\nerror-fixing mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unit test generation has become a promising and important use case of LLMs.\nHowever, existing evaluation benchmarks for assessing LLM unit test generation\ncapabilities focus on function- or class-level code rather than more practical\nand challenging project-level codebases. To address such limitation, we propose\nProjectTest, a project-level benchmark for unit test generation covering\nPython, Java, and JavaScript. ProjectTest features 20 moderate-sized and\nhigh-quality projects per language. We evaluate nine frontier LLMs on\nProjectTest and the results show that all frontier LLMs tested exhibit moderate\nperformance on ProjectTest on Python and Java, highlighting the difficulty of\nProjectTest. We also conduct a thorough error analysis, which shows that even\nfrontier LLMs, such as Claude-3.5-Sonnet, have significant simple errors,\nincluding compilation and cascade errors. Motivated by this observation, we\nfurther evaluate all frontier LLMs under manual error-fixing and\nself-error-fixing scenarios to assess their potential when equipped with\nerror-fixing mechanisms."
                },
                "authors": [
                    {
                        "name": "Yibo Wang"
                    },
                    {
                        "name": "Congying Xia"
                    },
                    {
                        "name": "Wenting Zhao"
                    },
                    {
                        "name": "Jiangshu Du"
                    },
                    {
                        "name": "Chunyu Miao"
                    },
                    {
                        "name": "Zhongfen Deng"
                    },
                    {
                        "name": "Philip S. Yu"
                    },
                    {
                        "name": "Chen Xing"
                    }
                ],
                "author_detail": {
                    "name": "Chen Xing"
                },
                "author": "Chen Xing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06556v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06556v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05878v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05878v2",
                "updated": "2025-02-11T15:45:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    45,
                    52,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-09T12:26:05Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    12,
                    26,
                    5,
                    6,
                    40,
                    0
                ],
                "title": "Enhancing Financial Time-Series Forecasting with Retrieval-Augmented\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Financial Time-Series Forecasting with Retrieval-Augmented\n  Large Language Models"
                },
                "summary": "Stock movement prediction, a critical task in financial time-series\nforecasting, relies on identifying and retrieving key influencing factors from\nvast and complex datasets. However, traditional text-trained or numeric\nsimilarity-based retrieval methods often struggle to handle the intricacies of\nfinancial data. To address this, we propose the first retrieval-augmented\ngeneration (RAG) framework specifically designed for financial time-series\nforecasting. Our framework incorporates three key innovations: a fine-tuned 1B\nlarge language model (StockLLM) as its backbone, a novel candidate selection\nmethod enhanced by LLM feedback, and a training objective that maximizes the\nsimilarity between queries and historically significant sequences. These\nadvancements enable our retriever, FinSeer, to uncover meaningful patterns\nwhile effectively minimizing noise in complex financial datasets. To support\nrobust evaluation, we also construct new datasets that integrate financial\nindicators and historical stock prices. Experimental results demonstrate that\nour RAG framework outperforms both the baseline StockLLM and random retrieval\nmethods, showcasing its effectiveness. FinSeer, as the retriever, achieves an\n8% higher accuracy on the BIGDATA22 benchmark and retrieves more impactful\nsequences compared to existing retrieval methods. This work highlights the\nimportance of tailored retrieval models in financial forecasting and provides a\nnovel, scalable framework for future research in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stock movement prediction, a critical task in financial time-series\nforecasting, relies on identifying and retrieving key influencing factors from\nvast and complex datasets. However, traditional text-trained or numeric\nsimilarity-based retrieval methods often struggle to handle the intricacies of\nfinancial data. To address this, we propose the first retrieval-augmented\ngeneration (RAG) framework specifically designed for financial time-series\nforecasting. Our framework incorporates three key innovations: a fine-tuned 1B\nlarge language model (StockLLM) as its backbone, a novel candidate selection\nmethod enhanced by LLM feedback, and a training objective that maximizes the\nsimilarity between queries and historically significant sequences. These\nadvancements enable our retriever, FinSeer, to uncover meaningful patterns\nwhile effectively minimizing noise in complex financial datasets. To support\nrobust evaluation, we also construct new datasets that integrate financial\nindicators and historical stock prices. Experimental results demonstrate that\nour RAG framework outperforms both the baseline StockLLM and random retrieval\nmethods, showcasing its effectiveness. FinSeer, as the retriever, achieves an\n8% higher accuracy on the BIGDATA22 benchmark and retrieves more impactful\nsequences compared to existing retrieval methods. This work highlights the\nimportance of tailored retrieval models in financial forecasting and provides a\nnovel, scalable framework for future research in the field."
                },
                "authors": [
                    {
                        "name": "Mengxi Xiao"
                    },
                    {
                        "name": "Zihao Jiang"
                    },
                    {
                        "name": "Lingfei Qian"
                    },
                    {
                        "name": "Zhengyu Chen"
                    },
                    {
                        "name": "Yueru He"
                    },
                    {
                        "name": "Yijing Xu"
                    },
                    {
                        "name": "Yuecheng Jiang"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Ruey-Ling Weng"
                    },
                    {
                        "name": "Min Peng"
                    },
                    {
                        "name": "Jimin Huang"
                    },
                    {
                        "name": "Sophia Ananiadou"
                    },
                    {
                        "name": "Qianqian Xie"
                    }
                ],
                "author_detail": {
                    "name": "Qianqian Xie"
                },
                "author": "Qianqian Xie",
                "arxiv_comment": "11 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05878v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05878v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08770v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08770v2",
                "updated": "2025-02-11T15:39:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    39,
                    8,
                    1,
                    42,
                    0
                ],
                "published": "2024-07-11T17:52:03Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    17,
                    52,
                    3,
                    3,
                    193,
                    0
                ],
                "title": "Model Surgery: Modulating LLM's Behavior Via Simple Parameter Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Surgery: Modulating LLM's Behavior Via Simple Parameter Editing"
                },
                "summary": "Large Language Models (LLMs) have demonstrated great potential as generalist\nassistants, showcasing powerful task understanding and problem-solving\ncapabilities. To deploy LLMs as AI assistants, it is crucial that these models\nexhibit desirable behavioral traits, such as non-toxicity and resilience\nagainst jailbreak attempts. Current approaches for detoxification or preventing\njailbreaking usually involve Supervised Fine-Tuning (SFT) or Reinforcement\nLearning from Human Feedback (RLHF), which requires finetuning billions of\nparameters through gradient descent with substantial computational cost.\nFurthermore, models modified through SFT and RLHF may deviate from the\npretrained models, potentially leading to a degradation in foundational LLM\ncapabilities. In this paper, we observe that surprisingly, directly editing a\nsmall subset of parameters can effectively modulate specific behaviors of LLMs,\nsuch as detoxification and resistance to jailbreaking, with only\ninference-level computational resources. Experiments demonstrate that in the\ndetoxification task, our approach achieves reductions of up to 90.0% in\ntoxicity on the RealToxicityPrompts dataset and 49.2% on ToxiGen, while\nmaintaining the LLM's general capabilities in areas such as common sense,\nquestion answering, and mathematics",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated great potential as generalist\nassistants, showcasing powerful task understanding and problem-solving\ncapabilities. To deploy LLMs as AI assistants, it is crucial that these models\nexhibit desirable behavioral traits, such as non-toxicity and resilience\nagainst jailbreak attempts. Current approaches for detoxification or preventing\njailbreaking usually involve Supervised Fine-Tuning (SFT) or Reinforcement\nLearning from Human Feedback (RLHF), which requires finetuning billions of\nparameters through gradient descent with substantial computational cost.\nFurthermore, models modified through SFT and RLHF may deviate from the\npretrained models, potentially leading to a degradation in foundational LLM\ncapabilities. In this paper, we observe that surprisingly, directly editing a\nsmall subset of parameters can effectively modulate specific behaviors of LLMs,\nsuch as detoxification and resistance to jailbreaking, with only\ninference-level computational resources. Experiments demonstrate that in the\ndetoxification task, our approach achieves reductions of up to 90.0% in\ntoxicity on the RealToxicityPrompts dataset and 49.2% on ToxiGen, while\nmaintaining the LLM's general capabilities in areas such as common sense,\nquestion answering, and mathematics"
                },
                "authors": [
                    {
                        "name": "Huanqian Wang"
                    },
                    {
                        "name": "Yang Yue"
                    },
                    {
                        "name": "Rui Lu"
                    },
                    {
                        "name": "Jingxin Shi"
                    },
                    {
                        "name": "Andrew Zhao"
                    },
                    {
                        "name": "Shenzhi Wang"
                    },
                    {
                        "name": "Shiji Song"
                    },
                    {
                        "name": "Gao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Gao Huang"
                },
                "author": "Gao Huang",
                "arxiv_comment": "23 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08770v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08770v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50 (Primary) 68T07, 62M45 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07644v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07644v1",
                "updated": "2025-02-11T15:34:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    34,
                    0,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T15:34:00Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    34,
                    0,
                    1,
                    42,
                    0
                ],
                "title": "SymGPT: Auditing Smart Contracts via Combining Symbolic Execution with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SymGPT: Auditing Smart Contracts via Combining Symbolic Execution with\n  Large Language Models"
                },
                "summary": "To govern smart contracts running on Ethereum, multiple Ethereum Request for\nComment (ERC) standards have been developed, each having a set of rules to\nguide the behaviors of smart contracts. Violating the ERC rules could cause\nserious security issues and financial loss, signifying the importance of\nverifying smart contracts follow ERCs. Today's practices of such verification\nare to manually audit each single contract, use expert-developed\nprogram-analysis tools, or use large language models (LLMs), all of which are\nfar from effective in identifying ERC rule violations. This paper introduces\nSymGPT, a tool that combines the natural language understanding of large\nlanguage models (LLMs) with the formal guarantees of symbolic execution to\nautomatically verify smart contracts' compliance with ERC rules. To develop\nSymGPT, we conduct an empirical study of 132 ERC rules from three widely used\nERC standards, examining their content, security implications, and natural\nlanguage descriptions. Based on this study, we design SymGPT by first\ninstructing an LLM to translate ERC rules into a defined EBNF grammar. We then\nsynthesize constraints from the formalized rules to represent scenarios where\nviolations may occur and use symbolic execution to detect them. Our evaluation\nshows that SymGPT identifies 5,783 ERC rule violations in 4,000 real-world\ncontracts, including 1,375 violations with clear attack paths for stealing\nfinancial assets, demonstrating its effectiveness. Furthermore, SymGPT\noutperforms six automated techniques and a security-expert auditing service,\nunderscoring its superiority over current smart contract analysis methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To govern smart contracts running on Ethereum, multiple Ethereum Request for\nComment (ERC) standards have been developed, each having a set of rules to\nguide the behaviors of smart contracts. Violating the ERC rules could cause\nserious security issues and financial loss, signifying the importance of\nverifying smart contracts follow ERCs. Today's practices of such verification\nare to manually audit each single contract, use expert-developed\nprogram-analysis tools, or use large language models (LLMs), all of which are\nfar from effective in identifying ERC rule violations. This paper introduces\nSymGPT, a tool that combines the natural language understanding of large\nlanguage models (LLMs) with the formal guarantees of symbolic execution to\nautomatically verify smart contracts' compliance with ERC rules. To develop\nSymGPT, we conduct an empirical study of 132 ERC rules from three widely used\nERC standards, examining their content, security implications, and natural\nlanguage descriptions. Based on this study, we design SymGPT by first\ninstructing an LLM to translate ERC rules into a defined EBNF grammar. We then\nsynthesize constraints from the formalized rules to represent scenarios where\nviolations may occur and use symbolic execution to detect them. Our evaluation\nshows that SymGPT identifies 5,783 ERC rule violations in 4,000 real-world\ncontracts, including 1,375 violations with clear attack paths for stealing\nfinancial assets, demonstrating its effectiveness. Furthermore, SymGPT\noutperforms six automated techniques and a security-expert auditing service,\nunderscoring its superiority over current smart contract analysis methods."
                },
                "authors": [
                    {
                        "name": "Shihao Xia"
                    },
                    {
                        "name": "Mengting He"
                    },
                    {
                        "name": "Shuai Shao"
                    },
                    {
                        "name": "Tingting Yu"
                    },
                    {
                        "name": "Yiying Zhang"
                    },
                    {
                        "name": "Linhai Song"
                    }
                ],
                "author_detail": {
                    "name": "Linhai Song"
                },
                "author": "Linhai Song",
                "arxiv_comment": "16 pages. arXiv admin note: text overlap with arXiv:2404.04306",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07644v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07644v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07642v1",
                "updated": "2025-02-11T15:33:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    33,
                    17,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T15:33:17Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    33,
                    17,
                    1,
                    42,
                    0
                ],
                "title": "FoQA: A Faroese Question-Answering Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FoQA: A Faroese Question-Answering Dataset"
                },
                "summary": "We present FoQA, a Faroese extractive question-answering (QA) dataset with\n2,000 samples, created using a semi-automated approach combining Large Language\nModels (LLMs) and human validation. The dataset was generated from Faroese\nWikipedia articles using GPT-4-turbo for initial QA generation, followed by\nquestion rephrasing to increase complexity and native speaker validation to\nensure quality. We provide baseline performance metrics for FoQA across\nmultiple models, including LLMs and BERT, demonstrating its effectiveness in\nevaluating Faroese QA performance. The dataset is released in three versions: a\nvalidated set of 2,000 samples, a complete set of all 10,001 generated samples,\nand a set of 2,395 rejected samples for error analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present FoQA, a Faroese extractive question-answering (QA) dataset with\n2,000 samples, created using a semi-automated approach combining Large Language\nModels (LLMs) and human validation. The dataset was generated from Faroese\nWikipedia articles using GPT-4-turbo for initial QA generation, followed by\nquestion rephrasing to increase complexity and native speaker validation to\nensure quality. We provide baseline performance metrics for FoQA across\nmultiple models, including LLMs and BERT, demonstrating its effectiveness in\nevaluating Faroese QA performance. The dataset is released in three versions: a\nvalidated set of 2,000 samples, a complete set of all 10,001 generated samples,\nand a set of 2,395 rejected samples for error analysis."
                },
                "authors": [
                    {
                        "name": "Annika Simonsen"
                    },
                    {
                        "name": "Dan Saattrup Nielsen"
                    },
                    {
                        "name": "Hafsteinn Einarsson"
                    }
                ],
                "author_detail": {
                    "name": "Hafsteinn Einarsson"
                },
                "author": "Hafsteinn Einarsson",
                "arxiv_comment": "Camera-ready version for RESOURCEFUL workshop, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07640v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07640v1",
                "updated": "2025-02-11T15:27:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    27,
                    35,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T15:27:35Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    27,
                    35,
                    1,
                    42,
                    0
                ],
                "title": "Goedel-Prover: A Frontier Model for Open-Source Automated Theorem\n  Proving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Goedel-Prover: A Frontier Model for Open-Source Automated Theorem\n  Proving"
                },
                "summary": "We introduce Goedel-Prover, an open-source large language model (LLM) that\nachieves the state-of-the-art (SOTA) performance in automated formal proof\ngeneration for mathematical problems. The key challenge in this field is the\nscarcity of formalized math statements and proofs, which we tackle in the\nfollowing ways. We train statement formalizers to translate the natural\nlanguage math problems from Numina into formal language (Lean 4), creating a\ndataset of 1.64 million formal statements. LLMs are used to check that the\nformal statements accurately preserve the content of the original natural\nlanguage problems. We then iteratively build a large dataset of formal proofs\nby training a series of provers. Each prover succeeds in proving many\nstatements that the previous ones could not, and these new proofs are added to\nthe training set for the next prover. The final prover outperforms all existing\nopen-source models in whole-proof generation. On the miniF2F benchmark, it\nachieves a 57.6% success rate (Pass@32), exceeding the previous best\nopen-source model by 7.6%. On PutnamBench, Goedel-Prover successfully solves 7\nproblems (Pass@512), ranking first on the leaderboard. Furthermore, it\ngenerates 29.7K formal proofs for Lean Workbook problems, nearly doubling the\n15.7K produced by earlier works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Goedel-Prover, an open-source large language model (LLM) that\nachieves the state-of-the-art (SOTA) performance in automated formal proof\ngeneration for mathematical problems. The key challenge in this field is the\nscarcity of formalized math statements and proofs, which we tackle in the\nfollowing ways. We train statement formalizers to translate the natural\nlanguage math problems from Numina into formal language (Lean 4), creating a\ndataset of 1.64 million formal statements. LLMs are used to check that the\nformal statements accurately preserve the content of the original natural\nlanguage problems. We then iteratively build a large dataset of formal proofs\nby training a series of provers. Each prover succeeds in proving many\nstatements that the previous ones could not, and these new proofs are added to\nthe training set for the next prover. The final prover outperforms all existing\nopen-source models in whole-proof generation. On the miniF2F benchmark, it\nachieves a 57.6% success rate (Pass@32), exceeding the previous best\nopen-source model by 7.6%. On PutnamBench, Goedel-Prover successfully solves 7\nproblems (Pass@512), ranking first on the leaderboard. Furthermore, it\ngenerates 29.7K formal proofs for Lean Workbook problems, nearly doubling the\n15.7K produced by earlier works."
                },
                "authors": [
                    {
                        "name": "Yong Lin"
                    },
                    {
                        "name": "Shange Tang"
                    },
                    {
                        "name": "Bohan Lyu"
                    },
                    {
                        "name": "Jiayun Wu"
                    },
                    {
                        "name": "Hongzhou Lin"
                    },
                    {
                        "name": "Kaiyu Yang"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Mengzhou Xia"
                    },
                    {
                        "name": "Danqi Chen"
                    },
                    {
                        "name": "Sanjeev Arora"
                    },
                    {
                        "name": "Chi Jin"
                    }
                ],
                "author_detail": {
                    "name": "Chi Jin"
                },
                "author": "Chi Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07640v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07640v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.08894v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.08894v2",
                "updated": "2025-02-11T15:18:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    18,
                    31,
                    1,
                    42,
                    0
                ],
                "published": "2024-05-14T18:12:09Z",
                "published_parsed": [
                    2024,
                    5,
                    14,
                    18,
                    12,
                    9,
                    1,
                    135,
                    0
                ],
                "title": "Global weight optimization of frame structures under free-vibration\n  eigenvalue constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Global weight optimization of frame structures under free-vibration\n  eigenvalue constraints"
                },
                "summary": "Topology optimization of frame structures under free-vibration eigenvalue\nconstraints constitutes a challenging nonconvex polynomial optimization problem\nwith disconnected feasible sets. In this article, we first formulate it as a\npolynomial semidefinite programming problem (SDP) of minimizing a linear\nfunction over a basic semi-algebraic feasible set. We then propose to solve\nthis problem by Lasserre hierarchy of linear semidefinite relaxations providing\na sequence of increasing lower bounds. To obtain also a sequence of upper\nbounds and thus conditions on global $\\varepsilon$-optimality, we propose a\nnovel technique. Namely, we provide a bilevel reformulation that exhibits a\nspecial structure: The lower level is quasiconvex univariate and its solution\nsatisfies the constraints of the upper-level problem. After deriving the\nconditions for the solvability of the lower-level problem, we thus provide a\nway to construct feasible points to the original SDP. Using such a feasible\npoint, we modify the original nonlinear SDP to satisfy the conditions for the\ndeployment of the Lasserre hierarchy. Solving arbitrary degree relaxation of\nthe hierarchy, we prove that scaled first-order moments associated with the\nproblem variables satisfy feasibility conditions for the lower-level problem\nand thus provide guaranteed upper and lower bounds on the objective function.\nUsing these bounds, we develop a simple sufficient condition for global\n$\\varepsilon$-optimality and prove that the optimality gap $\\varepsilon$\nconverges to zero if the set of global minimizers is convex. Finally, we\nillustrate these results with three representative problems for which the\nhierarchy converges in at most four relaxation degrees.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topology optimization of frame structures under free-vibration eigenvalue\nconstraints constitutes a challenging nonconvex polynomial optimization problem\nwith disconnected feasible sets. In this article, we first formulate it as a\npolynomial semidefinite programming problem (SDP) of minimizing a linear\nfunction over a basic semi-algebraic feasible set. We then propose to solve\nthis problem by Lasserre hierarchy of linear semidefinite relaxations providing\na sequence of increasing lower bounds. To obtain also a sequence of upper\nbounds and thus conditions on global $\\varepsilon$-optimality, we propose a\nnovel technique. Namely, we provide a bilevel reformulation that exhibits a\nspecial structure: The lower level is quasiconvex univariate and its solution\nsatisfies the constraints of the upper-level problem. After deriving the\nconditions for the solvability of the lower-level problem, we thus provide a\nway to construct feasible points to the original SDP. Using such a feasible\npoint, we modify the original nonlinear SDP to satisfy the conditions for the\ndeployment of the Lasserre hierarchy. Solving arbitrary degree relaxation of\nthe hierarchy, we prove that scaled first-order moments associated with the\nproblem variables satisfy feasibility conditions for the lower-level problem\nand thus provide guaranteed upper and lower bounds on the objective function.\nUsing these bounds, we develop a simple sufficient condition for global\n$\\varepsilon$-optimality and prove that the optimality gap $\\varepsilon$\nconverges to zero if the set of global minimizers is convex. Finally, we\nillustrate these results with three representative problems for which the\nhierarchy converges in at most four relaxation degrees."
                },
                "authors": [
                    {
                        "name": "Marek Tyburec"
                    },
                    {
                        "name": "Michal Kovara"
                    },
                    {
                        "name": "Marouan Handa"
                    },
                    {
                        "name": "Jan Zeman"
                    }
                ],
                "author_detail": {
                    "name": "Jan Zeman"
                },
                "author": "Jan Zeman",
                "arxiv_comment": "27 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.08894v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.08894v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07629v1",
                "updated": "2025-02-11T15:17:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    17,
                    0,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T15:17:00Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    17,
                    0,
                    1,
                    42,
                    0
                ],
                "title": "Exploring Mobile Touch Interaction with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Mobile Touch Interaction with Large Language Models"
                },
                "summary": "Interacting with Large Language Models (LLMs) for text editing on mobile\ndevices currently requires users to break out of their writing environment and\nswitch to a conversational AI interface. In this paper, we propose to control\nthe LLM via touch gestures performed directly on the text. We first chart a\ndesign space that covers fundamental touch input and text transformations. In\nthis space, we then concretely explore two control mappings: spread-to-generate\nand pinch-to-shorten, with visual feedback loops. We evaluate this concept in a\nuser study (N=14) that compares three feedback designs: no visualisation, text\nlength indicator, and length + word indicator. The results demonstrate that\ntouch-based control of LLMs is both feasible and user-friendly, with the length\n+ word indicator proving most effective for managing text generation. This work\nlays the foundation for further research into gesture-based interaction with\nLLMs on touch devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interacting with Large Language Models (LLMs) for text editing on mobile\ndevices currently requires users to break out of their writing environment and\nswitch to a conversational AI interface. In this paper, we propose to control\nthe LLM via touch gestures performed directly on the text. We first chart a\ndesign space that covers fundamental touch input and text transformations. In\nthis space, we then concretely explore two control mappings: spread-to-generate\nand pinch-to-shorten, with visual feedback loops. We evaluate this concept in a\nuser study (N=14) that compares three feedback designs: no visualisation, text\nlength indicator, and length + word indicator. The results demonstrate that\ntouch-based control of LLMs is both feasible and user-friendly, with the length\n+ word indicator proving most effective for managing text generation. This work\nlays the foundation for further research into gesture-based interaction with\nLLMs on touch devices."
                },
                "authors": [
                    {
                        "name": "Tim Zindulka"
                    },
                    {
                        "name": "Jannek Sekowski"
                    },
                    {
                        "name": "Florian Lehmann"
                    },
                    {
                        "name": "Daniel Buschek"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Buschek"
                },
                "author": "Daniel Buschek",
                "arxiv_doi": "10.1145/3706598.3713554",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706598.3713554",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.07629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "21 pages, 16 figures, 3 tables, ACM CHI 2025",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07608v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07608v1",
                "updated": "2025-02-11T14:58:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    58,
                    54,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T14:58:54Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    58,
                    54,
                    1,
                    42,
                    0
                ],
                "title": "Beyond Prompting: Time2Lang -- Bridging Time-Series Foundation Models\n  and Large Language Models for Health Sensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Prompting: Time2Lang -- Bridging Time-Series Foundation Models\n  and Large Language Models for Health Sensing"
                },
                "summary": "Large language models (LLMs) show promise for health applications when\ncombined with behavioral sensing data. Traditional approaches convert sensor\ndata into text prompts, but this process is prone to errors, computationally\nexpensive, and requires domain expertise. These challenges are particularly\nacute when processing extended time series data. While time series foundation\nmodels (TFMs) have recently emerged as powerful tools for learning\nrepresentations from temporal data, bridging TFMs and LLMs remains challenging.\nHere, we present Time2Lang, a framework that directly maps TFM outputs to LLM\nrepresentations without intermediate text conversion. Our approach first trains\non synthetic data using periodicity prediction as a pretext task, followed by\nevaluation on mental health classification tasks. We validate Time2Lang on two\nlongitudinal wearable and mobile sensing datasets: daily depression prediction\nusing step count data (17,251 days from 256 participants) and flourishing\nclassification based on conversation duration (46 participants over 10 weeks).\nTime2Lang maintains near constant inference times regardless of input length,\nunlike traditional prompting methods. The generated embeddings preserve\nessential time-series characteristics such as auto-correlation. Our results\ndemonstrate that TFMs and LLMs can be effectively integrated while minimizing\ninformation loss and enabling performance transfer across these distinct\nmodeling paradigms. To our knowledge, we are the first to integrate a TFM and\nan LLM for health, thus establishing a foundation for future research combining\ngeneral-purpose large models for complex healthcare tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) show promise for health applications when\ncombined with behavioral sensing data. Traditional approaches convert sensor\ndata into text prompts, but this process is prone to errors, computationally\nexpensive, and requires domain expertise. These challenges are particularly\nacute when processing extended time series data. While time series foundation\nmodels (TFMs) have recently emerged as powerful tools for learning\nrepresentations from temporal data, bridging TFMs and LLMs remains challenging.\nHere, we present Time2Lang, a framework that directly maps TFM outputs to LLM\nrepresentations without intermediate text conversion. Our approach first trains\non synthetic data using periodicity prediction as a pretext task, followed by\nevaluation on mental health classification tasks. We validate Time2Lang on two\nlongitudinal wearable and mobile sensing datasets: daily depression prediction\nusing step count data (17,251 days from 256 participants) and flourishing\nclassification based on conversation duration (46 participants over 10 weeks).\nTime2Lang maintains near constant inference times regardless of input length,\nunlike traditional prompting methods. The generated embeddings preserve\nessential time-series characteristics such as auto-correlation. Our results\ndemonstrate that TFMs and LLMs can be effectively integrated while minimizing\ninformation loss and enabling performance transfer across these distinct\nmodeling paradigms. To our knowledge, we are the first to integrate a TFM and\nan LLM for health, thus establishing a foundation for future research combining\ngeneral-purpose large models for complex healthcare tasks."
                },
                "authors": [
                    {
                        "name": "Arvind Pillai"
                    },
                    {
                        "name": "Dimitris Spathis"
                    },
                    {
                        "name": "Subigya Nepal"
                    },
                    {
                        "name": "Amanda C Collins"
                    },
                    {
                        "name": "Daniel M Mackin"
                    },
                    {
                        "name": "Michael V Heinz"
                    },
                    {
                        "name": "Tess Z Griffin"
                    },
                    {
                        "name": "Nicholas C Jacobson"
                    },
                    {
                        "name": "Andrew Campbell"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Campbell"
                },
                "author": "Andrew Campbell",
                "arxiv_comment": "Under review at CHIL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07608v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07608v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14317v2",
                "updated": "2025-02-11T14:51:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    51,
                    8,
                    1,
                    42,
                    0
                ],
                "published": "2024-08-26T14:45:03Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    14,
                    45,
                    3,
                    0,
                    239,
                    0
                ],
                "title": "Claim Verification in the Age of Large Language Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Claim Verification in the Age of Large Language Models: A Survey"
                },
                "summary": "The large and ever-increasing amount of data available on the Internet\ncoupled with the laborious task of manual claim and fact verification has\nsparked the interest in the development of automated claim verification\nsystems. Several deep learning and transformer-based models have been proposed\nfor this task over the years. With the introduction of Large Language Models\n(LLMs) and their superior performance in several NLP tasks, we have seen a\nsurge of LLM-based approaches to claim verification along with the use of novel\nmethods such as Retrieval Augmented Generation (RAG). In this survey, we\npresent a comprehensive account of recent claim verification frameworks using\nLLMs. We describe the different components of the claim verification pipeline\nused in these frameworks in detail including common approaches to retrieval,\nprompting, and fine-tuning. Finally, we describe publicly available English\ndatasets created for this task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The large and ever-increasing amount of data available on the Internet\ncoupled with the laborious task of manual claim and fact verification has\nsparked the interest in the development of automated claim verification\nsystems. Several deep learning and transformer-based models have been proposed\nfor this task over the years. With the introduction of Large Language Models\n(LLMs) and their superior performance in several NLP tasks, we have seen a\nsurge of LLM-based approaches to claim verification along with the use of novel\nmethods such as Retrieval Augmented Generation (RAG). In this survey, we\npresent a comprehensive account of recent claim verification frameworks using\nLLMs. We describe the different components of the claim verification pipeline\nused in these frameworks in detail including common approaches to retrieval,\nprompting, and fine-tuning. Finally, we describe publicly available English\ndatasets created for this task."
                },
                "authors": [
                    {
                        "name": "Alphaeus Dmonte"
                    },
                    {
                        "name": "Roland Oruche"
                    },
                    {
                        "name": "Marcos Zampieri"
                    },
                    {
                        "name": "Prasad Calyam"
                    },
                    {
                        "name": "Isabelle Augenstein"
                    }
                ],
                "author_detail": {
                    "name": "Isabelle Augenstein"
                },
                "author": "Isabelle Augenstein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07598v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07598v1",
                "updated": "2025-02-11T14:47:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    47,
                    32,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T14:47:32Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    47,
                    32,
                    1,
                    42,
                    0
                ],
                "title": "Towards spatial computing: recent advances in multimodal natural\n  interaction for XR headsets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards spatial computing: recent advances in multimodal natural\n  interaction for XR headsets"
                },
                "summary": "With the widespread adoption of Extended Reality (XR) headsets, spatial\ncomputing technologies are gaining increasing attention. Spatial computing\nenables interaction with virtual elements through natural input methods such as\neye tracking, hand gestures, and voice commands, thus placing natural\nhuman-computer interaction at its core. While previous surveys have reviewed\nconventional XR interaction techniques, recent advancements in natural\ninteraction, particularly driven by artificial intelligence (AI) and large\nlanguage models (LLMs), have introduced new paradigms and technologies. In this\npaper, we review research on multimodal natural interaction for wearable XR,\nfocusing on papers published between 2022 and 2024 in six top venues: ACM CHI,\nUIST, IMWUT (Ubicomp), IEEE VR, ISMAR, and TVCG. We classify and analyze these\nstudies based on application scenarios, operation types, and interaction\nmodalities. This analysis provides a structured framework for understanding how\nresearchers are designing advanced natural interaction techniques in XR. Based\non these findings, we discuss the challenges in natural interaction techniques\nand suggest potential directions for future research. This review provides\nvaluable insights for researchers aiming to design natural and efficient\ninteraction systems for XR, ultimately contributing to the advancement of\nspatial computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread adoption of Extended Reality (XR) headsets, spatial\ncomputing technologies are gaining increasing attention. Spatial computing\nenables interaction with virtual elements through natural input methods such as\neye tracking, hand gestures, and voice commands, thus placing natural\nhuman-computer interaction at its core. While previous surveys have reviewed\nconventional XR interaction techniques, recent advancements in natural\ninteraction, particularly driven by artificial intelligence (AI) and large\nlanguage models (LLMs), have introduced new paradigms and technologies. In this\npaper, we review research on multimodal natural interaction for wearable XR,\nfocusing on papers published between 2022 and 2024 in six top venues: ACM CHI,\nUIST, IMWUT (Ubicomp), IEEE VR, ISMAR, and TVCG. We classify and analyze these\nstudies based on application scenarios, operation types, and interaction\nmodalities. This analysis provides a structured framework for understanding how\nresearchers are designing advanced natural interaction techniques in XR. Based\non these findings, we discuss the challenges in natural interaction techniques\nand suggest potential directions for future research. This review provides\nvaluable insights for researchers aiming to design natural and efficient\ninteraction systems for XR, ultimately contributing to the advancement of\nspatial computing."
                },
                "authors": [
                    {
                        "name": "Zhimin Wang"
                    },
                    {
                        "name": "Maohang Rao"
                    },
                    {
                        "name": "Shanghua Ye"
                    },
                    {
                        "name": "Weitao Song"
                    },
                    {
                        "name": "Feng Lu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Lu"
                },
                "author": "Feng Lu",
                "arxiv_comment": "28 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07598v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07598v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07586v1",
                "updated": "2025-02-11T14:34:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    34,
                    5,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T14:34:05Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    34,
                    5,
                    1,
                    42,
                    0
                ],
                "title": "We Can't Understand AI Using our Existing Vocabulary",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We Can't Understand AI Using our Existing Vocabulary"
                },
                "summary": "This position paper argues that, in order to understand AI, we cannot rely on\nour existing vocabulary of human words. Instead, we should strive to develop\nneologisms: new words that represent precise human concepts that we want to\nteach machines, or machine concepts that we need to learn. We start from the\npremise that humans and machines have differing concepts. This means\ninterpretability can be framed as a communication problem: humans must be able\nto reference and control machine concepts, and communicate human concepts to\nmachines. Creating a shared human-machine language through developing\nneologisms, we believe, could solve this communication problem. Successful\nneologisms achieve a useful amount of abstraction: not too detailed, so they're\nreusable in many contexts, and not too high-level, so they convey precise\ninformation. As a proof of concept, we demonstrate how a \"length neologism\"\nenables controlling LLM response length, while a \"diversity neologism\" allows\nsampling more variable responses. Taken together, we argue that we cannot\nunderstand AI using our existing vocabulary, and expanding it through\nneologisms creates opportunities for both controlling and understanding\nmachines better.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This position paper argues that, in order to understand AI, we cannot rely on\nour existing vocabulary of human words. Instead, we should strive to develop\nneologisms: new words that represent precise human concepts that we want to\nteach machines, or machine concepts that we need to learn. We start from the\npremise that humans and machines have differing concepts. This means\ninterpretability can be framed as a communication problem: humans must be able\nto reference and control machine concepts, and communicate human concepts to\nmachines. Creating a shared human-machine language through developing\nneologisms, we believe, could solve this communication problem. Successful\nneologisms achieve a useful amount of abstraction: not too detailed, so they're\nreusable in many contexts, and not too high-level, so they convey precise\ninformation. As a proof of concept, we demonstrate how a \"length neologism\"\nenables controlling LLM response length, while a \"diversity neologism\" allows\nsampling more variable responses. Taken together, we argue that we cannot\nunderstand AI using our existing vocabulary, and expanding it through\nneologisms creates opportunities for both controlling and understanding\nmachines better."
                },
                "authors": [
                    {
                        "name": "John Hewitt"
                    },
                    {
                        "name": "Robert Geirhos"
                    },
                    {
                        "name": "Been Kim"
                    }
                ],
                "author_detail": {
                    "name": "Been Kim"
                },
                "author": "Been Kim",
                "arxiv_comment": "Position paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04964v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04964v2",
                "updated": "2025-02-11T14:32:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    32,
                    15,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-07T14:30:12Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    14,
                    30,
                    12,
                    4,
                    38,
                    0
                ],
                "title": "CoCoA: A Generalized Approach to Uncertainty Quantification by\n  Integrating Confidence and Consistency of LLM Outputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoCoA: A Generalized Approach to Uncertainty Quantification by\n  Integrating Confidence and Consistency of LLM Outputs"
                },
                "summary": "Uncertainty quantification (UQ) methods for Large Language Models (LLMs)\nencompasses a variety of approaches, with two major types being particularly\nprominent: information-based, which focus on model confidence expressed as\ntoken probabilities, and consistency-based, which assess the semantic\nrelationship between multiple outputs generated using repeated sampling.\nSeveral recent methods have combined these two approaches and shown impressive\nperformance in various applications. However, they sometimes fail to outperform\nmuch simpler baseline methods. Our investigation reveals distinctive\ncharacteristics of LLMs as probabilistic models, which help to explain why\nthese UQ methods underperform in certain tasks. Based on these findings, we\npropose a new way of synthesizing model confidence and output consistency that\nleads to a family of efficient and robust UQ methods. We evaluate our approach\nacross a variety of tasks such as question answering, abstractive\nsummarization, and machine translation, demonstrating sizable improvements over\nstate-of-the-art UQ approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty quantification (UQ) methods for Large Language Models (LLMs)\nencompasses a variety of approaches, with two major types being particularly\nprominent: information-based, which focus on model confidence expressed as\ntoken probabilities, and consistency-based, which assess the semantic\nrelationship between multiple outputs generated using repeated sampling.\nSeveral recent methods have combined these two approaches and shown impressive\nperformance in various applications. However, they sometimes fail to outperform\nmuch simpler baseline methods. Our investigation reveals distinctive\ncharacteristics of LLMs as probabilistic models, which help to explain why\nthese UQ methods underperform in certain tasks. Based on these findings, we\npropose a new way of synthesizing model confidence and output consistency that\nleads to a family of efficient and robust UQ methods. We evaluate our approach\nacross a variety of tasks such as question answering, abstractive\nsummarization, and machine translation, demonstrating sizable improvements over\nstate-of-the-art UQ approaches."
                },
                "authors": [
                    {
                        "name": "Roman Vashurin"
                    },
                    {
                        "name": "Maiya Goloburda"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Artem Shelmanov"
                    },
                    {
                        "name": "Maxim Panov"
                    }
                ],
                "author_detail": {
                    "name": "Maxim Panov"
                },
                "author": "Maxim Panov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04964v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04964v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03307v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03307v2",
                "updated": "2025-02-11T14:29:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    29,
                    44,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-05T16:08:05Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    8,
                    5,
                    2,
                    36,
                    0
                ],
                "title": "Intent Representation Learning with Large Language Model for\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intent Representation Learning with Large Language Model for\n  Recommendation"
                },
                "summary": "Intent-based recommender systems have garnered significant attention for\nuncovering latent fine-grained preferences. Intents, as underlying factors of\ninteractions, are crucial for improving recommendation interpretability. Most\nmethods define intents as learnable parameters updated alongside interactions.\nHowever, existing frameworks often overlook textual information (e.g., user\nreviews, item descriptions), which is crucial for alleviating the sparsity of\ninteraction intents. Exploring these multimodal intents, especially the\ninherent differences in representation spaces, poses two key challenges: i) How\nto align multimodal intents and effectively mitigate noise issues; ii) How to\nextract and match latent key intents across modalities. To tackle these\nchallenges, we propose a model-agnostic framework, Intent Representation\nLearning with Large Language Model (IRLLRec), which leverages large language\nmodels (LLMs) to construct multimodal intents and enhance recommendations.\nSpecifically, IRLLRec employs a dual-tower architecture to learn multimodal\nintent representations. Next, we propose pairwise and translation alignment to\neliminate inter-modal differences and enhance robustness against noisy input\nfeatures. Finally, to better match textual and interaction-based intents, we\nemploy momentum distillation to perform teacher-student learning on fused\nintent representations. Empirical evaluations on three datasets show that our\nIRLLRec framework outperforms baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intent-based recommender systems have garnered significant attention for\nuncovering latent fine-grained preferences. Intents, as underlying factors of\ninteractions, are crucial for improving recommendation interpretability. Most\nmethods define intents as learnable parameters updated alongside interactions.\nHowever, existing frameworks often overlook textual information (e.g., user\nreviews, item descriptions), which is crucial for alleviating the sparsity of\ninteraction intents. Exploring these multimodal intents, especially the\ninherent differences in representation spaces, poses two key challenges: i) How\nto align multimodal intents and effectively mitigate noise issues; ii) How to\nextract and match latent key intents across modalities. To tackle these\nchallenges, we propose a model-agnostic framework, Intent Representation\nLearning with Large Language Model (IRLLRec), which leverages large language\nmodels (LLMs) to construct multimodal intents and enhance recommendations.\nSpecifically, IRLLRec employs a dual-tower architecture to learn multimodal\nintent representations. Next, we propose pairwise and translation alignment to\neliminate inter-modal differences and enhance robustness against noisy input\nfeatures. Finally, to better match textual and interaction-based intents, we\nemploy momentum distillation to perform teacher-student learning on fused\nintent representations. Empirical evaluations on three datasets show that our\nIRLLRec framework outperforms baselines."
                },
                "authors": [
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Lei Sang"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Yiwen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yiwen Zhang"
                },
                "author": "Yiwen Zhang",
                "arxiv_comment": "11 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03307v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03307v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07578v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07578v1",
                "updated": "2025-02-11T14:25:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    25,
                    20,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T14:25:20Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    25,
                    20,
                    1,
                    42,
                    0
                ],
                "title": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference"
                },
                "summary": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users to pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.3$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users to pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.3$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs."
                },
                "authors": [
                    {
                        "name": "Yufeng Gu"
                    },
                    {
                        "name": "Alireza Khadem"
                    },
                    {
                        "name": "Sumanth Umesh"
                    },
                    {
                        "name": "Ning Liang"
                    },
                    {
                        "name": "Xavier Servot"
                    },
                    {
                        "name": "Onur Mutlu"
                    },
                    {
                        "name": "Ravi Iyer"
                    },
                    {
                        "name": "Reetuparna Das"
                    }
                ],
                "author_detail": {
                    "name": "Reetuparna Das"
                },
                "author": "Reetuparna Das",
                "arxiv_doi": "10.1145/3676641.3716267",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3716267",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.07578v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07578v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, Volume\n  2 (ASPLOS'25)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04315v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04315v3",
                "updated": "2025-02-11T14:01:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    1,
                    39,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-06T18:57:06Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    57,
                    6,
                    3,
                    37,
                    0
                ],
                "title": "ChameleonLLM: Batch-Aware Dynamic Low-Rank Adaptation via Inference-Time\n  Clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChameleonLLM: Batch-Aware Dynamic Low-Rank Adaptation via Inference-Time\n  Clusters"
                },
                "summary": "Recent advances in large language models (LLMs) have shown remarkable\nperformance across diverse tasks. However, these models are typically deployed\nwith fixed weights, which limits their ability to adapt dynamically to the\nvariability inherent in real-world data during inference. This paper introduces\nChameleonLLM, a novel framework that enables inference-time adaptation of LLMs\nby leveraging batch-aware clustering and on-the-fly generation of low-rank\nupdates. Unlike traditional fine-tuning approaches such as Low-Rank Adaptation\n(LoRA) or methods that rely on a fixed set of pre-learned uniforms (changeable\nmasks), our method dynamically generates adaptive modifications to the decoder\nweights based on the aggregated statistics of clustered batches. By\nintelligently grouping similar inputs and computing context-aware low-rank\nupdates via a hyper-network, ChameleonLLM achieves significant performance\ngains, outperforming conventional LoRA methods while eliminating the overhead\nof maintaining multiple expert models. Our experiments highlight the potential\nof our approach to serve as a versatile and highly adaptive solution for\nlanguage model inference. ChameleonLLM is open-sourced to ensure the\nreproducibility of our experiments:\nhttps://anonymous.4open.science/r/ChamaleonLLM/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have shown remarkable\nperformance across diverse tasks. However, these models are typically deployed\nwith fixed weights, which limits their ability to adapt dynamically to the\nvariability inherent in real-world data during inference. This paper introduces\nChameleonLLM, a novel framework that enables inference-time adaptation of LLMs\nby leveraging batch-aware clustering and on-the-fly generation of low-rank\nupdates. Unlike traditional fine-tuning approaches such as Low-Rank Adaptation\n(LoRA) or methods that rely on a fixed set of pre-learned uniforms (changeable\nmasks), our method dynamically generates adaptive modifications to the decoder\nweights based on the aggregated statistics of clustered batches. By\nintelligently grouping similar inputs and computing context-aware low-rank\nupdates via a hyper-network, ChameleonLLM achieves significant performance\ngains, outperforming conventional LoRA methods while eliminating the overhead\nof maintaining multiple expert models. Our experiments highlight the potential\nof our approach to serve as a versatile and highly adaptive solution for\nlanguage model inference. ChameleonLLM is open-sourced to ensure the\nreproducibility of our experiments:\nhttps://anonymous.4open.science/r/ChamaleonLLM/"
                },
                "authors": [
                    {
                        "name": "Kamer Ali Yuksel"
                    },
                    {
                        "name": "Hassan Sawaf"
                    }
                ],
                "author_detail": {
                    "name": "Hassan Sawaf"
                },
                "author": "Hassan Sawaf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04315v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04315v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18005v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18005v3",
                "updated": "2025-02-11T13:58:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    13,
                    58,
                    39,
                    1,
                    42,
                    0
                ],
                "published": "2025-01-29T21:40:32Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    21,
                    40,
                    32,
                    2,
                    29,
                    0
                ],
                "title": "Fault Localization via Fine-tuning Large Language Models with Mutation\n  Generated Stack Traces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fault Localization via Fine-tuning Large Language Models with Mutation\n  Generated Stack Traces"
                },
                "summary": "Abrupt and unexpected terminations of software are termed as software\ncrashes. They can be challenging to analyze. Finding the root cause requires\nextensive manual effort and expertise to connect information sources like stack\ntraces, source code, and logs. Typical approaches to fault localization require\neither test failures or source code. Crashes occurring in production\nenvironments, such as that of SAP HANA, provide solely crash logs and stack\ntraces. We present a novel approach to localize faults based only on the stack\ntrace information and no additional runtime information, by fine-tuning large\nlanguage models (LLMs). We address complex cases where the root cause of a\ncrash differs from the technical cause, and is not located in the innermost\nframe of the stack trace. As the number of historic crashes is insufficient to\nfine-tune LLMs, we augment our dataset by leveraging code mutators to inject\nsynthetic crashes into the code base. By fine-tuning on 64,369 crashes\nresulting from 4.1 million mutations of the HANA code base, we can correctly\npredict the root cause location of a crash with an accuracy of 66.9\\% while\nbaselines only achieve 12.6% and 10.6%. We substantiate the generalizability of\nour approach by evaluating on two additional open-source databases, SQLite and\nDuckDB, achieving accuracies of 63% and 74%, respectively. Across all our\nexperiments, fine-tuning consistently outperformed prompting non-finetuned LLMs\nfor localizing faults in our datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abrupt and unexpected terminations of software are termed as software\ncrashes. They can be challenging to analyze. Finding the root cause requires\nextensive manual effort and expertise to connect information sources like stack\ntraces, source code, and logs. Typical approaches to fault localization require\neither test failures or source code. Crashes occurring in production\nenvironments, such as that of SAP HANA, provide solely crash logs and stack\ntraces. We present a novel approach to localize faults based only on the stack\ntrace information and no additional runtime information, by fine-tuning large\nlanguage models (LLMs). We address complex cases where the root cause of a\ncrash differs from the technical cause, and is not located in the innermost\nframe of the stack trace. As the number of historic crashes is insufficient to\nfine-tune LLMs, we augment our dataset by leveraging code mutators to inject\nsynthetic crashes into the code base. By fine-tuning on 64,369 crashes\nresulting from 4.1 million mutations of the HANA code base, we can correctly\npredict the root cause location of a crash with an accuracy of 66.9\\% while\nbaselines only achieve 12.6% and 10.6%. We substantiate the generalizability of\nour approach by evaluating on two additional open-source databases, SQLite and\nDuckDB, achieving accuracies of 63% and 74%, respectively. Across all our\nexperiments, fine-tuning consistently outperformed prompting non-finetuned LLMs\nfor localizing faults in our datasets."
                },
                "authors": [
                    {
                        "name": "Neetha Jambigi"
                    },
                    {
                        "name": "Bartosz Bogacz"
                    },
                    {
                        "name": "Moritz Mueller"
                    },
                    {
                        "name": "Thomas Bach"
                    },
                    {
                        "name": "Michael Felderer"
                    }
                ],
                "author_detail": {
                    "name": "Michael Felderer"
                },
                "author": "Michael Felderer",
                "arxiv_comment": "I do not have the necessary approvals to out the paper on Arxiv from\n  my organization yet. I was too soon to do this",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18005v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18005v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07557v1",
                "updated": "2025-02-11T13:50:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    13,
                    50,
                    50,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T13:50:50Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    13,
                    50,
                    50,
                    1,
                    42,
                    0
                ],
                "title": "JBShield: Defending Large Language Models from Jailbreak Attacks through\n  Activated Concept Analysis and Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JBShield: Defending Large Language Models from Jailbreak Attacks through\n  Activated Concept Analysis and Manipulation"
                },
                "summary": "Despite the implementation of safety alignment strategies, large language\nmodels (LLMs) remain vulnerable to jailbreak attacks, which undermine these\nsafety guardrails and pose significant security threats. Some defenses have\nbeen proposed to detect or mitigate jailbreaks, but they are unable to\nwithstand the test of time due to an insufficient understanding of jailbreak\nmechanisms. In this work, we investigate the mechanisms behind jailbreaks based\non the Linear Representation Hypothesis (LRH), which states that neural\nnetworks encode high-level concepts as subspaces in their hidden\nrepresentations. We define the toxic semantics in harmful and jailbreak prompts\nas toxic concepts and describe the semantics in jailbreak prompts that\nmanipulate LLMs to comply with unsafe requests as jailbreak concepts. Through\nconcept extraction and analysis, we reveal that LLMs can recognize the toxic\nconcepts in both harmful and jailbreak prompts. However, unlike harmful\nprompts, jailbreak prompts activate the jailbreak concepts and alter the LLM\noutput from rejection to compliance. Building on our analysis, we propose a\ncomprehensive jailbreak defense framework, JBShield, consisting of two key\ncomponents: jailbreak detection JBShield-D and mitigation JBShield-M.\nJBShield-D identifies jailbreak prompts by determining whether the input\nactivates both toxic and jailbreak concepts. When a jailbreak prompt is\ndetected, JBShield-M adjusts the hidden representations of the target LLM by\nenhancing the toxic concept and weakening the jailbreak concept, ensuring LLMs\nproduce safe content. Extensive experiments demonstrate the superior\nperformance of JBShield, achieving an average detection accuracy of 0.95 and\nreducing the average attack success rate of various jailbreak attacks to 2%\nfrom 61% across distinct LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the implementation of safety alignment strategies, large language\nmodels (LLMs) remain vulnerable to jailbreak attacks, which undermine these\nsafety guardrails and pose significant security threats. Some defenses have\nbeen proposed to detect or mitigate jailbreaks, but they are unable to\nwithstand the test of time due to an insufficient understanding of jailbreak\nmechanisms. In this work, we investigate the mechanisms behind jailbreaks based\non the Linear Representation Hypothesis (LRH), which states that neural\nnetworks encode high-level concepts as subspaces in their hidden\nrepresentations. We define the toxic semantics in harmful and jailbreak prompts\nas toxic concepts and describe the semantics in jailbreak prompts that\nmanipulate LLMs to comply with unsafe requests as jailbreak concepts. Through\nconcept extraction and analysis, we reveal that LLMs can recognize the toxic\nconcepts in both harmful and jailbreak prompts. However, unlike harmful\nprompts, jailbreak prompts activate the jailbreak concepts and alter the LLM\noutput from rejection to compliance. Building on our analysis, we propose a\ncomprehensive jailbreak defense framework, JBShield, consisting of two key\ncomponents: jailbreak detection JBShield-D and mitigation JBShield-M.\nJBShield-D identifies jailbreak prompts by determining whether the input\nactivates both toxic and jailbreak concepts. When a jailbreak prompt is\ndetected, JBShield-M adjusts the hidden representations of the target LLM by\nenhancing the toxic concept and weakening the jailbreak concept, ensuring LLMs\nproduce safe content. Extensive experiments demonstrate the superior\nperformance of JBShield, achieving an average detection accuracy of 0.95 and\nreducing the average attack success rate of various jailbreak attacks to 2%\nfrom 61% across distinct LLMs."
                },
                "authors": [
                    {
                        "name": "Shenyi Zhang"
                    },
                    {
                        "name": "Yuchen Zhai"
                    },
                    {
                        "name": "Keyan Guo"
                    },
                    {
                        "name": "Hongxin Hu"
                    },
                    {
                        "name": "Shengnan Guo"
                    },
                    {
                        "name": "Zheng Fang"
                    },
                    {
                        "name": "Lingchen Zhao"
                    },
                    {
                        "name": "Chao Shen"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Qian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qian Wang"
                },
                "author": "Qian Wang",
                "arxiv_comment": "To Appear in the 34rd USENIX Security Symposium, August 13-15, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07555v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07555v1",
                "updated": "2025-02-11T13:48:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    13,
                    48,
                    10,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T13:48:10Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    13,
                    48,
                    10,
                    1,
                    42,
                    0
                ],
                "title": "O1 Embedder: Let Retrievers Think Before Action",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "O1 Embedder: Let Retrievers Think Before Action"
                },
                "summary": "The growing power of large language models (LLMs) has revolutionized how\npeople access and utilize information. Notably, the LLMs excel at performing\nfine-grained data representation, which facilitates precise retrieval of\ninformation. They also generate high-quality answers based on external\nreferences, enabling the production of useful knowledge. The recent\nintroduction of reasoning models, like OpenAI O1 and DeepSeek R1, marks another\nleap forward, highlighting LLMs' ability to think progressively before\ndelivering final answers. This breakthrough significantly improves the ability\nto address complex tasks, e.g., coding and math proofs.\n  Inspired by this progress, we aim to develop similar capabilities for\nretrieval models, which hold great promise for tackling critical challenges in\nthe field, including multi-task retrieval, zero-shot retrieval, and tasks\nrequiring intensive reasoning of complex relationships. With this motivation,\nwe propose a novel approach called O1 Embedder, which generates useful thoughts\nfor the input query before making retrieval for the target documents. To\nrealize this objective, we conquer two technical difficulties. First, we design\na data synthesis workflow, creating training signals for O1 Embedder by\ngenerating initial thoughts from an LLM-expert and subsequently refining them\nusing a retrieval committee. Second, we optimize the training process, enabling\na pre-trained model to be jointly fine-tuned to generate retrieval thoughts via\nbehavior cloning and perform dense retrieval through contrastive learning. Our\napproach is evaluated by comprehensive experiments, where substantial\nimprovements are achieved across 12 popular datasets, spanning both in-domain\nand out-of-domain scenarios. These results highlight O1 Embedder's remarkable\naccuracy and generalizability, paving the way for the development of\nnext-generation IR foundation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing power of large language models (LLMs) has revolutionized how\npeople access and utilize information. Notably, the LLMs excel at performing\nfine-grained data representation, which facilitates precise retrieval of\ninformation. They also generate high-quality answers based on external\nreferences, enabling the production of useful knowledge. The recent\nintroduction of reasoning models, like OpenAI O1 and DeepSeek R1, marks another\nleap forward, highlighting LLMs' ability to think progressively before\ndelivering final answers. This breakthrough significantly improves the ability\nto address complex tasks, e.g., coding and math proofs.\n  Inspired by this progress, we aim to develop similar capabilities for\nretrieval models, which hold great promise for tackling critical challenges in\nthe field, including multi-task retrieval, zero-shot retrieval, and tasks\nrequiring intensive reasoning of complex relationships. With this motivation,\nwe propose a novel approach called O1 Embedder, which generates useful thoughts\nfor the input query before making retrieval for the target documents. To\nrealize this objective, we conquer two technical difficulties. First, we design\na data synthesis workflow, creating training signals for O1 Embedder by\ngenerating initial thoughts from an LLM-expert and subsequently refining them\nusing a retrieval committee. Second, we optimize the training process, enabling\na pre-trained model to be jointly fine-tuned to generate retrieval thoughts via\nbehavior cloning and perform dense retrieval through contrastive learning. Our\napproach is evaluated by comprehensive experiments, where substantial\nimprovements are achieved across 12 popular datasets, spanning both in-domain\nand out-of-domain scenarios. These results highlight O1 Embedder's remarkable\naccuracy and generalizability, paving the way for the development of\nnext-generation IR foundation models."
                },
                "authors": [
                    {
                        "name": "Ruin Yan"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Defu Lian"
                    }
                ],
                "author_detail": {
                    "name": "Defu Lian"
                },
                "author": "Defu Lian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07555v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07555v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06693v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06693v2",
                "updated": "2025-02-11T13:43:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    13,
                    43,
                    25,
                    1,
                    42,
                    0
                ],
                "published": "2024-10-09T08:59:57Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    8,
                    59,
                    57,
                    2,
                    283,
                    0
                ],
                "title": "Autonomous localization of multiple ionizing radiation sources using\n  miniature single-layer Compton cameras onboard a group of micro aerial\n  vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous localization of multiple ionizing radiation sources using\n  miniature single-layer Compton cameras onboard a group of micro aerial\n  vehicles"
                },
                "summary": "A novel method for autonomous localization of multiple sources of gamma\nradiation using a group of Micro Aerial Vehicles (MAVs) is presented in this\npaper. The method utilizes an extremely lightweight (44 g) Compton camera\nMiniPIX TPX3. The compact size of the detector allows for deployment onboard\nsafe and agile small-scale Unmanned Aerial Vehicles (UAVs). The proposed\nradiation mapping approach fuses measurements from multiple distributed Compton\ncamera sensors to accurately estimate the positions of multiple radioactive\nsources in real time. Unlike commonly used intensity-based detectors, the\nCompton camera reconstructs the set of possible directions towards a radiation\nsource from just a single ionizing particle. Therefore, the proposed approach\ncan localize radiation sources without having to estimate the gradient of a\nradiation field or contour lines, which require longer measurements. The\ninstant estimation is able to fully exploit the potential of highly mobile\nMAVs. The radiation mapping method is combined with an active search strategy,\nwhich coordinates the future actions of the MAVs in order to improve the\nquality of the estimate of the sources' positions, as well as to explore the\narea of interest faster. The proposed solution is evaluated in simulation and\nreal world experiments with multiple Cesium-137 radiation sources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A novel method for autonomous localization of multiple sources of gamma\nradiation using a group of Micro Aerial Vehicles (MAVs) is presented in this\npaper. The method utilizes an extremely lightweight (44 g) Compton camera\nMiniPIX TPX3. The compact size of the detector allows for deployment onboard\nsafe and agile small-scale Unmanned Aerial Vehicles (UAVs). The proposed\nradiation mapping approach fuses measurements from multiple distributed Compton\ncamera sensors to accurately estimate the positions of multiple radioactive\nsources in real time. Unlike commonly used intensity-based detectors, the\nCompton camera reconstructs the set of possible directions towards a radiation\nsource from just a single ionizing particle. Therefore, the proposed approach\ncan localize radiation sources without having to estimate the gradient of a\nradiation field or contour lines, which require longer measurements. The\ninstant estimation is able to fully exploit the potential of highly mobile\nMAVs. The radiation mapping method is combined with an active search strategy,\nwhich coordinates the future actions of the MAVs in order to improve the\nquality of the estimate of the sources' positions, as well as to explore the\narea of interest faster. The proposed solution is evaluated in simulation and\nreal world experiments with multiple Cesium-137 radiation sources."
                },
                "authors": [
                    {
                        "name": "Michal Werner"
                    },
                    {
                        "name": "Tom Ba"
                    },
                    {
                        "name": "Petr tibinger"
                    },
                    {
                        "name": "Daniela Doubravov"
                    },
                    {
                        "name": "Jaroslav olc"
                    },
                    {
                        "name": "Jan Rusk"
                    },
                    {
                        "name": "Martin Saska"
                    }
                ],
                "author_detail": {
                    "name": "Martin Saska"
                },
                "author": "Martin Saska",
                "arxiv_doi": "10.1109/IROS58592.2024.10802808",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IROS58592.2024.10802808",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.06693v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06693v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "2024 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS)",
                "arxiv_journal_ref": "2024 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS)",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04916v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04916v2",
                "updated": "2025-02-11T13:16:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    13,
                    16,
                    29,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-07T13:33:40Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    33,
                    40,
                    4,
                    38,
                    0
                ],
                "title": "Classification or Prompting: A Case Study on Legal Requirements\n  Traceability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classification or Prompting: A Case Study on Legal Requirements\n  Traceability"
                },
                "summary": "New regulations are continuously introduced to ensure that software\ndevelopment complies with the ethical concerns and prioritizes public safety. A\nprerequisite for demonstrating compliance involves tracing software\nrequirements to legal provisions. Requirements traceability is a fundamental\ntask where requirements engineers are supposed to analyze technical\nrequirements against target artifacts, often under limited time budget. Doing\nthis analysis manually for complex systems with hundreds of requirements is\ninfeasible. The legal dimension introduces additional challenges that only\nexacerbate manual effort.\n  In this paper, we investigate two automated solutions based on large language\nmodels (LLMs) to predict trace links between requirements and legal provisions.\nThe first solution, Kashif, is a classifier that leverages sentence\ntransformers. The second solution prompts a recent generative LLM based on\nRice, a prompt engineering framework.\n  On a benchmark dataset, we empirically evaluate Kashif and compare it against\na baseline classifier from the literature. Kashif can identify trace links with\nan average recall of ~67%, outperforming the baseline with a substantial gain\nof 54 percentage points (pp) in recall. However, on unseen, more complex\nrequirements documents traced to the European general data protection\nregulation (GDPR), Kashif performs poorly, yielding an average recall of 15%.\nOn the same documents, however, our Rice-based solution yields an average\nrecall of 84%, with a remarkable gain of about 69 pp over Kashif. Our results\nsuggest that requirements traceability in the legal context cannot be simply\naddressed by building classifiers, as such solutions do not generalize and fail\nto perform well on complex regulations and requirements. Resorting to\ngenerative LLMs, with careful prompt engineering, is thus a more promising\nalternative.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "New regulations are continuously introduced to ensure that software\ndevelopment complies with the ethical concerns and prioritizes public safety. A\nprerequisite for demonstrating compliance involves tracing software\nrequirements to legal provisions. Requirements traceability is a fundamental\ntask where requirements engineers are supposed to analyze technical\nrequirements against target artifacts, often under limited time budget. Doing\nthis analysis manually for complex systems with hundreds of requirements is\ninfeasible. The legal dimension introduces additional challenges that only\nexacerbate manual effort.\n  In this paper, we investigate two automated solutions based on large language\nmodels (LLMs) to predict trace links between requirements and legal provisions.\nThe first solution, Kashif, is a classifier that leverages sentence\ntransformers. The second solution prompts a recent generative LLM based on\nRice, a prompt engineering framework.\n  On a benchmark dataset, we empirically evaluate Kashif and compare it against\na baseline classifier from the literature. Kashif can identify trace links with\nan average recall of ~67%, outperforming the baseline with a substantial gain\nof 54 percentage points (pp) in recall. However, on unseen, more complex\nrequirements documents traced to the European general data protection\nregulation (GDPR), Kashif performs poorly, yielding an average recall of 15%.\nOn the same documents, however, our Rice-based solution yields an average\nrecall of 84%, with a remarkable gain of about 69 pp over Kashif. Our results\nsuggest that requirements traceability in the legal context cannot be simply\naddressed by building classifiers, as such solutions do not generalize and fail\nto perform well on complex regulations and requirements. Resorting to\ngenerative LLMs, with careful prompt engineering, is thus a more promising\nalternative."
                },
                "authors": [
                    {
                        "name": "Romina Etezadi"
                    },
                    {
                        "name": "Sallam Abualhaija"
                    },
                    {
                        "name": "Chetan Arora"
                    },
                    {
                        "name": "Lionel Briand"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Briand"
                },
                "author": "Lionel Briand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04916v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04916v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00641v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00641v2",
                "updated": "2025-02-11T13:12:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    13,
                    12,
                    16,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-02T03:07:45Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    3,
                    7,
                    45,
                    6,
                    33,
                    0
                ],
                "title": "Evaluating Small Language Models for News Summarization: Implications\n  and Factors Influencing Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Small Language Models for News Summarization: Implications\n  and Factors Influencing Performance"
                },
                "summary": "The increasing demand for efficient summarization tools in\nresource-constrained environments highlights the need for effective solutions.\nWhile large language models (LLMs) deliver superior summarization quality,\ntheir high computational resource requirements limit practical use\napplications. In contrast, small language models (SLMs) present a more\naccessible alternative, capable of real-time summarization on edge devices.\nHowever, their summarization capabilities and comparative performance against\nLLMs remain underexplored. This paper addresses this gap by presenting a\ncomprehensive evaluation of 19 SLMs for news summarization across 2,000 news\nsamples, focusing on relevance, coherence, factual consistency, and summary\nlength. Our findings reveal significant variations in SLM performance, with\ntop-performing models such as Phi3-Mini and Llama3.2-3B-Ins achieving results\ncomparable to those of 70B LLMs while generating more concise summaries.\nNotably, SLMs are better suited for simple prompts, as overly complex prompts\nmay lead to a decline in summary quality. Additionally, our analysis indicates\nthat instruction tuning does not consistently enhance the news summarization\ncapabilities of SLMs. This research not only contributes to the understanding\nof SLMs but also provides practical insights for researchers seeking efficient\nsummarization solutions that balance performance and resource use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for efficient summarization tools in\nresource-constrained environments highlights the need for effective solutions.\nWhile large language models (LLMs) deliver superior summarization quality,\ntheir high computational resource requirements limit practical use\napplications. In contrast, small language models (SLMs) present a more\naccessible alternative, capable of real-time summarization on edge devices.\nHowever, their summarization capabilities and comparative performance against\nLLMs remain underexplored. This paper addresses this gap by presenting a\ncomprehensive evaluation of 19 SLMs for news summarization across 2,000 news\nsamples, focusing on relevance, coherence, factual consistency, and summary\nlength. Our findings reveal significant variations in SLM performance, with\ntop-performing models such as Phi3-Mini and Llama3.2-3B-Ins achieving results\ncomparable to those of 70B LLMs while generating more concise summaries.\nNotably, SLMs are better suited for simple prompts, as overly complex prompts\nmay lead to a decline in summary quality. Additionally, our analysis indicates\nthat instruction tuning does not consistently enhance the news summarization\ncapabilities of SLMs. This research not only contributes to the understanding\nof SLMs but also provides practical insights for researchers seeking efficient\nsummarization solutions that balance performance and resource use."
                },
                "authors": [
                    {
                        "name": "Borui Xu"
                    },
                    {
                        "name": "Yao Chen"
                    },
                    {
                        "name": "Zeyi Wen"
                    },
                    {
                        "name": "Weiguo Liu"
                    },
                    {
                        "name": "Bingsheng He"
                    }
                ],
                "author_detail": {
                    "name": "Bingsheng He"
                },
                "author": "Bingsheng He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00641v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00641v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13381v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13381v2",
                "updated": "2025-02-11T12:44:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    12,
                    44,
                    39,
                    1,
                    42,
                    0
                ],
                "published": "2025-01-23T04:50:03Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    4,
                    50,
                    3,
                    3,
                    23,
                    0
                ],
                "title": "Do as We Do, Not as You Think: the Conformity of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do as We Do, Not as You Think: the Conformity of Large Language Models"
                },
                "summary": "Recent advancements in large language models (LLMs) revolutionize the field\nof intelligent agents, enabling collaborative multi-agent systems capable of\ntackling complex problems across various domains. However, the potential of\nconformity within these systems, analogous to phenomena like conformity bias\nand groupthink in human group dynamics, remains largely unexplored, raising\nconcerns about their collective problem-solving capabilities and possible\nethical implications. This paper presents a comprehensive study on conformity\nin LLM-driven multi-agent systems, focusing on three aspects: the existence of\nconformity, the factors influencing conformity, and potential mitigation\nstrategies. In particular, we introduce BenchForm, a new conformity-oriented\nbenchmark, featuring reasoning-intensive tasks and five distinct interaction\nprotocols designed to probe LLMs' behavior in collaborative scenarios. Several\nrepresentative LLMs are evaluated on BenchForm, using metrics such as\nconformity rate and independence rate to quantify conformity's impact. Our\nanalysis delves into factors influencing conformity, including interaction time\nand majority size, and examines how the subject agent rationalizes its\nconforming behavior. Furthermore, we explore two strategies to mitigate\nconformity effects, i.e., developing enhanced personas and implementing a\nreflection mechanism. Several interesting findings regarding LLMs' conformity\nare derived from empirical results and case studies. We hope that these\ninsights can pave the way for more robust and ethically-aligned collaborative\nAI systems. Our benchmark and code are available at BenchForm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) revolutionize the field\nof intelligent agents, enabling collaborative multi-agent systems capable of\ntackling complex problems across various domains. However, the potential of\nconformity within these systems, analogous to phenomena like conformity bias\nand groupthink in human group dynamics, remains largely unexplored, raising\nconcerns about their collective problem-solving capabilities and possible\nethical implications. This paper presents a comprehensive study on conformity\nin LLM-driven multi-agent systems, focusing on three aspects: the existence of\nconformity, the factors influencing conformity, and potential mitigation\nstrategies. In particular, we introduce BenchForm, a new conformity-oriented\nbenchmark, featuring reasoning-intensive tasks and five distinct interaction\nprotocols designed to probe LLMs' behavior in collaborative scenarios. Several\nrepresentative LLMs are evaluated on BenchForm, using metrics such as\nconformity rate and independence rate to quantify conformity's impact. Our\nanalysis delves into factors influencing conformity, including interaction time\nand majority size, and examines how the subject agent rationalizes its\nconforming behavior. Furthermore, we explore two strategies to mitigate\nconformity effects, i.e., developing enhanced personas and implementing a\nreflection mechanism. Several interesting findings regarding LLMs' conformity\nare derived from empirical results and case studies. We hope that these\ninsights can pave the way for more robust and ethically-aligned collaborative\nAI systems. Our benchmark and code are available at BenchForm."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Weng"
                    },
                    {
                        "name": "Guikun Chen"
                    },
                    {
                        "name": "Wenguan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenguan Wang"
                },
                "author": "Wenguan Wang",
                "arxiv_comment": "ICLR 2025 (Oral). Code: https://github.com/Zhiyuan-Weng/BenchForm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13381v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13381v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19018v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19018v4",
                "updated": "2025-02-11T12:39:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    12,
                    39,
                    22,
                    1,
                    42,
                    0
                ],
                "published": "2024-12-26T01:56:42Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    1,
                    56,
                    42,
                    3,
                    361,
                    0
                ],
                "title": "Let the Fuzzy Rule Speak: Enhancing In-context Learning Debiasing with\n  Interpretability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let the Fuzzy Rule Speak: Enhancing In-context Learning Debiasing with\n  Interpretability"
                },
                "summary": "Large language models (LLMs) often struggle with balanced class accuracy in\ntext classification tasks using in-context learning (ICL), hindering some\npractical uses due to user dissatisfaction or safety risks caused by\nmisclassifications. Retraining LLMs to address root causes in data or model\npriors is neither easy nor cost-effective. This paper delves deeper into the\nclass accuracy imbalance issue, identifying that it arises because certain\nclasses consistently receive disproportionately high ICL probabilities, causing\nunder-prediction and lower accuracy for others. More importantly, probability\nranges affect the imbalance differently, allowing for precise, range-specific\ncorrections. We introduce FuRud (Fuzzy Rule Optimization-based Debiasing), a\nmethod for sample-level class probability correction. FuRud tackles\ninterpretability challenges by determining why certain classes need corrections\nand tailoring adjustments for each instance's class probabilities which is\npowered by fuzzy sets with triangular membership functions, transforming a\nclass probability based on the range it belongs to. By solving a nonlinear\ninteger programming problem with a labeled set of ICL class probabilities to\nminimize class accuracy bias (COBias) and maximize overall accuracy, each class\nselects an optimal correction function from 19 triangular membership functions\nwithout updating an LLM, and the selected functions correct test instances at\ninference. Across seven benchmark datasets, FuRud reduces COBias by over half\n(56%) and improves overall accuracy by 21% relatively, outperforming\nstate-of-the-art debiasing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often struggle with balanced class accuracy in\ntext classification tasks using in-context learning (ICL), hindering some\npractical uses due to user dissatisfaction or safety risks caused by\nmisclassifications. Retraining LLMs to address root causes in data or model\npriors is neither easy nor cost-effective. This paper delves deeper into the\nclass accuracy imbalance issue, identifying that it arises because certain\nclasses consistently receive disproportionately high ICL probabilities, causing\nunder-prediction and lower accuracy for others. More importantly, probability\nranges affect the imbalance differently, allowing for precise, range-specific\ncorrections. We introduce FuRud (Fuzzy Rule Optimization-based Debiasing), a\nmethod for sample-level class probability correction. FuRud tackles\ninterpretability challenges by determining why certain classes need corrections\nand tailoring adjustments for each instance's class probabilities which is\npowered by fuzzy sets with triangular membership functions, transforming a\nclass probability based on the range it belongs to. By solving a nonlinear\ninteger programming problem with a labeled set of ICL class probabilities to\nminimize class accuracy bias (COBias) and maximize overall accuracy, each class\nselects an optimal correction function from 19 triangular membership functions\nwithout updating an LLM, and the selected functions correct test instances at\ninference. Across seven benchmark datasets, FuRud reduces COBias by over half\n(56%) and improves overall accuracy by 21% relatively, outperforming\nstate-of-the-art debiasing methods."
                },
                "authors": [
                    {
                        "name": "Ruixi Lin"
                    },
                    {
                        "name": "Yang You"
                    }
                ],
                "author_detail": {
                    "name": "Yang You"
                },
                "author": "Yang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19018v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19018v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00233v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00233v2",
                "updated": "2025-02-11T12:33:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    12,
                    33,
                    13,
                    1,
                    42,
                    0
                ],
                "published": "2024-12-31T02:53:27Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    2,
                    53,
                    27,
                    1,
                    366,
                    0
                ],
                "title": "Zero-Shot Strategies for Length-Controllable Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Strategies for Length-Controllable Summarization"
                },
                "summary": "Large language models (LLMs) struggle with precise length control,\nparticularly in zero-shot settings. We conduct a comprehensive study evaluating\nLLMs' length control capabilities across multiple measures and propose\npractical methods to improve controllability. Our experiments with LLaMA 3\nreveal stark differences in length adherence across measures and highlight\ninherent biases of the model. To address these challenges, we introduce a set\nof methods: length approximation, target adjustment, sample filtering, and\nautomated revisions. By combining these methods, we demonstrate substantial\nimprovements in length compliance while maintaining or enhancing summary\nquality, providing highly effective zero-shot strategies for precise length\ncontrol without the need for model fine-tuning or architectural changes. With\nour work, we not only advance our understanding of LLM behavior in controlled\ntext generation but also pave the way for more reliable and adaptable\nsummarization systems in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) struggle with precise length control,\nparticularly in zero-shot settings. We conduct a comprehensive study evaluating\nLLMs' length control capabilities across multiple measures and propose\npractical methods to improve controllability. Our experiments with LLaMA 3\nreveal stark differences in length adherence across measures and highlight\ninherent biases of the model. To address these challenges, we introduce a set\nof methods: length approximation, target adjustment, sample filtering, and\nautomated revisions. By combining these methods, we demonstrate substantial\nimprovements in length compliance while maintaining or enhancing summary\nquality, providing highly effective zero-shot strategies for precise length\ncontrol without the need for model fine-tuning or architectural changes. With\nour work, we not only advance our understanding of LLM behavior in controlled\ntext generation but also pave the way for more reliable and adaptable\nsummarization systems in real-world applications."
                },
                "authors": [
                    {
                        "name": "Fabian Retkowski"
                    },
                    {
                        "name": "Alexander Waibel"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Waibel"
                },
                "author": "Alexander Waibel",
                "arxiv_comment": "Accepted to NAACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00233v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00233v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00696v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00696v3",
                "updated": "2025-02-11T12:21:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    12,
                    21,
                    13,
                    1,
                    42,
                    0
                ],
                "published": "2024-09-01T11:24:54Z",
                "published_parsed": [
                    2024,
                    9,
                    1,
                    11,
                    24,
                    54,
                    6,
                    245,
                    0
                ],
                "title": "Polyrating: A Cost-Effective and Bias-Aware Rating System for LLM\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Polyrating: A Cost-Effective and Bias-Aware Rating System for LLM\n  Evaluation"
                },
                "summary": "Rating-based human evaluation has become an essential tool to accurately\nevaluate the impressive performance of large language models (LLMs). However,\ncurrent rating systems suffer from several important limitations: first, they\nfail to account for biases that significantly influence evaluation results,\nsecond, they require large and expensive preference datasets to obtain accurate\nratings, and third, they do not facilitate meaningful comparisons of model\nratings across different tasks. To address these issues, we introduce\nPolyrating, an expressive and flexible rating system based on maximum a\nposteriori estimation that enables a more nuanced and thorough analysis of\nmodel performance at lower costs. Polyrating can detect and quantify biases\naffecting human preferences, ensuring fairer model comparisons. Further,\nPolyrating can reduce the cost of human evaluations by up to $41\\%$ for new\nmodels and up to $77\\%$ for new tasks by leveraging existing benchmark scores.\nLastly, Polyrating enables direct comparisons of ratings across different\ntasks, providing a comprehensive understanding of an LLMs' strengths,\nweaknesses, and relative performance across different applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rating-based human evaluation has become an essential tool to accurately\nevaluate the impressive performance of large language models (LLMs). However,\ncurrent rating systems suffer from several important limitations: first, they\nfail to account for biases that significantly influence evaluation results,\nsecond, they require large and expensive preference datasets to obtain accurate\nratings, and third, they do not facilitate meaningful comparisons of model\nratings across different tasks. To address these issues, we introduce\nPolyrating, an expressive and flexible rating system based on maximum a\nposteriori estimation that enables a more nuanced and thorough analysis of\nmodel performance at lower costs. Polyrating can detect and quantify biases\naffecting human preferences, ensuring fairer model comparisons. Further,\nPolyrating can reduce the cost of human evaluations by up to $41\\%$ for new\nmodels and up to $77\\%$ for new tasks by leveraging existing benchmark scores.\nLastly, Polyrating enables direct comparisons of ratings across different\ntasks, providing a comprehensive understanding of an LLMs' strengths,\nweaknesses, and relative performance across different applications."
                },
                "authors": [
                    {
                        "name": "Jasper Dekoninck"
                    },
                    {
                        "name": "Maximilian Baader"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00696v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00696v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07503v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07503v1",
                "updated": "2025-02-11T12:11:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    12,
                    11,
                    40,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T12:11:40Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    12,
                    11,
                    40,
                    1,
                    42,
                    0
                ],
                "title": "Harnessing Language's Fractal Geometry with Recursive Inference Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Language's Fractal Geometry with Recursive Inference Scaling"
                },
                "summary": "Recent research in language modeling reveals two scaling effects: the\nwell-known improvement from increased training compute, and a lesser-known\nboost from applying more sophisticated or computationally intensive inference\nmethods. Inspired by recent findings on the fractal geometry of language, we\nintroduce Recursive INference Scaling (RINS) as a complementary, plug-in recipe\nfor scaling inference time. For a given fixed model architecture and training\ncompute budget, RINS substantially improves language modeling performance. It\nalso generalizes beyond pure language tasks, delivering gains in multimodal\nsystems, including a +2% improvement in 0-shot ImageNet accuracy for\nSigLIP-B/16. Additionally, by deriving data scaling laws, we show that RINS\nimproves both the asymptotic performance limits and the scaling exponents.\nThese advantages are maintained even when compared to state-of-the-art\nrecursive techniques like the \"repeat-all-over\" (RAO) strategy in Mobile LLM.\nFinally, stochastic RINS not only can enhance performance further but also\nprovides the flexibility to optionally forgo increased inference computation at\ntest time with minimal performance degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research in language modeling reveals two scaling effects: the\nwell-known improvement from increased training compute, and a lesser-known\nboost from applying more sophisticated or computationally intensive inference\nmethods. Inspired by recent findings on the fractal geometry of language, we\nintroduce Recursive INference Scaling (RINS) as a complementary, plug-in recipe\nfor scaling inference time. For a given fixed model architecture and training\ncompute budget, RINS substantially improves language modeling performance. It\nalso generalizes beyond pure language tasks, delivering gains in multimodal\nsystems, including a +2% improvement in 0-shot ImageNet accuracy for\nSigLIP-B/16. Additionally, by deriving data scaling laws, we show that RINS\nimproves both the asymptotic performance limits and the scaling exponents.\nThese advantages are maintained even when compared to state-of-the-art\nrecursive techniques like the \"repeat-all-over\" (RAO) strategy in Mobile LLM.\nFinally, stochastic RINS not only can enhance performance further but also\nprovides the flexibility to optionally forgo increased inference computation at\ntest time with minimal performance degradation."
                },
                "authors": [
                    {
                        "name": "Ibrahim Alabdulmohsin"
                    },
                    {
                        "name": "Xiaohua Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohua Zhai"
                },
                "author": "Xiaohua Zhai",
                "arxiv_comment": "18 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07503v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07503v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04411v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04411v2",
                "updated": "2025-02-11T12:09:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    12,
                    9,
                    51,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-06T11:26:30Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    11,
                    26,
                    30,
                    3,
                    37,
                    0
                ],
                "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and\n  Uncertainty Based Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and\n  Uncertainty Based Routing"
                },
                "summary": "Model merging aggregates Large Language Models (LLMs) finetuned on different\ntasks into a stronger one. However, parameter conflicts between models leads to\nperformance degradation in averaging. While model routing addresses this issue\nby selecting individual models during inference, it imposes excessive storage\nand compute costs, and fails to leverage the common knowledge from different\nmodels. In this work, we observe that different layers exhibit varying levels\nof parameter conflicts. Building on this insight, we average layers with\nminimal parameter conflicts and use a novel task-level expert routing for\nlayers with significant conflicts. To further reduce storage costs, inspired by\ntask arithmetic sparsity, we decouple multiple fine-tuned experts into a dense\nexpert and several sparse experts. Considering the out-of-distribution samples,\nwe select and merge appropriate experts based on the task uncertainty of the\ninput data. We conduct extensive experiments on both LLaMA and Qwen with\nvarying parameter scales, and evaluate on real-world reasoning tasks. Results\ndemonstrate that our method consistently achieves significant performance\nimprovements while requiring less system cost compared to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model merging aggregates Large Language Models (LLMs) finetuned on different\ntasks into a stronger one. However, parameter conflicts between models leads to\nperformance degradation in averaging. While model routing addresses this issue\nby selecting individual models during inference, it imposes excessive storage\nand compute costs, and fails to leverage the common knowledge from different\nmodels. In this work, we observe that different layers exhibit varying levels\nof parameter conflicts. Building on this insight, we average layers with\nminimal parameter conflicts and use a novel task-level expert routing for\nlayers with significant conflicts. To further reduce storage costs, inspired by\ntask arithmetic sparsity, we decouple multiple fine-tuned experts into a dense\nexpert and several sparse experts. Considering the out-of-distribution samples,\nwe select and merge appropriate experts based on the task uncertainty of the\ninput data. We conduct extensive experiments on both LLaMA and Qwen with\nvarying parameter scales, and evaluate on real-world reasoning tasks. Results\ndemonstrate that our method consistently achieves significant performance\nimprovements while requiring less system cost compared to existing methods."
                },
                "authors": [
                    {
                        "name": "Kunfeng Lai"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Xinglin Pan"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Haolan Chen"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "work in progress. arXiv admin note: text overlap with\n  arXiv:2405.09673 by other authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04411v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04411v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07495v1",
                "updated": "2025-02-11T11:54:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    11,
                    54,
                    56,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T11:54:56Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    11,
                    54,
                    56,
                    1,
                    42,
                    0
                ],
                "title": "LLM-Sketch: Enhancing Network Sketches with LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Sketch: Enhancing Network Sketches with LLM"
                },
                "summary": "Network stream mining is fundamental to many network operations. Sketches, as\ncompact data structures that offer low memory overhead with bounded accuracy,\nhave emerged as a promising solution for network stream mining. Recent studies\nattempt to optimize sketches using machine learning; however, these approaches\nface the challenges of lacking adaptivity to dynamic networks and incurring\nhigh training costs. In this paper, we propose LLM-Sketch, based on the insight\nthat fields beyond the flow IDs in packet headers can also help infer flow\nsizes. By using a two-tier data structure and separately recording large and\nsmall flows, LLM-Sketch improves accuracy while minimizing memory usage.\nFurthermore, it leverages fine-tuned large language models (LLMs) to reliably\nestimate flow sizes. We evaluate LLM-Sketch on three representative tasks, and\nthe results demonstrate that LLM-Sketch outperforms state-of-the-art methods by\nachieving a $7.5\\times$ accuracy improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network stream mining is fundamental to many network operations. Sketches, as\ncompact data structures that offer low memory overhead with bounded accuracy,\nhave emerged as a promising solution for network stream mining. Recent studies\nattempt to optimize sketches using machine learning; however, these approaches\nface the challenges of lacking adaptivity to dynamic networks and incurring\nhigh training costs. In this paper, we propose LLM-Sketch, based on the insight\nthat fields beyond the flow IDs in packet headers can also help infer flow\nsizes. By using a two-tier data structure and separately recording large and\nsmall flows, LLM-Sketch improves accuracy while minimizing memory usage.\nFurthermore, it leverages fine-tuned large language models (LLMs) to reliably\nestimate flow sizes. We evaluate LLM-Sketch on three representative tasks, and\nthe results demonstrate that LLM-Sketch outperforms state-of-the-art methods by\nachieving a $7.5\\times$ accuracy improvement."
                },
                "authors": [
                    {
                        "name": "Yuanpeng Li"
                    },
                    {
                        "name": "Zhen Xu"
                    },
                    {
                        "name": "Zongwei Lv"
                    },
                    {
                        "name": "Yannan Hu"
                    },
                    {
                        "name": "Yong Cui"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12956v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12956v2",
                "updated": "2025-02-11T11:50:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    11,
                    50,
                    15,
                    1,
                    42,
                    0
                ],
                "published": "2025-01-22T15:29:09Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    29,
                    9,
                    2,
                    22,
                    0
                ],
                "title": "GANQ: GPU-Adaptive Non-Uniform Quantization for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GANQ: GPU-Adaptive Non-Uniform Quantization for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) face significant deployment challenges due to\ntheir substantial resource requirements. While low-bit quantized weights can\nreduce memory usage and improve inference efficiency, current hardware lacks\nnative support for mixed-precision General Matrix Multiplication (mpGEMM),\nresulting in inefficient dequantization-based implementations. Moreover,\nuniform quantization methods often fail to capture weight distributions\nadequately, leading to performance degradation. We propose GANQ (GPU-Adaptive\nNon-Uniform Quantization), a layer-wise post-training non-uniform quantization\nframework optimized for hardware-efficient lookup table-based mpGEMM. GANQ\nachieves superior quantization performance by utilizing a training-free,\nGPU-adaptive optimization algorithm to efficiently reduce layer-wise\nquantization errors. Extensive experiments demonstrate GANQ's ability to reduce\nthe perplexity gap from the FP16 baseline compared to state-of-the-art methods\nfor both 3-bit and 4-bit quantization. Furthermore, when deployed on a single\nNVIDIA RTX 4090 GPU, GANQ's quantized models achieve up to 2.57$\\times$ speedup\nover the baseline, advancing memory and inference efficiency in LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) face significant deployment challenges due to\ntheir substantial resource requirements. While low-bit quantized weights can\nreduce memory usage and improve inference efficiency, current hardware lacks\nnative support for mixed-precision General Matrix Multiplication (mpGEMM),\nresulting in inefficient dequantization-based implementations. Moreover,\nuniform quantization methods often fail to capture weight distributions\nadequately, leading to performance degradation. We propose GANQ (GPU-Adaptive\nNon-Uniform Quantization), a layer-wise post-training non-uniform quantization\nframework optimized for hardware-efficient lookup table-based mpGEMM. GANQ\nachieves superior quantization performance by utilizing a training-free,\nGPU-adaptive optimization algorithm to efficiently reduce layer-wise\nquantization errors. Extensive experiments demonstrate GANQ's ability to reduce\nthe perplexity gap from the FP16 baseline compared to state-of-the-art methods\nfor both 3-bit and 4-bit quantization. Furthermore, when deployed on a single\nNVIDIA RTX 4090 GPU, GANQ's quantized models achieve up to 2.57$\\times$ speedup\nover the baseline, advancing memory and inference efficiency in LLM deployment."
                },
                "authors": [
                    {
                        "name": "Pengxiang Zhao"
                    },
                    {
                        "name": "Xiaoming Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoming Yuan"
                },
                "author": "Xiaoming Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12956v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12956v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07490v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07490v1",
                "updated": "2025-02-11T11:49:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    11,
                    49,
                    3,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T11:49:03Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    11,
                    49,
                    3,
                    1,
                    42,
                    0
                ],
                "title": "Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn\n  More",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn\n  More"
                },
                "summary": "Large Language Models (LLMs) are discovered to suffer from accurately\nretrieving key information. To address this, we propose Mask-Enhanced\nAutoregressive Prediction (MEAP), a simple yet effective training paradigm that\nseamlessly integrates Masked Language Modeling (MLM) into Next-Token Prediction\n(NTP) to enhance the latter's in-context retrieval capabilities. Specifically,\nMEAP first randomly masks a small fraction of input tokens and then directly\nperforms the standard next-token prediction autoregressive using a decoder-only\nTransformer. MEAP eliminates the need for bidirectional attention or\nencoder-decoder architectures for MLM, incurring no additional computational\noverhead during pre-training or inference. Intensive experiments demonstrate\nthat MEAP substantially outperforms NTP on key information retrieval and\nlong-context reasoning tasks, while performing on par or better on commonsense\nreasoning tasks. The benefits of MEAP also extend to supervised fine-tuning,\nwhere it shows remarkable advantages in lost-in-the-middle scenarios,\noutperforming NTP by 11.77 percentage points. Our analysis indicates that\nMEAP's effectiveness arises from its ability to promote more distinguishable\nattention scores by concentrating on a reduced set of non-masked tokens. This\nmechanism improves the model's focus on task-relevant signals while mitigating\nthe influence of peripheral context. These findings position MEAP as a\npromising training paradigm for large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are discovered to suffer from accurately\nretrieving key information. To address this, we propose Mask-Enhanced\nAutoregressive Prediction (MEAP), a simple yet effective training paradigm that\nseamlessly integrates Masked Language Modeling (MLM) into Next-Token Prediction\n(NTP) to enhance the latter's in-context retrieval capabilities. Specifically,\nMEAP first randomly masks a small fraction of input tokens and then directly\nperforms the standard next-token prediction autoregressive using a decoder-only\nTransformer. MEAP eliminates the need for bidirectional attention or\nencoder-decoder architectures for MLM, incurring no additional computational\noverhead during pre-training or inference. Intensive experiments demonstrate\nthat MEAP substantially outperforms NTP on key information retrieval and\nlong-context reasoning tasks, while performing on par or better on commonsense\nreasoning tasks. The benefits of MEAP also extend to supervised fine-tuning,\nwhere it shows remarkable advantages in lost-in-the-middle scenarios,\noutperforming NTP by 11.77 percentage points. Our analysis indicates that\nMEAP's effectiveness arises from its ability to promote more distinguishable\nattention scores by concentrating on a reduced set of non-masked tokens. This\nmechanism improves the model's focus on task-relevant signals while mitigating\nthe influence of peripheral context. These findings position MEAP as a\npromising training paradigm for large language models."
                },
                "authors": [
                    {
                        "name": "Xialie Zhuang"
                    },
                    {
                        "name": "Zhikai Jia"
                    },
                    {
                        "name": "Jianjin Li"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Zheng Cao"
                    },
                    {
                        "name": "Shiwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Shiwei Liu"
                },
                "author": "Shiwei Liu",
                "arxiv_comment": "15 pages,7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07490v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07490v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07487v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07487v1",
                "updated": "2025-02-11T11:46:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    11,
                    46,
                    38,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T11:46:38Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    11,
                    46,
                    38,
                    1,
                    42,
                    0
                ],
                "title": "Multi-Agent Collaboration for Multilingual Code Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Collaboration for Multilingual Code Instruction Tuning"
                },
                "summary": "Recent advancement in code understanding and generation demonstrates that\ncode LLMs fine-tuned on a high-quality instruction dataset can gain powerful\ncapabilities to address wide-ranging code-related tasks. However, most previous\nexisting methods mainly view each programming language in isolation and ignore\nthe knowledge transfer among different programming languages. To bridge the gap\namong different programming languages, we introduce a novel multi-agent\ncollaboration framework to enhance multilingual instruction tuning for code\nLLMs, where multiple language-specific intelligent agent components with\ngeneration memory work together to transfer knowledge from one language to\nanother efficiently and effectively. Specifically, we first generate the\nlanguage-specific instruction data from the code snippets and then provide the\ngenerated data as the seed data for language-specific agents. Multiple\nlanguage-specific agents discuss and collaborate to formulate a new instruction\nand its corresponding solution (A new programming language or existing\nprogramming language), To further encourage the cross-lingual transfer, each\nagent stores its generation history as memory and then summarizes its merits\nand faults. Finally, the high-quality multilingual instruction data is used to\nencourage knowledge transfer among different programming languages to train\nQwen2.5-xCoder. Experimental results on multilingual programming benchmarks\ndemonstrate the superior performance of Qwen2.5-xCoder in sharing common\nknowledge, highlighting its potential to reduce the cross-lingual gap.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancement in code understanding and generation demonstrates that\ncode LLMs fine-tuned on a high-quality instruction dataset can gain powerful\ncapabilities to address wide-ranging code-related tasks. However, most previous\nexisting methods mainly view each programming language in isolation and ignore\nthe knowledge transfer among different programming languages. To bridge the gap\namong different programming languages, we introduce a novel multi-agent\ncollaboration framework to enhance multilingual instruction tuning for code\nLLMs, where multiple language-specific intelligent agent components with\ngeneration memory work together to transfer knowledge from one language to\nanother efficiently and effectively. Specifically, we first generate the\nlanguage-specific instruction data from the code snippets and then provide the\ngenerated data as the seed data for language-specific agents. Multiple\nlanguage-specific agents discuss and collaborate to formulate a new instruction\nand its corresponding solution (A new programming language or existing\nprogramming language), To further encourage the cross-lingual transfer, each\nagent stores its generation history as memory and then summarizes its merits\nand faults. Finally, the high-quality multilingual instruction data is used to\nencourage knowledge transfer among different programming languages to train\nQwen2.5-xCoder. Experimental results on multilingual programming benchmarks\ndemonstrate the superior performance of Qwen2.5-xCoder in sharing common\nknowledge, highlighting its potential to reduce the cross-lingual gap."
                },
                "authors": [
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Jiaxi Yang"
                    },
                    {
                        "name": "Yibo Miao"
                    },
                    {
                        "name": "Shanghaoran Quan"
                    },
                    {
                        "name": "Zhenhe Wu"
                    },
                    {
                        "name": "Qiyao Peng"
                    },
                    {
                        "name": "Liqun Yang"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Zeyu Cui"
                    },
                    {
                        "name": "Binyuan Hui"
                    },
                    {
                        "name": "Junyang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Junyang Lin"
                },
                "author": "Junyang Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07487v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07487v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13341v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13341v2",
                "updated": "2025-02-11T11:39:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    11,
                    39,
                    18,
                    1,
                    42,
                    0
                ],
                "published": "2024-10-17T08:49:42Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    8,
                    49,
                    42,
                    3,
                    291,
                    0
                ],
                "title": "Limits to scalable evaluation at the frontier: LLM as Judge won't beat\n  twice the data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Limits to scalable evaluation at the frontier: LLM as Judge won't beat\n  twice the data"
                },
                "summary": "High quality annotations are increasingly a bottleneck in the explosively\ngrowing machine learning ecosystem. Scalable evaluation methods that avoid\ncostly annotation have therefore become an important research ambition. Many\nhope to use strong existing models in lieu of costly labels to provide cheap\nmodel evaluations. Unfortunately, this method of using models as judges\nintroduces biases, such as self-preferencing, that can distort model\ncomparisons. An emerging family of debiasing tools promises to fix these issues\nby using a few high quality labels to debias a large number of model judgments.\nIn this paper, we study how far such debiasing methods, in principle, can go.\nOur main result shows that when the judge is no more accurate than the\nevaluated model, no debiasing method can decrease the required amount of ground\ntruth labels by more than half. Our result speaks to the severe limitations of\nthe LLM-as-a-judge paradigm at the evaluation frontier where the goal is to\nassess newly released models that are possibly better than the judge. Through\nan empirical evaluation, we demonstrate that the sample size savings achievable\nin practice are even more modest than what our theoretical limit suggests.\nAlong the way, our work provides new observations about debiasing methods for\nmodel evaluation, and points out promising avenues for future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High quality annotations are increasingly a bottleneck in the explosively\ngrowing machine learning ecosystem. Scalable evaluation methods that avoid\ncostly annotation have therefore become an important research ambition. Many\nhope to use strong existing models in lieu of costly labels to provide cheap\nmodel evaluations. Unfortunately, this method of using models as judges\nintroduces biases, such as self-preferencing, that can distort model\ncomparisons. An emerging family of debiasing tools promises to fix these issues\nby using a few high quality labels to debias a large number of model judgments.\nIn this paper, we study how far such debiasing methods, in principle, can go.\nOur main result shows that when the judge is no more accurate than the\nevaluated model, no debiasing method can decrease the required amount of ground\ntruth labels by more than half. Our result speaks to the severe limitations of\nthe LLM-as-a-judge paradigm at the evaluation frontier where the goal is to\nassess newly released models that are possibly better than the judge. Through\nan empirical evaluation, we demonstrate that the sample size savings achievable\nin practice are even more modest than what our theoretical limit suggests.\nAlong the way, our work provides new observations about debiasing methods for\nmodel evaluation, and points out promising avenues for future work."
                },
                "authors": [
                    {
                        "name": "Florian E. Dorner"
                    },
                    {
                        "name": "Vivian Y. Nastl"
                    },
                    {
                        "name": "Moritz Hardt"
                    }
                ],
                "author_detail": {
                    "name": "Moritz Hardt"
                },
                "author": "Moritz Hardt",
                "arxiv_comment": "ICLR 2025; 28 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13341v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13341v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07474v1",
                "updated": "2025-02-11T11:34:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    11,
                    34,
                    33,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T11:34:33Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    11,
                    34,
                    33,
                    1,
                    42,
                    0
                ],
                "title": "ETimeline: An Extensive Timeline Generation Dataset based on Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ETimeline: An Extensive Timeline Generation Dataset based on Large\n  Language Model"
                },
                "summary": "Timeline generation is of great significance for a comprehensive\nunderstanding of the development of events over time. Its goal is to organize\nnews chronologically, which helps to identify patterns and trends that may be\nobscured when viewing news in isolation, making it easier to track the\ndevelopment of stories and understand the interrelationships between key\nevents. Timelines are now common in various commercial products, but academic\nresearch in this area is notably scarce. Additionally, the current datasets are\nin need of refinement for enhanced utility and expanded coverage. In this\npaper, we propose ETimeline, which encompasses over $13,000$ news articles,\nspanning $600$ bilingual timelines across $28$ news domains. Specifically, we\ngather a candidate pool of more than $120,000$ news articles and employ the\nlarge language model (LLM) Pipeline to improve performance, ultimately yielding\nthe ETimeline. The data analysis underscores the appeal of ETimeline.\nAdditionally, we also provide the news pool data for further research and\nanalysis. This work contributes to the advancement of timeline generation\nresearch and supports a wide range of tasks, including topic generation and\nevent relationships. We believe that this dataset will serve as a catalyst for\ninnovative research and bridge the gap between academia and industry in\nunderstanding the practical application of technology services. The dataset is\navailable at https://zenodo.org/records/11392212",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timeline generation is of great significance for a comprehensive\nunderstanding of the development of events over time. Its goal is to organize\nnews chronologically, which helps to identify patterns and trends that may be\nobscured when viewing news in isolation, making it easier to track the\ndevelopment of stories and understand the interrelationships between key\nevents. Timelines are now common in various commercial products, but academic\nresearch in this area is notably scarce. Additionally, the current datasets are\nin need of refinement for enhanced utility and expanded coverage. In this\npaper, we propose ETimeline, which encompasses over $13,000$ news articles,\nspanning $600$ bilingual timelines across $28$ news domains. Specifically, we\ngather a candidate pool of more than $120,000$ news articles and employ the\nlarge language model (LLM) Pipeline to improve performance, ultimately yielding\nthe ETimeline. The data analysis underscores the appeal of ETimeline.\nAdditionally, we also provide the news pool data for further research and\nanalysis. This work contributes to the advancement of timeline generation\nresearch and supports a wide range of tasks, including topic generation and\nevent relationships. We believe that this dataset will serve as a catalyst for\ninnovative research and bridge the gap between academia and industry in\nunderstanding the practical application of technology services. The dataset is\navailable at https://zenodo.org/records/11392212"
                },
                "authors": [
                    {
                        "name": "Xiaochen Liu"
                    },
                    {
                        "name": "Yanan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yanan Zhang"
                },
                "author": "Yanan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07469v1",
                "updated": "2025-02-11T11:25:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    11,
                    25,
                    10,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T11:25:10Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    11,
                    25,
                    10,
                    1,
                    42,
                    0
                ],
                "title": "5D Neural Surrogates for Nonlinear Gyrokinetic Simulations of Plasma\n  Turbulence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "5D Neural Surrogates for Nonlinear Gyrokinetic Simulations of Plasma\n  Turbulence"
                },
                "summary": "Nuclear fusion plays a pivotal role in the quest for reliable and sustainable\nenergy production. A major roadblock to achieving commercially viable fusion\npower is understanding plasma turbulence, which can significantly degrade\nplasma confinement. Modelling turbulence is crucial to design performing plasma\nscenarios for next-generation reactor-class devices and current experimental\nmachines. The nonlinear gyrokinetic equation underpinning turbulence modelling\nevolves a 5D distribution function over time. Solving this equation numerically\nis extremely expensive, requiring up to weeks for a single run to converge,\nmaking it unfeasible for iterative optimisation and control studies. In this\nwork, we propose a method for training neural surrogates for 5D gyrokinetic\nsimulations. Our method extends a hierarchical vision transformer to five\ndimensions and is trained on the 5D distribution function for the adiabatic\nelectron approximation. We demonstrate that our model can accurately infer\ndownstream physical quantities such as heat flux time trace and electrostatic\npotentials for single-step predictions two orders of magnitude faster than\nnumerical codes. Our work paves the way towards neural surrogates for plasma\nturbulence simulations to accelerate deployment of commercial energy production\nvia nuclear fusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nuclear fusion plays a pivotal role in the quest for reliable and sustainable\nenergy production. A major roadblock to achieving commercially viable fusion\npower is understanding plasma turbulence, which can significantly degrade\nplasma confinement. Modelling turbulence is crucial to design performing plasma\nscenarios for next-generation reactor-class devices and current experimental\nmachines. The nonlinear gyrokinetic equation underpinning turbulence modelling\nevolves a 5D distribution function over time. Solving this equation numerically\nis extremely expensive, requiring up to weeks for a single run to converge,\nmaking it unfeasible for iterative optimisation and control studies. In this\nwork, we propose a method for training neural surrogates for 5D gyrokinetic\nsimulations. Our method extends a hierarchical vision transformer to five\ndimensions and is trained on the 5D distribution function for the adiabatic\nelectron approximation. We demonstrate that our model can accurately infer\ndownstream physical quantities such as heat flux time trace and electrostatic\npotentials for single-step predictions two orders of magnitude faster than\nnumerical codes. Our work paves the way towards neural surrogates for plasma\nturbulence simulations to accelerate deployment of commercial energy production\nvia nuclear fusion."
                },
                "authors": [
                    {
                        "name": "Gianluca Galletti"
                    },
                    {
                        "name": "Fabian Paischer"
                    },
                    {
                        "name": "Paul Setinek"
                    },
                    {
                        "name": "William Hornsby"
                    },
                    {
                        "name": "Lorenzo Zanisi"
                    },
                    {
                        "name": "Naomi Carey"
                    },
                    {
                        "name": "Stanislas Pamela"
                    },
                    {
                        "name": "Johannes Brandstetter"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Brandstetter"
                },
                "author": "Johannes Brandstetter",
                "arxiv_comment": "6 pages (+ references and appendix)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16125v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16125v3",
                "updated": "2025-02-11T11:19:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    11,
                    19,
                    40,
                    1,
                    42,
                    0
                ],
                "published": "2025-01-27T15:12:27Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    12,
                    27,
                    0,
                    27,
                    0
                ],
                "title": "SampleLLM: Optimizing Tabular Data Synthesis in Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SampleLLM: Optimizing Tabular Data Synthesis in Recommendations"
                },
                "summary": "Tabular data synthesis is crucial in machine learning, yet existing general\nmethods-primarily based on statistical or deep learning models-are highly\ndata-dependent and often fall short in recommender systems. This limitation\narises from their difficulty in capturing complex distributions and\nunderstanding feature relationships from sparse and limited data, along with\ntheir inability to grasp semantic feature relations. Recently, Large Language\nModels (LLMs) have shown potential in generating synthetic data samples through\nfew-shot learning and semantic understanding. However, they often suffer from\ninconsistent distribution and lack of diversity due to their inherent\ndistribution disparity with the target dataset. To address these challenges and\nenhance tabular data synthesis for recommendation tasks, we propose a novel\ntwo-stage framework named SampleLLM to improve the quality of LLM-based tabular\ndata synthesis for recommendations by ensuring better distribution alignment.\nIn the first stage, SampleLLM employs LLMs with Chain-of-Thought prompts and\ndiverse exemplars to generate data that closely aligns with the target dataset\ndistribution, even when input samples are limited. The second stage uses an\nadvanced feature attribution-based importance sampling method to refine feature\nrelationships within the synthesized data, reducing any distribution biases\nintroduced by the LLM. Experimental results on three recommendation datasets,\ntwo general datasets, and online deployment illustrate that SampleLLM\nsignificantly surpasses existing methods for recommendation tasks and holds\npromise for a broader range of tabular data scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular data synthesis is crucial in machine learning, yet existing general\nmethods-primarily based on statistical or deep learning models-are highly\ndata-dependent and often fall short in recommender systems. This limitation\narises from their difficulty in capturing complex distributions and\nunderstanding feature relationships from sparse and limited data, along with\ntheir inability to grasp semantic feature relations. Recently, Large Language\nModels (LLMs) have shown potential in generating synthetic data samples through\nfew-shot learning and semantic understanding. However, they often suffer from\ninconsistent distribution and lack of diversity due to their inherent\ndistribution disparity with the target dataset. To address these challenges and\nenhance tabular data synthesis for recommendation tasks, we propose a novel\ntwo-stage framework named SampleLLM to improve the quality of LLM-based tabular\ndata synthesis for recommendations by ensuring better distribution alignment.\nIn the first stage, SampleLLM employs LLMs with Chain-of-Thought prompts and\ndiverse exemplars to generate data that closely aligns with the target dataset\ndistribution, even when input samples are limited. The second stage uses an\nadvanced feature attribution-based importance sampling method to refine feature\nrelationships within the synthesized data, reducing any distribution biases\nintroduced by the LLM. Experimental results on three recommendation datasets,\ntwo general datasets, and online deployment illustrate that SampleLLM\nsignificantly surpasses existing methods for recommendation tasks and holds\npromise for a broader range of tabular data scenarios."
                },
                "authors": [
                    {
                        "name": "Jingtong Gao"
                    },
                    {
                        "name": "Zhaocheng Du"
                    },
                    {
                        "name": "Xiaopeng Li"
                    },
                    {
                        "name": "Yichao Wang"
                    },
                    {
                        "name": "Xiangyang Li"
                    },
                    {
                        "name": "Huifeng Guo"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhao"
                },
                "author": "Xiangyu Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16125v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16125v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07460v1",
                "updated": "2025-02-11T11:11:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    11,
                    11,
                    5,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T11:11:05Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    11,
                    11,
                    5,
                    1,
                    42,
                    0
                ],
                "title": "Logarithmic Regret for Online KL-Regularized Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logarithmic Regret for Online KL-Regularized Reinforcement Learning"
                },
                "summary": "Recent advances in Reinforcement Learning from Human Feedback (RLHF) have\nshown that KL-regularization plays a pivotal role in improving the efficiency\nof RL fine-tuning for large language models (LLMs). Despite its empirical\nadvantage, the theoretical difference between KL-regularized RL and standard RL\nremains largely under-explored. While there is a recent line of work on the\ntheoretical analysis of KL-regularized objective in decision making\n\\citep{xiong2024iterative, xie2024exploratory,zhao2024sharp}, these analyses\neither reduce to the traditional RL setting or rely on strong coverage\nassumptions. In this paper, we propose an optimism-based KL-regularized online\ncontextual bandit algorithm, and provide a novel analysis of its regret. By\ncarefully leveraging the benign optimization landscape induced by the\nKL-regularization and the optimistic reward estimation, our algorithm achieves\nan $\\mathcal{O}\\big(\\eta\\log (N_{\\mathcal R} T)\\cdot d_{\\mathcal R}\\big)$\nlogarithmic regret bound, where $\\eta, N_{\\mathcal R},T,d_{\\mathcal R}$ denote\nthe KL-regularization parameter, the cardinality of the reward function class,\nnumber of rounds, and the complexity of the reward function class. Furthermore,\nwe extend our algorithm and analysis to reinforcement learning by developing a\nnovel decomposition over transition steps and also obtain a similar logarithmic\nregret bound.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Reinforcement Learning from Human Feedback (RLHF) have\nshown that KL-regularization plays a pivotal role in improving the efficiency\nof RL fine-tuning for large language models (LLMs). Despite its empirical\nadvantage, the theoretical difference between KL-regularized RL and standard RL\nremains largely under-explored. While there is a recent line of work on the\ntheoretical analysis of KL-regularized objective in decision making\n\\citep{xiong2024iterative, xie2024exploratory,zhao2024sharp}, these analyses\neither reduce to the traditional RL setting or rely on strong coverage\nassumptions. In this paper, we propose an optimism-based KL-regularized online\ncontextual bandit algorithm, and provide a novel analysis of its regret. By\ncarefully leveraging the benign optimization landscape induced by the\nKL-regularization and the optimistic reward estimation, our algorithm achieves\nan $\\mathcal{O}\\big(\\eta\\log (N_{\\mathcal R} T)\\cdot d_{\\mathcal R}\\big)$\nlogarithmic regret bound, where $\\eta, N_{\\mathcal R},T,d_{\\mathcal R}$ denote\nthe KL-regularization parameter, the cardinality of the reward function class,\nnumber of rounds, and the complexity of the reward function class. Furthermore,\nwe extend our algorithm and analysis to reinforcement learning by developing a\nnovel decomposition over transition steps and also obtain a similar logarithmic\nregret bound."
                },
                "authors": [
                    {
                        "name": "Heyang Zhao"
                    },
                    {
                        "name": "Chenlu Ye"
                    },
                    {
                        "name": "Wei Xiong"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07459v1",
                "updated": "2025-02-11T11:07:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    11,
                    7,
                    44,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T11:07:44Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    11,
                    7,
                    44,
                    1,
                    42,
                    0
                ],
                "title": "PerCul: A Story-Driven Cultural Evaluation of LLMs in Persian",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PerCul: A Story-Driven Cultural Evaluation of LLMs in Persian"
                },
                "summary": "Large language models predominantly reflect Western cultures, largely due to\nthe dominance of English-centric training data. This imbalance presents a\nsignificant challenge, as LLMs are increasingly used across diverse contexts\nwithout adequate evaluation of their cultural competence in non-English\nlanguages, including Persian. To address this gap, we introduce PerCul, a\ncarefully constructed dataset designed to assess the sensitivity of LLMs toward\nPersian culture. PerCul features story-based, multiple-choice questions that\ncapture culturally nuanced scenarios. Unlike existing benchmarks, PerCul is\ncurated with input from native Persian annotators to ensure authenticity and to\nprevent the use of translation as a shortcut. We evaluate several\nstate-of-the-art multilingual and Persian-specific LLMs, establishing a\nfoundation for future research in cross-cultural NLP evaluation. Our\nexperiments demonstrate a 11.3% gap between best closed source model and\nlayperson baseline while the gap increases to 21.3% by using the best\nopen-weight model. You can access the dataset from here:\nhttps://huggingface.co/datasets/teias-ai/percul",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models predominantly reflect Western cultures, largely due to\nthe dominance of English-centric training data. This imbalance presents a\nsignificant challenge, as LLMs are increasingly used across diverse contexts\nwithout adequate evaluation of their cultural competence in non-English\nlanguages, including Persian. To address this gap, we introduce PerCul, a\ncarefully constructed dataset designed to assess the sensitivity of LLMs toward\nPersian culture. PerCul features story-based, multiple-choice questions that\ncapture culturally nuanced scenarios. Unlike existing benchmarks, PerCul is\ncurated with input from native Persian annotators to ensure authenticity and to\nprevent the use of translation as a shortcut. We evaluate several\nstate-of-the-art multilingual and Persian-specific LLMs, establishing a\nfoundation for future research in cross-cultural NLP evaluation. Our\nexperiments demonstrate a 11.3% gap between best closed source model and\nlayperson baseline while the gap increases to 21.3% by using the best\nopen-weight model. You can access the dataset from here:\nhttps://huggingface.co/datasets/teias-ai/percul"
                },
                "authors": [
                    {
                        "name": "Erfan Moosavi Monazzah"
                    },
                    {
                        "name": "Vahid Rahimzadeh"
                    },
                    {
                        "name": "Yadollah Yaghoobzadeh"
                    },
                    {
                        "name": "Azadeh Shakery"
                    },
                    {
                        "name": "Mohammad Taher Pilehvar"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Taher Pilehvar"
                },
                "author": "Mohammad Taher Pilehvar",
                "arxiv_comment": "Accepted at NAACL 2025 Main Conference, the dataset is available on\n  HuggingFace (see https://huggingface.co/datasets/teias-ai/percul)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07222v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07222v2",
                "updated": "2025-02-11T11:02:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    11,
                    2,
                    10,
                    1,
                    42,
                    0
                ],
                "published": "2024-06-11T13:01:50Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    13,
                    1,
                    50,
                    1,
                    163,
                    0
                ],
                "title": "Improving Autoformalization using Type Checking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Autoformalization using Type Checking"
                },
                "summary": "Autoformalization, the automatic translation of unconstrained natural\nlanguage into formal languages, has garnered significant attention due to its\npotential applications in theorem proving, formal verification, and LLM output\nchecking. In this work, we analyze both current autoformalization methods and\nthe processes used to evaluate them, focusing specifically on the Lean 4\ntheorem proving language. We demonstrate that scaling type-check filtering with\nself-consistency techniques on top of existing methods significantly improves\nperformance, achieving absolute accuracy gains of up to +18.4\\% on ProofNet. To\nsupport reproducibility and further research, we release our code, including\nnew symbolic equivalence for Lean formulas. We also release new benchmarks: a\nnew research-level mathematics dataset RLM25, a corrected ProofNet, and\nProofNetVerif with labeled correct and incorrect autoformalization pairs for\nevaluating metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoformalization, the automatic translation of unconstrained natural\nlanguage into formal languages, has garnered significant attention due to its\npotential applications in theorem proving, formal verification, and LLM output\nchecking. In this work, we analyze both current autoformalization methods and\nthe processes used to evaluate them, focusing specifically on the Lean 4\ntheorem proving language. We demonstrate that scaling type-check filtering with\nself-consistency techniques on top of existing methods significantly improves\nperformance, achieving absolute accuracy gains of up to +18.4\\% on ProofNet. To\nsupport reproducibility and further research, we release our code, including\nnew symbolic equivalence for Lean formulas. We also release new benchmarks: a\nnew research-level mathematics dataset RLM25, a corrected ProofNet, and\nProofNetVerif with labeled correct and incorrect autoformalization pairs for\nevaluating metrics."
                },
                "authors": [
                    {
                        "name": "Auguste Poiroux"
                    },
                    {
                        "name": "Gail Weiss"
                    },
                    {
                        "name": "Viktor Kunak"
                    },
                    {
                        "name": "Antoine Bosselut"
                    }
                ],
                "author_detail": {
                    "name": "Antoine Bosselut"
                },
                "author": "Antoine Bosselut",
                "arxiv_comment": "New benchmarks released, see\n  https://github.com/augustepoiroux/RLMEval ,\n  https://huggingface.co/datasets/PAug/ProofNetSharp , and\n  https://huggingface.co/datasets/PAug/ProofNetVerif . For code, see\n  https://github.com/augustepoiroux/LeanInteract",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07222v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07222v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16701v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16701v3",
                "updated": "2025-02-11T10:53:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    10,
                    53,
                    20,
                    1,
                    42,
                    0
                ],
                "published": "2024-09-25T07:47:01Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    47,
                    1,
                    2,
                    269,
                    0
                ],
                "title": "Vulnerability-Triggering Test Case Generation from Third-Party Libraries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vulnerability-Triggering Test Case Generation from Third-Party Libraries"
                },
                "summary": "Open-source third-party libraries are widely used in software development.\nThese libraries offer substantial advantages in terms of time and resource\nsavings. However, a significant concern arises due to the publicly disclosed\nvulnerabilities within these libraries. Existing automated vulnerability\ndetection tools often suffer from false positives and fail to accurately assess\nthe propagation of inputs capable of triggering vulnerabilities from client\nprojects to vulnerable code in libraries. In this paper, we propose a novel\napproach called VULEUT (Vulnerability Exploit Unit Test Generation), which\ncombines vulnerability exploitation reachability analysis and LLM-based unit\ntest generation. VULEUT is designed to automatically verify the exploitability\nof vulnerabilities in third-party libraries commonly used in client software\nprojects. VULEUT first analyzes the client projects to determine the\nreachability of vulnerability conditions. And then, it leverages the Large\nLanguage Model (LLM) to generate unit tests for vulnerability confirmation. To\nevaluate the effectiveness of VULEUT, we collect 32 vulnerabilities from\nvarious third-party libraries and conduct experiments on 70 real client\nprojects. Besides, we also compare our approach with two representative tools,\ni.e., TRANSFER and VESTA. Our results demonstrate the effectiveness of VULEUT,\nwith 229 out of 292 generated unit tests successfully confirming vulnerability\nexploitation across 70 client projects, which outperforms baselines by 24%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-source third-party libraries are widely used in software development.\nThese libraries offer substantial advantages in terms of time and resource\nsavings. However, a significant concern arises due to the publicly disclosed\nvulnerabilities within these libraries. Existing automated vulnerability\ndetection tools often suffer from false positives and fail to accurately assess\nthe propagation of inputs capable of triggering vulnerabilities from client\nprojects to vulnerable code in libraries. In this paper, we propose a novel\napproach called VULEUT (Vulnerability Exploit Unit Test Generation), which\ncombines vulnerability exploitation reachability analysis and LLM-based unit\ntest generation. VULEUT is designed to automatically verify the exploitability\nof vulnerabilities in third-party libraries commonly used in client software\nprojects. VULEUT first analyzes the client projects to determine the\nreachability of vulnerability conditions. And then, it leverages the Large\nLanguage Model (LLM) to generate unit tests for vulnerability confirmation. To\nevaluate the effectiveness of VULEUT, we collect 32 vulnerabilities from\nvarious third-party libraries and conduct experiments on 70 real client\nprojects. Besides, we also compare our approach with two representative tools,\ni.e., TRANSFER and VESTA. Our results demonstrate the effectiveness of VULEUT,\nwith 229 out of 292 generated unit tests successfully confirming vulnerability\nexploitation across 70 client projects, which outperforms baselines by 24%."
                },
                "authors": [
                    {
                        "name": "Yi Gao"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Zirui Chen"
                    },
                    {
                        "name": "Xiaohu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohu Yang"
                },
                "author": "Xiaohu Yang",
                "arxiv_comment": "Published in 2nd Conference on AI Foundation Models and Software\n  Engineering (FORGE 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16701v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16701v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16739v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16739v2",
                "updated": "2025-02-11T10:52:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    10,
                    52,
                    54,
                    1,
                    42,
                    0
                ],
                "published": "2024-09-25T08:42:29Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    42,
                    29,
                    2,
                    269,
                    0
                ],
                "title": "Automated Unit Test Refactoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Unit Test Refactoring"
                },
                "summary": "Test smells arise from poor design practices and insufficient domain\nknowledge, which can lower the quality of test code and make it harder to\nmaintain and update. Manually refactoring test smells is time-consuming and\nerror-prone, highlighting the necessity for automated approaches. Current\nrule-based refactoring methods often struggle in scenarios not covered by\npredefined rules and lack the flexibility needed to handle diverse cases\neffectively. In this paper, we propose a novel approach called UTRefactor, a\ncontext-enhanced, LLM-based framework for automatic test refactoring in Java\nprojects. UTRefactor extracts relevant context from test code and leverages an\nexternal knowledge base that includes test smell definitions, descriptions, and\nDSL-based refactoring rules. By simulating the manual refactoring process\nthrough a chain-of-thought approach, UTRefactor guides the LLM to eliminate\ntest smells in a step-by-step process, ensuring both accuracy and consistency\nthroughout the refactoring. Additionally, we implement a checkpoint mechanism\nto facilitate comprehensive refactoring, particularly when multiple smells are\npresent. We evaluate UTRefactor on 879 tests from six open-source Java\nprojects, reducing the number of test smells from 2,375 to 265, achieving an\n89% reduction. UTRefactor outperforms direct LLM-based refactoring methods by\n61.82% in smell elimination and significantly surpasses the performance of a\nrule-based test smell refactoring tool. Our results demonstrate the\neffectiveness of UTRefactor in enhancing test code quality while minimizing\nmanual involvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test smells arise from poor design practices and insufficient domain\nknowledge, which can lower the quality of test code and make it harder to\nmaintain and update. Manually refactoring test smells is time-consuming and\nerror-prone, highlighting the necessity for automated approaches. Current\nrule-based refactoring methods often struggle in scenarios not covered by\npredefined rules and lack the flexibility needed to handle diverse cases\neffectively. In this paper, we propose a novel approach called UTRefactor, a\ncontext-enhanced, LLM-based framework for automatic test refactoring in Java\nprojects. UTRefactor extracts relevant context from test code and leverages an\nexternal knowledge base that includes test smell definitions, descriptions, and\nDSL-based refactoring rules. By simulating the manual refactoring process\nthrough a chain-of-thought approach, UTRefactor guides the LLM to eliminate\ntest smells in a step-by-step process, ensuring both accuracy and consistency\nthroughout the refactoring. Additionally, we implement a checkpoint mechanism\nto facilitate comprehensive refactoring, particularly when multiple smells are\npresent. We evaluate UTRefactor on 879 tests from six open-source Java\nprojects, reducing the number of test smells from 2,375 to 265, achieving an\n89% reduction. UTRefactor outperforms direct LLM-based refactoring methods by\n61.82% in smell elimination and significantly surpasses the performance of a\nrule-based test smell refactoring tool. Our results demonstrate the\neffectiveness of UTRefactor in enhancing test code quality while minimizing\nmanual involvement."
                },
                "authors": [
                    {
                        "name": "Yi Gao"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Xiaohu Yang"
                    },
                    {
                        "name": "Xin Xia"
                    }
                ],
                "author_detail": {
                    "name": "Xin Xia"
                },
                "author": "Xin Xia",
                "arxiv_comment": "Published in International Conference on the Foundations of Software\n  Engineering (FSE 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16739v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16739v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07445v1",
                "updated": "2025-02-11T10:43:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    10,
                    43,
                    36,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T10:43:36Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    10,
                    43,
                    36,
                    1,
                    42,
                    0
                ],
                "title": "Forget What You Know about LLMs Evaluations - LLMs are Like a Chameleon",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forget What You Know about LLMs Evaluations - LLMs are Like a Chameleon"
                },
                "summary": "Large language models (LLMs) often appear to excel on public benchmarks, but\nthese high scores may mask an overreliance on dataset-specific surface cues\nrather than true language understanding. We introduce the Chameleon Benchmark\nOverfit Detector (C-BOD), a meta-evaluation framework that systematically\ndistorts benchmark prompts via a parametric transformation and detects\noverfitting of LLMs. By rephrasing inputs while preserving their semantic\ncontent and labels, C-BOD exposes whether a model's performance is driven by\nmemorized patterns. Evaluated on the MMLU benchmark using 26 leading LLMs, our\nmethod reveals an average performance degradation of 2.15% under modest\nperturbations, with 20 out of 26 models exhibiting statistically significant\ndifferences. Notably, models with higher baseline accuracy exhibit larger\nperformance differences under perturbation, and larger LLMs tend to be more\nsensitive to rephrasings indicating that both cases may overrely on fixed\nprompt patterns. In contrast, the Llama family and models with lower baseline\naccuracy show insignificant degradation, suggesting reduced dependency on\nsuperficial cues. Moreover, C-BOD's dataset- and model-agnostic design allows\neasy integration into training pipelines to promote more robust language\nunderstanding. Our findings challenge the community to look beyond leaderboard\nscores and prioritize resilience and generalization in LLM evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often appear to excel on public benchmarks, but\nthese high scores may mask an overreliance on dataset-specific surface cues\nrather than true language understanding. We introduce the Chameleon Benchmark\nOverfit Detector (C-BOD), a meta-evaluation framework that systematically\ndistorts benchmark prompts via a parametric transformation and detects\noverfitting of LLMs. By rephrasing inputs while preserving their semantic\ncontent and labels, C-BOD exposes whether a model's performance is driven by\nmemorized patterns. Evaluated on the MMLU benchmark using 26 leading LLMs, our\nmethod reveals an average performance degradation of 2.15% under modest\nperturbations, with 20 out of 26 models exhibiting statistically significant\ndifferences. Notably, models with higher baseline accuracy exhibit larger\nperformance differences under perturbation, and larger LLMs tend to be more\nsensitive to rephrasings indicating that both cases may overrely on fixed\nprompt patterns. In contrast, the Llama family and models with lower baseline\naccuracy show insignificant degradation, suggesting reduced dependency on\nsuperficial cues. Moreover, C-BOD's dataset- and model-agnostic design allows\neasy integration into training pipelines to promote more robust language\nunderstanding. Our findings challenge the community to look beyond leaderboard\nscores and prioritize resilience and generalization in LLM evaluation."
                },
                "authors": [
                    {
                        "name": "Nurit Cohen-Inger"
                    },
                    {
                        "name": "Yehonatan Elisha"
                    },
                    {
                        "name": "Bracha Shapira"
                    },
                    {
                        "name": "Lior Rokach"
                    },
                    {
                        "name": "Seffi Cohen"
                    }
                ],
                "author_detail": {
                    "name": "Seffi Cohen"
                },
                "author": "Seffi Cohen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07443v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07443v1",
                "updated": "2025-02-11T10:37:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    10,
                    37,
                    20,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T10:37:20Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    10,
                    37,
                    20,
                    1,
                    42,
                    0
                ],
                "title": "Approximating Human Strategic Reasoning with LLM-Enhanced Recursive\n  Reasoners Leveraging Multi-agent Hypergames",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximating Human Strategic Reasoning with LLM-Enhanced Recursive\n  Reasoners Leveraging Multi-agent Hypergames"
                },
                "summary": "LLM-driven multi-agent-based simulations have been gaining traction with\napplications in game-theoretic and social simulations. While most\nimplementations seek to exploit or evaluate LLM-agentic reasoning, they often\ndo so with a weak notion of agency and simplified architectures. We implement a\nrole-based multi-agent strategic interaction framework tailored to\nsophisticated recursive reasoners, providing the means for systematic in-depth\ndevelopment and evaluation of strategic reasoning. Our game environment is\ngoverned by the umpire responsible for facilitating games, from matchmaking\nthrough move validation to environment management. Players incorporate\nstate-of-the-art LLMs in their decision mechanism, relying on a formal\nhypergame-based model of hierarchical beliefs. We use one-shot, 2-player beauty\ncontests to evaluate the recursive reasoning capabilities of the latest LLMs,\nproviding a comparison to an established baseline model from economics and data\nfrom human experiments. Furthermore, we introduce the foundations of an\nalternative semantic measure of reasoning to the k-level theory. Our\nexperiments show that artificial reasoners can outperform the baseline model in\nterms of both approximating human behaviour and reaching the optimal solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-driven multi-agent-based simulations have been gaining traction with\napplications in game-theoretic and social simulations. While most\nimplementations seek to exploit or evaluate LLM-agentic reasoning, they often\ndo so with a weak notion of agency and simplified architectures. We implement a\nrole-based multi-agent strategic interaction framework tailored to\nsophisticated recursive reasoners, providing the means for systematic in-depth\ndevelopment and evaluation of strategic reasoning. Our game environment is\ngoverned by the umpire responsible for facilitating games, from matchmaking\nthrough move validation to environment management. Players incorporate\nstate-of-the-art LLMs in their decision mechanism, relying on a formal\nhypergame-based model of hierarchical beliefs. We use one-shot, 2-player beauty\ncontests to evaluate the recursive reasoning capabilities of the latest LLMs,\nproviding a comparison to an established baseline model from economics and data\nfrom human experiments. Furthermore, we introduce the foundations of an\nalternative semantic measure of reasoning to the k-level theory. Our\nexperiments show that artificial reasoners can outperform the baseline model in\nterms of both approximating human behaviour and reaching the optimal solution."
                },
                "authors": [
                    {
                        "name": "Vince Trencsenyi"
                    },
                    {
                        "name": "Agnieszka Mensfelt"
                    },
                    {
                        "name": "Kostas Stathis"
                    }
                ],
                "author_detail": {
                    "name": "Kostas Stathis"
                },
                "author": "Kostas Stathis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07443v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07443v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04476v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04476v2",
                "updated": "2025-02-11T10:35:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    10,
                    35,
                    2,
                    1,
                    42,
                    0
                ],
                "published": "2024-11-19T15:40:16Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    15,
                    40,
                    16,
                    1,
                    324,
                    0
                ],
                "title": "The Moral Mind(s) of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Moral Mind(s) of Large Language Models"
                },
                "summary": "As large language models (LLMs) become integrated into decision-making across\nvarious sectors, key questions arise: do they exhibit an emergent \"moral mind\"\n- a consistent set of moral principles guiding their ethical judgments - and is\nthis reasoning uniform or diverse across models? To investigate this, we\npresented approximately forty models from major providers with a structured set\nof ethical scenarios, creating one of the largest datasets of its kind. Our\nrationality tests revealed that at least one model from each provider exhibited\nbehavior consistent with approximately stable moral principles, effectively\nacting as if nearly optimizing a utility function encoding ethical reasoning.\nWe estimated these utility functions and found that models tend to cluster\naround neutral ethical stances. To further characterize moral heterogeneity, we\napplied a non-parametric permutation approach, constructing a probabilistic\nsimilarity network based on revealed preference patterns. This analysis showed\nthat while approximately rational models share a core ethical structure,\ndifferences emerged: roughly half displayed greater moral adaptability,\nbridging diverse perspectives, while the remainder adhered to more rigid\nethical structures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become integrated into decision-making across\nvarious sectors, key questions arise: do they exhibit an emergent \"moral mind\"\n- a consistent set of moral principles guiding their ethical judgments - and is\nthis reasoning uniform or diverse across models? To investigate this, we\npresented approximately forty models from major providers with a structured set\nof ethical scenarios, creating one of the largest datasets of its kind. Our\nrationality tests revealed that at least one model from each provider exhibited\nbehavior consistent with approximately stable moral principles, effectively\nacting as if nearly optimizing a utility function encoding ethical reasoning.\nWe estimated these utility functions and found that models tend to cluster\naround neutral ethical stances. To further characterize moral heterogeneity, we\napplied a non-parametric permutation approach, constructing a probabilistic\nsimilarity network based on revealed preference patterns. This analysis showed\nthat while approximately rational models share a core ethical structure,\ndifferences emerged: roughly half displayed greater moral adaptability,\nbridging diverse perspectives, while the remainder adhered to more rigid\nethical structures."
                },
                "authors": [
                    {
                        "name": "Avner Seror"
                    }
                ],
                "author_detail": {
                    "name": "Avner Seror"
                },
                "author": "Avner Seror",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04476v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04476v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07436v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07436v1",
                "updated": "2025-02-11T10:24:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    10,
                    24,
                    57,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T10:24:57Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    10,
                    24,
                    57,
                    1,
                    42,
                    0
                ],
                "title": "Optimizing Knowledge Distillation in Transformers: Enabling Multi-Head\n  Attention without Alignment Barriers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Knowledge Distillation in Transformers: Enabling Multi-Head\n  Attention without Alignment Barriers"
                },
                "summary": "Knowledge distillation (KD) in transformers often faces challenges due to\nmisalignment in the number of attention heads between teacher and student\nmodels. Existing methods either require identical head counts or introduce\nprojectors to bridge dimensional gaps, limiting flexibility and efficiency. We\npropose Squeezing-Heads Distillation (SHD), a novel approach that enables\nseamless knowledge transfer between models with varying head counts by\ncompressing multi-head attention maps via efficient linear approximation.\nUnlike prior work, SHD eliminates alignment barriers without additional\nparameters or architectural modifications. Our method dynamically approximates\nthe combined effect of multiple teacher heads into fewer student heads,\npreserving fine-grained attention patterns while reducing redundancy.\nExperiments across language (LLaMA, GPT) and vision (DiT, MDT) generative and\nvision (DeiT) discriminative tasks demonstrate SHD's effectiveness: it\noutperforms logit-based and feature-alignment KD baselines, achieving\nstate-of-the-art results in image classification, image generation language\nfine-tuning, and language pre-training. The key innovations of flexible head\ncompression, projector-free design, and linear-time complexity make SHD a\nversatile and scalable solution for distilling modern transformers. This work\nbridges a critical gap in KD, enabling efficient deployment of compact models\nwithout compromising performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation (KD) in transformers often faces challenges due to\nmisalignment in the number of attention heads between teacher and student\nmodels. Existing methods either require identical head counts or introduce\nprojectors to bridge dimensional gaps, limiting flexibility and efficiency. We\npropose Squeezing-Heads Distillation (SHD), a novel approach that enables\nseamless knowledge transfer between models with varying head counts by\ncompressing multi-head attention maps via efficient linear approximation.\nUnlike prior work, SHD eliminates alignment barriers without additional\nparameters or architectural modifications. Our method dynamically approximates\nthe combined effect of multiple teacher heads into fewer student heads,\npreserving fine-grained attention patterns while reducing redundancy.\nExperiments across language (LLaMA, GPT) and vision (DiT, MDT) generative and\nvision (DeiT) discriminative tasks demonstrate SHD's effectiveness: it\noutperforms logit-based and feature-alignment KD baselines, achieving\nstate-of-the-art results in image classification, image generation language\nfine-tuning, and language pre-training. The key innovations of flexible head\ncompression, projector-free design, and linear-time complexity make SHD a\nversatile and scalable solution for distilling modern transformers. This work\nbridges a critical gap in KD, enabling efficient deployment of compact models\nwithout compromising performance."
                },
                "authors": [
                    {
                        "name": "Zhaodong Bing"
                    },
                    {
                        "name": "Linze Li"
                    },
                    {
                        "name": "Jiajun Liang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Liang"
                },
                "author": "Jiajun Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07436v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07436v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12663v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12663v2",
                "updated": "2025-02-11T10:21:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    10,
                    21,
                    16,
                    1,
                    42,
                    0
                ],
                "published": "2024-07-17T15:47:25Z",
                "published_parsed": [
                    2024,
                    7,
                    17,
                    15,
                    47,
                    25,
                    2,
                    199,
                    0
                ],
                "title": "Is That Rain? Understanding Effects on Visual Odometry Performance for\n  Autonomous UAVs and Efficient DNN-based Rain Classification at the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is That Rain? Understanding Effects on Visual Odometry Performance for\n  Autonomous UAVs and Efficient DNN-based Rain Classification at the Edge"
                },
                "summary": "The development of safe and reliable autonomous unmanned aerial vehicles\nrelies on the ability of the system to recognise and adapt to changes in the\nlocal environment based on sensor inputs. State-of-the-art local tracking and\ntrajectory planning are typically performed using camera sensor input to the\nflight control algorithm, but the extent to which environmental disturbances\nlike rain affect the performance of these systems is largely unknown. In this\npaper, we first describe the development of an open dataset comprising ~335k\nimages to examine these effects for seven different classes of precipitation\nconditions and show that a worst-case average tracking error of 1.5 m is\npossible for a state-of-the-art visual odometry system (VINS-Fusion). We then\nuse the dataset to train a set of deep neural network models suited to mobile\nand constrained deployment scenarios to determine the extent to which it may be\npossible to efficiently and accurately classify these `rainy' conditions. The\nmost lightweight of these models (MobileNetV3 small) can achieve an accuracy of\n90% with a memory footprint of just 1.28 MB and a frame rate of 93 FPS, which\nis suitable for deployment in resource-constrained and latency-sensitive\nsystems. We demonstrate a classification latency in the order of milliseconds\nusing typical flight computer hardware. Accordingly, such a model can feed into\nthe disturbance estimation component of an autonomous flight controller. In\naddition, data from unmanned aerial vehicles with the ability to accurately\ndetermine environmental conditions in real time may contribute to developing\nmore granular timely localised weather forecasting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of safe and reliable autonomous unmanned aerial vehicles\nrelies on the ability of the system to recognise and adapt to changes in the\nlocal environment based on sensor inputs. State-of-the-art local tracking and\ntrajectory planning are typically performed using camera sensor input to the\nflight control algorithm, but the extent to which environmental disturbances\nlike rain affect the performance of these systems is largely unknown. In this\npaper, we first describe the development of an open dataset comprising ~335k\nimages to examine these effects for seven different classes of precipitation\nconditions and show that a worst-case average tracking error of 1.5 m is\npossible for a state-of-the-art visual odometry system (VINS-Fusion). We then\nuse the dataset to train a set of deep neural network models suited to mobile\nand constrained deployment scenarios to determine the extent to which it may be\npossible to efficiently and accurately classify these `rainy' conditions. The\nmost lightweight of these models (MobileNetV3 small) can achieve an accuracy of\n90% with a memory footprint of just 1.28 MB and a frame rate of 93 FPS, which\nis suitable for deployment in resource-constrained and latency-sensitive\nsystems. We demonstrate a classification latency in the order of milliseconds\nusing typical flight computer hardware. Accordingly, such a model can feed into\nthe disturbance estimation component of an autonomous flight controller. In\naddition, data from unmanned aerial vehicles with the ability to accurately\ndetermine environmental conditions in real time may contribute to developing\nmore granular timely localised weather forecasting."
                },
                "authors": [
                    {
                        "name": "Andrea Albanese"
                    },
                    {
                        "name": "Yanran Wang"
                    },
                    {
                        "name": "Davide Brunelli"
                    },
                    {
                        "name": "David Boyle"
                    }
                ],
                "author_detail": {
                    "name": "David Boyle"
                },
                "author": "David Boyle",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12663v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12663v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07424v1",
                "updated": "2025-02-11T10:10:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    10,
                    10,
                    26,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T10:10:26Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    10,
                    10,
                    26,
                    1,
                    42,
                    0
                ],
                "title": "RomanLens: Latent Romanization and its role in Multilinguality in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RomanLens: Latent Romanization and its role in Multilinguality in LLMs"
                },
                "summary": "Large Language Models (LLMs) exhibit remarkable multilingual generalization\ndespite being predominantly trained on English-centric corpora. A fundamental\nquestion arises: how do LLMs achieve such robust multilingual capabilities? For\nnon-Latin script languages, we investigate the role of romanization - the\nrepresentation of non-Latin scripts using Latin characters - as a bridge in\nmultilingual processing. Using mechanistic interpretability techniques, we\nanalyze next-token generation and find that intermediate layers frequently\nrepresent target words in romanized form before transitioning to native script,\na phenomenon we term Latent Romanization. Further, through activation patching\nexperiments, we demonstrate that LLMs encode semantic concepts similarly across\nnative and romanized scripts, suggesting a shared underlying representation.\nAdditionally in translation towards non Latin languages, our findings reveal\nthat when the target language is in romanized form, its representations emerge\nearlier in the model's layers compared to native script. These insights\ncontribute to a deeper understanding of multilingual representation in LLMs and\nhighlight the implicit role of romanization in facilitating language transfer.\nOur work provides new directions for potentially improving multilingual\nlanguage modeling and interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit remarkable multilingual generalization\ndespite being predominantly trained on English-centric corpora. A fundamental\nquestion arises: how do LLMs achieve such robust multilingual capabilities? For\nnon-Latin script languages, we investigate the role of romanization - the\nrepresentation of non-Latin scripts using Latin characters - as a bridge in\nmultilingual processing. Using mechanistic interpretability techniques, we\nanalyze next-token generation and find that intermediate layers frequently\nrepresent target words in romanized form before transitioning to native script,\na phenomenon we term Latent Romanization. Further, through activation patching\nexperiments, we demonstrate that LLMs encode semantic concepts similarly across\nnative and romanized scripts, suggesting a shared underlying representation.\nAdditionally in translation towards non Latin languages, our findings reveal\nthat when the target language is in romanized form, its representations emerge\nearlier in the model's layers compared to native script. These insights\ncontribute to a deeper understanding of multilingual representation in LLMs and\nhighlight the implicit role of romanization in facilitating language transfer.\nOur work provides new directions for potentially improving multilingual\nlanguage modeling and interpretability."
                },
                "authors": [
                    {
                        "name": "Alan Saji"
                    },
                    {
                        "name": "Jaavid Aktar Husain"
                    },
                    {
                        "name": "Thanmay Jayakumar"
                    },
                    {
                        "name": "Raj Dabre"
                    },
                    {
                        "name": "Anoop Kunchukuttan"
                    },
                    {
                        "name": "Mitesh M. Khapra"
                    },
                    {
                        "name": "Ratish Puduppully"
                    }
                ],
                "author_detail": {
                    "name": "Ratish Puduppully"
                },
                "arxiv_affiliation": "IT University of Copenhagen",
                "author": "Ratish Puduppully",
                "arxiv_comment": "18 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02577v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02577v2",
                "updated": "2025-02-11T10:06:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    10,
                    6,
                    14,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-04T18:53:42Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    53,
                    42,
                    1,
                    35,
                    0
                ],
                "title": "A comparison of translation performance between DeepL and Supertext",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A comparison of translation performance between DeepL and Supertext"
                },
                "summary": "As strong machine translation (MT) systems are increasingly based on large\nlanguage models (LLMs), reliable quality benchmarking requires methods that\ncapture their ability to leverage extended context. This study compares two\ncommercial MT systems -- DeepL and Supertext -- by assessing their performance\non unsegmented texts. We evaluate translation quality across four language\ndirections with professional translators assessing segments with full\ndocument-level context. While segment-level assessments indicate no strong\npreference between the systems in most cases, document-level analysis reveals a\npreference for Supertext in three out of four language directions, suggesting\nsuperior consistency across longer texts. We advocate for more\ncontext-sensitive evaluation methodologies to ensure that MT quality\nassessments reflect real-world usability. We release all evaluation data and\nscripts for further analysis and reproduction at\nhttps://github.com/supertext/evaluation_deepl_supertext.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As strong machine translation (MT) systems are increasingly based on large\nlanguage models (LLMs), reliable quality benchmarking requires methods that\ncapture their ability to leverage extended context. This study compares two\ncommercial MT systems -- DeepL and Supertext -- by assessing their performance\non unsegmented texts. We evaluate translation quality across four language\ndirections with professional translators assessing segments with full\ndocument-level context. While segment-level assessments indicate no strong\npreference between the systems in most cases, document-level analysis reveals a\npreference for Supertext in three out of four language directions, suggesting\nsuperior consistency across longer texts. We advocate for more\ncontext-sensitive evaluation methodologies to ensure that MT quality\nassessments reflect real-world usability. We release all evaluation data and\nscripts for further analysis and reproduction at\nhttps://github.com/supertext/evaluation_deepl_supertext."
                },
                "authors": [
                    {
                        "name": "Alex Flckiger"
                    },
                    {
                        "name": "Chantal Amrhein"
                    },
                    {
                        "name": "Tim Graf"
                    },
                    {
                        "name": "Frdric Odermatt"
                    },
                    {
                        "name": "Martin Pmsl"
                    },
                    {
                        "name": "Philippe Schlpfer"
                    },
                    {
                        "name": "Florian Schottmann"
                    },
                    {
                        "name": "Samuel Lubli"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Lubli"
                },
                "author": "Samuel Lubli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02577v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02577v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00560v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00560v2",
                "updated": "2025-02-11T10:02:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    10,
                    2,
                    55,
                    1,
                    42,
                    0
                ],
                "published": "2024-12-31T17:46:51Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    17,
                    46,
                    51,
                    1,
                    366,
                    0
                ],
                "title": "Re-evaluating Automatic LLM System Ranking for Alignment with Human\n  Preference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-evaluating Automatic LLM System Ranking for Alignment with Human\n  Preference"
                },
                "summary": "Evaluating and ranking the capabilities of different LLMs is crucial for\nunderstanding their performance and alignment with human preferences. Due to\nthe high cost and time-consuming nature of human evaluations, an automatic LLM\nbencher (i.e., an automatic evaluation framework that aims to rank LLMs based\non their alignment with human preferences) is indispensable. An automatic LLM\nbencher consists of four components: the input set (e.g., a user instruction),\nthe evaluation model (e.g., an LLM), the evaluation type (e.g., pairwise\ncomparison), and the aggregation method (e.g., the ELO rating system). However,\nprevious work has not thoroughly explored how to select these components or how\ntheir different combinations influence the results. In this work, through\ncontrolled experiments, we provide a series of recommendations on how to choose\neach component to better automate the evaluation of LLMs. Furthermore, we\ndiscovered that when evaluating LLMs with similar performance, the performance\nof the automatic LLM bencher declines sharply, underscoring the limitations of\ncurrent benchers and calling for future work. Lastly, we found that the\nevaluation models' performance at the instance level (e.g., the accuracy of\nselecting the best output) does not always align with their effectiveness when\nused as a component of a bencher, highlighting the importance of dedicated\nsystem-level evaluation of benchers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating and ranking the capabilities of different LLMs is crucial for\nunderstanding their performance and alignment with human preferences. Due to\nthe high cost and time-consuming nature of human evaluations, an automatic LLM\nbencher (i.e., an automatic evaluation framework that aims to rank LLMs based\non their alignment with human preferences) is indispensable. An automatic LLM\nbencher consists of four components: the input set (e.g., a user instruction),\nthe evaluation model (e.g., an LLM), the evaluation type (e.g., pairwise\ncomparison), and the aggregation method (e.g., the ELO rating system). However,\nprevious work has not thoroughly explored how to select these components or how\ntheir different combinations influence the results. In this work, through\ncontrolled experiments, we provide a series of recommendations on how to choose\neach component to better automate the evaluation of LLMs. Furthermore, we\ndiscovered that when evaluating LLMs with similar performance, the performance\nof the automatic LLM bencher declines sharply, underscoring the limitations of\ncurrent benchers and calling for future work. Lastly, we found that the\nevaluation models' performance at the instance level (e.g., the accuracy of\nselecting the best output) does not always align with their effectiveness when\nused as a component of a bencher, highlighting the importance of dedicated\nsystem-level evaluation of benchers."
                },
                "authors": [
                    {
                        "name": "Mingqi Gao"
                    },
                    {
                        "name": "Yixin Liu"
                    },
                    {
                        "name": "Xinyu Hu"
                    },
                    {
                        "name": "Xiaojun Wan"
                    },
                    {
                        "name": "Jonathan Bragg"
                    },
                    {
                        "name": "Arman Cohan"
                    }
                ],
                "author_detail": {
                    "name": "Arman Cohan"
                },
                "author": "Arman Cohan",
                "arxiv_comment": "Findings of NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00560v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00560v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07418v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07418v1",
                "updated": "2025-02-11T09:54:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    9,
                    54,
                    39,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T09:54:39Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    9,
                    54,
                    39,
                    1,
                    42,
                    0
                ],
                "title": "Entity Linking using LLMs for Automated Product Carbon Footprint\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entity Linking using LLMs for Automated Product Carbon Footprint\n  Estimation"
                },
                "summary": "Growing concerns about climate change and sustainability are driving\nmanufacturers to take significant steps toward reducing their carbon\nfootprints. For these manufacturers, a first step towards this goal is to\nidentify the environmental impact of the individual components of their\nproducts. We propose a system leveraging large language models (LLMs) to\nautomatically map components from manufacturer Bills of Materials (BOMs) to\nLife Cycle Assessment (LCA) database entries by using LLMs to expand on\navailable component information. Our approach reduces the need for manual data\nprocessing, paving the way for more accessible sustainability practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Growing concerns about climate change and sustainability are driving\nmanufacturers to take significant steps toward reducing their carbon\nfootprints. For these manufacturers, a first step towards this goal is to\nidentify the environmental impact of the individual components of their\nproducts. We propose a system leveraging large language models (LLMs) to\nautomatically map components from manufacturer Bills of Materials (BOMs) to\nLife Cycle Assessment (LCA) database entries by using LLMs to expand on\navailable component information. Our approach reduces the need for manual data\nprocessing, paving the way for more accessible sustainability practices."
                },
                "authors": [
                    {
                        "name": "Steffen Castle"
                    },
                    {
                        "name": "Julian Moreno Schneider"
                    },
                    {
                        "name": "Leonhard Hennig"
                    },
                    {
                        "name": "Georg Rehm"
                    }
                ],
                "author_detail": {
                    "name": "Georg Rehm"
                },
                "author": "Georg Rehm",
                "arxiv_journal_ref": "Proceedings of The 1st Workshop on Ecology, Environment, and\n  Natural Language Processing (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07418v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07418v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07403v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07403v1",
                "updated": "2025-02-11T09:32:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    9,
                    32,
                    31,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T09:32:31Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    9,
                    32,
                    31,
                    1,
                    42,
                    0
                ],
                "title": "Extended monocular 3D imaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extended monocular 3D imaging"
                },
                "summary": "3D vision is of paramount importance for numerous applications ranging from\nmachine intelligence to precision metrology. Despite much recent progress, the\nmajority of 3D imaging hardware remains bulky and complicated and provides much\nlower image resolution compared to their 2D counterparts. Moreover, there are\nmany well-known scenarios that existing 3D imaging solutions frequently fail.\nHere, we introduce an extended monocular 3D imaging (EM3D) framework that fully\nexploits the vectorial wave nature of light. Via the multi-stage fusion of\ndiffraction- and polarization-based depth cues, using a compact monocular\ncamera equipped with a diffractive-refractive hybrid lens, we experimentally\ndemonstrate the snapshot acquisition of a million-pixel and accurate 3D point\ncloud for extended scenes that are traditionally challenging, including those\nwith low texture, being highly reflective, or nearly transparent, without a\ndata prior. Furthermore, we discover that the combination of depth and\npolarization information can unlock unique new opportunities in material\nidentification, which may further expand machine intelligence for applications\nlike target recognition and face anti-spoofing. The straightforward yet\npowerful architecture thus opens up a new path for a higher-dimensional machine\nvision in a minimal form factor, facilitating the deployment of monocular\ncameras for applications in much more diverse scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D vision is of paramount importance for numerous applications ranging from\nmachine intelligence to precision metrology. Despite much recent progress, the\nmajority of 3D imaging hardware remains bulky and complicated and provides much\nlower image resolution compared to their 2D counterparts. Moreover, there are\nmany well-known scenarios that existing 3D imaging solutions frequently fail.\nHere, we introduce an extended monocular 3D imaging (EM3D) framework that fully\nexploits the vectorial wave nature of light. Via the multi-stage fusion of\ndiffraction- and polarization-based depth cues, using a compact monocular\ncamera equipped with a diffractive-refractive hybrid lens, we experimentally\ndemonstrate the snapshot acquisition of a million-pixel and accurate 3D point\ncloud for extended scenes that are traditionally challenging, including those\nwith low texture, being highly reflective, or nearly transparent, without a\ndata prior. Furthermore, we discover that the combination of depth and\npolarization information can unlock unique new opportunities in material\nidentification, which may further expand machine intelligence for applications\nlike target recognition and face anti-spoofing. The straightforward yet\npowerful architecture thus opens up a new path for a higher-dimensional machine\nvision in a minimal form factor, facilitating the deployment of monocular\ncameras for applications in much more diverse scenarios."
                },
                "authors": [
                    {
                        "name": "Zicheng Shen"
                    },
                    {
                        "name": "Feng Zhao"
                    },
                    {
                        "name": "Yibo Ni"
                    },
                    {
                        "name": "Yuanmu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yuanmu Yang"
                },
                "author": "Yuanmu Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07403v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07403v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07399v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07399v1",
                "updated": "2025-02-11T09:27:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    9,
                    27,
                    0,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T09:27:00Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    9,
                    27,
                    0,
                    1,
                    42,
                    0
                ],
                "title": "On Iterative Evaluation and Enhancement of Code Quality Using GPT-4o",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Iterative Evaluation and Enhancement of Code Quality Using GPT-4o"
                },
                "summary": "This paper introduces CodeQUEST, a novel framework leveraging Large Language\nModels (LLMs) to iteratively evaluate and enhance code quality across multiple\ndimensions, including readability, maintainability, efficiency, and security.\nThe framework is divided into two main components: an Evaluator that assesses\ncode quality across ten dimensions, providing both quantitative scores and\nqualitative summaries, and an Optimizer that iteratively improves the code\nbased on the Evaluator's feedback. Our study demonstrates that CodeQUEST can\neffectively and robustly evaluate code quality, with its assessments aligning\nclosely with established code quality metrics. Through a series of experiments\nusing a curated dataset of Python and JavaScript examples, CodeQUEST\ndemonstrated significant improvements in code quality, achieving a mean\nrelative percentage improvement of 52.6%. The framework's evaluations were\nvalidated against a set of proxy metrics comprising of Pylint Score, Radon\nMaintainability Index, and Bandit output logs, showing a meaningful\ncorrelation. This highlights the potential of LLMs in automating code quality\nevaluation and improvement processes, presenting a significant advancement\ntoward enhancing software development practices. The code implementation of the\nframework is available at: https://github.com/jpmorganchase/CodeQuest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces CodeQUEST, a novel framework leveraging Large Language\nModels (LLMs) to iteratively evaluate and enhance code quality across multiple\ndimensions, including readability, maintainability, efficiency, and security.\nThe framework is divided into two main components: an Evaluator that assesses\ncode quality across ten dimensions, providing both quantitative scores and\nqualitative summaries, and an Optimizer that iteratively improves the code\nbased on the Evaluator's feedback. Our study demonstrates that CodeQUEST can\neffectively and robustly evaluate code quality, with its assessments aligning\nclosely with established code quality metrics. Through a series of experiments\nusing a curated dataset of Python and JavaScript examples, CodeQUEST\ndemonstrated significant improvements in code quality, achieving a mean\nrelative percentage improvement of 52.6%. The framework's evaluations were\nvalidated against a set of proxy metrics comprising of Pylint Score, Radon\nMaintainability Index, and Bandit output logs, showing a meaningful\ncorrelation. This highlights the potential of LLMs in automating code quality\nevaluation and improvement processes, presenting a significant advancement\ntoward enhancing software development practices. The code implementation of the\nframework is available at: https://github.com/jpmorganchase/CodeQuest."
                },
                "authors": [
                    {
                        "name": "Rundong Liu"
                    },
                    {
                        "name": "Andre Frade"
                    },
                    {
                        "name": "Amal Vaidya"
                    },
                    {
                        "name": "Maxime Labonne"
                    },
                    {
                        "name": "Marcus Kaiser"
                    },
                    {
                        "name": "Bismayan Chakrabarti"
                    },
                    {
                        "name": "Jonathan Budd"
                    },
                    {
                        "name": "Sean Moran"
                    }
                ],
                "author_detail": {
                    "name": "Sean Moran"
                },
                "author": "Sean Moran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07399v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07399v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07393v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07393v1",
                "updated": "2025-02-11T09:23:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    9,
                    23,
                    14,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T09:23:14Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    9,
                    23,
                    14,
                    1,
                    42,
                    0
                ],
                "title": "FinRL-DeepSeek: LLM-Infused Risk-Sensitive Reinforcement Learning for\n  Trading Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FinRL-DeepSeek: LLM-Infused Risk-Sensitive Reinforcement Learning for\n  Trading Agents"
                },
                "summary": "This paper presents a novel risk-sensitive trading agent combining\nreinforcement learning and large language models (LLMs). We extend the\nConditional Value-at-Risk Proximal Policy Optimization (CPPO) algorithm, by\nadding risk assessment and trading recommendation signals generated by a LLM\nfrom financial news. Our approach is backtested on the Nasdaq-100 index\nbenchmark, using financial news data from the FNSPID dataset and the DeepSeek\nV3, Qwen 2.5 and Llama 3.3 language models. The code, data, and trading agents\nare available at: https://github.com/benstaf/FinRL_DeepSeek",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel risk-sensitive trading agent combining\nreinforcement learning and large language models (LLMs). We extend the\nConditional Value-at-Risk Proximal Policy Optimization (CPPO) algorithm, by\nadding risk assessment and trading recommendation signals generated by a LLM\nfrom financial news. Our approach is backtested on the Nasdaq-100 index\nbenchmark, using financial news data from the FNSPID dataset and the DeepSeek\nV3, Qwen 2.5 and Llama 3.3 language models. The code, data, and trading agents\nare available at: https://github.com/benstaf/FinRL_DeepSeek"
                },
                "authors": [
                    {
                        "name": "Mostapha Benhenda"
                    }
                ],
                "author_detail": {
                    "name": "Mostapha Benhenda"
                },
                "arxiv_affiliation": "LAGA",
                "author": "Mostapha Benhenda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07393v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07393v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.TR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.TR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07391v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07391v1",
                "updated": "2025-02-11T09:19:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    9,
                    19,
                    46,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T09:19:46Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    9,
                    19,
                    46,
                    1,
                    42,
                    0
                ],
                "title": "Target-Augmented Shared Fusion-based Multimodal Sarcasm Explanation\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Target-Augmented Shared Fusion-based Multimodal Sarcasm Explanation\n  Generation"
                },
                "summary": "Sarcasm is a linguistic phenomenon that intends to ridicule a target (e.g.,\nentity, event, or person) in an inherent way. Multimodal Sarcasm Explanation\n(MuSE) aims at revealing the intended irony in a sarcastic post using a natural\nlanguage explanation. Though important, existing systems overlooked the\nsignificance of the target of sarcasm in generating explanations. In this\npaper, we propose a Target-aUgmented shaRed fusion-Based sarcasm explanatiOn\nmodel, aka. TURBO. We design a novel shared-fusion mechanism to leverage the\ninter-modality relationships between an image and its caption. TURBO assumes\nthe target of the sarcasm and guides the multimodal shared fusion mechanism in\nlearning intricacies of the intended irony for explanations. We evaluate our\nproposed TURBO model on the MORE+ dataset. Comparison against multiple\nbaselines and state-of-the-art models signifies the performance improvement of\nTURBO by an average margin of $+3.3\\%$. Moreover, we explore LLMs in zero and\none-shot settings for our task and observe that LLM-generated explanation,\nthough remarkable, often fails to capture the critical nuances of the sarcasm.\nFurthermore, we supplement our study with extensive human evaluation on TURBO's\ngenerated explanations and find them out to be comparatively better than other\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sarcasm is a linguistic phenomenon that intends to ridicule a target (e.g.,\nentity, event, or person) in an inherent way. Multimodal Sarcasm Explanation\n(MuSE) aims at revealing the intended irony in a sarcastic post using a natural\nlanguage explanation. Though important, existing systems overlooked the\nsignificance of the target of sarcasm in generating explanations. In this\npaper, we propose a Target-aUgmented shaRed fusion-Based sarcasm explanatiOn\nmodel, aka. TURBO. We design a novel shared-fusion mechanism to leverage the\ninter-modality relationships between an image and its caption. TURBO assumes\nthe target of the sarcasm and guides the multimodal shared fusion mechanism in\nlearning intricacies of the intended irony for explanations. We evaluate our\nproposed TURBO model on the MORE+ dataset. Comparison against multiple\nbaselines and state-of-the-art models signifies the performance improvement of\nTURBO by an average margin of $+3.3\\%$. Moreover, we explore LLMs in zero and\none-shot settings for our task and observe that LLM-generated explanation,\nthough remarkable, often fails to capture the critical nuances of the sarcasm.\nFurthermore, we supplement our study with extensive human evaluation on TURBO's\ngenerated explanations and find them out to be comparatively better than other\nsystems."
                },
                "authors": [
                    {
                        "name": "Palaash Goel"
                    },
                    {
                        "name": "Dushyant Singh Chauhan"
                    },
                    {
                        "name": "Md Shad Akhtar"
                    }
                ],
                "author_detail": {
                    "name": "Md Shad Akhtar"
                },
                "author": "Md Shad Akhtar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07391v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07391v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07374v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07374v1",
                "updated": "2025-02-11T08:48:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    8,
                    48,
                    48,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T08:48:48Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    8,
                    48,
                    48,
                    1,
                    42,
                    0
                ],
                "title": "LLMs Can Easily Learn to Reason from Demonstrations Structure, not\n  content, is what matters!",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Can Easily Learn to Reason from Demonstrations Structure, not\n  content, is what matters!"
                },
                "summary": "Large reasoning models (LRMs) tackle complex reasoning problems by following\nlong chain-of-thoughts (Long CoT) that incorporate reflection, backtracking,\nand self-validation. However, the training techniques and data requirements to\nelicit Long CoT remain poorly understood. In this work, we find that a Large\nLanguage model (LLM) can effectively learn Long CoT reasoning through\ndata-efficient supervised fine-tuning (SFT) and parameter-efficient low-rank\nadaptation (LoRA). With just 17k long CoT training samples, the\nQwen2.5-32B-Instruct model achieves significant improvements on a wide range of\nmath and coding benchmarks, including 56.7% (+40.0%) on AIME 2024 and 57.0%\n(+8.1%) on LiveCodeBench, competitive to the proprietary o1-preview model's\nscore of 44.6% and 59.1%. More importantly, we find that the structure of Long\nCoT is critical to the learning process, whereas the content of individual\nreasoning steps has minimal impact. Perturbations affecting content, such as\ntraining on incorrect samples or removing reasoning keywords, have little\nimpact on performance. In contrast, structural modifications that disrupt\nlogical consistency in the Long CoT, such as shuffling or deleting reasoning\nsteps, significantly degrade accuracy. For example, a model trained on Long CoT\nsamples with incorrect answers still achieves only 3.2% lower accuracy compared\nto training with fully correct samples. These insights deepen our understanding\nof how to elicit reasoning capabilities in LLMs and highlight key\nconsiderations for efficiently training the next generation of reasoning\nmodels. This is the academic paper of our previous released Sky-T1-32B-Preview\nmodel. Codes are available at https://github.com/NovaSky-AI/SkyThought.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large reasoning models (LRMs) tackle complex reasoning problems by following\nlong chain-of-thoughts (Long CoT) that incorporate reflection, backtracking,\nand self-validation. However, the training techniques and data requirements to\nelicit Long CoT remain poorly understood. In this work, we find that a Large\nLanguage model (LLM) can effectively learn Long CoT reasoning through\ndata-efficient supervised fine-tuning (SFT) and parameter-efficient low-rank\nadaptation (LoRA). With just 17k long CoT training samples, the\nQwen2.5-32B-Instruct model achieves significant improvements on a wide range of\nmath and coding benchmarks, including 56.7% (+40.0%) on AIME 2024 and 57.0%\n(+8.1%) on LiveCodeBench, competitive to the proprietary o1-preview model's\nscore of 44.6% and 59.1%. More importantly, we find that the structure of Long\nCoT is critical to the learning process, whereas the content of individual\nreasoning steps has minimal impact. Perturbations affecting content, such as\ntraining on incorrect samples or removing reasoning keywords, have little\nimpact on performance. In contrast, structural modifications that disrupt\nlogical consistency in the Long CoT, such as shuffling or deleting reasoning\nsteps, significantly degrade accuracy. For example, a model trained on Long CoT\nsamples with incorrect answers still achieves only 3.2% lower accuracy compared\nto training with fully correct samples. These insights deepen our understanding\nof how to elicit reasoning capabilities in LLMs and highlight key\nconsiderations for efficiently training the next generation of reasoning\nmodels. This is the academic paper of our previous released Sky-T1-32B-Preview\nmodel. Codes are available at https://github.com/NovaSky-AI/SkyThought."
                },
                "authors": [
                    {
                        "name": "Dacheng Li"
                    },
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Tyler Griggs"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Xiangxi Mo"
                    },
                    {
                        "name": "Shishir G. Patil"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07374v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07374v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07373v1",
                "updated": "2025-02-11T08:48:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    8,
                    48,
                    46,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T08:48:46Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    8,
                    48,
                    46,
                    1,
                    42,
                    0
                ],
                "title": "EvoFlow: Evolving Diverse Agentic Workflows On The Fly",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvoFlow: Evolving Diverse Agentic Workflows On The Fly"
                },
                "summary": "The past two years have witnessed the evolution of large language model\n(LLM)-based multi-agent systems from labor-intensive manual design to partial\nautomation (\\textit{e.g.}, prompt engineering, communication topology) and\neventually to fully automated design. However, existing agentic automation\npipelines often lack LLM heterogeneity and focus on single-objective\nperformance optimization, limiting their potential to combine weaker models for\nmore customized and cost-effective solutions. To address this challenge, we\npropose EvoFlow, a niching evolutionary algorithm-based framework to\nautomatically search a population of heterogeneous and complexity-adaptive\nagentic workflows, rather than a single homogeneous, complex workflow.\nTechnically, EvoFlow performs \\textit{(1) tag-based retrieval} to extract\nparent workflows from an agentic population, evolves new workflows through\n\\textit{(2) crossover} and \\textit{(3) mutation}, and employs \\textit{(4)\nniching-based selection} to maintain population diversity and quality.\nExtensive evaluations across seven benchmarks demonstrate that EvoFlow is:\n\\textbf{(I) diverse}, evolving a population of workflows ranging from simple\nI/O tasks to complex multi-turn interactions; \\textbf{(II) high-performing},\noutperforming previous handcrafted and automated workflows by\n$1.23\\%\\sim29.86\\%$; \\textbf{(III) economical}, surpassing powerful\n\\llmname{o1-preview} at $12.4\\%$ of its inference cost using weaker open-source\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The past two years have witnessed the evolution of large language model\n(LLM)-based multi-agent systems from labor-intensive manual design to partial\nautomation (\\textit{e.g.}, prompt engineering, communication topology) and\neventually to fully automated design. However, existing agentic automation\npipelines often lack LLM heterogeneity and focus on single-objective\nperformance optimization, limiting their potential to combine weaker models for\nmore customized and cost-effective solutions. To address this challenge, we\npropose EvoFlow, a niching evolutionary algorithm-based framework to\nautomatically search a population of heterogeneous and complexity-adaptive\nagentic workflows, rather than a single homogeneous, complex workflow.\nTechnically, EvoFlow performs \\textit{(1) tag-based retrieval} to extract\nparent workflows from an agentic population, evolves new workflows through\n\\textit{(2) crossover} and \\textit{(3) mutation}, and employs \\textit{(4)\nniching-based selection} to maintain population diversity and quality.\nExtensive evaluations across seven benchmarks demonstrate that EvoFlow is:\n\\textbf{(I) diverse}, evolving a population of workflows ranging from simple\nI/O tasks to complex multi-turn interactions; \\textbf{(II) high-performing},\noutperforming previous handcrafted and automated workflows by\n$1.23\\%\\sim29.86\\%$; \\textbf{(III) economical}, surpassing powerful\n\\llmname{o1-preview} at $12.4\\%$ of its inference cost using weaker open-source\nmodels."
                },
                "authors": [
                    {
                        "name": "Guibin Zhang"
                    },
                    {
                        "name": "Kaijie Chen"
                    },
                    {
                        "name": "Guancheng Wan"
                    },
                    {
                        "name": "Heng Chang"
                    },
                    {
                        "name": "Hong Cheng"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Shuyue Hu"
                    },
                    {
                        "name": "Lei Bai"
                    }
                ],
                "author_detail": {
                    "name": "Lei Bai"
                },
                "author": "Lei Bai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07372v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07372v1",
                "updated": "2025-02-11T08:47:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    8,
                    47,
                    58,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T08:47:58Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    8,
                    47,
                    58,
                    1,
                    42,
                    0
                ],
                "title": "USRNet: Unified Scene Recovery Network for Enhancing Traffic Imaging\n  under Multiple Adverse Weather Conditions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "USRNet: Unified Scene Recovery Network for Enhancing Traffic Imaging\n  under Multiple Adverse Weather Conditions"
                },
                "summary": "Advancements in computer vision technology have facilitated the extensive\ndeployment of intelligent transportation systems and visual surveillance\nsystems across various applications, including autonomous driving, public\nsafety, and environmental monitoring. However, adverse weather conditions such\nas haze, rain, snow, and more complex mixed degradation can significantly\ndegrade image quality. The degradation compromises the accuracy and reliability\nof these systems across various scenarios. To tackle the challenge of\ndeveloping adaptable models for scene restoration, we introduce the unified\nscene recovery network (USRNet), capable of handling multiple types of image\ndegradation. The USRNet features a sophisticated architecture consisting of a\nscene encoder, an attention-driven node independent learning mechanism (NILM),\nan edge decoder, and a scene restoration module. The scene encoder, powered by\nadvanced residual blocks, extracts deep features from degraded images in a\nprogressive manner, ensuring thorough encoding of degradation information. To\nenhance the USRNet's adaptability in diverse weather conditions, we introduce\nNILM, which enables the network to learn and respond to different scenarios\nwith precision, thereby increasing its robustness. The edge decoder is designed\nto extract edge features with precision, which is essential for maintaining\nimage sharpness. Experimental results demonstrate that USRNet surpasses\nexisting methods in handling complex imaging degradations, thereby improving\nthe accuracy and reliability of visual systems across diverse scenarios. The\ncode resources for this work can be accessed in\nhttps://github.com/LouisYxLu/USRNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in computer vision technology have facilitated the extensive\ndeployment of intelligent transportation systems and visual surveillance\nsystems across various applications, including autonomous driving, public\nsafety, and environmental monitoring. However, adverse weather conditions such\nas haze, rain, snow, and more complex mixed degradation can significantly\ndegrade image quality. The degradation compromises the accuracy and reliability\nof these systems across various scenarios. To tackle the challenge of\ndeveloping adaptable models for scene restoration, we introduce the unified\nscene recovery network (USRNet), capable of handling multiple types of image\ndegradation. The USRNet features a sophisticated architecture consisting of a\nscene encoder, an attention-driven node independent learning mechanism (NILM),\nan edge decoder, and a scene restoration module. The scene encoder, powered by\nadvanced residual blocks, extracts deep features from degraded images in a\nprogressive manner, ensuring thorough encoding of degradation information. To\nenhance the USRNet's adaptability in diverse weather conditions, we introduce\nNILM, which enables the network to learn and respond to different scenarios\nwith precision, thereby increasing its robustness. The edge decoder is designed\nto extract edge features with precision, which is essential for maintaining\nimage sharpness. Experimental results demonstrate that USRNet surpasses\nexisting methods in handling complex imaging degradations, thereby improving\nthe accuracy and reliability of visual systems across diverse scenarios. The\ncode resources for this work can be accessed in\nhttps://github.com/LouisYxLu/USRNet."
                },
                "authors": [
                    {
                        "name": "Yuxu Lu"
                    },
                    {
                        "name": "Ai Chen"
                    },
                    {
                        "name": "Dong Yang"
                    },
                    {
                        "name": "Ryan Wen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ryan Wen Liu"
                },
                "author": "Ryan Wen Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07372v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07372v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07365v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07365v1",
                "updated": "2025-02-11T08:37:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    8,
                    37,
                    16,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T08:37:16Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    8,
                    37,
                    16,
                    1,
                    42,
                    0
                ],
                "title": "LongReD: Mitigating Short-Text Degradation of Long-Context Large\n  Language Models via Restoration Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongReD: Mitigating Short-Text Degradation of Long-Context Large\n  Language Models via Restoration Distillation"
                },
                "summary": "Large language models (LLMs) have gained extended context windows through\nscaling positional encodings and lightweight continual pre-training. However,\nthis often leads to degraded performance on short-text tasks, while the reasons\nfor this degradation remain insufficiently explored. In this work, we identify\ntwo primary factors contributing to this issue: distribution drift in hidden\nstates and attention scores, and catastrophic forgetting during continual\npre-training. To address these challenges, we propose Long Context Pre-training\nwith Restoration Distillation (LongReD), a novel approach designed to mitigate\nshort-text performance degradation through minimizing the distribution\ndiscrepancy between the extended and original models. Besides training on long\ntexts, LongReD distills the hidden state of selected layers from the original\nmodel on short texts. Additionally, LongReD also introduces a short-to-long\ndistillation, aligning the output distribution on short texts with that on long\ntexts by leveraging skipped positional indices. Experiments on common text\nbenchmarks demonstrate that LongReD effectively preserves the model's\nshort-text performance while maintaining comparable or even better capacity to\nhandle long texts than baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have gained extended context windows through\nscaling positional encodings and lightweight continual pre-training. However,\nthis often leads to degraded performance on short-text tasks, while the reasons\nfor this degradation remain insufficiently explored. In this work, we identify\ntwo primary factors contributing to this issue: distribution drift in hidden\nstates and attention scores, and catastrophic forgetting during continual\npre-training. To address these challenges, we propose Long Context Pre-training\nwith Restoration Distillation (LongReD), a novel approach designed to mitigate\nshort-text performance degradation through minimizing the distribution\ndiscrepancy between the extended and original models. Besides training on long\ntexts, LongReD distills the hidden state of selected layers from the original\nmodel on short texts. Additionally, LongReD also introduces a short-to-long\ndistillation, aligning the output distribution on short texts with that on long\ntexts by leveraging skipped positional indices. Experiments on common text\nbenchmarks demonstrate that LongReD effectively preserves the model's\nshort-text performance while maintaining comparable or even better capacity to\nhandle long texts than baselines."
                },
                "authors": [
                    {
                        "name": "Zican Dong"
                    },
                    {
                        "name": "Junyi Li"
                    },
                    {
                        "name": "Jinhao Jiang"
                    },
                    {
                        "name": "Mingyu Xu"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Bingning Wang"
                    },
                    {
                        "name": "Weipeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Weipeng Chen"
                },
                "author": "Weipeng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07365v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07352v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07352v1",
                "updated": "2025-02-11T08:23:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    8,
                    23,
                    56,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T08:23:56Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    8,
                    23,
                    56,
                    1,
                    42,
                    0
                ],
                "title": "Bridging the Evaluation Gap: Leveraging Large Language Models for Topic\n  Model Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Evaluation Gap: Leveraging Large Language Models for Topic\n  Model Evaluation"
                },
                "summary": "This study presents a framework for automated evaluation of dynamically\nevolving topic taxonomies in scientific literature using Large Language Models\n(LLMs). In digital library systems, topic modeling plays a crucial role in\nefficiently organizing and retrieving scholarly content, guiding researchers\nthrough complex knowledge landscapes. As research domains proliferate and\nshift, traditional human centric and static evaluation methods struggle to\nmaintain relevance. The proposed approach harnesses LLMs to measure key quality\ndimensions, such as coherence, repetitiveness, diversity, and topic-document\nalignment, without heavy reliance on expert annotators or narrow statistical\nmetrics. Tailored prompts guide LLM assessments, ensuring consistent and\ninterpretable evaluations across various datasets and modeling techniques.\nExperiments on benchmark corpora demonstrate the method's robustness,\nscalability, and adaptability, underscoring its value as a more holistic and\ndynamic alternative to conventional evaluation strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents a framework for automated evaluation of dynamically\nevolving topic taxonomies in scientific literature using Large Language Models\n(LLMs). In digital library systems, topic modeling plays a crucial role in\nefficiently organizing and retrieving scholarly content, guiding researchers\nthrough complex knowledge landscapes. As research domains proliferate and\nshift, traditional human centric and static evaluation methods struggle to\nmaintain relevance. The proposed approach harnesses LLMs to measure key quality\ndimensions, such as coherence, repetitiveness, diversity, and topic-document\nalignment, without heavy reliance on expert annotators or narrow statistical\nmetrics. Tailored prompts guide LLM assessments, ensuring consistent and\ninterpretable evaluations across various datasets and modeling techniques.\nExperiments on benchmark corpora demonstrate the method's robustness,\nscalability, and adaptability, underscoring its value as a more holistic and\ndynamic alternative to conventional evaluation strategies."
                },
                "authors": [
                    {
                        "name": "Zhiyin Tan"
                    },
                    {
                        "name": "Jennifer D'Souza"
                    }
                ],
                "author_detail": {
                    "name": "Jennifer D'Souza"
                },
                "author": "Jennifer D'Souza",
                "arxiv_comment": "accepted by IRCDL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07352v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07352v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12109v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12109v2",
                "updated": "2025-02-11T08:22:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    8,
                    22,
                    35,
                    1,
                    42,
                    0
                ],
                "published": "2024-06-17T21:37:09Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    21,
                    37,
                    9,
                    0,
                    169,
                    0
                ],
                "title": "Can LLMs Learn Macroeconomic Narratives from Social Media?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Learn Macroeconomic Narratives from Social Media?"
                },
                "summary": "This study empirically tests the $\\textit{Narrative Economics}$ hypothesis,\nwhich posits that narratives (ideas that are spread virally and affect public\nbeliefs) can influence economic fluctuations. We introduce two curated datasets\ncontaining posts from X (formerly Twitter) which capture economy-related\nnarratives (Data will be shared upon paper acceptance). Employing Natural\nLanguage Processing (NLP) methods, we extract and summarize narratives from the\ntweets. We test their predictive power for $\\textit{macroeconomic}$ forecasting\nby incorporating the tweets' or the extracted narratives' representations in\ndownstream financial prediction tasks. Our work highlights the challenges in\nimproving macroeconomic models with narrative data, paving the way for the\nresearch community to realistically address this important challenge. From a\nscientific perspective, our investigation offers valuable insights and NLP\ntools for narrative extraction and summarization using Large Language Models\n(LLMs), contributing to future research on the role of narratives in economics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study empirically tests the $\\textit{Narrative Economics}$ hypothesis,\nwhich posits that narratives (ideas that are spread virally and affect public\nbeliefs) can influence economic fluctuations. We introduce two curated datasets\ncontaining posts from X (formerly Twitter) which capture economy-related\nnarratives (Data will be shared upon paper acceptance). Employing Natural\nLanguage Processing (NLP) methods, we extract and summarize narratives from the\ntweets. We test their predictive power for $\\textit{macroeconomic}$ forecasting\nby incorporating the tweets' or the extracted narratives' representations in\ndownstream financial prediction tasks. Our work highlights the challenges in\nimproving macroeconomic models with narrative data, paving the way for the\nresearch community to realistically address this important challenge. From a\nscientific perspective, our investigation offers valuable insights and NLP\ntools for narrative extraction and summarization using Large Language Models\n(LLMs), contributing to future research on the role of narratives in economics."
                },
                "authors": [
                    {
                        "name": "Almog Gueta"
                    },
                    {
                        "name": "Amir Feder"
                    },
                    {
                        "name": "Zorik Gekhman"
                    },
                    {
                        "name": "Ariel Goldstein"
                    },
                    {
                        "name": "Roi Reichart"
                    }
                ],
                "author_detail": {
                    "name": "Roi Reichart"
                },
                "author": "Roi Reichart",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12109v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12109v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07346v1",
                "updated": "2025-02-11T08:17:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    8,
                    17,
                    19,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T08:17:19Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    8,
                    17,
                    19,
                    1,
                    42,
                    0
                ],
                "title": "BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large\n  Language Models"
                },
                "summary": "Previous multilingual benchmarks focus primarily on simple understanding\ntasks, but for large language models(LLMs), we emphasize proficiency in\ninstruction following, reasoning, long context understanding, code generation,\nand so on. However, measuring these advanced capabilities across languages is\nunderexplored. To address the disparity, we introduce BenchMAX, a multi-way\nmultilingual evaluation benchmark that allows for fair comparisons of these\nimportant abilities across languages. To maintain high quality, three distinct\nnative-speaking annotators independently annotate each sample within all tasks\nafter the data was machine-translated from English into 16 other languages.\nAdditionally, we present a novel translation challenge stemming from dataset\nconstruction. Extensive experiments on BenchMAX reveal varying effectiveness of\ncore capabilities across languages, highlighting performance gaps that cannot\nbe bridged by simply scaling up model size. BenchMAX serves as a comprehensive\nmultilingual evaluation platform, providing a promising test bed to promote the\ndevelopment of multilingual language models. The dataset and code are publicly\naccessible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous multilingual benchmarks focus primarily on simple understanding\ntasks, but for large language models(LLMs), we emphasize proficiency in\ninstruction following, reasoning, long context understanding, code generation,\nand so on. However, measuring these advanced capabilities across languages is\nunderexplored. To address the disparity, we introduce BenchMAX, a multi-way\nmultilingual evaluation benchmark that allows for fair comparisons of these\nimportant abilities across languages. To maintain high quality, three distinct\nnative-speaking annotators independently annotate each sample within all tasks\nafter the data was machine-translated from English into 16 other languages.\nAdditionally, we present a novel translation challenge stemming from dataset\nconstruction. Extensive experiments on BenchMAX reveal varying effectiveness of\ncore capabilities across languages, highlighting performance gaps that cannot\nbe bridged by simply scaling up model size. BenchMAX serves as a comprehensive\nmultilingual evaluation platform, providing a promising test bed to promote the\ndevelopment of multilingual language models. The dataset and code are publicly\naccessible."
                },
                "authors": [
                    {
                        "name": "Xu Huang"
                    },
                    {
                        "name": "Wenhao Zhu"
                    },
                    {
                        "name": "Hanxu Hu"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Shujian Huang"
                    },
                    {
                        "name": "Fei Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Fei Yuan"
                },
                "author": "Fei Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07340v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07340v1",
                "updated": "2025-02-11T08:05:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    8,
                    5,
                    56,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T08:05:56Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    8,
                    5,
                    56,
                    1,
                    42,
                    0
                ],
                "title": "Aligning Large Language Models to Follow Instructions and Hallucinate\n  Less via Effective Data Filtering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Large Language Models to Follow Instructions and Hallucinate\n  Less via Effective Data Filtering"
                },
                "summary": "Training LLMs on data that contains unfamiliar knowledge during the\ninstruction tuning stage can make LLMs overconfident and encourage\nhallucinations. To address this challenge, we introduce a novel framework,\nNOVA, which identifies high-quality data that aligns well with the LLM's\nlearned knowledge to reduce hallucinations. NOVA includes Internal Consistency\nProbing (ICP) and Semantic Equivalence Identification (SEI) to measure how\nfamiliar the LLM is with instruction data. Specifically, ICP evaluates the\nLLM's understanding of the given instruction by calculating the tailored\nconsistency among multiple self-generated responses. SEI further assesses the\nfamiliarity of the LLM with the target response by comparing it to the\ngenerated responses, using the proposed semantic clustering and well-designed\nvoting strategy. Finally, we introduce an expert-aligned reward model,\nconsidering characteristics beyond just familiarity to enhance data quality. By\nconsidering data quality and avoiding unfamiliar data, we can utilize the\nselected data to effectively align LLMs to follow instructions and hallucinate\nless. Extensive experiments and analysis show that NOVA significantly reduces\nhallucinations and allows LLMs to maintain a strong ability to follow\ninstructions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training LLMs on data that contains unfamiliar knowledge during the\ninstruction tuning stage can make LLMs overconfident and encourage\nhallucinations. To address this challenge, we introduce a novel framework,\nNOVA, which identifies high-quality data that aligns well with the LLM's\nlearned knowledge to reduce hallucinations. NOVA includes Internal Consistency\nProbing (ICP) and Semantic Equivalence Identification (SEI) to measure how\nfamiliar the LLM is with instruction data. Specifically, ICP evaluates the\nLLM's understanding of the given instruction by calculating the tailored\nconsistency among multiple self-generated responses. SEI further assesses the\nfamiliarity of the LLM with the target response by comparing it to the\ngenerated responses, using the proposed semantic clustering and well-designed\nvoting strategy. Finally, we introduce an expert-aligned reward model,\nconsidering characteristics beyond just familiarity to enhance data quality. By\nconsidering data quality and avoiding unfamiliar data, we can utilize the\nselected data to effectively align LLMs to follow instructions and hallucinate\nless. Extensive experiments and analysis show that NOVA significantly reduces\nhallucinations and allows LLMs to maintain a strong ability to follow\ninstructions."
                },
                "authors": [
                    {
                        "name": "Shuzheng Si"
                    },
                    {
                        "name": "Haozhe Zhao"
                    },
                    {
                        "name": "Gang Chen"
                    },
                    {
                        "name": "Cheng Gao"
                    },
                    {
                        "name": "Yuzhuo Bai"
                    },
                    {
                        "name": "Zhitong Wang"
                    },
                    {
                        "name": "Kaikai An"
                    },
                    {
                        "name": "Kangyang Luo"
                    },
                    {
                        "name": "Chen Qian"
                    },
                    {
                        "name": "Fanchao Qi"
                    },
                    {
                        "name": "Baobao Chang"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07340v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07340v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07323v1",
                "updated": "2025-02-11T07:42:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    7,
                    42,
                    44,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T07:42:44Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    7,
                    42,
                    44,
                    1,
                    42,
                    0
                ],
                "title": "Semantic to Structure: Learning Structural Representations for\n  Infringement Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic to Structure: Learning Structural Representations for\n  Infringement Detection"
                },
                "summary": "Structural information in images is crucial for aesthetic assessment, and it\nis widely recognized in the artistic field that imitating the structure of\nother works significantly infringes on creators' rights. The advancement of\ndiffusion models has led to AI-generated content imitating artists' structural\ncreations, yet effective detection methods are still lacking. In this paper, we\ndefine this phenomenon as \"structural infringement\" and propose a corresponding\ndetection method. Additionally, we develop quantitative metrics and create\nmanually annotated datasets for evaluation: the SIA dataset of synthesized\ndata, and the SIR dataset of real data. Due to the current lack of datasets for\nstructural infringement detection, we propose a new data synthesis strategy\nbased on diffusion models and LLM, successfully training a structural\ninfringement detection model. Experimental results show that our method can\nsuccessfully detect structural infringements and achieve notable improvements\non annotated test sets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structural information in images is crucial for aesthetic assessment, and it\nis widely recognized in the artistic field that imitating the structure of\nother works significantly infringes on creators' rights. The advancement of\ndiffusion models has led to AI-generated content imitating artists' structural\ncreations, yet effective detection methods are still lacking. In this paper, we\ndefine this phenomenon as \"structural infringement\" and propose a corresponding\ndetection method. Additionally, we develop quantitative metrics and create\nmanually annotated datasets for evaluation: the SIA dataset of synthesized\ndata, and the SIR dataset of real data. Due to the current lack of datasets for\nstructural infringement detection, we propose a new data synthesis strategy\nbased on diffusion models and LLM, successfully training a structural\ninfringement detection model. Experimental results show that our method can\nsuccessfully detect structural infringements and achieve notable improvements\non annotated test sets."
                },
                "authors": [
                    {
                        "name": "Chuanwei Huang"
                    },
                    {
                        "name": "Zexi Jia"
                    },
                    {
                        "name": "Hongyan Fei"
                    },
                    {
                        "name": "Yeshuang Zhu"
                    },
                    {
                        "name": "Zhiqiang Yuan"
                    },
                    {
                        "name": "Jinchao Zhang"
                    },
                    {
                        "name": "Jie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhou"
                },
                "author": "Jie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07322v1",
                "updated": "2025-02-11T07:42:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    7,
                    42,
                    9,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T07:42:09Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    7,
                    42,
                    9,
                    1,
                    42,
                    0
                ],
                "title": "MEMIT-Merge: Addressing MEMIT's Key-Value Conflicts in Same-Subject\n  Batch Editing for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEMIT-Merge: Addressing MEMIT's Key-Value Conflicts in Same-Subject\n  Batch Editing for LLMs"
                },
                "summary": "As large language models continue to scale up, knowledge editing techniques\nthat modify models' internal knowledge without full retraining have gained\nsignificant attention. MEMIT, a prominent batch editing algorithm, stands out\nfor its capability to perform mass knowledge modifications. However, we uncover\na critical limitation that MEMIT's editing efficacy significantly deteriorates\nwhen processing batches containing multiple edits sharing the same subject. Our\nanalysis reveals that the root cause lies in MEMIT's key value modeling\nframework: When multiple facts with the same subject in a batch are modeled\nthrough MEMIT's key value mechanism, identical keys (derived from the shared\nsubject) are forced to represent different values (corresponding to different\nknowledge), resulting in updates conflicts during editing. Addressing this\nissue, we propose MEMIT-Merge, an enhanced approach that merges value\ncomputation processes for facts sharing the same subject, effectively resolving\nthe performance degradation in same-subject batch editing scenarios.\nExperimental results demonstrate that when MEMIT's edit success rate drops to\naround 50% at larger batch sizes, MEMIT-Merge maintains a success rate\nexceeding 90%, showcasing remarkable robustness to subject entity collisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models continue to scale up, knowledge editing techniques\nthat modify models' internal knowledge without full retraining have gained\nsignificant attention. MEMIT, a prominent batch editing algorithm, stands out\nfor its capability to perform mass knowledge modifications. However, we uncover\na critical limitation that MEMIT's editing efficacy significantly deteriorates\nwhen processing batches containing multiple edits sharing the same subject. Our\nanalysis reveals that the root cause lies in MEMIT's key value modeling\nframework: When multiple facts with the same subject in a batch are modeled\nthrough MEMIT's key value mechanism, identical keys (derived from the shared\nsubject) are forced to represent different values (corresponding to different\nknowledge), resulting in updates conflicts during editing. Addressing this\nissue, we propose MEMIT-Merge, an enhanced approach that merges value\ncomputation processes for facts sharing the same subject, effectively resolving\nthe performance degradation in same-subject batch editing scenarios.\nExperimental results demonstrate that when MEMIT's edit success rate drops to\naround 50% at larger batch sizes, MEMIT-Merge maintains a success rate\nexceeding 90%, showcasing remarkable robustness to subject entity collisions."
                },
                "authors": [
                    {
                        "name": "Zilu Dong"
                    },
                    {
                        "name": "Xiangqing Shen"
                    },
                    {
                        "name": "Rui Xia"
                    }
                ],
                "author_detail": {
                    "name": "Rui Xia"
                },
                "author": "Rui Xia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05558v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05558v2",
                "updated": "2025-02-11T07:36:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    7,
                    36,
                    7,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-08T13:08:11Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    13,
                    8,
                    11,
                    5,
                    39,
                    0
                ],
                "title": "Large Memory Network for Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Memory Network for Recommendation"
                },
                "summary": "Modeling user behavior sequences in recommender systems is essential for\nunderstanding user preferences over time, enabling personalized and accurate\nrecommendations for improving user retention and enhancing business values.\nDespite its significance, there are two challenges for current sequential\nmodeling approaches. From the spatial dimension, it is difficult to mutually\nperceive similar users' interests for a generalized intention understanding;\nfrom the temporal dimension, current methods are generally prone to forgetting\nlong-term interests due to the fixed-length input sequence. In this paper, we\npresent Large Memory Network (LMN), providing a novel idea by compressing and\nstoring user history behavior information in a large-scale memory block. With\nthe elaborated online deployment strategy, the memory block can be easily\nscaled up to million-scale in the industry. Extensive offline comparison\nexperiments, memory scaling up experiments, and online A/B test on Douyin\nE-Commerce Search (ECS) are performed, validating the superior performance of\nLMN. Currently, LMN has been fully deployed in Douyin ECS, serving millions of\nusers each day.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling user behavior sequences in recommender systems is essential for\nunderstanding user preferences over time, enabling personalized and accurate\nrecommendations for improving user retention and enhancing business values.\nDespite its significance, there are two challenges for current sequential\nmodeling approaches. From the spatial dimension, it is difficult to mutually\nperceive similar users' interests for a generalized intention understanding;\nfrom the temporal dimension, current methods are generally prone to forgetting\nlong-term interests due to the fixed-length input sequence. In this paper, we\npresent Large Memory Network (LMN), providing a novel idea by compressing and\nstoring user history behavior information in a large-scale memory block. With\nthe elaborated online deployment strategy, the memory block can be easily\nscaled up to million-scale in the industry. Extensive offline comparison\nexperiments, memory scaling up experiments, and online A/B test on Douyin\nE-Commerce Search (ECS) are performed, validating the superior performance of\nLMN. Currently, LMN has been fully deployed in Douyin ECS, serving millions of\nusers each day."
                },
                "authors": [
                    {
                        "name": "Hui Lu"
                    },
                    {
                        "name": "Zheng Chai"
                    },
                    {
                        "name": "Yuchao Zheng"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Deping Xie"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Xun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xun Zhou"
                },
                "author": "Xun Zhou",
                "arxiv_journal_ref": "WWW 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05558v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05558v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01523v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01523v3",
                "updated": "2025-02-11T07:31:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    7,
                    31,
                    3,
                    1,
                    42,
                    0
                ],
                "published": "2024-12-02T14:16:03Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    14,
                    16,
                    3,
                    0,
                    337,
                    0
                ],
                "title": "FlexSP: Accelerating Large Language Model Training via Flexible Sequence\n  Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexSP: Accelerating Large Language Model Training via Flexible Sequence\n  Parallelism"
                },
                "summary": "Extending the context length (i.e., the maximum supported sequence length) of\nLLMs is of paramount significance. To facilitate long context training of LLMs,\nsequence parallelism has emerged as an essential technique, which scatters each\ninput sequence across multiple devices and necessitates communication to\nprocess the sequence. In essence, existing sequence parallelism methods assume\nhomogeneous sequence lengths (i.e., all input sequences are equal in length)\nand therefore leverages a single, static scattering strategy for all input\nsequences. However, in reality, the sequence lengths in LLM training corpora\nexhibit substantial variability, often following a long-tail distribution,\nwhich leads to workload heterogeneity.\n  In this paper, we show that employing a single, static strategy results in\ninefficiency and resource under-utilization, highlighting the need for adaptive\napproaches to handle the heterogeneous workloads across sequences. To address\nthis, we propose a heterogeneity-adaptive sequence parallelism method. For each\ntraining step, our approach captures the variability in sequence lengths and\nassigns the optimal combination of scattering strategies based on workload\ncharacteristics. We model this problem as a linear programming optimization and\ndesign an efficient and effective solver to find the optimal solution.\nFurthermore, we implement our method in a high-performance system that supports\nadaptive parallelization in distributed LLM training. Experimental results\ndemonstrate that our system outperforms state-of-the-art training frameworks by\nup to 1.98x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extending the context length (i.e., the maximum supported sequence length) of\nLLMs is of paramount significance. To facilitate long context training of LLMs,\nsequence parallelism has emerged as an essential technique, which scatters each\ninput sequence across multiple devices and necessitates communication to\nprocess the sequence. In essence, existing sequence parallelism methods assume\nhomogeneous sequence lengths (i.e., all input sequences are equal in length)\nand therefore leverages a single, static scattering strategy for all input\nsequences. However, in reality, the sequence lengths in LLM training corpora\nexhibit substantial variability, often following a long-tail distribution,\nwhich leads to workload heterogeneity.\n  In this paper, we show that employing a single, static strategy results in\ninefficiency and resource under-utilization, highlighting the need for adaptive\napproaches to handle the heterogeneous workloads across sequences. To address\nthis, we propose a heterogeneity-adaptive sequence parallelism method. For each\ntraining step, our approach captures the variability in sequence lengths and\nassigns the optimal combination of scattering strategies based on workload\ncharacteristics. We model this problem as a linear programming optimization and\ndesign an efficient and effective solver to find the optimal solution.\nFurthermore, we implement our method in a high-performance system that supports\nadaptive parallelization in distributed LLM training. Experimental results\ndemonstrate that our system outperforms state-of-the-art training frameworks by\nup to 1.98x."
                },
                "authors": [
                    {
                        "name": "Yujie Wang"
                    },
                    {
                        "name": "Shiju Wang"
                    },
                    {
                        "name": "Shenhan Zhu"
                    },
                    {
                        "name": "Fangcheng Fu"
                    },
                    {
                        "name": "Xinyi Liu"
                    },
                    {
                        "name": "Xuefeng Xiao"
                    },
                    {
                        "name": "Huixia Li"
                    },
                    {
                        "name": "Jiashi Li"
                    },
                    {
                        "name": "Faming Wu"
                    },
                    {
                        "name": "Bin Cui"
                    }
                ],
                "author_detail": {
                    "name": "Bin Cui"
                },
                "author": "Bin Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01523v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01523v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07315v1",
                "updated": "2025-02-11T07:25:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    7,
                    25,
                    57,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T07:25:57Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    7,
                    25,
                    57,
                    1,
                    42,
                    0
                ],
                "title": "Prompt-Based Document Modifications In Ranking Competitions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt-Based Document Modifications In Ranking Competitions"
                },
                "summary": "We study prompting-based approaches with Large Language Models (LLMs) for\nmodifying documents so as to promote their ranking in a competitive search\nsetting. Our methods are inspired by prior work on leveraging LLMs as rankers.\nWe evaluate our approach by deploying it as a bot in previous ranking\ncompetitions and in competitions we organized. Our findings demonstrate that\nour approach effectively improves document ranking while preserving high levels\nof faithfulness to the original content and maintaining overall document\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study prompting-based approaches with Large Language Models (LLMs) for\nmodifying documents so as to promote their ranking in a competitive search\nsetting. Our methods are inspired by prior work on leveraging LLMs as rankers.\nWe evaluate our approach by deploying it as a bot in previous ranking\ncompetitions and in competitions we organized. Our findings demonstrate that\nour approach effectively improves document ranking while preserving high levels\nof faithfulness to the original content and maintaining overall document\nquality."
                },
                "authors": [
                    {
                        "name": "Niv Bardas"
                    },
                    {
                        "name": "Tommy Mordo"
                    },
                    {
                        "name": "Oren Kurland"
                    },
                    {
                        "name": "Moshe Tennenholtz"
                    },
                    {
                        "name": "Gal Zur"
                    }
                ],
                "author_detail": {
                    "name": "Gal Zur"
                },
                "author": "Gal Zur",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07307v1",
                "updated": "2025-02-11T07:09:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    7,
                    9,
                    49,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T07:09:49Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    7,
                    9,
                    49,
                    1,
                    42,
                    0
                ],
                "title": "CreAgent: Towards Long-Term Evaluation of Recommender System under\n  Platform-Creator Information Asymmetry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CreAgent: Towards Long-Term Evaluation of Recommender System under\n  Platform-Creator Information Asymmetry"
                },
                "summary": "Ensuring the long-term sustainability of recommender systems (RS) emerges as\na crucial issue. Traditional offline evaluation methods for RS typically focus\non immediate user feedback, such as clicks, but they often neglect the\nlong-term impact of content creators. On real-world content platforms, creators\ncan strategically produce and upload new items based on user feedback and\npreference trends. While previous studies have attempted to model creator\nbehavior, they often overlook the role of information asymmetry. This asymmetry\narises because creators primarily have access to feedback on the items they\nproduce, while platforms possess data on the entire spectrum of user feedback.\nCurrent RS simulators, however, fail to account for this asymmetry, leading to\ninaccurate long-term evaluations. To address this gap, we propose CreAgent, a\nLarge Language Model (LLM)-empowered creator simulation agent. By incorporating\ngame theory's belief mechanism and the fast-and-slow thinking framework,\nCreAgent effectively simulates creator behavior under conditions of information\nasymmetry. Additionally, we enhance CreAgent's simulation ability by\nfine-tuning it using Proximal Policy Optimization (PPO). Our credibility\nvalidation experiments show that CreAgent aligns well with the behaviors\nbetween real-world platform and creator, thus improving the reliability of\nlong-term RS evaluations. Moreover, through the simulation of RS involving\nCreAgents, we can explore how fairness- and diversity-aware RS algorithms\ncontribute to better long-term performance for various stakeholders. CreAgent\nand the simulation platform are publicly available at\nhttps://github.com/shawnye2000/CreAgent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the long-term sustainability of recommender systems (RS) emerges as\na crucial issue. Traditional offline evaluation methods for RS typically focus\non immediate user feedback, such as clicks, but they often neglect the\nlong-term impact of content creators. On real-world content platforms, creators\ncan strategically produce and upload new items based on user feedback and\npreference trends. While previous studies have attempted to model creator\nbehavior, they often overlook the role of information asymmetry. This asymmetry\narises because creators primarily have access to feedback on the items they\nproduce, while platforms possess data on the entire spectrum of user feedback.\nCurrent RS simulators, however, fail to account for this asymmetry, leading to\ninaccurate long-term evaluations. To address this gap, we propose CreAgent, a\nLarge Language Model (LLM)-empowered creator simulation agent. By incorporating\ngame theory's belief mechanism and the fast-and-slow thinking framework,\nCreAgent effectively simulates creator behavior under conditions of information\nasymmetry. Additionally, we enhance CreAgent's simulation ability by\nfine-tuning it using Proximal Policy Optimization (PPO). Our credibility\nvalidation experiments show that CreAgent aligns well with the behaviors\nbetween real-world platform and creator, thus improving the reliability of\nlong-term RS evaluations. Moreover, through the simulation of RS involving\nCreAgents, we can explore how fairness- and diversity-aware RS algorithms\ncontribute to better long-term performance for various stakeholders. CreAgent\nand the simulation platform are publicly available at\nhttps://github.com/shawnye2000/CreAgent."
                },
                "authors": [
                    {
                        "name": "Xiaopeng Ye"
                    },
                    {
                        "name": "Chen Xu"
                    },
                    {
                        "name": "Zhongxiang Sun"
                    },
                    {
                        "name": "Jun Xu"
                    },
                    {
                        "name": "Gang Wang"
                    },
                    {
                        "name": "Zhenhua Dong"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]