[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.10819v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10819v1",
                "updated": "2024-10-14T17:59:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    58,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:59:58Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    58,
                    0,
                    288,
                    0
                ],
                "title": "DuoAttention: Efficient Long-Context LLM Inference with Retrieval and\n  Streaming Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DuoAttention: Efficient Long-Context LLM Inference with Retrieval and\n  Streaming Heads"
                },
                "summary": "Deploying long-context large language models (LLMs) is essential but poses\nsignificant computational and memory challenges. Caching all Key and Value (KV)\nstates across all attention heads consumes substantial memory. Existing KV\ncache pruning methods either damage the long-context capabilities of LLMs or\noffer only limited efficiency improvements. In this paper, we identify that\nonly a fraction of attention heads, a.k.a, Retrieval Heads, are critical for\nprocessing long contexts and require full attention across all tokens. In\ncontrast, all other heads, which primarily focus on recent tokens and attention\nsinks--referred to as Streaming Heads--do not require full attention. Based on\nthis insight, we introduce DuoAttention, a framework that only applies a full\nKV cache to retrieval heads while using a light-weight, constant-length KV\ncache for streaming heads, which reduces both LLM's decoding and pre-filling\nmemory and latency without compromising its long-context abilities.\nDuoAttention uses a lightweight, optimization-based algorithm with synthetic\ndata to identify retrieval heads accurately. Our method significantly reduces\nlong-context inference memory by up to 2.55x for MHA and 1.67x for GQA models\nwhile speeding up decoding by up to 2.18x and 1.50x and accelerating\npre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with\nminimal accuracy loss compared to full attention. Notably, combined with\nquantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context\nlength on a single A100 GPU. Code is provided in\nhttps://github.com/mit-han-lab/duo-attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying long-context large language models (LLMs) is essential but poses\nsignificant computational and memory challenges. Caching all Key and Value (KV)\nstates across all attention heads consumes substantial memory. Existing KV\ncache pruning methods either damage the long-context capabilities of LLMs or\noffer only limited efficiency improvements. In this paper, we identify that\nonly a fraction of attention heads, a.k.a, Retrieval Heads, are critical for\nprocessing long contexts and require full attention across all tokens. In\ncontrast, all other heads, which primarily focus on recent tokens and attention\nsinks--referred to as Streaming Heads--do not require full attention. Based on\nthis insight, we introduce DuoAttention, a framework that only applies a full\nKV cache to retrieval heads while using a light-weight, constant-length KV\ncache for streaming heads, which reduces both LLM's decoding and pre-filling\nmemory and latency without compromising its long-context abilities.\nDuoAttention uses a lightweight, optimization-based algorithm with synthetic\ndata to identify retrieval heads accurately. Our method significantly reduces\nlong-context inference memory by up to 2.55x for MHA and 1.67x for GQA models\nwhile speeding up decoding by up to 2.18x and 1.50x and accelerating\npre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with\nminimal accuracy loss compared to full attention. Notably, combined with\nquantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context\nlength on a single A100 GPU. Code is provided in\nhttps://github.com/mit-han-lab/duo-attention."
                },
                "authors": [
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Jingwei Zuo"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10819v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10781v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10781v1",
                "updated": "2024-10-14T17:50:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    50,
                    28,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:50:28Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    50,
                    28,
                    0,
                    288,
                    0
                ],
                "title": "When Attention Sink Emerges in Language Models: An Empirical View",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Attention Sink Emerges in Language Models: An Empirical View"
                },
                "summary": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink."
                },
                "authors": [
                    {
                        "name": "Xiangming Gu"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10781v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10781v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10511v1",
                "updated": "2024-10-14T13:49:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    49,
                    6,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T13:49:06Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    49,
                    6,
                    0,
                    288,
                    0
                ],
                "title": "Customize Your Visual Autoregressive Recipe with Set Autoregressive\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Customize Your Visual Autoregressive Recipe with Set Autoregressive\n  Modeling"
                },
                "summary": "We introduce a new paradigm for AutoRegressive (AR) image generation, termed\nSet AutoRegressive Modeling (SAR). SAR generalizes the conventional AR to the\nnext-set setting, i.e., splitting the sequence into arbitrary sets containing\nmultiple tokens, rather than outputting each token in a fixed raster order. To\naccommodate SAR, we develop a straightforward architecture termed Fully Masked\nTransformer. We reveal that existing AR variants correspond to specific design\nchoices of sequence order and output intervals within the SAR framework, with\nAR and Masked AR (MAR) as two extreme instances. Notably, SAR facilitates a\nseamless transition from AR to MAR, where intermediate states allow for\ntraining a causal model that benefits from both few-step inference and KV cache\nacceleration, thus leveraging the advantages of both AR and MAR. On the\nImageNet benchmark, we carefully explore the properties of SAR by analyzing the\nimpact of sequence order and output intervals on performance, as well as the\ngeneralization ability regarding inference order and steps. We further validate\nthe potential of SAR by training a 900M text-to-image model capable of\nsynthesizing photo-realistic images with any resolution. We hope our work may\ninspire more exploration and application of AR-based modeling across diverse\nmodalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a new paradigm for AutoRegressive (AR) image generation, termed\nSet AutoRegressive Modeling (SAR). SAR generalizes the conventional AR to the\nnext-set setting, i.e., splitting the sequence into arbitrary sets containing\nmultiple tokens, rather than outputting each token in a fixed raster order. To\naccommodate SAR, we develop a straightforward architecture termed Fully Masked\nTransformer. We reveal that existing AR variants correspond to specific design\nchoices of sequence order and output intervals within the SAR framework, with\nAR and Masked AR (MAR) as two extreme instances. Notably, SAR facilitates a\nseamless transition from AR to MAR, where intermediate states allow for\ntraining a causal model that benefits from both few-step inference and KV cache\nacceleration, thus leveraging the advantages of both AR and MAR. On the\nImageNet benchmark, we carefully explore the properties of SAR by analyzing the\nimpact of sequence order and output intervals on performance, as well as the\ngeneralization ability regarding inference order and steps. We further validate\nthe potential of SAR by training a 900M text-to-image model capable of\nsynthesizing photo-realistic images with any resolution. We hope our work may\ninspire more exploration and application of AR-based modeling across diverse\nmodalities."
                },
                "authors": [
                    {
                        "name": "Wenze Liu"
                    },
                    {
                        "name": "Le Zhuo"
                    },
                    {
                        "name": "Yi Xin"
                    },
                    {
                        "name": "Sheng Xia"
                    },
                    {
                        "name": "Peng Gao"
                    },
                    {
                        "name": "Xiangyu Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Yue"
                },
                "author": "Xiangyu Yue",
                "arxiv_comment": "19 pages, 17 figures, 8 tables, github repo:\n  https://github.com/poppuppy/SAR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05317v2",
                "updated": "2024-10-14T09:35:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    9,
                    35,
                    35,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-05T03:47:06Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    47,
                    6,
                    5,
                    279,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Token-wise Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Token-wise Feature Caching"
                },
                "summary": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality."
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13378v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13378v2",
                "updated": "2024-10-14T07:58:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    7,
                    58,
                    39,
                    0,
                    288,
                    0
                ],
                "published": "2024-05-22T06:19:43Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    6,
                    19,
                    43,
                    2,
                    143,
                    0
                ],
                "title": "FedCache 2.0: Federated Edge Learning with Knowledge Caching and Dataset\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedCache 2.0: Federated Edge Learning with Knowledge Caching and Dataset\n  Distillation"
                },
                "summary": "Federated Edge Learning (FEL) has emerged as a promising approach for\nenabling edge devices to collaboratively train machine learning models while\npreserving data privacy. Despite its advantages, practical FEL deployment faces\nsignificant challenges related to device constraints and device-server\ninteractions, necessitating heterogeneous, user-adaptive model training with\nlimited and uncertain communication. In this paper, we introduce FedCache 2.0,\na novel personalized FEL architecture that simultaneously addresses these\nchallenges. FedCache 2.0 incorporates the benefits of both dataset distillation\nand knowledge cache-driven federated learning by storing and organizing\ndistilled data as knowledge in the server-side knowledge cache. Moreover, a\ndevice-centric cache sampling strategy is introduced to tailor transferred\nknowledge for individual devices within controlled communication bandwidth.\nExtensive experiments on five datasets covering image recognition, audio\nunderstanding, and mobile sensor data mining tasks demonstrate that (1)\nFedCache 2.0 significantly outperforms state-of-the-art methods regardless of\nmodel structures, data distributions, and modalities. (2) FedCache 2.0 can\ntrain splendid personalized on-device models with at least $\\times$28.6\nimprovement in communication efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Edge Learning (FEL) has emerged as a promising approach for\nenabling edge devices to collaboratively train machine learning models while\npreserving data privacy. Despite its advantages, practical FEL deployment faces\nsignificant challenges related to device constraints and device-server\ninteractions, necessitating heterogeneous, user-adaptive model training with\nlimited and uncertain communication. In this paper, we introduce FedCache 2.0,\na novel personalized FEL architecture that simultaneously addresses these\nchallenges. FedCache 2.0 incorporates the benefits of both dataset distillation\nand knowledge cache-driven federated learning by storing and organizing\ndistilled data as knowledge in the server-side knowledge cache. Moreover, a\ndevice-centric cache sampling strategy is introduced to tailor transferred\nknowledge for individual devices within controlled communication bandwidth.\nExtensive experiments on five datasets covering image recognition, audio\nunderstanding, and mobile sensor data mining tasks demonstrate that (1)\nFedCache 2.0 significantly outperforms state-of-the-art methods regardless of\nmodel structures, data distributions, and modalities. (2) FedCache 2.0 can\ntrain splendid personalized on-device models with at least $\\times$28.6\nimprovement in communication efficiency."
                },
                "authors": [
                    {
                        "name": "Quyang Pan"
                    },
                    {
                        "name": "Sheng Sun"
                    },
                    {
                        "name": "Zhiyuan Wu"
                    },
                    {
                        "name": "Yuwei Wang"
                    },
                    {
                        "name": "Min Liu"
                    },
                    {
                        "name": "Bo Gao"
                    },
                    {
                        "name": "Jingyuan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jingyuan Wang"
                },
                "author": "Jingyuan Wang",
                "arxiv_comment": "17 pages, 7 figures, 14 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13378v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13378v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10157v1",
                "updated": "2024-10-14T04:49:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    49,
                    22,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T04:49:22Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    49,
                    22,
                    0,
                    288,
                    0
                ],
                "title": "Caching Content Placement and Beamforming Co-design for IRS-Aided MIMO\n  Systems with Imperfect CSI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching Content Placement and Beamforming Co-design for IRS-Aided MIMO\n  Systems with Imperfect CSI"
                },
                "summary": "When offloading links encounter deep fading and obstruction, edge caching\ncannot fully enhance wireless network performance and improve the QoS of edge\nnodes, as it fails to effectively reduce backhaul burden. The emerging\ntechnology of intelligent reflecting surfaces (IRS) compensates for this\ndisadvantage by creating a smart and reconfigurable wireless environment.\nSubsequently, we jointly design content placement and active/passive\nbeamforming to minimize network costs under imperfect channel state information\n(CSI) in the IRS-oriented edge caching system. This minimization problem is\ndecomposed into two subproblems. The content placement subproblem is addressed\nby applying KKT optimality conditions. We then develop the alternating\noptimization method to resolve precoder and reflection beamforming.\nSpecifically, we reduce transmission power by first fixing the phase shift,\nreducing the problem to a convex one relative to the precoder, which is solved\nthrough convex optimization. Next, we fix the precoder and resolve the\nresulting reflection beamforming problem using the penalty convex-concave\nprocedure (CCP) method. Results demonstrate that our proposed method\noutperforms uniform caching and random phase approaches in reducing\ntransmission power and saving network costs. Eventually, the proposed approach\noffers potential improvements in the caching optimization and transmission\nrobustness of wireless communication with imperfect CSI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When offloading links encounter deep fading and obstruction, edge caching\ncannot fully enhance wireless network performance and improve the QoS of edge\nnodes, as it fails to effectively reduce backhaul burden. The emerging\ntechnology of intelligent reflecting surfaces (IRS) compensates for this\ndisadvantage by creating a smart and reconfigurable wireless environment.\nSubsequently, we jointly design content placement and active/passive\nbeamforming to minimize network costs under imperfect channel state information\n(CSI) in the IRS-oriented edge caching system. This minimization problem is\ndecomposed into two subproblems. The content placement subproblem is addressed\nby applying KKT optimality conditions. We then develop the alternating\noptimization method to resolve precoder and reflection beamforming.\nSpecifically, we reduce transmission power by first fixing the phase shift,\nreducing the problem to a convex one relative to the precoder, which is solved\nthrough convex optimization. Next, we fix the precoder and resolve the\nresulting reflection beamforming problem using the penalty convex-concave\nprocedure (CCP) method. Results demonstrate that our proposed method\noutperforms uniform caching and random phase approaches in reducing\ntransmission power and saving network costs. Eventually, the proposed approach\noffers potential improvements in the caching optimization and transmission\nrobustness of wireless communication with imperfect CSI."
                },
                "authors": [
                    {
                        "name": "Meng Gao"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Huafu Li"
                    },
                    {
                        "name": "Junqi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Junqi Guo"
                },
                "author": "Junqi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10149v1",
                "updated": "2024-10-14T04:30:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    30,
                    38,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T04:30:38Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    30,
                    38,
                    0,
                    288,
                    0
                ],
                "title": "Fast and Accurate Neural Rendering Using Semi-Gradients",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Accurate Neural Rendering Using Semi-Gradients"
                },
                "summary": "We propose a simple yet effective neural network-based framework for global\nillumination rendering. Recently, rendering techniques that learn neural\nradiance caches by minimizing the difference (i.e., residual) between the left\nand right sides of the rendering equation have been suggested. Due to their\nease of implementation and the advantage of excluding path integral\ncalculations, these techniques have been applied to various fields, such as\nfree-viewpoint rendering, differentiable rendering, and real-time rendering.\nHowever, issues of slow training and occasionally darkened renders have been\nnoted. We identify the cause of these issues as the bias and high variance\npresent in the gradient estimates of the existing residual-based objective\nfunction. To address this, we introduce a new objective function that maintains\nthe same global optimum as before but allows for unbiased and low-variance\ngradient estimates, enabling faster and more accurate training of neural\nnetworks. In conclusion, this method is simply implemented by ignoring the\npartial derivatives of the right-hand side, and theoretical and experimental\nanalyses demonstrate the effectiveness of the proposed loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a simple yet effective neural network-based framework for global\nillumination rendering. Recently, rendering techniques that learn neural\nradiance caches by minimizing the difference (i.e., residual) between the left\nand right sides of the rendering equation have been suggested. Due to their\nease of implementation and the advantage of excluding path integral\ncalculations, these techniques have been applied to various fields, such as\nfree-viewpoint rendering, differentiable rendering, and real-time rendering.\nHowever, issues of slow training and occasionally darkened renders have been\nnoted. We identify the cause of these issues as the bias and high variance\npresent in the gradient estimates of the existing residual-based objective\nfunction. To address this, we introduce a new objective function that maintains\nthe same global optimum as before but allows for unbiased and low-variance\ngradient estimates, enabling faster and more accurate training of neural\nnetworks. In conclusion, this method is simply implemented by ignoring the\npartial derivatives of the right-hand side, and theoretical and experimental\nanalyses demonstrate the effectiveness of the proposed loss."
                },
                "authors": [
                    {
                        "name": "In-Young Cho"
                    },
                    {
                        "name": "Jaewoong Cho"
                    }
                ],
                "author_detail": {
                    "name": "Jaewoong Cho"
                },
                "author": "Jaewoong Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10071v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10071v1",
                "updated": "2024-10-14T01:25:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    1,
                    25,
                    56,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T01:25:56Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    1,
                    25,
                    56,
                    0,
                    288,
                    0
                ],
                "title": "Content Caching-Assisted Vehicular Edge Computing Using Multi-Agent\n  Graph Attention Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content Caching-Assisted Vehicular Edge Computing Using Multi-Agent\n  Graph Attention Reinforcement Learning"
                },
                "summary": "In order to avoid repeated task offloading and realize the reuse of popular\ntask computing results, we construct a novel content caching-assisted vehicular\nedge computing (VEC) framework. In the face of irregular network topology and\nunknown environmental dynamics, we further propose a multi-agent graph\nattention reinforcement learning (MGARL) based edge caching scheme, which\nutilizes the graph attention convolution kernel to integrate the neighboring\nnodes' features of each agent and further enhance the cooperation among agents.\nOur simulation results show that our proposed scheme is capable of improving\nthe utilization of caching resources while reducing the long-term task\ncomputing latency compared to the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In order to avoid repeated task offloading and realize the reuse of popular\ntask computing results, we construct a novel content caching-assisted vehicular\nedge computing (VEC) framework. In the face of irregular network topology and\nunknown environmental dynamics, we further propose a multi-agent graph\nattention reinforcement learning (MGARL) based edge caching scheme, which\nutilizes the graph attention convolution kernel to integrate the neighboring\nnodes' features of each agent and further enhance the cooperation among agents.\nOur simulation results show that our proposed scheme is capable of improving\nthe utilization of caching resources while reducing the long-term task\ncomputing latency compared to the baselines."
                },
                "authors": [
                    {
                        "name": "Jinjin Shen"
                    },
                    {
                        "name": "Yan Lin"
                    },
                    {
                        "name": "Yijin Zhang"
                    },
                    {
                        "name": "Weibin Zhang"
                    },
                    {
                        "name": "Feng Shu"
                    },
                    {
                        "name": "Jun Li"
                    }
                ],
                "author_detail": {
                    "name": "Jun Li"
                },
                "author": "Jun Li",
                "arxiv_comment": "6 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10071v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10071v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09533v1",
                "updated": "2024-10-12T13:45:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    12,
                    13,
                    45,
                    26,
                    5,
                    286,
                    0
                ],
                "published": "2024-10-12T13:45:26Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    13,
                    45,
                    26,
                    5,
                    286,
                    0
                ],
                "title": "Leveraging Semantic Cues from Foundation Vision Models for Enhanced\n  Local Feature Correspondence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Semantic Cues from Foundation Vision Models for Enhanced\n  Local Feature Correspondence"
                },
                "summary": "Visual correspondence is a crucial step in key computer vision tasks,\nincluding camera localization, image registration, and structure from motion.\nThe most effective techniques for matching keypoints currently involve using\nlearned sparse or dense matchers, which need pairs of images. These neural\nnetworks have a good general understanding of features from both images, but\nthey often struggle to match points from different semantic areas. This paper\npresents a new method that uses semantic cues from foundation vision model\nfeatures (like DINOv2) to enhance local feature matching by incorporating\nsemantic reasoning into existing descriptors. Therefore, the learned\ndescriptors do not require image pairs at inference time, allowing feature\ncaching and fast matching using similarity search, unlike learned matchers. We\npresent adapted versions of six existing descriptors, with an average increase\nin performance of 29% in camera localization, with comparable accuracy to\nexisting matchers as LightGlue and LoFTR in two existing benchmarks. Both code\nand trained models are available at\nhttps://www.verlab.dcc.ufmg.br/descriptors/reasoning_accv24",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual correspondence is a crucial step in key computer vision tasks,\nincluding camera localization, image registration, and structure from motion.\nThe most effective techniques for matching keypoints currently involve using\nlearned sparse or dense matchers, which need pairs of images. These neural\nnetworks have a good general understanding of features from both images, but\nthey often struggle to match points from different semantic areas. This paper\npresents a new method that uses semantic cues from foundation vision model\nfeatures (like DINOv2) to enhance local feature matching by incorporating\nsemantic reasoning into existing descriptors. Therefore, the learned\ndescriptors do not require image pairs at inference time, allowing feature\ncaching and fast matching using similarity search, unlike learned matchers. We\npresent adapted versions of six existing descriptors, with an average increase\nin performance of 29% in camera localization, with comparable accuracy to\nexisting matchers as LightGlue and LoFTR in two existing benchmarks. Both code\nand trained models are available at\nhttps://www.verlab.dcc.ufmg.br/descriptors/reasoning_accv24"
                },
                "authors": [
                    {
                        "name": "Felipe Cadar"
                    },
                    {
                        "name": "Guilherme Potje"
                    },
                    {
                        "name": "Renato Martins"
                    },
                    {
                        "name": "Cdric Demonceaux"
                    },
                    {
                        "name": "Erickson R. Nascimento"
                    }
                ],
                "author_detail": {
                    "name": "Erickson R. Nascimento"
                },
                "author": "Erickson R. Nascimento",
                "arxiv_comment": "Accepted in ACCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09479v1",
                "updated": "2024-10-12T10:38:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    12,
                    10,
                    38,
                    39,
                    5,
                    286,
                    0
                ],
                "published": "2024-10-12T10:38:39Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    10,
                    38,
                    39,
                    5,
                    286,
                    0
                ],
                "title": "Viscoelastic Effects on the Hydrodynamics of an Active Compound Particle",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Viscoelastic Effects on the Hydrodynamics of an Active Compound Particle"
                },
                "summary": "Understanding the hydrodynamics of microswimmers in viscoelastic fluids and\nconfined environments is crucial for interpreting their behaviour in natural\nsettings and designing synthetic microswimmers for practical applications like\ncargo transport. In this study, we explore the hydrodynamics of a concentric\nactive compound particle - a model microswimmer (a squirmer) positioned at the\ncentre of a viscoelastic fluid droplet (a model cargo) suspended in another\nviscoelastic medium. We consider the Oldroyd-B constitutive model to\ncharacterize the fluids and employ a perturbative approach in the Deborah\nnumber to analyze viscoelastic effects analytically, assuming a small Capillary\nnumber so that the droplet remains spherical and does not deform. We examine\nthree cases: (i) a squirmer confined within a viscoelastic fluid droplet\nsuspended in a Newtonian fluid, (ii) a squirmer confined within a Newtonian\nfluid droplet suspended in a viscoelastic fluid, and (iii) a squirmer confined\nwithin a viscoelastic fluid droplet suspended in another viscoelastic fluid.\nOur findings reveal that the swimming speeds of the squirmer and the droplet\nare determined by the complex interplay of viscoelasticity, the size ratio of\nthe droplet to the squirmer (confinement strength), and the viscosity ratio of\nthe surrounding fluid to the droplet fluid. A critical aspect of this\ninteraction is the positioning of stagnation points within the fluid flow,\nwhich governs the distribution of polymeric stress. This distribution, in turn,\nplays a crucial role in determining the influence of viscoelasticity on the\nsquirmer's dynamics. Our analysis suggests that viscoelastic effects can either\nenhance or hinder the swimming speed of the squirmer when confined in a\ndroplet, depending on the specific configuration of the system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the hydrodynamics of microswimmers in viscoelastic fluids and\nconfined environments is crucial for interpreting their behaviour in natural\nsettings and designing synthetic microswimmers for practical applications like\ncargo transport. In this study, we explore the hydrodynamics of a concentric\nactive compound particle - a model microswimmer (a squirmer) positioned at the\ncentre of a viscoelastic fluid droplet (a model cargo) suspended in another\nviscoelastic medium. We consider the Oldroyd-B constitutive model to\ncharacterize the fluids and employ a perturbative approach in the Deborah\nnumber to analyze viscoelastic effects analytically, assuming a small Capillary\nnumber so that the droplet remains spherical and does not deform. We examine\nthree cases: (i) a squirmer confined within a viscoelastic fluid droplet\nsuspended in a Newtonian fluid, (ii) a squirmer confined within a Newtonian\nfluid droplet suspended in a viscoelastic fluid, and (iii) a squirmer confined\nwithin a viscoelastic fluid droplet suspended in another viscoelastic fluid.\nOur findings reveal that the swimming speeds of the squirmer and the droplet\nare determined by the complex interplay of viscoelasticity, the size ratio of\nthe droplet to the squirmer (confinement strength), and the viscosity ratio of\nthe surrounding fluid to the droplet fluid. A critical aspect of this\ninteraction is the positioning of stagnation points within the fluid flow,\nwhich governs the distribution of polymeric stress. This distribution, in turn,\nplays a crucial role in determining the influence of viscoelasticity on the\nsquirmer's dynamics. Our analysis suggests that viscoelastic effects can either\nenhance or hinder the swimming speed of the squirmer when confined in a\ndroplet, depending on the specific configuration of the system."
                },
                "authors": [
                    {
                        "name": "KVS Chaithanya"
                    },
                    {
                        "name": "Sumesh P. Thampi"
                    }
                ],
                "author_detail": {
                    "name": "Sumesh P. Thampi"
                },
                "author": "Sumesh P. Thampi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09397v1",
                "updated": "2024-10-12T07:01:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    12,
                    7,
                    1,
                    30,
                    5,
                    286,
                    0
                ],
                "published": "2024-10-12T07:01:30Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    7,
                    1,
                    30,
                    5,
                    286,
                    0
                ],
                "title": "Fine-grained Attention I/O Complexity: Comprehensive Analysis for\n  Backward Passes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-grained Attention I/O Complexity: Comprehensive Analysis for\n  Backward Passes"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nprocessing long-context information. However, the quadratic complexity of\nattention computation with respect to sequence length poses significant\ncomputational challenges, and I/O aware algorithms have been proposed. This\npaper presents a comprehensive analysis of the I/O complexity for attention\nmechanisms, focusing on backward passes by categorizing into small and large\ncache scenarios. Using the red-blue pebble game framework, we establish tight\nbounds on I/O complexity across all cache sizes. We confirm that the de facto\nstandard I/O aware algorithm FlashAttention is optimal for both forward and\nbackward passes for the large cache size scenario. For small cache sizes, we\nprovide an algorithm that improves over existing methods and achieves the tight\nbounds. Additionally, we extend our analysis to sparse attention, a mainstream\nspeeding-up approach, deriving fine-grained lower bounds for both forward and\nbackward passes and both small and large caches. Our findings complete the\ntheoretical foundation for I/O complexity in attention mechanisms, offering\ninsights for designing efficient algorithms of LLM training and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nprocessing long-context information. However, the quadratic complexity of\nattention computation with respect to sequence length poses significant\ncomputational challenges, and I/O aware algorithms have been proposed. This\npaper presents a comprehensive analysis of the I/O complexity for attention\nmechanisms, focusing on backward passes by categorizing into small and large\ncache scenarios. Using the red-blue pebble game framework, we establish tight\nbounds on I/O complexity across all cache sizes. We confirm that the de facto\nstandard I/O aware algorithm FlashAttention is optimal for both forward and\nbackward passes for the large cache size scenario. For small cache sizes, we\nprovide an algorithm that improves over existing methods and achieves the tight\nbounds. Additionally, we extend our analysis to sparse attention, a mainstream\nspeeding-up approach, deriving fine-grained lower bounds for both forward and\nbackward passes and both small and large caches. Our findings complete the\ntheoretical foundation for I/O complexity in attention mechanisms, offering\ninsights for designing efficient algorithms of LLM training and inference."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Yingyu Liang"
                    },
                    {
                        "name": "Zhenmei Shi"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yufa Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yufa Zhou"
                },
                "author": "Yufa Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14360v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14360v3",
                "updated": "2024-10-12T02:11:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    12,
                    2,
                    11,
                    14,
                    5,
                    286,
                    0
                ],
                "published": "2024-09-22T08:30:43Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    8,
                    30,
                    43,
                    6,
                    266,
                    0
                ],
                "title": "In-place Switch: Reprogramming based SLC Cache Design for Hybrid 3D SSDs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-place Switch: Reprogramming based SLC Cache Design for Hybrid 3D SSDs"
                },
                "summary": "Recently, 3D SSDs are widely adopted in PCs, data centers, and cloud storage\nsystems. To increase capacity, high bit-density cells, such as Triple-Level\nCell (TLC), are utilized within 3D SSDs. However, due to the inferior\nperformance of TLC, a portion of TLCs is configured to operate as Single-Level\nCell (SLC) to provide high performance, with host data initially directed to\nthe SLCs. In SLC/TLC hybrid 3D SSDs, a portion of the TLC space is designated\nas an SLC cache to achieve high SSD performance by writing host data at the SLC\nspeed. Given the limited size of the SLC cache, block reclamation is necessary\nto free up the SLC cache during idle periods. However, our preliminary studies\nindicate that the SLC cache can lead to a performance cliff if filled rapidly\nand cause significant write amplification when data migration occurs during\nidle times.\n  In this work, we propose leveraging a reprogram operation to address these\nchallenges. Specifically, when the SLC cache is full or during idle periods, a\nreprogram operation is performed to switch used SLC pages to TLC pages in place\n(termed In-place Switch, IPS). Subsequently, other free TLC space is allocated\nas the new SLC cache. IPS can continuously provide sufficient SLC cache within\nSSDs, significantly improving write performance and reducing write\namplification. Experimental results demonstrate that IPS can reduce write\nlatency and write amplification by up to 0.75 times and 0.53 times,\nrespectively, compared to state-of-the-art SLC cache technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, 3D SSDs are widely adopted in PCs, data centers, and cloud storage\nsystems. To increase capacity, high bit-density cells, such as Triple-Level\nCell (TLC), are utilized within 3D SSDs. However, due to the inferior\nperformance of TLC, a portion of TLCs is configured to operate as Single-Level\nCell (SLC) to provide high performance, with host data initially directed to\nthe SLCs. In SLC/TLC hybrid 3D SSDs, a portion of the TLC space is designated\nas an SLC cache to achieve high SSD performance by writing host data at the SLC\nspeed. Given the limited size of the SLC cache, block reclamation is necessary\nto free up the SLC cache during idle periods. However, our preliminary studies\nindicate that the SLC cache can lead to a performance cliff if filled rapidly\nand cause significant write amplification when data migration occurs during\nidle times.\n  In this work, we propose leveraging a reprogram operation to address these\nchallenges. Specifically, when the SLC cache is full or during idle periods, a\nreprogram operation is performed to switch used SLC pages to TLC pages in place\n(termed In-place Switch, IPS). Subsequently, other free TLC space is allocated\nas the new SLC cache. IPS can continuously provide sufficient SLC cache within\nSSDs, significantly improving write performance and reducing write\namplification. Experimental results demonstrate that IPS can reduce write\nlatency and write amplification by up to 0.75 times and 0.53 times,\nrespectively, compared to state-of-the-art SLC cache technologies."
                },
                "authors": [
                    {
                        "name": "Xufeng Yang"
                    },
                    {
                        "name": "Zhengjian Cong"
                    },
                    {
                        "name": "Congming Gao"
                    }
                ],
                "author_detail": {
                    "name": "Congming Gao"
                },
                "author": "Congming Gao",
                "arxiv_comment": "This paper has been submitted to NAS'24 (The 17th International\n  Conference on Networking, Architecture and Storage)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14360v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14360v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09237v1",
                "updated": "2024-10-11T20:23:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    20,
                    23,
                    0,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T20:23:00Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    20,
                    23,
                    0,
                    4,
                    285,
                    0
                ],
                "title": "Foundation Model-Powered 3D Few-Shot Class Incremental Learning via\n  Training-free Adaptor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation Model-Powered 3D Few-Shot Class Incremental Learning via\n  Training-free Adaptor"
                },
                "summary": "Recent advances in deep learning for processing point clouds hold increased\ninterest in Few-Shot Class Incremental Learning (FSCIL) for 3D computer vision.\nThis paper introduces a new method to tackle the Few-Shot Continual Incremental\nLearning (FSCIL) problem in 3D point cloud environments. We leverage a\nfoundational 3D model trained extensively on point cloud data. Drawing from\nrecent improvements in foundation models, known for their ability to work well\nacross different tasks, we propose a novel strategy that does not require\nadditional training to adapt to new tasks. Our approach uses a dual cache\nsystem: first, it uses previous test samples based on how confident the model\nwas in its predictions to prevent forgetting, and second, it includes a small\nnumber of new task samples to prevent overfitting. This dynamic adaptation\nensures strong performance across different learning tasks without needing lots\nof fine-tuning. We tested our approach on datasets like ModelNet, ShapeNet,\nScanObjectNN, and CO3D, showing that it outperforms other FSCIL methods and\ndemonstrating its effectiveness and versatility. The code is available at\n\\url{https://github.com/ahmadisahar/ACCV_FCIL3D}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in deep learning for processing point clouds hold increased\ninterest in Few-Shot Class Incremental Learning (FSCIL) for 3D computer vision.\nThis paper introduces a new method to tackle the Few-Shot Continual Incremental\nLearning (FSCIL) problem in 3D point cloud environments. We leverage a\nfoundational 3D model trained extensively on point cloud data. Drawing from\nrecent improvements in foundation models, known for their ability to work well\nacross different tasks, we propose a novel strategy that does not require\nadditional training to adapt to new tasks. Our approach uses a dual cache\nsystem: first, it uses previous test samples based on how confident the model\nwas in its predictions to prevent forgetting, and second, it includes a small\nnumber of new task samples to prevent overfitting. This dynamic adaptation\nensures strong performance across different learning tasks without needing lots\nof fine-tuning. We tested our approach on datasets like ModelNet, ShapeNet,\nScanObjectNN, and CO3D, showing that it outperforms other FSCIL methods and\ndemonstrating its effectiveness and versatility. The code is available at\n\\url{https://github.com/ahmadisahar/ACCV_FCIL3D}."
                },
                "authors": [
                    {
                        "name": "Sahar Ahmadi"
                    },
                    {
                        "name": "Ali Cheraghian"
                    },
                    {
                        "name": "Morteza Saberi"
                    },
                    {
                        "name": "Md. Towsif Abir"
                    },
                    {
                        "name": "Hamidreza Dastmalchi"
                    },
                    {
                        "name": "Farookh Hussain"
                    },
                    {
                        "name": "Shafin Rahman"
                    }
                ],
                "author_detail": {
                    "name": "Shafin Rahman"
                },
                "author": "Shafin Rahman",
                "arxiv_comment": "ACCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08895v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08895v1",
                "updated": "2024-10-11T15:12:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    12,
                    30,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T15:12:30Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    12,
                    30,
                    4,
                    285,
                    0
                ],
                "title": "Calibrated Cache Model for Few-Shot Vision-Language Model Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibrated Cache Model for Few-Shot Vision-Language Model Adaptation"
                },
                "summary": "Cache-based approaches stand out as both effective and efficient for adapting\nvision-language models (VLMs). Nonetheless, the existing cache model overlooks\nthree crucial aspects. 1) Pre-trained VLMs are mainly optimized for image-text\nsimilarity, neglecting the importance of image-image similarity, leading to a\ngap between pre-training and adaptation. 2) The current cache model is based on\nthe Nadaraya-Watson (N-W) estimator, which disregards the intricate\nrelationships among training samples while constructing weight function. 3)\nUnder the condition of limited samples, the logits generated by cache model are\nof high uncertainty, directly using these logits without accounting for the\nconfidence could be problematic. This work presents three calibration modules\naimed at addressing the above challenges. Similarity Calibration refines the\nimage-image similarity by using unlabeled images. We add a learnable projection\nlayer with residual connection on top of the pre-trained image encoder of CLIP\nand optimize the parameters by minimizing self-supervised contrastive loss.\nWeight Calibration introduces a precision matrix into the weight function to\nadequately model the relation between training samples, transforming the\nexisting cache model to a Gaussian Process (GP) regressor, which could be more\naccurate than N-W estimator. Confidence Calibration leverages the predictive\nvariances computed by GP Regression to dynamically re-scale the logits of cache\nmodel, ensuring that the cache model's outputs are appropriately adjusted based\non their confidence levels. Besides, to reduce the high complexity of GPs, we\nfurther propose a group-based learning strategy. Integrating the above designs,\nwe propose both training-free and training-required variants. Extensive\nexperiments on 11 few-shot classification datasets validate that the proposed\nmethods can achieve state-of-the-art performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-based approaches stand out as both effective and efficient for adapting\nvision-language models (VLMs). Nonetheless, the existing cache model overlooks\nthree crucial aspects. 1) Pre-trained VLMs are mainly optimized for image-text\nsimilarity, neglecting the importance of image-image similarity, leading to a\ngap between pre-training and adaptation. 2) The current cache model is based on\nthe Nadaraya-Watson (N-W) estimator, which disregards the intricate\nrelationships among training samples while constructing weight function. 3)\nUnder the condition of limited samples, the logits generated by cache model are\nof high uncertainty, directly using these logits without accounting for the\nconfidence could be problematic. This work presents three calibration modules\naimed at addressing the above challenges. Similarity Calibration refines the\nimage-image similarity by using unlabeled images. We add a learnable projection\nlayer with residual connection on top of the pre-trained image encoder of CLIP\nand optimize the parameters by minimizing self-supervised contrastive loss.\nWeight Calibration introduces a precision matrix into the weight function to\nadequately model the relation between training samples, transforming the\nexisting cache model to a Gaussian Process (GP) regressor, which could be more\naccurate than N-W estimator. Confidence Calibration leverages the predictive\nvariances computed by GP Regression to dynamically re-scale the logits of cache\nmodel, ensuring that the cache model's outputs are appropriately adjusted based\non their confidence levels. Besides, to reduce the high complexity of GPs, we\nfurther propose a group-based learning strategy. Integrating the above designs,\nwe propose both training-free and training-required variants. Extensive\nexperiments on 11 few-shot classification datasets validate that the proposed\nmethods can achieve state-of-the-art performance."
                },
                "authors": [
                    {
                        "name": "Kun Ding"
                    },
                    {
                        "name": "Qiang Yu"
                    },
                    {
                        "name": "Haojian Zhang"
                    },
                    {
                        "name": "Gaofeng Meng"
                    },
                    {
                        "name": "Shiming Xiang"
                    }
                ],
                "author_detail": {
                    "name": "Shiming Xiang"
                },
                "author": "Shiming Xiang",
                "arxiv_comment": "submitted to IJCV",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08895v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08760v1",
                "updated": "2024-10-11T12:19:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    19,
                    18,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T12:19:18Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    19,
                    18,
                    4,
                    285,
                    0
                ],
                "title": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation"
                },
                "summary": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL."
                },
                "authors": [
                    {
                        "name": "Konstantin Burlachenko"
                    },
                    {
                        "name": "Peter Richtrik"
                    }
                ],
                "author_detail": {
                    "name": "Peter Richtrik"
                },
                "author": "Peter Richtrik",
                "arxiv_comment": "55 pages, 12 figures, 12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.4; C.3; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08618v1",
                "updated": "2024-10-11T08:33:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    8,
                    33,
                    58,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T08:33:58Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    8,
                    33,
                    58,
                    4,
                    285,
                    0
                ],
                "title": "AsyncFS: Metadata Updates Made Asynchronous for Distributed Filesystems\n  with In-Network Coordination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsyncFS: Metadata Updates Made Asynchronous for Distributed Filesystems\n  with In-Network Coordination"
                },
                "summary": "Distributed filesystems typically employ synchronous metadata updates, facing\ninherent challenges for access efficiency, load balancing, and directory\ncontention, especially under dynamic and skewed workloads. This paper argues\nthat synchronous updates are overly conservative for distributed filesystems.\nWe propose AsyncFS with asynchronous metadata updates, allowing operations to\nreturn early and defer directory updates until respective read to enable\nlatency hiding and conflict resolution. The key challenge is efficiently\nmaintaining the synchronous semantics of metadata updates. To address this,\nAsyncFS is co-designed with a programmable switch, leveraging the constrained\non-switch resources to holistically track directory states in the network with\nnegligible cost. This allows AsyncFS to timely aggregate and efficiently apply\ndelayed updates using batching and consolidation before directory reads.\nEvaluation shows that AsyncFS achieves up to 13.34$\\times$ and 3.85$\\times$\nhigher throughput, and 61.6% and 57.3% lower latency than two state-of-the-art\ndistributed filesystems, InfiniFS and CFS-KV, respectively, on skewed\nworkloads. For real-world workloads, AsyncFS improves end-to-end throughput by\n21.1$\\times$, 1.1$\\times$ and 30.1% over Ceph, IndexFS and CFS-KV,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed filesystems typically employ synchronous metadata updates, facing\ninherent challenges for access efficiency, load balancing, and directory\ncontention, especially under dynamic and skewed workloads. This paper argues\nthat synchronous updates are overly conservative for distributed filesystems.\nWe propose AsyncFS with asynchronous metadata updates, allowing operations to\nreturn early and defer directory updates until respective read to enable\nlatency hiding and conflict resolution. The key challenge is efficiently\nmaintaining the synchronous semantics of metadata updates. To address this,\nAsyncFS is co-designed with a programmable switch, leveraging the constrained\non-switch resources to holistically track directory states in the network with\nnegligible cost. This allows AsyncFS to timely aggregate and efficiently apply\ndelayed updates using batching and consolidation before directory reads.\nEvaluation shows that AsyncFS achieves up to 13.34$\\times$ and 3.85$\\times$\nhigher throughput, and 61.6% and 57.3% lower latency than two state-of-the-art\ndistributed filesystems, InfiniFS and CFS-KV, respectively, on skewed\nworkloads. For real-world workloads, AsyncFS improves end-to-end throughput by\n21.1$\\times$, 1.1$\\times$ and 30.1% over Ceph, IndexFS and CFS-KV,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Mingkai Dong"
                    },
                    {
                        "name": "Qiulin Tian"
                    },
                    {
                        "name": "Ziyi Tian"
                    },
                    {
                        "name": "Tong Xin"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08584v1",
                "updated": "2024-10-11T07:24:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    24,
                    21,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T07:24:21Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    24,
                    21,
                    4,
                    285,
                    0
                ],
                "title": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification and KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification and KV Cache Compression"
                },
                "summary": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs that resolves both computation\nand memory bottlenecks through a dynamic ratio allocation strategy of important\ntokens. This ratio is adaptively determined based on the layer-specific\ndistribution of attention scores, rather than fixed hyper-parameters, thereby\nimproving efficiency for less complex tasks while maintaining high performance\nfor more challenging ones. Then we select important tokens based on their\nnormalized attention scores and perform attention mechanism solely on those\nimportant tokens to accelerate the prefill phase. To mitigate the memory\nbottleneck in the decoding phase, we employ mixed-precision quantization to the\nKV cache, where high-bit quantization is used for caches of important tokens,\nwhile low-bit quantization is applied to those of less importance. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.6$\\times$ and reduce GPU memory usage by 50.0%, with a minimal accuracy\nreduction of only 0.2% on Video-MME benchmark over LongVA-7B model, effectively\nenhancing the generation efficiency of LVLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs that resolves both computation\nand memory bottlenecks through a dynamic ratio allocation strategy of important\ntokens. This ratio is adaptively determined based on the layer-specific\ndistribution of attention scores, rather than fixed hyper-parameters, thereby\nimproving efficiency for less complex tasks while maintaining high performance\nfor more challenging ones. Then we select important tokens based on their\nnormalized attention scores and perform attention mechanism solely on those\nimportant tokens to accelerate the prefill phase. To mitigate the memory\nbottleneck in the decoding phase, we employ mixed-precision quantization to the\nKV cache, where high-bit quantization is used for caches of important tokens,\nwhile low-bit quantization is applied to those of less importance. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.6$\\times$ and reduce GPU memory usage by 50.0%, with a minimal accuracy\nreduction of only 0.2% on Video-MME benchmark over LongVA-7B model, effectively\nenhancing the generation efficiency of LVLMs."
                },
                "authors": [
                    {
                        "name": "Yefei He"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Hong Zhou"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.03462v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.03462v3",
                "updated": "2024-10-11T02:18:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    2,
                    18,
                    24,
                    4,
                    285,
                    0
                ],
                "published": "2024-01-07T11:57:40Z",
                "published_parsed": [
                    2024,
                    1,
                    7,
                    11,
                    57,
                    40,
                    6,
                    7,
                    0
                ],
                "title": "Long Context Compression with Activation Beacon",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long Context Compression with Activation Beacon"
                },
                "summary": "Long context compression is a critical research problem due to its\nsignificance in reducing the high computational and memory costs associated\nwith LLMs. In this paper, we propose Activation Beacon, a plug-in module for\ntransformer-based LLMs that targets effective, efficient, and flexible\ncompression of long contexts. To achieve this, our method introduces the\nfollowing technical designs. 1) We directly compress the activations (i.e. keys\nand values at every layer), rather than leveraging soft prompts to relay\ninformation (which constitute a major bottleneck to encapsulate the complex\ninformation within long contexts). 2) We tailor the compression workflow, where\neach fine-grained input unit is progressively compressed, enabling high-quality\ncompression and efficient computation during both training and inference. 3) We\ntrain the model through compression-based auto-regression, making full use of\nplain texts and instructional data to optimize the model's compression\nperformance. 4) During training, we randomly sample a compression ratio at each\nstep, teaching the model to support a wide range of compression configurations.\nExtensive evaluations are conducted on various long-context tasks whose lengths\n(e.g., 128K) may far exceed the maximum training length (20K), such as document\nunderstanding, few-shot learning, and Needle-in-a-Haystack. Whilst existing\nmethods struggle to handle these challenging tasks, Activation Beacon maintains\na comparable performance to the uncompressed baseline across various scenarios,\nachieving a 2x acceleration in inference time and an 8x reduction of memory\ncosts for KV cache. Our data, model, and code have been released at\n\\url{https://github.com/FlagOpen/FlagEmbedding/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context compression is a critical research problem due to its\nsignificance in reducing the high computational and memory costs associated\nwith LLMs. In this paper, we propose Activation Beacon, a plug-in module for\ntransformer-based LLMs that targets effective, efficient, and flexible\ncompression of long contexts. To achieve this, our method introduces the\nfollowing technical designs. 1) We directly compress the activations (i.e. keys\nand values at every layer), rather than leveraging soft prompts to relay\ninformation (which constitute a major bottleneck to encapsulate the complex\ninformation within long contexts). 2) We tailor the compression workflow, where\neach fine-grained input unit is progressively compressed, enabling high-quality\ncompression and efficient computation during both training and inference. 3) We\ntrain the model through compression-based auto-regression, making full use of\nplain texts and instructional data to optimize the model's compression\nperformance. 4) During training, we randomly sample a compression ratio at each\nstep, teaching the model to support a wide range of compression configurations.\nExtensive evaluations are conducted on various long-context tasks whose lengths\n(e.g., 128K) may far exceed the maximum training length (20K), such as document\nunderstanding, few-shot learning, and Needle-in-a-Haystack. Whilst existing\nmethods struggle to handle these challenging tasks, Activation Beacon maintains\na comparable performance to the uncompressed baseline across various scenarios,\nachieving a 2x acceleration in inference time and an 8x reduction of memory\ncosts for KV cache. Our data, model, and code have been released at\n\\url{https://github.com/FlagOpen/FlagEmbedding/}."
                },
                "authors": [
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Ninglu Shao"
                    },
                    {
                        "name": "Qiwei Ye"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou",
                "arxiv_comment": "Newer version of Activation Beacon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.03462v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.03462v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08391v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08391v1",
                "updated": "2024-10-10T21:55:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    21,
                    55,
                    11,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T21:55:11Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    21,
                    55,
                    11,
                    3,
                    284,
                    0
                ],
                "title": "KV Prediction for Improved Time to First Token",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Prediction for Improved Time to First Token"
                },
                "summary": "Inference with transformer-based language models begins with a prompt\nprocessing step. In this step, the model generates the first output token and\nstores the KV cache needed for future generation steps. This prompt processing\nstep can be computationally expensive, taking 10s of seconds or more for\nbillion-parameter models on edge devices when prompt lengths or batch sizes\nrise. This degrades user experience by introducing significant latency into the\nmodel's outputs. To reduce the time spent producing the first output (known as\nthe ``time to first token'', or TTFT) of a pretrained model, we introduce a\nnovel method called KV Prediction. In our method, a small auxiliary model is\nused to process the prompt and produce an approximation of the KV cache used by\na base model. This approximated KV cache is then used with the base model for\nautoregressive generation without the need to query the auxiliary model again.\nWe demonstrate that our method produces a pareto-optimal efficiency-accuracy\ntrade-off when compared to baselines. On TriviaQA, we demonstrate relative\naccuracy improvements in the range of $15\\%-50\\%$ across a range of TTFT FLOPs\nbudgets. We also demonstrate accuracy improvements of up to $30\\%$ on HumanEval\npython code completion at fixed TTFT FLOPs budgets. Additionally, we benchmark\nmodels on an Apple M2 Pro CPU and demonstrate that our improvement in FLOPs\ntranslates to a TTFT speedup on hardware. We release our code at\nhttps://github.com/apple/corenet/tree/main/projects/kv-prediction .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with transformer-based language models begins with a prompt\nprocessing step. In this step, the model generates the first output token and\nstores the KV cache needed for future generation steps. This prompt processing\nstep can be computationally expensive, taking 10s of seconds or more for\nbillion-parameter models on edge devices when prompt lengths or batch sizes\nrise. This degrades user experience by introducing significant latency into the\nmodel's outputs. To reduce the time spent producing the first output (known as\nthe ``time to first token'', or TTFT) of a pretrained model, we introduce a\nnovel method called KV Prediction. In our method, a small auxiliary model is\nused to process the prompt and produce an approximation of the KV cache used by\na base model. This approximated KV cache is then used with the base model for\nautoregressive generation without the need to query the auxiliary model again.\nWe demonstrate that our method produces a pareto-optimal efficiency-accuracy\ntrade-off when compared to baselines. On TriviaQA, we demonstrate relative\naccuracy improvements in the range of $15\\%-50\\%$ across a range of TTFT FLOPs\nbudgets. We also demonstrate accuracy improvements of up to $30\\%$ on HumanEval\npython code completion at fixed TTFT FLOPs budgets. Additionally, we benchmark\nmodels on an Apple M2 Pro CPU and demonstrate that our improvement in FLOPs\ntranslates to a TTFT speedup on hardware. We release our code at\nhttps://github.com/apple/corenet/tree/main/projects/kv-prediction ."
                },
                "authors": [
                    {
                        "name": "Maxwell Horton"
                    },
                    {
                        "name": "Qingqing Cao"
                    },
                    {
                        "name": "Chenfan Sun"
                    },
                    {
                        "name": "Yanzi Jin"
                    },
                    {
                        "name": "Sachin Mehta"
                    },
                    {
                        "name": "Mohammad Rastegari"
                    },
                    {
                        "name": "Moin Nabi"
                    }
                ],
                "author_detail": {
                    "name": "Moin Nabi"
                },
                "author": "Moin Nabi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08391v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08391v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11284v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11284v3",
                "updated": "2024-10-10T16:57:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    57,
                    34,
                    3,
                    284,
                    0
                ],
                "published": "2024-04-17T11:48:14Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    11,
                    48,
                    14,
                    2,
                    108,
                    0
                ],
                "title": "Amplifying Main Memory-Based Timing Covert and Side Channels using\n  Processing-in-Memory Operations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amplifying Main Memory-Based Timing Covert and Side Channels using\n  Processing-in-Memory Operations"
                },
                "summary": "The adoption of processing-in-memory (PiM) architectures has been gaining\nmomentum because they provide high performance and low energy consumption by\nalleviating the data movement bottleneck. Yet, the security of such\narchitectures has not been thoroughly explored. The adoption of PiM solutions\nprovides a new way to directly access main memory, which malicious user\napplications can exploit. We show that this new way to access main memory opens\nopportunities for high-throughput timing attacks that are hard-to-mitigate\nwithout significant performance overhead.\n  We introduce IMPACT, a set of high-throughput main memory-based timing\nattacks that leverage characteristics of PiM architectures to establish covert\nand side channels. IMPACT enables high-throughput communication and private\ninformation leakage by exploiting the shared DRAM row buffer. To achieve high\nthroughput, IMPACT (i) eliminates cache bypassing steps required by\nprocessor-centric main memory and cache-based timing attacks and (ii) leverages\nthe intrinsic parallelism of PiM operations. We showcase two applications of\nIMPACT. First, we build two covert-channel attacks that run on the host CPU and\nleverage different PiM approaches to gain direct and fast access to main memory\nand establish high-throughput communication covert channels. Second, we\nshowcase a side-channel attack that leaks private information of concurrently\nrunning victim applications that are accelerated with PiM. Our results\ndemonstrate that (i) our covert channels achieve 12.87 Mb/s and 14.16 Mb/s\ncommunication throughput, respectively, which is up to 4.91x and 5.41x faster\nthan the state-of-the-art main memory-based covert channels, and (ii) our\nside-channel attack allows the attacker to leak secrets with a low error rate.\nTo avoid such covert and side channels in emerging PiM systems, we propose and\nevaluate three defenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The adoption of processing-in-memory (PiM) architectures has been gaining\nmomentum because they provide high performance and low energy consumption by\nalleviating the data movement bottleneck. Yet, the security of such\narchitectures has not been thoroughly explored. The adoption of PiM solutions\nprovides a new way to directly access main memory, which malicious user\napplications can exploit. We show that this new way to access main memory opens\nopportunities for high-throughput timing attacks that are hard-to-mitigate\nwithout significant performance overhead.\n  We introduce IMPACT, a set of high-throughput main memory-based timing\nattacks that leverage characteristics of PiM architectures to establish covert\nand side channels. IMPACT enables high-throughput communication and private\ninformation leakage by exploiting the shared DRAM row buffer. To achieve high\nthroughput, IMPACT (i) eliminates cache bypassing steps required by\nprocessor-centric main memory and cache-based timing attacks and (ii) leverages\nthe intrinsic parallelism of PiM operations. We showcase two applications of\nIMPACT. First, we build two covert-channel attacks that run on the host CPU and\nleverage different PiM approaches to gain direct and fast access to main memory\nand establish high-throughput communication covert channels. Second, we\nshowcase a side-channel attack that leaks private information of concurrently\nrunning victim applications that are accelerated with PiM. Our results\ndemonstrate that (i) our covert channels achieve 12.87 Mb/s and 14.16 Mb/s\ncommunication throughput, respectively, which is up to 4.91x and 5.41x faster\nthan the state-of-the-art main memory-based covert channels, and (ii) our\nside-channel attack allows the attacker to leak secrets with a low error rate.\nTo avoid such covert and side channels in emerging PiM systems, we propose and\nevaluate three defenses."
                },
                "authors": [
                    {
                        "name": "Konstantinos Kanellopoulos"
                    },
                    {
                        "name": "F. Nisa Bostanci"
                    },
                    {
                        "name": "Ataberk Olgun"
                    },
                    {
                        "name": "A. Giray Yaglikci"
                    },
                    {
                        "name": "Ismail Emir Yuksel"
                    },
                    {
                        "name": "Nika Mansouri Ghiasi"
                    },
                    {
                        "name": "Zulal Bingol"
                    },
                    {
                        "name": "Mohammad Sadrosadati"
                    },
                    {
                        "name": "Onur Mutlu"
                    }
                ],
                "author_detail": {
                    "name": "Onur Mutlu"
                },
                "author": "Onur Mutlu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11284v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11284v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01195v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01195v2",
                "updated": "2024-10-10T11:01:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    1,
                    44,
                    3,
                    284,
                    0
                ],
                "published": "2024-06-03T10:58:32Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    10,
                    58,
                    32,
                    0,
                    155,
                    0
                ],
                "title": "C$^3$P-VoxelMap: Compact, Cumulative and Coalescible Probabilistic Voxel\n  Mapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "C$^3$P-VoxelMap: Compact, Cumulative and Coalescible Probabilistic Voxel\n  Mapping"
                },
                "summary": "This work presents a compact, cumulative and coalescible probabilistic voxel\nmapping method to enhance performance, accuracy and memory efficiency in LiDAR\nodometry. Probabilistic voxel mapping requires storing past point clouds and\nre-iterating on them to update the uncertainty every iteration, which consumes\nlarge memory space and CPU cycles. To solve this problem, we propose a\ntwo-folded strategy. First, we introduce a compact point-free representation\nfor probabilistic voxels and derive a cumulative update of the planar\nuncertainty without caching original point clouds. Our voxel structure only\nkeeps track of a predetermined set of statistics for points that lie inside it.\nThis method reduces the runtime complexity from $O(MN)$ to $O(N)$ and the space\ncomplexity from $O(N)$ to $O(1)$ where $M$ is the number of iterations and $N$\nis the number of points. Second, to further minimize memory usage and enhance\nmapping accuracy, we provide a strategy to dynamically merge voxels associated\nwith the same physical planes by taking advantage of the geometric features in\nthe real world. Rather than scanning for these coalescible voxels constantly at\nevery iteration, our merging strategy accumulates voxels in a\nlocality-sensitive hash and triggers merging lazily. On-demand merging not only\nreduces memory footprint with minimal computational overhead but also improves\nlocalization accuracy thanks to cross-voxel denoising. Experiments exhibit 20%\nhigher accuracy, 20% faster performance and 70% lower memory consumption than\nthe state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a compact, cumulative and coalescible probabilistic voxel\nmapping method to enhance performance, accuracy and memory efficiency in LiDAR\nodometry. Probabilistic voxel mapping requires storing past point clouds and\nre-iterating on them to update the uncertainty every iteration, which consumes\nlarge memory space and CPU cycles. To solve this problem, we propose a\ntwo-folded strategy. First, we introduce a compact point-free representation\nfor probabilistic voxels and derive a cumulative update of the planar\nuncertainty without caching original point clouds. Our voxel structure only\nkeeps track of a predetermined set of statistics for points that lie inside it.\nThis method reduces the runtime complexity from $O(MN)$ to $O(N)$ and the space\ncomplexity from $O(N)$ to $O(1)$ where $M$ is the number of iterations and $N$\nis the number of points. Second, to further minimize memory usage and enhance\nmapping accuracy, we provide a strategy to dynamically merge voxels associated\nwith the same physical planes by taking advantage of the geometric features in\nthe real world. Rather than scanning for these coalescible voxels constantly at\nevery iteration, our merging strategy accumulates voxels in a\nlocality-sensitive hash and triggers merging lazily. On-demand merging not only\nreduces memory footprint with minimal computational overhead but also improves\nlocalization accuracy thanks to cross-voxel denoising. Experiments exhibit 20%\nhigher accuracy, 20% faster performance and 70% lower memory consumption than\nthe state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Xu Yang"
                    },
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Qijie Ge"
                    },
                    {
                        "name": "Lulu Suo"
                    },
                    {
                        "name": "Weijie Tang"
                    },
                    {
                        "name": "Zhengyu Wei"
                    },
                    {
                        "name": "Longxiang Huang"
                    },
                    {
                        "name": "Bo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Wang"
                },
                "author": "Bo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01195v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01195v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.04793v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.04793v2",
                "updated": "2024-10-10T05:11:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    5,
                    11,
                    52,
                    3,
                    284,
                    0
                ],
                "published": "2024-04-07T03:08:14Z",
                "published_parsed": [
                    2024,
                    4,
                    7,
                    3,
                    8,
                    14,
                    6,
                    98,
                    0
                ],
                "title": "SqueezeAttention: 2D Management of KV-Cache in LLM Inference via\n  Layer-wise Optimal Budget",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SqueezeAttention: 2D Management of KV-Cache in LLM Inference via\n  Layer-wise Optimal Budget"
                },
                "summary": "Optimizing the Key-Value (KV) cache of the Large Language Model (LLM) has\nbeen considered critical to saving the cost of inference. Most of the existing\nKV-cache compression algorithms attempted to sparsify the sequence of tokens by\ntaking advantage of the different importance of tokens. However, most of these\nmethods treat all layers equally, allocating the same KV budget to each layer.\nThis approach is suboptimal, as some layers may be less sensitive to input\ntokens yet still receive the same budget as others. In this work, we found that\nby identifying the importance of attention layers, we could optimize the\nKV-cache jointly from two dimensions, i.e., sequence-wise and layer-wise. Based\non our observations regarding layer-wise importance in inference, we propose\nSqueezeAttention to precisely optimize the allocation of KV-cache budget among\nlayers on-the-fly and then incorporate three representative sequence-wise\nalgorithms to compress the KV-cache for each layer with its very own budget.\nSpecifically, we first measure each layer's importance by calculating the\ncosine similarity of the input prompt differences before and after the\nself-attention layers. Based on this similarity, we then categorize the layers\ninto two groups and adjust their KV budgets accordingly. By optimizing the\nKV-cache from both sequence's and layer's dimensions, SqueezeAttention achieves\naround 30% to 70% of the memory reductions and up to 2.2 times of throughput\nimprovements in a wide range of LLMs and benchmarks. The code is available at\nhttps://github.com/hetailang/SqueezeAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing the Key-Value (KV) cache of the Large Language Model (LLM) has\nbeen considered critical to saving the cost of inference. Most of the existing\nKV-cache compression algorithms attempted to sparsify the sequence of tokens by\ntaking advantage of the different importance of tokens. However, most of these\nmethods treat all layers equally, allocating the same KV budget to each layer.\nThis approach is suboptimal, as some layers may be less sensitive to input\ntokens yet still receive the same budget as others. In this work, we found that\nby identifying the importance of attention layers, we could optimize the\nKV-cache jointly from two dimensions, i.e., sequence-wise and layer-wise. Based\non our observations regarding layer-wise importance in inference, we propose\nSqueezeAttention to precisely optimize the allocation of KV-cache budget among\nlayers on-the-fly and then incorporate three representative sequence-wise\nalgorithms to compress the KV-cache for each layer with its very own budget.\nSpecifically, we first measure each layer's importance by calculating the\ncosine similarity of the input prompt differences before and after the\nself-attention layers. Based on this similarity, we then categorize the layers\ninto two groups and adjust their KV budgets accordingly. By optimizing the\nKV-cache from both sequence's and layer's dimensions, SqueezeAttention achieves\naround 30% to 70% of the memory reductions and up to 2.2 times of throughput\nimprovements in a wide range of LLMs and benchmarks. The code is available at\nhttps://github.com/hetailang/SqueezeAttention."
                },
                "authors": [
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Shaoduo Gan"
                    }
                ],
                "author_detail": {
                    "name": "Shaoduo Gan"
                },
                "author": "Shaoduo Gan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.04793v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.04793v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07590v1",
                "updated": "2024-10-10T03:52:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    3,
                    52,
                    54,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T03:52:54Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    3,
                    52,
                    54,
                    3,
                    284,
                    0
                ],
                "title": "TurboRAG: Accelerating Retrieval-Augmented Generation with Precomputed\n  KV Caches for Chunked Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurboRAG: Accelerating Retrieval-Augmented Generation with Precomputed\n  KV Caches for Chunked Text"
                },
                "summary": "Current Retrieval-Augmented Generation (RAG) systems concatenate and process\nnumerous retrieved document chunks for prefill which requires a large volume of\ncomputation, therefore leading to significant latency in time-to-first-token\n(TTFT). To reduce the computation overhead as well as TTFT, we introduce\nTurboRAG, a novel RAG system that redesigns the inference paradigm of the\ncurrent RAG system by first pre-computing and storing the key-value (KV) caches\nof documents offline, and then directly retrieving the saved KV cache for\nprefill. Hence, online computation of KV caches is eliminated during inference.\nIn addition, we provide a number of insights into the mask matrix and\npositional embedding mechanisms, plus fine-tune a pretrained language model to\nmaintain model accuracy of TurboRAG. Our approach is applicable to most\nexisting large language models and their applications without any requirement\nin modification of models and inference systems. Experimental results across a\nsuite of RAG benchmarks demonstrate that TurboRAG reduces TTFT by up to 9.4x\ncompared to the conventional RAG systems (on an average of 8.6x), but reserving\ncomparable performance to the standard RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Retrieval-Augmented Generation (RAG) systems concatenate and process\nnumerous retrieved document chunks for prefill which requires a large volume of\ncomputation, therefore leading to significant latency in time-to-first-token\n(TTFT). To reduce the computation overhead as well as TTFT, we introduce\nTurboRAG, a novel RAG system that redesigns the inference paradigm of the\ncurrent RAG system by first pre-computing and storing the key-value (KV) caches\nof documents offline, and then directly retrieving the saved KV cache for\nprefill. Hence, online computation of KV caches is eliminated during inference.\nIn addition, we provide a number of insights into the mask matrix and\npositional embedding mechanisms, plus fine-tune a pretrained language model to\nmaintain model accuracy of TurboRAG. Our approach is applicable to most\nexisting large language models and their applications without any requirement\nin modification of models and inference systems. Experimental results across a\nsuite of RAG benchmarks demonstrate that TurboRAG reduces TTFT by up to 9.4x\ncompared to the conventional RAG systems (on an average of 8.6x), but reserving\ncomparable performance to the standard RAG systems."
                },
                "authors": [
                    {
                        "name": "Songshuo Lu"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Yutian Rong"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Yaohua Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yaohua Tang"
                },
                "author": "Yaohua Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07579v1",
                "updated": "2024-10-10T03:28:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    3,
                    28,
                    46,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T03:28:46Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    3,
                    28,
                    46,
                    3,
                    284,
                    0
                ],
                "title": "Teddy: Efficient Large-Scale Dataset Distillation via\n  Taylor-Approximated Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teddy: Efficient Large-Scale Dataset Distillation via\n  Taylor-Approximated Matching"
                },
                "summary": "Dataset distillation or condensation refers to compressing a large-scale\ndataset into a much smaller one, enabling models trained on this synthetic\ndataset to generalize effectively on real data. Tackling this challenge, as\ndefined, relies on a bi-level optimization algorithm: a novel model is trained\nin each iteration within a nested loop, with gradients propagated through an\nunrolled computation graph. However, this approach incurs high memory and time\ncomplexity, posing difficulties in scaling up to large datasets such as\nImageNet. Addressing these concerns, this paper introduces Teddy, a\nTaylor-approximated dataset distillation framework designed to handle\nlarge-scale dataset and enhance efficiency. On the one hand, backed up by\ntheoretical analysis, we propose a memory-efficient approximation derived from\nTaylor expansion, which transforms the original form dependent on multi-step\ngradients to a first-order one. On the other hand, rather than repeatedly\ntraining a novel model in each iteration, we unveil that employing a pre-cached\npool of weak models, which can be generated from a single base model, enhances\nboth time efficiency and performance concurrently, particularly when dealing\nwith large-scale datasets. Extensive experiments demonstrate that the proposed\nTeddy attains state-of-the-art efficiency and performance on the Tiny-ImageNet\nand original-sized ImageNet-1K dataset, notably surpassing prior methods by up\nto 12.8%, while reducing 46.6% runtime. Our code will be available at\nhttps://github.com/Lexie-YU/Teddy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dataset distillation or condensation refers to compressing a large-scale\ndataset into a much smaller one, enabling models trained on this synthetic\ndataset to generalize effectively on real data. Tackling this challenge, as\ndefined, relies on a bi-level optimization algorithm: a novel model is trained\nin each iteration within a nested loop, with gradients propagated through an\nunrolled computation graph. However, this approach incurs high memory and time\ncomplexity, posing difficulties in scaling up to large datasets such as\nImageNet. Addressing these concerns, this paper introduces Teddy, a\nTaylor-approximated dataset distillation framework designed to handle\nlarge-scale dataset and enhance efficiency. On the one hand, backed up by\ntheoretical analysis, we propose a memory-efficient approximation derived from\nTaylor expansion, which transforms the original form dependent on multi-step\ngradients to a first-order one. On the other hand, rather than repeatedly\ntraining a novel model in each iteration, we unveil that employing a pre-cached\npool of weak models, which can be generated from a single base model, enhances\nboth time efficiency and performance concurrently, particularly when dealing\nwith large-scale datasets. Extensive experiments demonstrate that the proposed\nTeddy attains state-of-the-art efficiency and performance on the Tiny-ImageNet\nand original-sized ImageNet-1K dataset, notably surpassing prior methods by up\nto 12.8%, while reducing 46.6% runtime. Our code will be available at\nhttps://github.com/Lexie-YU/Teddy."
                },
                "authors": [
                    {
                        "name": "Ruonan Yu"
                    },
                    {
                        "name": "Songhua Liu"
                    },
                    {
                        "name": "Jingwen Ye"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "arxiv_comment": "Accepted by ECCV2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.19519v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19519v3",
                "updated": "2024-10-09T15:57:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    57,
                    3,
                    2,
                    283,
                    0
                ],
                "published": "2024-03-28T15:52:15Z",
                "published_parsed": [
                    2024,
                    3,
                    28,
                    15,
                    52,
                    15,
                    3,
                    88,
                    0
                ],
                "title": "Laser Interactions with Gas Jets: EMP Emission and Nozzle Damage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Laser Interactions with Gas Jets: EMP Emission and Nozzle Damage"
                },
                "summary": "Understanding the physics of electromagnetic pulse emission and nozzle damage\nis critical for the long-term operation of laser experiments with gas targets,\nparticularly at facilities looking to produce stable sources of radiation at\nhigh repetition rate. We present a theoretical model of plasma formation and\nelectrostatic charging when high-power lasers are focused inside gases. The\nmodel can be used to estimate the amplitude of gigahertz electromagnetic pulses\n(EMPs) produced by the laser and the extent of damage to the gas jet nozzle.\nLooking at a range of laser and target properties relevant to existing\nhigh-power laser systems, we find that EMP fields of tens to hundreds of kV/m\ncan be generated several metres from the gas jet. Model predictions are\ncompared with measurements of EMP, plasma formation and nozzle damage from two\nexperiments on the VEGA-3 laser and one experiment on the Vulcan Petawatt\nlaser.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the physics of electromagnetic pulse emission and nozzle damage\nis critical for the long-term operation of laser experiments with gas targets,\nparticularly at facilities looking to produce stable sources of radiation at\nhigh repetition rate. We present a theoretical model of plasma formation and\nelectrostatic charging when high-power lasers are focused inside gases. The\nmodel can be used to estimate the amplitude of gigahertz electromagnetic pulses\n(EMPs) produced by the laser and the extent of damage to the gas jet nozzle.\nLooking at a range of laser and target properties relevant to existing\nhigh-power laser systems, we find that EMP fields of tens to hundreds of kV/m\ncan be generated several metres from the gas jet. Model predictions are\ncompared with measurements of EMP, plasma formation and nozzle damage from two\nexperiments on the VEGA-3 laser and one experiment on the Vulcan Petawatt\nlaser."
                },
                "authors": [
                    {
                        "name": "Philip Wykeham Bradford"
                    },
                    {
                        "name": "Valeria Ospina-Bohorquez"
                    },
                    {
                        "name": "Michael Ehret"
                    },
                    {
                        "name": "Jose-Luis Henares"
                    },
                    {
                        "name": "Pilar Puyuelo-Valdes"
                    },
                    {
                        "name": "Tomasz Chodukowski"
                    },
                    {
                        "name": "Tadeusz Pisarczyk"
                    },
                    {
                        "name": "Zofia Rusiniak"
                    },
                    {
                        "name": "Carlos Salgado-Lopez"
                    },
                    {
                        "name": "Christos Vlachos"
                    },
                    {
                        "name": "Massimiliano Sciscio"
                    },
                    {
                        "name": "Martina Salvadori"
                    },
                    {
                        "name": "Claudio Verona"
                    },
                    {
                        "name": "George Hicks"
                    },
                    {
                        "name": "Oliver Ettlinger"
                    },
                    {
                        "name": "Zulfikar Najmudin"
                    },
                    {
                        "name": "Jean-Raphael Marques"
                    },
                    {
                        "name": "Laurent Gremillet"
                    },
                    {
                        "name": "Joao Jorge Santos"
                    },
                    {
                        "name": "Fabrizio Consoli"
                    },
                    {
                        "name": "Vladimir Tikhonchuk"
                    }
                ],
                "author_detail": {
                    "name": "Vladimir Tikhonchuk"
                },
                "author": "Vladimir Tikhonchuk",
                "arxiv_comment": "18 pages (total), 12 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.19519v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19519v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06934v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06934v1",
                "updated": "2024-10-09T14:28:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    28,
                    59,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T14:28:59Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    28,
                    59,
                    2,
                    283,
                    0
                ],
                "title": "VEC-Sim: A Simulation Platform for Evaluating Service Caching and\n  Computation Offloading Policies in Vehicular Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VEC-Sim: A Simulation Platform for Evaluating Service Caching and\n  Computation Offloading Policies in Vehicular Edge Networks"
                },
                "summary": "Computer simulation platforms offer an alternative solution by emulating\ncomplex systems in a controlled manner. However, existing Edge Computing (EC)\nsimulators, as well as general-purpose vehicular network simulators, are not\ntailored for VEC and lack dedicated support for modeling the distinct access\npattern, entity mobility trajectory and other unique characteristics of VEC\nnetworks. To fill this gap, this paper proposes VEC-Sim, a versatile simulation\nplatform for in-depth evaluation and analysis of various service caching and\ncomputation offloading policies in VEC networks. VEC-Sim incorporates realistic\nmechanisms to replicate real-world access patterns, including service feature\nvector, vehicle mobility modeling, evolving service popularity, new service\nupload and user preference shifts, etc. Moreover, its modular architecture and\nextensive Application Programming Interfaces (APIs) allow seamless integration\nof customized scheduling policies and user-defined metrics. A comprehensive\nevaluation of VEC-Sim's capabilities is undertaken in comparison to real-world\nground truths. Results prove it to be accurate in reproducing classical\nscheduling algorithms and extremely effective in conducting case studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer simulation platforms offer an alternative solution by emulating\ncomplex systems in a controlled manner. However, existing Edge Computing (EC)\nsimulators, as well as general-purpose vehicular network simulators, are not\ntailored for VEC and lack dedicated support for modeling the distinct access\npattern, entity mobility trajectory and other unique characteristics of VEC\nnetworks. To fill this gap, this paper proposes VEC-Sim, a versatile simulation\nplatform for in-depth evaluation and analysis of various service caching and\ncomputation offloading policies in VEC networks. VEC-Sim incorporates realistic\nmechanisms to replicate real-world access patterns, including service feature\nvector, vehicle mobility modeling, evolving service popularity, new service\nupload and user preference shifts, etc. Moreover, its modular architecture and\nextensive Application Programming Interfaces (APIs) allow seamless integration\nof customized scheduling policies and user-defined metrics. A comprehensive\nevaluation of VEC-Sim's capabilities is undertaken in comparison to real-world\nground truths. Results prove it to be accurate in reproducing classical\nscheduling algorithms and extremely effective in conducting case studies."
                },
                "authors": [
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Xiaolong Xu"
                    },
                    {
                        "name": "Muhammad Bilal"
                    },
                    {
                        "name": "Xiangwei Wang"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Siyu Wu"
                    }
                ],
                "author_detail": {
                    "name": "Siyu Wu"
                },
                "author": "Siyu Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06934v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06934v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00428v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00428v3",
                "updated": "2024-10-09T11:40:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    11,
                    40,
                    31,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-01T06:23:17Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    6,
                    23,
                    17,
                    1,
                    275,
                    0
                ],
                "title": "LayerKV: Optimizing Large Language Model Serving with Layer-wise KV\n  Cache Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LayerKV: Optimizing Large Language Model Serving with Layer-wise KV\n  Cache Management"
                },
                "summary": "The expanding context windows in large language models (LLMs) have greatly\nenhanced their capabilities in various applications, but they also introduce\nsignificant challenges in maintaining low latency, particularly in Time to\nFirst Token (TTFT). This paper identifies that the sharp rise in TTFT as\ncontext length increases is predominantly driven by queuing delays, which are\ncaused by the growing demands for GPU Key-Value (KV) cache allocation clashing\nwith the limited availability of KV cache blocks. To address this issue, we\npropose LayerKV, a simple yet effective plug-in method that effectively reduces\nTTFT without requiring additional hardware or compromising output performance,\nwhile seamlessly integrating with existing parallelism strategies and\nscheduling techniques. Specifically, LayerKV introduces layer-wise KV block\nallocation, management, and offloading for fine-grained control over system\nmemory, coupled with an SLO-aware scheduler to optimize overall Service Level\nObjectives (SLOs). Comprehensive evaluations on representative models, ranging\nfrom 7B to 70B parameters, across various GPU configurations, demonstrate that\nLayerKV improves TTFT latency up to 69x and reduces SLO violation rates by\n28.7%, significantly enhancing the user experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expanding context windows in large language models (LLMs) have greatly\nenhanced their capabilities in various applications, but they also introduce\nsignificant challenges in maintaining low latency, particularly in Time to\nFirst Token (TTFT). This paper identifies that the sharp rise in TTFT as\ncontext length increases is predominantly driven by queuing delays, which are\ncaused by the growing demands for GPU Key-Value (KV) cache allocation clashing\nwith the limited availability of KV cache blocks. To address this issue, we\npropose LayerKV, a simple yet effective plug-in method that effectively reduces\nTTFT without requiring additional hardware or compromising output performance,\nwhile seamlessly integrating with existing parallelism strategies and\nscheduling techniques. Specifically, LayerKV introduces layer-wise KV block\nallocation, management, and offloading for fine-grained control over system\nmemory, coupled with an SLO-aware scheduler to optimize overall Service Level\nObjectives (SLOs). Comprehensive evaluations on representative models, ranging\nfrom 7B to 70B parameters, across various GPU configurations, demonstrate that\nLayerKV improves TTFT latency up to 69x and reduces SLO violation rates by\n28.7%, significantly enhancing the user experience."
                },
                "authors": [
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Changxu Shao"
                    },
                    {
                        "name": "Ziqing Wang"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Yuhong Guo"
                    },
                    {
                        "name": "Junping Zhao"
                    },
                    {
                        "name": "Ke Zhang"
                    },
                    {
                        "name": "Zhenxuan Pan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenxuan Pan"
                },
                "author": "Zhenxuan Pan",
                "arxiv_comment": "11 pages, 7 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00428v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00428v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06627v1",
                "updated": "2024-10-09T07:22:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    7,
                    22,
                    40,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T07:22:40Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    7,
                    22,
                    40,
                    2,
                    283,
                    0
                ],
                "title": "Variations in Multi-Agent Actor-Critic Frameworks for Joint\n  Optimizations in UAV Swarm Networks: Recent Evolution, Challenges, and\n  Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variations in Multi-Agent Actor-Critic Frameworks for Joint\n  Optimizations in UAV Swarm Networks: Recent Evolution, Challenges, and\n  Directions"
                },
                "summary": "Autonomous unmanned aerial vehicle (UAV) swarm networks (UAVSNs) can\neffectively execute surveillance, connectivity, and computing services to\nground users (GUs). These missions require trajectory planning, UAV-GUs\nassociation, task offloading, next-hop selection, and resources such as\ntransmit power, bandwidth, caching, and computing allocation to improve network\nperformances. Owing to the highly dynamic topology, limited resources, and\nnon-availability of global knowledge, optimizing network performance in UAVSNs\nis very intricate. Hence, it requires an adaptive joint optimization framework\nthat can tackle both discrete and continuous decision variables to ensure\noptimal network performance under dynamic constraints. Multi-agent deep\nreinforcement learning-based adaptive actor-critic framework can efficiently\naddress these problems. This paper investigates the recent evolutions of\nactor-critic frameworks to deal with joint optimization problems in UAVSNs. In\naddition, challenges and potential solutions are addressed as research\ndirections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous unmanned aerial vehicle (UAV) swarm networks (UAVSNs) can\neffectively execute surveillance, connectivity, and computing services to\nground users (GUs). These missions require trajectory planning, UAV-GUs\nassociation, task offloading, next-hop selection, and resources such as\ntransmit power, bandwidth, caching, and computing allocation to improve network\nperformances. Owing to the highly dynamic topology, limited resources, and\nnon-availability of global knowledge, optimizing network performance in UAVSNs\nis very intricate. Hence, it requires an adaptive joint optimization framework\nthat can tackle both discrete and continuous decision variables to ensure\noptimal network performance under dynamic constraints. Multi-agent deep\nreinforcement learning-based adaptive actor-critic framework can efficiently\naddress these problems. This paper investigates the recent evolutions of\nactor-critic frameworks to deal with joint optimization problems in UAVSNs. In\naddition, challenges and potential solutions are addressed as research\ndirections."
                },
                "authors": [
                    {
                        "name": "Muhammad Morshed Alam"
                    },
                    {
                        "name": "Muhammad Yeasir Aarafat"
                    },
                    {
                        "name": "Tamim Hossain"
                    }
                ],
                "author_detail": {
                    "name": "Tamim Hossain"
                },
                "author": "Tamim Hossain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13941v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13941v2",
                "updated": "2024-10-09T04:11:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    4,
                    11,
                    28,
                    2,
                    283,
                    0
                ],
                "published": "2024-06-20T02:20:21Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    2,
                    20,
                    21,
                    3,
                    172,
                    0
                ],
                "title": "UpDLRM: Accelerating Personalized Recommendation using Real-World PIM\n  Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UpDLRM: Accelerating Personalized Recommendation using Real-World PIM\n  Architecture"
                },
                "summary": "Deep Learning Recommendation Models (DLRMs) have gained popularity in\nrecommendation systems due to their effectiveness in handling large-scale\nrecommendation tasks. The embedding layers of DLRMs have become the performance\nbottleneck due to their intensive needs on memory capacity and memory\nbandwidth. In this paper, we propose UpDLRM, which utilizes real-world\nprocessingin-memory (PIM) hardware, UPMEM DPU, to boost the memory bandwidth\nand reduce recommendation latency. The parallel nature of the DPU memory can\nprovide high aggregated bandwidth for the large number of irregular memory\naccesses in embedding lookups, thus offering great potential to reduce the\ninference latency. To fully utilize the DPU memory bandwidth, we further\nstudied the embedding table partitioning problem to achieve good\nworkload-balance and efficient data caching. Evaluations using real-world\ndatasets show that, UpDLRM achieves much lower inference time for DLRM compared\nto both CPU-only and CPU-GPU hybrid counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning Recommendation Models (DLRMs) have gained popularity in\nrecommendation systems due to their effectiveness in handling large-scale\nrecommendation tasks. The embedding layers of DLRMs have become the performance\nbottleneck due to their intensive needs on memory capacity and memory\nbandwidth. In this paper, we propose UpDLRM, which utilizes real-world\nprocessingin-memory (PIM) hardware, UPMEM DPU, to boost the memory bandwidth\nand reduce recommendation latency. The parallel nature of the DPU memory can\nprovide high aggregated bandwidth for the large number of irregular memory\naccesses in embedding lookups, thus offering great potential to reduce the\ninference latency. To fully utilize the DPU memory bandwidth, we further\nstudied the embedding table partitioning problem to achieve good\nworkload-balance and efficient data caching. Evaluations using real-world\ndatasets show that, UpDLRM achieves much lower inference time for DLRM compared\nto both CPU-only and CPU-GPU hybrid counterparts."
                },
                "authors": [
                    {
                        "name": "Sitian Chen"
                    },
                    {
                        "name": "Haobin Tan"
                    },
                    {
                        "name": "Amelie Chi Zhou"
                    },
                    {
                        "name": "Yusen Li"
                    },
                    {
                        "name": "Pavan Balaji"
                    }
                ],
                "author_detail": {
                    "name": "Pavan Balaji"
                },
                "author": "Pavan Balaji",
                "arxiv_doi": "10.1145/3649329.3658266",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3649329.3658266",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.13941v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13941v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by DAC 2024",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06497v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06497v1",
                "updated": "2024-10-09T02:51:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    2,
                    51,
                    27,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T02:51:27Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    2,
                    51,
                    27,
                    2,
                    283,
                    0
                ],
                "title": "ERCache: An Efficient and Reliable Caching Framework for Large-Scale\n  User Representations in Meta's Ads System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ERCache: An Efficient and Reliable Caching Framework for Large-Scale\n  User Representations in Meta's Ads System"
                },
                "summary": "The increasing complexity of deep learning models used for calculating user\nrepresentations presents significant challenges, particularly with limited\ncomputational resources and strict service-level agreements (SLAs). Previous\nresearch efforts have focused on optimizing model inference but have overlooked\na critical question: is it necessary to perform user model inference for every\nad request in large-scale social networks? To address this question and these\nchallenges, we first analyze user access patterns at Meta and find that most\nuser model inferences occur within a short timeframe. T his observation reveals\na triangular relationship among model complexity, embedding freshness, and\nservice SLAs. Building on this insight, we designed, implemented, and evaluated\nERCache, an efficient and robust caching framework for large-scale user\nrepresentations in ads recommendation systems on social networks. ERCache\ncategorizes cache into direct and failover types and applies customized\nsettings and eviction policies for each model, effectively balancing model\ncomplexity, embedding freshness, and service SLAs, even considering the\nstaleness introduced by caching. ERCache has been deployed at Meta for over six\nmonths, supporting more than 30 ranking models while efficiently conserving\ncomputational resources and complying with service SLA requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of deep learning models used for calculating user\nrepresentations presents significant challenges, particularly with limited\ncomputational resources and strict service-level agreements (SLAs). Previous\nresearch efforts have focused on optimizing model inference but have overlooked\na critical question: is it necessary to perform user model inference for every\nad request in large-scale social networks? To address this question and these\nchallenges, we first analyze user access patterns at Meta and find that most\nuser model inferences occur within a short timeframe. T his observation reveals\na triangular relationship among model complexity, embedding freshness, and\nservice SLAs. Building on this insight, we designed, implemented, and evaluated\nERCache, an efficient and robust caching framework for large-scale user\nrepresentations in ads recommendation systems on social networks. ERCache\ncategorizes cache into direct and failover types and applies customized\nsettings and eviction policies for each model, effectively balancing model\ncomplexity, embedding freshness, and service SLAs, even considering the\nstaleness introduced by caching. ERCache has been deployed at Meta for over six\nmonths, supporting more than 30 ranking models while efficiently conserving\ncomputational resources and complying with service SLA requirements."
                },
                "authors": [
                    {
                        "name": "Fang Zhou"
                    },
                    {
                        "name": "Yaning Huang"
                    },
                    {
                        "name": "Dong Liang"
                    },
                    {
                        "name": "Dai Li"
                    },
                    {
                        "name": "Zhongke Zhang"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Xiao Xin"
                    },
                    {
                        "name": "Abdallah Aboelela"
                    },
                    {
                        "name": "Zheliang Jiang"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Jeff Song"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Chen Liang"
                    },
                    {
                        "name": "Huayu Li"
                    },
                    {
                        "name": "ChongLin Sun"
                    },
                    {
                        "name": "Hang Yang"
                    },
                    {
                        "name": "Lei Qu"
                    },
                    {
                        "name": "Zhan Shu"
                    },
                    {
                        "name": "Mindi Yuan"
                    },
                    {
                        "name": "Emanuele Maccherani"
                    },
                    {
                        "name": "Taha Hayat"
                    },
                    {
                        "name": "John Guo"
                    },
                    {
                        "name": "Varna Puvvada"
                    },
                    {
                        "name": "Uladzimir Pashkevich"
                    }
                ],
                "author_detail": {
                    "name": "Uladzimir Pashkevich"
                },
                "author": "Uladzimir Pashkevich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06497v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06497v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10443v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10443v4",
                "updated": "2024-10-09T01:12:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    1,
                    12,
                    19,
                    2,
                    283,
                    0
                ],
                "published": "2024-05-16T21:07:42Z",
                "published_parsed": [
                    2024,
                    5,
                    16,
                    21,
                    7,
                    42,
                    3,
                    137,
                    0
                ],
                "title": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation"
                },
                "summary": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost."
                },
                "authors": [
                    {
                        "name": "Matthew Raffel"
                    },
                    {
                        "name": "Victor Agostinelli"
                    },
                    {
                        "name": "Lizhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lizhong Chen"
                },
                "author": "Lizhong Chen",
                "arxiv_comment": "Accepted at EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10443v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10443v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01527v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01527v2",
                "updated": "2024-10-08T19:34:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    19,
                    34,
                    3,
                    1,
                    282,
                    0
                ],
                "published": "2024-07-01T17:59:47Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    17,
                    59,
                    47,
                    0,
                    183,
                    0
                ],
                "title": "KV Cache Compression, But What Must We Give in Return? A Comprehensive\n  Benchmark of Long Context Capable Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Compression, But What Must We Give in Return? A Comprehensive\n  Benchmark of Long Context Capable Approaches"
                },
                "summary": "Long context capability is a crucial competency for large language models\n(LLMs) as it mitigates the human struggle to digest long-form texts. This\ncapability enables complex task-solving scenarios such as book summarization,\ncode assistance, and many more tasks that are traditionally manpower-intensive.\nHowever, transformer-based LLMs face significant challenges with long context\ninput due to the growing size of the KV cache and the intrinsic complexity of\nattending to extended inputs; where multiple schools of efficiency-driven\napproaches - such as KV cache quantization, token dropping, prompt compression,\nlinear-time sequence models, and hybrid architectures - have been proposed to\nproduce efficient yet long context-capable models. Despite these advancements,\nno existing work has comprehensively benchmarked these methods in a reasonably\naligned environment. In this work, we fill this gap by providing a taxonomy of\ncurrent methods and evaluating 10+ state-of-the-art approaches across seven\ncategories of long context tasks. Our work reveals numerous previously unknown\nphenomena and offers insights - as well as a friendly workbench - for the\nfuture development of long context-capable LLMs. The source code is available\nat https://github.com/henryzhongsc/longctx_bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context capability is a crucial competency for large language models\n(LLMs) as it mitigates the human struggle to digest long-form texts. This\ncapability enables complex task-solving scenarios such as book summarization,\ncode assistance, and many more tasks that are traditionally manpower-intensive.\nHowever, transformer-based LLMs face significant challenges with long context\ninput due to the growing size of the KV cache and the intrinsic complexity of\nattending to extended inputs; where multiple schools of efficiency-driven\napproaches - such as KV cache quantization, token dropping, prompt compression,\nlinear-time sequence models, and hybrid architectures - have been proposed to\nproduce efficient yet long context-capable models. Despite these advancements,\nno existing work has comprehensively benchmarked these methods in a reasonably\naligned environment. In this work, we fill this gap by providing a taxonomy of\ncurrent methods and evaluating 10+ state-of-the-art approaches across seven\ncategories of long context tasks. Our work reveals numerous previously unknown\nphenomena and offers insights - as well as a friendly workbench - for the\nfuture development of long context-capable LLMs. The source code is available\nat https://github.com/henryzhongsc/longctx_bench."
                },
                "authors": [
                    {
                        "name": "Jiayi Yuan"
                    },
                    {
                        "name": "Hongyi Liu"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Yu-Neng Chuang"
                    },
                    {
                        "name": "Songchen Li"
                    },
                    {
                        "name": "Guanchu Wang"
                    },
                    {
                        "name": "Duy Le"
                    },
                    {
                        "name": "Hongye Jin"
                    },
                    {
                        "name": "Vipin Chaudhary"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Xia Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xia Hu"
                },
                "author": "Xia Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01527v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01527v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05927v1",
                "updated": "2024-10-08T11:28:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    11,
                    28,
                    30,
                    1,
                    282,
                    0
                ],
                "published": "2024-10-08T11:28:30Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    11,
                    28,
                    30,
                    1,
                    282,
                    0
                ],
                "title": "Numerical analysis of partial discharge ignition in H2 bubbles floating\n  in dielectric oils, for High-Voltage Solid State Transformer applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerical analysis of partial discharge ignition in H2 bubbles floating\n  in dielectric oils, for High-Voltage Solid State Transformer applications"
                },
                "summary": "We report on a self-consistent numerical analysis campaign of partial\ndischarge (PD) ignition in H2 bubbles floating in biobased dielectric oils. We\ninvestigate various configurations (bubble sizes, bubble position, existence of\nprotrusion) on a cylinder-to-cylinder setup that emulates a specific SST module\n(from SSTAR Horizon Europe project) under transient overvoltage as well as in\nits design operational conditions (VRMS = 66 kV, AC excitation of 50 Hz). Our\nresults on electrical characteristics and plasma dynamics leading to the PD\nignition, indicate that under transient overvoltage and for mm size bubbles\n(diameter 1 -4.5 mm), the smaller the bubble the less the inception voltage,\nwhile the peak inception voltage is higher than 70 kV. The existence of\nmetallic protrusion can affect the inception voltage of a remote floating\nbubble only slightly and when this is close to the sharp tip. The extreme\nscenario of a protrusion in contact (inside) a gas bubble severely affects the\ninsulation properties and drops the PD inception voltage remarkably. The larger\nthe bubble and the sharper the tip of the protrusion the lower the inception\npeak voltage, that can reach values well below 40 kV. On the contrary and under\ndesign operation, larger bubbles increase the severity and probability of PD\nevents, leading to lower instantaneous inception voltages. Current pulses\nproduced in bubbles can quickly transit to intense streamer discharges (which\ncan also transit to catastrophic arcing) if the operational frequency is\nreduced and/or under transient, HF overvoltage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on a self-consistent numerical analysis campaign of partial\ndischarge (PD) ignition in H2 bubbles floating in biobased dielectric oils. We\ninvestigate various configurations (bubble sizes, bubble position, existence of\nprotrusion) on a cylinder-to-cylinder setup that emulates a specific SST module\n(from SSTAR Horizon Europe project) under transient overvoltage as well as in\nits design operational conditions (VRMS = 66 kV, AC excitation of 50 Hz). Our\nresults on electrical characteristics and plasma dynamics leading to the PD\nignition, indicate that under transient overvoltage and for mm size bubbles\n(diameter 1 -4.5 mm), the smaller the bubble the less the inception voltage,\nwhile the peak inception voltage is higher than 70 kV. The existence of\nmetallic protrusion can affect the inception voltage of a remote floating\nbubble only slightly and when this is close to the sharp tip. The extreme\nscenario of a protrusion in contact (inside) a gas bubble severely affects the\ninsulation properties and drops the PD inception voltage remarkably. The larger\nthe bubble and the sharper the tip of the protrusion the lower the inception\npeak voltage, that can reach values well below 40 kV. On the contrary and under\ndesign operation, larger bubbles increase the severity and probability of PD\nevents, leading to lower instantaneous inception voltages. Current pulses\nproduced in bubbles can quickly transit to intense streamer discharges (which\ncan also transit to catastrophic arcing) if the operational frequency is\nreduced and/or under transient, HF overvoltage."
                },
                "authors": [
                    {
                        "name": "Konstantinos Kourtzanidis"
                    },
                    {
                        "name": "Panagiotis Dimitrakellis"
                    },
                    {
                        "name": "Dimitrios Rakopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Rakopoulos"
                },
                "author": "Dimitrios Rakopoulos",
                "arxiv_comment": "Submitted to IEEE Transactions on Dielectrics and Electrical\n  Insulation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05863v1",
                "updated": "2024-10-08T09:53:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    9,
                    53,
                    10,
                    1,
                    282,
                    0
                ],
                "published": "2024-10-08T09:53:10Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    9,
                    53,
                    10,
                    1,
                    282,
                    0
                ],
                "title": "Enhancing Playback Performance in Video Recommender Systems with an\n  On-Device Gating and Ranking Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Playback Performance in Video Recommender Systems with an\n  On-Device Gating and Ranking Framework"
                },
                "summary": "Video recommender systems (RSs) have gained increasing attention in recent\nyears. Existing mainstream RSs focus on optimizing the matching function\nbetween users and items. However, we noticed that users frequently encounter\nplayback issues such as slow loading or stuttering while browsing the videos,\nespecially in weak network conditions, which will lead to a subpar browsing\nexperience, and may cause users to leave, even when the video content and\nrecommendations are superior. It is quite a serious issue, yet easily\noverlooked. To tackle this issue, we propose an on-device Gating and Ranking\nFramework (GRF) that cooperates with server-side RS. Specifically, we utilize a\ngate model to identify videos that may have playback issues in real-time, and\nthen we employ a ranking model to select the optimal result from a\nlocally-cached pool to replace the stuttering videos. Our solution has been\nfully deployed on Kwai, a large-scale short video platform with hundreds of\nmillions of users globally. Moreover, it significantly enhances video playback\nperformance and improves overall user experience and retention rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video recommender systems (RSs) have gained increasing attention in recent\nyears. Existing mainstream RSs focus on optimizing the matching function\nbetween users and items. However, we noticed that users frequently encounter\nplayback issues such as slow loading or stuttering while browsing the videos,\nespecially in weak network conditions, which will lead to a subpar browsing\nexperience, and may cause users to leave, even when the video content and\nrecommendations are superior. It is quite a serious issue, yet easily\noverlooked. To tackle this issue, we propose an on-device Gating and Ranking\nFramework (GRF) that cooperates with server-side RS. Specifically, we utilize a\ngate model to identify videos that may have playback issues in real-time, and\nthen we employ a ranking model to select the optimal result from a\nlocally-cached pool to replace the stuttering videos. Our solution has been\nfully deployed on Kwai, a large-scale short video platform with hundreds of\nmillions of users globally. Moreover, it significantly enhances video playback\nperformance and improves overall user experience and retention rates."
                },
                "authors": [
                    {
                        "name": "Yunfei Yang"
                    },
                    {
                        "name": "Zhenghao Qi"
                    },
                    {
                        "name": "Honghuan Wu"
                    },
                    {
                        "name": "Qi Song"
                    },
                    {
                        "name": "Tieyao Zhang"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Yimin Tu"
                    },
                    {
                        "name": "Kaiqiao Zhan"
                    },
                    {
                        "name": "Ben Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ben Wang"
                },
                "author": "Ben Wang",
                "arxiv_comment": "CIKM 2024 applied research track, 7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05854v1",
                "updated": "2024-10-08T09:46:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    9,
                    46,
                    38,
                    1,
                    282,
                    0
                ],
                "published": "2024-10-08T09:46:38Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    9,
                    46,
                    38,
                    1,
                    282,
                    0
                ],
                "title": "A Scalable State Sharing Protocol for Low-Resource Validator Nodes in\n  Blockchain Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Scalable State Sharing Protocol for Low-Resource Validator Nodes in\n  Blockchain Networks"
                },
                "summary": "The perpetual growth of data stored on popular blockchains such as Ethereum\nleads to significant scalability challenges and substantial storage costs for\noperators of full nodes. Increasing costs may lead to fewer independently\noperated nodes in the network, which poses risks to decentralization (and hence\nnetwork security), but also pushes decentralized app developers towards\ncentrally hosted API services.\n  This paper introduces a new protocol that allows validator nodes to\nparticipate in a blockchain network without the need to store the full state of\nthe network on each node. The key idea is to use the blockchain network as both\na replicated state machine and as a distributed storage system. By distributing\nstates across nodes and enabling efficient data retrieval through a\nKademlia-inspired routing protocol, we reduce storage costs for validators.\nCryptographic proofs (such as Merkle proofs) are used to allow nodes to verify\ndata stored by other nodes without having to trust those nodes directly. While\nthe protocol trades off data storage for increased network bandwidth, we show\nhow gossiping and caching can minimize the increased bandwidth needs.\n  To validate our state sharing protocol, we conduct an extensive quantitative\nanalysis of Ethereum's data storage and data access patterns. Our findings\nindicate that while our protocol significantly lowers storage needs, it comes\nwith an increased bandwidth usage ranging from 1.5 MB to 5 MB per block,\ntranslating to an additional monthly bandwidth of 319 GB to 1,065 GB. Despite\nthis, the size remains small enough such that it can be passed to all nodes and\nvalidated within Ethereum's 12-second block validation window. Further analysis\nshows that Merkle proofs are the most significant contributor to the additional\nbandwidth. To address this concern, we also analyze the impact of switching to\nthe more space-efficient Verkle Proofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The perpetual growth of data stored on popular blockchains such as Ethereum\nleads to significant scalability challenges and substantial storage costs for\noperators of full nodes. Increasing costs may lead to fewer independently\noperated nodes in the network, which poses risks to decentralization (and hence\nnetwork security), but also pushes decentralized app developers towards\ncentrally hosted API services.\n  This paper introduces a new protocol that allows validator nodes to\nparticipate in a blockchain network without the need to store the full state of\nthe network on each node. The key idea is to use the blockchain network as both\na replicated state machine and as a distributed storage system. By distributing\nstates across nodes and enabling efficient data retrieval through a\nKademlia-inspired routing protocol, we reduce storage costs for validators.\nCryptographic proofs (such as Merkle proofs) are used to allow nodes to verify\ndata stored by other nodes without having to trust those nodes directly. While\nthe protocol trades off data storage for increased network bandwidth, we show\nhow gossiping and caching can minimize the increased bandwidth needs.\n  To validate our state sharing protocol, we conduct an extensive quantitative\nanalysis of Ethereum's data storage and data access patterns. Our findings\nindicate that while our protocol significantly lowers storage needs, it comes\nwith an increased bandwidth usage ranging from 1.5 MB to 5 MB per block,\ntranslating to an additional monthly bandwidth of 319 GB to 1,065 GB. Despite\nthis, the size remains small enough such that it can be passed to all nodes and\nvalidated within Ethereum's 12-second block validation window. Further analysis\nshows that Merkle proofs are the most significant contributor to the additional\nbandwidth. To address this concern, we also analyze the impact of switching to\nthe more space-efficient Verkle Proofs."
                },
                "authors": [
                    {
                        "name": "Ruben Hias"
                    },
                    {
                        "name": "Weihong Wang"
                    },
                    {
                        "name": "Jan Vanhoof"
                    },
                    {
                        "name": "Tom Van Cutsem"
                    }
                ],
                "author_detail": {
                    "name": "Tom Van Cutsem"
                },
                "author": "Tom Van Cutsem",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12018v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12018v2",
                "updated": "2024-10-08T04:25:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    4,
                    25,
                    41,
                    1,
                    282,
                    0
                ],
                "published": "2024-06-17T18:34:58Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    18,
                    34,
                    58,
                    0,
                    169,
                    0
                ],
                "title": "CItruS: Chunked Instruction-aware State Eviction for Long Sequence\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CItruS: Chunked Instruction-aware State Eviction for Long Sequence\n  Modeling"
                },
                "summary": "Long sequence modeling has gained broad interest as large language models\n(LLMs) continue to advance. Recent research has identified that a large portion\nof hidden states within the key-value caches of Transformer models can be\ndiscarded (also termed evicted) without affecting the perplexity performance in\ngenerating long sequences. However, we show that these methods, despite\npreserving perplexity performance, often drop information that is important for\nsolving downstream tasks, a problem which we call information neglect. To\naddress this issue, we introduce Chunked Instruction-aware State Eviction\n(CItruS), a novel modeling technique that integrates the attention preferences\nuseful for a downstream task into the eviction process of hidden states. In\naddition, we design a method for chunked sequence processing to further improve\nefficiency. Our training-free method exhibits superior performance on long\nsequence comprehension and retrieval tasks over several strong baselines under\nthe same memory budget, while preserving language modeling perplexity. The code\nand data have been released at https://github.com/ybai-nlp/CItruS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long sequence modeling has gained broad interest as large language models\n(LLMs) continue to advance. Recent research has identified that a large portion\nof hidden states within the key-value caches of Transformer models can be\ndiscarded (also termed evicted) without affecting the perplexity performance in\ngenerating long sequences. However, we show that these methods, despite\npreserving perplexity performance, often drop information that is important for\nsolving downstream tasks, a problem which we call information neglect. To\naddress this issue, we introduce Chunked Instruction-aware State Eviction\n(CItruS), a novel modeling technique that integrates the attention preferences\nuseful for a downstream task into the eviction process of hidden states. In\naddition, we design a method for chunked sequence processing to further improve\nefficiency. Our training-free method exhibits superior performance on long\nsequence comprehension and retrieval tasks over several strong baselines under\nthe same memory budget, while preserving language modeling perplexity. The code\nand data have been released at https://github.com/ybai-nlp/CItruS."
                },
                "authors": [
                    {
                        "name": "Yu Bai"
                    },
                    {
                        "name": "Xiyuan Zou"
                    },
                    {
                        "name": "Heyan Huang"
                    },
                    {
                        "name": "Sanxing Chen"
                    },
                    {
                        "name": "Marc-Antoine Rondeau"
                    },
                    {
                        "name": "Yang Gao"
                    },
                    {
                        "name": "Jackie Chi Kit Cheung"
                    }
                ],
                "author_detail": {
                    "name": "Jackie Chi Kit Cheung"
                },
                "author": "Jackie Chi Kit Cheung",
                "arxiv_comment": "EMNLP 2024 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12018v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12018v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05265v1",
                "updated": "2024-10-07T17:59:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    35,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T17:59:35Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    35,
                    0,
                    281,
                    0
                ],
                "title": "PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers\n  in LLMs"
                },
                "summary": "Quantization is essential for deploying Large Language Models (LLMs) by\nenhancing memory efficiency and inference speed. Existing methods for\nactivation quantization mainly address channel-wise outliers, often neglecting\ntoken-wise outliers, leading to reliance on costly per-token dynamic\nquantization. To address this, we introduce PrefixQuant, a novel technique that\nisolates outlier tokens offline without re-training. Specifically, PrefixQuant\nidentifies high-frequency outlier tokens and prefixes them in the KV cache,\npreventing the generation of outlier tokens during inference and simplifying\nquantization. To our knowledge, PrefixQuant is the first to enable efficient\nper-tensor static quantization to outperform expensive per-token dynamic\nquantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and\n4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization\nachieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5\ncommon-sense reasoning tasks, outperforming previous per-token dynamic\nquantization methods like QuaRot with 0.98 perplexity improvement and +5.98\npoints accuracy. Additionally, the inference speed of W4A4 quantized models\nusing PrefixQuant is 1.60x to 2.81x faster than FP16 models and exceeds QuaRot\nmodels by 1.2x to 1.3x. Our code is available at\n\\url{https://github.com/ChenMnZ/PrefixQuant}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization is essential for deploying Large Language Models (LLMs) by\nenhancing memory efficiency and inference speed. Existing methods for\nactivation quantization mainly address channel-wise outliers, often neglecting\ntoken-wise outliers, leading to reliance on costly per-token dynamic\nquantization. To address this, we introduce PrefixQuant, a novel technique that\nisolates outlier tokens offline without re-training. Specifically, PrefixQuant\nidentifies high-frequency outlier tokens and prefixes them in the KV cache,\npreventing the generation of outlier tokens during inference and simplifying\nquantization. To our knowledge, PrefixQuant is the first to enable efficient\nper-tensor static quantization to outperform expensive per-token dynamic\nquantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and\n4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization\nachieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5\ncommon-sense reasoning tasks, outperforming previous per-token dynamic\nquantization methods like QuaRot with 0.98 perplexity improvement and +5.98\npoints accuracy. Additionally, the inference speed of W4A4 quantized models\nusing PrefixQuant is 1.60x to 2.81x faster than FP16 models and exceeds QuaRot\nmodels by 1.2x to 1.3x. Our code is available at\n\\url{https://github.com/ChenMnZ/PrefixQuant}."
                },
                "authors": [
                    {
                        "name": "Mengzhao Chen"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Yi Bin"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Ping Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ping Luo"
                },
                "author": "Ping Luo",
                "arxiv_comment": "A PTQ method to significantly boost the performance of static\n  activation quantization",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05516v3",
                "updated": "2024-10-07T17:21:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    21,
                    57,
                    0,
                    281,
                    0
                ],
                "published": "2023-12-09T09:55:07Z",
                "published_parsed": [
                    2023,
                    12,
                    9,
                    9,
                    55,
                    7,
                    5,
                    343,
                    0
                ],
                "title": "Stateful Large Language Model Serving with Pensieve",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stateful Large Language Model Serving with Pensieve"
                },
                "summary": "Large Language Models (LLMs) are wildly popular today and it is important to\nserve them efficiently. Existing LLM serving systems are stateless across\nrequests. Consequently, when LLMs are used in the common setting of multi-turn\nconversations, a growing log of the conversation history must be processed\nalongside any request by the serving system at each turn, resulting in repeated\nprocessing.\n  In this paper, we design $Pensieve$, a system optimized for multi-turn\nconversation LLM serving. $Pensieve$ maintains the conversation state across\nrequests by caching previously processed history to avoid duplicate processing.\n$Pensieve$'s multi-tier caching strategy can utilize both GPU and CPU memory to\nefficiently store and retrieve cached data. $Pensieve$ also generalizes the\nrecent PagedAttention kernel to support attention between multiple input tokens\nwith a GPU cache spread over non-contiguous memory. Our evaluation shows that\n$Pensieve$ can achieve $1.14$-$3.0\\times$ the throughput of vLLM and\nTensorRT-LLM and significantly reduce latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are wildly popular today and it is important to\nserve them efficiently. Existing LLM serving systems are stateless across\nrequests. Consequently, when LLMs are used in the common setting of multi-turn\nconversations, a growing log of the conversation history must be processed\nalongside any request by the serving system at each turn, resulting in repeated\nprocessing.\n  In this paper, we design $Pensieve$, a system optimized for multi-turn\nconversation LLM serving. $Pensieve$ maintains the conversation state across\nrequests by caching previously processed history to avoid duplicate processing.\n$Pensieve$'s multi-tier caching strategy can utilize both GPU and CPU memory to\nefficiently store and retrieve cached data. $Pensieve$ also generalizes the\nrecent PagedAttention kernel to support attention between multiple input tokens\nwith a GPU cache spread over non-contiguous memory. Our evaluation shows that\n$Pensieve$ can achieve $1.14$-$3.0\\times$ the throughput of vLLM and\nTensorRT-LLM and significantly reduce latency."
                },
                "authors": [
                    {
                        "name": "Lingfan Yu"
                    },
                    {
                        "name": "Jinkun Lin"
                    },
                    {
                        "name": "Jinyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinyang Li"
                },
                "author": "Jinyang Li",
                "arxiv_doi": "10.1145/3689031.3696086",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689031.3696086",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.05516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00161v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00161v2",
                "updated": "2024-10-07T15:07:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    15,
                    7,
                    9,
                    0,
                    281,
                    0
                ],
                "published": "2024-09-30T19:09:13Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    19,
                    9,
                    13,
                    0,
                    274,
                    0
                ],
                "title": "KV-Compress: Paged KV-Cache Compression with Variable Compression Rates\n  per Attention Head",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Compress: Paged KV-Cache Compression with Variable Compression Rates\n  per Attention Head"
                },
                "summary": "Context lengths of Large Language Models (LLMs) have exploded in recent\nyears, with 128k-token context becoming a standard and million-token context\nbecoming a reality. Efficiently supporting long-context inference remains\nchallenging as the memory that must be allocated in key-value (KV) cache for a\ngeneration scales with its context length, limiting the number of long-context\nrequests that can be served concurrently under a given memory budget. KV cache\ncompression can mitigate this issue by removing under-utilized KVs from each\nattention head's cache and reducing its memory footprint. Higher theoretical\ncompression rates can be achieved when the number of removed KVs varies across\nattention heads, but application of such a strategy within existing inference\nframeworks adds fragmentation and cannot realize the theoretical compression\nrates in physical memory. We introduce KV-Compress, a novel compression method\nthat evicts contiguous KV blocks within a PagedAttention framework, reducing\nthe memory footprint of the KV cache proportionally to this theoretical\ncompression rate. Our method achieves state-of-the-art performance on LongBench\nfor both Mistral-7B-Instruct-v0.2 and Llama-3.1-8B-Instruct while lowering the\ntotal number of compressed KVs by 4x compared with prior methods. Evaluations\non Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct-FP8 achieve compression\nrates up to 8x with negligible impact on performance, and up to 64x while\nretaining over 90% of full-cache performance for all but three of the suite's\nsubsets. We benchmark an integration of our method with vLLM that increases\ntotal throughput by up to 5.18x by enabling larger decoding batches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context lengths of Large Language Models (LLMs) have exploded in recent\nyears, with 128k-token context becoming a standard and million-token context\nbecoming a reality. Efficiently supporting long-context inference remains\nchallenging as the memory that must be allocated in key-value (KV) cache for a\ngeneration scales with its context length, limiting the number of long-context\nrequests that can be served concurrently under a given memory budget. KV cache\ncompression can mitigate this issue by removing under-utilized KVs from each\nattention head's cache and reducing its memory footprint. Higher theoretical\ncompression rates can be achieved when the number of removed KVs varies across\nattention heads, but application of such a strategy within existing inference\nframeworks adds fragmentation and cannot realize the theoretical compression\nrates in physical memory. We introduce KV-Compress, a novel compression method\nthat evicts contiguous KV blocks within a PagedAttention framework, reducing\nthe memory footprint of the KV cache proportionally to this theoretical\ncompression rate. Our method achieves state-of-the-art performance on LongBench\nfor both Mistral-7B-Instruct-v0.2 and Llama-3.1-8B-Instruct while lowering the\ntotal number of compressed KVs by 4x compared with prior methods. Evaluations\non Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct-FP8 achieve compression\nrates up to 8x with negligible impact on performance, and up to 64x while\nretaining over 90% of full-cache performance for all but three of the suite's\nsubsets. We benchmark an integration of our method with vLLM that increases\ntotal throughput by up to 5.18x by enabling larger decoding batches."
                },
                "authors": [
                    {
                        "name": "Isaac Rehg"
                    }
                ],
                "author_detail": {
                    "name": "Isaac Rehg"
                },
                "author": "Isaac Rehg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00161v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00161v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05076v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05076v1",
                "updated": "2024-10-07T14:30:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    30,
                    27,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T14:30:27Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    30,
                    27,
                    0,
                    281,
                    0
                ],
                "title": "TidalDecode: Fast and Accurate LLM Decoding with Position Persistent\n  Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TidalDecode: Fast and Accurate LLM Decoding with Position Persistent\n  Sparse Attention"
                },
                "summary": "Large language models (LLMs) have driven significant advancements across\ndiverse NLP tasks, with long-context models gaining prominence for handling\nextended inputs. However, the expanding key-value (KV) cache size required by\nTransformer architectures intensifies the memory constraints, particularly\nduring the decoding phase, creating a significant bottleneck. Existing sparse\nattention mechanisms designed to address this bottleneck have two limitations:\n(1) they often fail to reliably identify the most relevant tokens for\nattention, and (2) they overlook the spatial coherence of token selection\nacross consecutive Transformer layers, which can lead to performance\ndegradation and substantial overhead in token selection. This paper introduces\nTidalDecode, a simple yet effective algorithm and system for fast and accurate\nLLM decoding through position persistent sparse attention. TidalDecode\nleverages the spatial coherence of tokens selected by existing sparse attention\nmethods and introduces a few token selection layers that perform full attention\nto identify the tokens with the highest attention scores, while all other\nlayers perform sparse attention with the pre-selected tokens. This design\nenables TidalDecode to substantially reduce the overhead of token selection for\nsparse attention without sacrificing the quality of the generated results.\nEvaluation on a diverse set of LLMs and tasks shows that TidalDecode closely\nmatches the generative performance of full attention methods while reducing the\nLLM decoding latency by up to 2.1x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have driven significant advancements across\ndiverse NLP tasks, with long-context models gaining prominence for handling\nextended inputs. However, the expanding key-value (KV) cache size required by\nTransformer architectures intensifies the memory constraints, particularly\nduring the decoding phase, creating a significant bottleneck. Existing sparse\nattention mechanisms designed to address this bottleneck have two limitations:\n(1) they often fail to reliably identify the most relevant tokens for\nattention, and (2) they overlook the spatial coherence of token selection\nacross consecutive Transformer layers, which can lead to performance\ndegradation and substantial overhead in token selection. This paper introduces\nTidalDecode, a simple yet effective algorithm and system for fast and accurate\nLLM decoding through position persistent sparse attention. TidalDecode\nleverages the spatial coherence of tokens selected by existing sparse attention\nmethods and introduces a few token selection layers that perform full attention\nto identify the tokens with the highest attention scores, while all other\nlayers perform sparse attention with the pre-selected tokens. This design\nenables TidalDecode to substantially reduce the overhead of token selection for\nsparse attention without sacrificing the quality of the generated results.\nEvaluation on a diverse set of LLMs and tasks shows that TidalDecode closely\nmatches the generative performance of full attention methods while reducing the\nLLM decoding latency by up to 2.1x."
                },
                "authors": [
                    {
                        "name": "Lijie Yang"
                    },
                    {
                        "name": "Zhihao Zhang"
                    },
                    {
                        "name": "Zhuofu Chen"
                    },
                    {
                        "name": "Zikun Li"
                    },
                    {
                        "name": "Zhihao Jia"
                    }
                ],
                "author_detail": {
                    "name": "Zhihao Jia"
                },
                "author": "Zhihao Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05076v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05033v1",
                "updated": "2024-10-07T13:33:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    33,
                    23,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T13:33:23Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    33,
                    23,
                    0,
                    281,
                    0
                ],
                "title": "Extended Functional Representation Lemma: A Tool For Privacy, Semantic\n  Representation, Caching, and Compression Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extended Functional Representation Lemma: A Tool For Privacy, Semantic\n  Representation, Caching, and Compression Design"
                },
                "summary": "This paper provides an overview of a problem in information-theoretic privacy\nmechanism design, addressing two scenarios in which private data is either\nobservable or hidden. In each scenario, different privacy measures are used,\nincluding bounded mutual information and two types of per-letter privacy\nconstraints. Considering the first scenario, an agent observes useful data that\nis correlated with private data, and wants to disclose the useful information\nto a user. Due to the privacy concerns, direct disclosure is prohibited. Hence,\na privacy mechanism is designed to generate disclosed data which maximizes the\nrevealed information about the useful data while satisfying a privacy\nconstraint. In the second scenario, the agent has additionally access to the\nprivate data. We discuss how the Functional Representation Lemma, the Strong\nFunctional Representation Lemma, and their extended versions are useful for\ndesigning low-complexity privacy mechanisms that achieve optimal\nprivacy-utility trade-offs under certain constraints. Furthermore, another\nprivacy design problem is presented where part of the private attribute is more\nprivate than the remaining part. Finally, we provide applications including\nsemantic communications, caching and delivery, and compression designs, where\nthe approach can be applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides an overview of a problem in information-theoretic privacy\nmechanism design, addressing two scenarios in which private data is either\nobservable or hidden. In each scenario, different privacy measures are used,\nincluding bounded mutual information and two types of per-letter privacy\nconstraints. Considering the first scenario, an agent observes useful data that\nis correlated with private data, and wants to disclose the useful information\nto a user. Due to the privacy concerns, direct disclosure is prohibited. Hence,\na privacy mechanism is designed to generate disclosed data which maximizes the\nrevealed information about the useful data while satisfying a privacy\nconstraint. In the second scenario, the agent has additionally access to the\nprivate data. We discuss how the Functional Representation Lemma, the Strong\nFunctional Representation Lemma, and their extended versions are useful for\ndesigning low-complexity privacy mechanisms that achieve optimal\nprivacy-utility trade-offs under certain constraints. Furthermore, another\nprivacy design problem is presented where part of the private attribute is more\nprivate than the remaining part. Finally, we provide applications including\nsemantic communications, caching and delivery, and compression designs, where\nthe approach can be applied."
                },
                "authors": [
                    {
                        "name": "Amirreza Zamani"
                    },
                    {
                        "name": "Mikael Skoglund"
                    }
                ],
                "author_detail": {
                    "name": "Mikael Skoglund"
                },
                "author": "Mikael Skoglund",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2212.12475",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05004v1",
                "updated": "2024-10-07T13:03:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    3,
                    45,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T13:03:45Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    3,
                    45,
                    0,
                    281,
                    0
                ],
                "title": "Fast State Restoration in LLM Serving with HCache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast State Restoration in LLM Serving with HCache"
                },
                "summary": "The growing complexity of LLM usage today, e.g., multi-round conversation and\nretrieval-augmented generation (RAG), makes contextual states (i.e., KV cache)\nreusable across user requests. Given the capacity constraints of GPU memory,\nonly a limited number of contexts can be cached on GPU for reusing. Existing\ninference systems typically evict part of the KV cache and restore it by\nrecomputing it from the original tokens or offloading it to host storage for\nlater retrieval, both of which introduce substantial computational or I/O\noverheads. We propose HCache, a novel LLM state restoration method. Its key\nidea is to restore LLM states from intermediate activations and thus utilize\ncomputational and I/O resources with low overhead. We enhance HCache with two\ntechniques, including i) a bubble-free restoration scheduler that integrates\nresource-complementary methods to optimize the balance between computation and\nIO tasks; and ii) a chunk-based storage manager to address the layout mismatch\nissue (i.e., layer-before-token saving versus token-before-layer restoration).\nOur evaluations, conducted using real-world tasks, show that HCache reduces the\nTTFT by up to 1.93X compared to KV offload while consuming 1.92-2.40X less\nstorage space; compared to token recomputation, HCache achieves up to 5.73X\nreduction in TTFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing complexity of LLM usage today, e.g., multi-round conversation and\nretrieval-augmented generation (RAG), makes contextual states (i.e., KV cache)\nreusable across user requests. Given the capacity constraints of GPU memory,\nonly a limited number of contexts can be cached on GPU for reusing. Existing\ninference systems typically evict part of the KV cache and restore it by\nrecomputing it from the original tokens or offloading it to host storage for\nlater retrieval, both of which introduce substantial computational or I/O\noverheads. We propose HCache, a novel LLM state restoration method. Its key\nidea is to restore LLM states from intermediate activations and thus utilize\ncomputational and I/O resources with low overhead. We enhance HCache with two\ntechniques, including i) a bubble-free restoration scheduler that integrates\nresource-complementary methods to optimize the balance between computation and\nIO tasks; and ii) a chunk-based storage manager to address the layout mismatch\nissue (i.e., layer-before-token saving versus token-before-layer restoration).\nOur evaluations, conducted using real-world tasks, show that HCache reduces the\nTTFT by up to 1.93X compared to KV offload while consuming 1.92-2.40X less\nstorage space; compared to token recomputation, HCache achieves up to 5.73X\nreduction in TTFT."
                },
                "authors": [
                    {
                        "name": "Shiwei Gao"
                    },
                    {
                        "name": "Youmin Chen"
                    },
                    {
                        "name": "Jiwu Shu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwu Shu"
                },
                "author": "Jiwu Shu",
                "arxiv_comment": "EuroSys 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16406v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16406v3",
                "updated": "2024-10-07T01:27:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    1,
                    27,
                    59,
                    0,
                    281,
                    0
                ],
                "published": "2024-05-26T02:15:49Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    2,
                    15,
                    49,
                    6,
                    147,
                    0
                ],
                "title": "SpinQuant: LLM quantization with learned rotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpinQuant: LLM quantization with learned rotations"
                },
                "summary": "Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot."
                },
                "authors": [
                    {
                        "name": "Zechun Liu"
                    },
                    {
                        "name": "Changsheng Zhao"
                    },
                    {
                        "name": "Igor Fedorov"
                    },
                    {
                        "name": "Bilge Soran"
                    },
                    {
                        "name": "Dhruv Choudhary"
                    },
                    {
                        "name": "Raghuraman Krishnamoorthi"
                    },
                    {
                        "name": "Vikas Chandra"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Tijmen Blankevoort"
                    }
                ],
                "author_detail": {
                    "name": "Tijmen Blankevoort"
                },
                "author": "Tijmen Blankevoort",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16406v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16406v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18400v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18400v4",
                "updated": "2024-10-06T22:13:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    6,
                    22,
                    13,
                    16,
                    6,
                    280,
                    0
                ],
                "published": "2024-05-28T17:40:48Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    17,
                    40,
                    48,
                    1,
                    149,
                    0
                ],
                "title": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass"
                },
                "summary": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding."
                },
                "authors": [
                    {
                        "name": "Ethan Shen"
                    },
                    {
                        "name": "Alan Fan"
                    },
                    {
                        "name": "Sarah M. Pratt"
                    },
                    {
                        "name": "Jae Sung Park"
                    },
                    {
                        "name": "Matthew Wallingford"
                    },
                    {
                        "name": "Sham M. Kakade"
                    },
                    {
                        "name": "Ari Holtzman"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Ali Farhadi"
                    },
                    {
                        "name": "Aditya Kusupati"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Kusupati"
                },
                "author": "Aditya Kusupati",
                "arxiv_comment": "23 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18400v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18400v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04603v1",
                "updated": "2024-10-06T19:36:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    6,
                    19,
                    36,
                    34,
                    6,
                    280,
                    0
                ],
                "published": "2024-10-06T19:36:34Z",
                "published_parsed": [
                    2024,
                    10,
                    6,
                    19,
                    36,
                    34,
                    6,
                    280,
                    0
                ],
                "title": "Self-compensating Light Calorimetry with Liquid Argon Time Projection\n  Chamber for GeV Neutrino Physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-compensating Light Calorimetry with Liquid Argon Time Projection\n  Chamber for GeV Neutrino Physics"
                },
                "summary": "Liquid Argon Time Projection Chamber (LArTPC) is an exceptional dual\ncalorimeter capable of estimating the energy of incident particles through both\nthe ionization charge and the scintillation light. Our studies show that due to\nthe mechanisms of charge recombination and light generation involved in the\nenergy dissipation in liquid argon, light calorimetry in LArTPCs is inherently\nself-compensating: the missing energy in the hadronic component is compensated\nfor by the extra recombination luminescence compared to the electromagnetic\ncomponent. Good compensation of the electron-to-hadron response ratio (e/h)\naround unity can be achieved across a broad range of drift electric fields from\n0.2 to 1.8 kV/cm.This inherent self-compensation enhances the appeal of light\ncalorimetry in LArTPCs, complementing the well-established charge calorimetry.\nUsing GeV neutrinos as a case study, we show that light calorimetry can achieve\nan energy resolution comparable to the more sophisticated charge imaging\ncalorimetry. The synergy between light and charge calorimetry offers a novel\napproach to evaluating and mitigating systematic uncertainties in energy\nmeasurements with LArTPCs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Liquid Argon Time Projection Chamber (LArTPC) is an exceptional dual\ncalorimeter capable of estimating the energy of incident particles through both\nthe ionization charge and the scintillation light. Our studies show that due to\nthe mechanisms of charge recombination and light generation involved in the\nenergy dissipation in liquid argon, light calorimetry in LArTPCs is inherently\nself-compensating: the missing energy in the hadronic component is compensated\nfor by the extra recombination luminescence compared to the electromagnetic\ncomponent. Good compensation of the electron-to-hadron response ratio (e/h)\naround unity can be achieved across a broad range of drift electric fields from\n0.2 to 1.8 kV/cm.This inherent self-compensation enhances the appeal of light\ncalorimetry in LArTPCs, complementing the well-established charge calorimetry.\nUsing GeV neutrinos as a case study, we show that light calorimetry can achieve\nan energy resolution comparable to the more sophisticated charge imaging\ncalorimetry. The synergy between light and charge calorimetry offers a novel\napproach to evaluating and mitigating systematic uncertainties in energy\nmeasurements with LArTPCs."
                },
                "authors": [
                    {
                        "name": "Xuyang Ning"
                    },
                    {
                        "name": "Wei Shi"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Ciro Riccio"
                    },
                    {
                        "name": "Jay Hyun Jo"
                    }
                ],
                "author_detail": {
                    "name": "Jay Hyun Jo"
                },
                "author": "Jay Hyun Jo",
                "arxiv_comment": "15 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04252v1",
                "updated": "2024-10-05T18:20:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    5,
                    18,
                    20,
                    37,
                    5,
                    279,
                    0
                ],
                "published": "2024-10-05T18:20:37Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    18,
                    20,
                    37,
                    5,
                    279,
                    0
                ],
                "title": "Lazy Qubit Reordering for Accelerating Parallel State-Vector-based\n  Quantum Circuit Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lazy Qubit Reordering for Accelerating Parallel State-Vector-based\n  Quantum Circuit Simulation"
                },
                "summary": "This paper proposes two quantum operation scheduling methods for accelerating\nparallel state-vector-based quantum circuit simulation using multiple graphics\nprocessing units (GPUs). The proposed methods reduce all-to-all communication\ncaused by qubit reordering (QR), which can dominate the overhead of parallel\nsimulation. Our approach eliminates redundant QRs by introducing intentional\ndelays in QR communications such that multiple QRs can be aggregated into a\nsingle QR. The delays are carefully introduced based on the principles of\ntime-space tiling, or a cache optimization technique for classical computers,\nwhich we use to arrange the execution order of quantum operations. Moreover, we\npresent an extended scheduling method for the hierarchical interconnection of\nGPU cluster systems to avoid slow inter-node communication. We develop these\nmethods tailored for two primary procedures in variational quantum eigensolver\n(VQE) simulation: quantum state update (QSU) and expectation value computation\n(EVC). Experimental validation on 32-GPU executions demonstrates acceleration\nin QSU and EVC -- up to 54$\\times$ and 606$\\times$, respectively -- compared to\nexisting methods. Moreover, our extended scheduling method further reduced\ncommunication time by up to 15\\% in a two-layered interconnected cluster\nsystem. Our approach is useful for any quantum circuit simulations, including\nQSU and/or EVC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes two quantum operation scheduling methods for accelerating\nparallel state-vector-based quantum circuit simulation using multiple graphics\nprocessing units (GPUs). The proposed methods reduce all-to-all communication\ncaused by qubit reordering (QR), which can dominate the overhead of parallel\nsimulation. Our approach eliminates redundant QRs by introducing intentional\ndelays in QR communications such that multiple QRs can be aggregated into a\nsingle QR. The delays are carefully introduced based on the principles of\ntime-space tiling, or a cache optimization technique for classical computers,\nwhich we use to arrange the execution order of quantum operations. Moreover, we\npresent an extended scheduling method for the hierarchical interconnection of\nGPU cluster systems to avoid slow inter-node communication. We develop these\nmethods tailored for two primary procedures in variational quantum eigensolver\n(VQE) simulation: quantum state update (QSU) and expectation value computation\n(EVC). Experimental validation on 32-GPU executions demonstrates acceleration\nin QSU and EVC -- up to 54$\\times$ and 606$\\times$, respectively -- compared to\nexisting methods. Moreover, our extended scheduling method further reduced\ncommunication time by up to 15\\% in a two-layered interconnected cluster\nsystem. Our approach is useful for any quantum circuit simulations, including\nQSU and/or EVC."
                },
                "authors": [
                    {
                        "name": "Yusuke Teranishi"
                    },
                    {
                        "name": "Shoma Hiraoka"
                    },
                    {
                        "name": "Wataru Mizukami"
                    },
                    {
                        "name": "Masao Okita"
                    },
                    {
                        "name": "Fumihiko Ino"
                    }
                ],
                "author_detail": {
                    "name": "Fumihiko Ino"
                },
                "author": "Fumihiko Ino",
                "arxiv_comment": "24 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03960v1",
                "updated": "2024-10-04T22:45:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    22,
                    45,
                    26,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-04T22:45:26Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    22,
                    45,
                    26,
                    4,
                    278,
                    0
                ],
                "title": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation"
                },
                "summary": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs."
                },
                "authors": [
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Zhewei Yao"
                    },
                    {
                        "name": "Samyam Rajbhandari"
                    },
                    {
                        "name": "Yuxiong He"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiong He"
                },
                "author": "Yuxiong He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01723v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01723v2",
                "updated": "2024-10-04T10:14:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    10,
                    14,
                    17,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-02T16:34:29Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    34,
                    29,
                    2,
                    276,
                    0
                ],
                "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Cache\n  in Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmoniCa: Harmonizing Training and Inference for Better Feature Cache\n  in Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion Transformers (DiTs) have gained prominence for outstanding\nscalability and extraordinary performance in generative tasks. However, their\nconsiderable inference costs impede practical deployment. The feature cache\nmechanism, which involves storing and retrieving redundant computations across\ntimesteps, holds promise for reducing per-step inference time in diffusion\nmodels. Most existing caching methods for DiT are manually designed. Although\nthe learning-based approach attempts to optimize strategies adaptively, it\nsuffers from discrepancies between training and inference, which hampers both\nthe performance and acceleration ratio. Upon detailed analysis, we pinpoint\nthat these discrepancies primarily stem from two aspects: (1) Prior Timestep\nDisregard, where training ignores the effect of cache usage at earlier\ntimesteps, and (2) Objective Mismatch, where the training target (align\npredicted noise in each timestep) deviates from the goal of inference (generate\nthe high-quality image). To alleviate these discrepancies, we propose\nHarmoniCa, a novel method that Harmonizes training and inference with a novel\nlearning-based Caching framework built upon Step-Wise Denoising Training (SDT)\nand Image Error Proxy-Guided Objective (IEPO). Compared to the traditional\ntraining paradigm, the newly proposed SDT maintains the continuity of the\ndenoising process, enabling the model to leverage information from prior\ntimesteps during training, similar to the way it operates during inference.\nFurthermore, we design IEPO, which integrates an efficient proxy mechanism to\napproximate the final image error caused by reusing the cached feature.\nTherefore, IEPO helps balance final image quality and cache utilization,\nresolving the issue of training that only considers the impact of cache usage\non the predicted output at each timestep.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have gained prominence for outstanding\nscalability and extraordinary performance in generative tasks. However, their\nconsiderable inference costs impede practical deployment. The feature cache\nmechanism, which involves storing and retrieving redundant computations across\ntimesteps, holds promise for reducing per-step inference time in diffusion\nmodels. Most existing caching methods for DiT are manually designed. Although\nthe learning-based approach attempts to optimize strategies adaptively, it\nsuffers from discrepancies between training and inference, which hampers both\nthe performance and acceleration ratio. Upon detailed analysis, we pinpoint\nthat these discrepancies primarily stem from two aspects: (1) Prior Timestep\nDisregard, where training ignores the effect of cache usage at earlier\ntimesteps, and (2) Objective Mismatch, where the training target (align\npredicted noise in each timestep) deviates from the goal of inference (generate\nthe high-quality image). To alleviate these discrepancies, we propose\nHarmoniCa, a novel method that Harmonizes training and inference with a novel\nlearning-based Caching framework built upon Step-Wise Denoising Training (SDT)\nand Image Error Proxy-Guided Objective (IEPO). Compared to the traditional\ntraining paradigm, the newly proposed SDT maintains the continuity of the\ndenoising process, enabling the model to leverage information from prior\ntimesteps during training, similar to the way it operates during inference.\nFurthermore, we design IEPO, which integrates an efficient proxy mechanism to\napproximate the final image error caused by reusing the cached feature.\nTherefore, IEPO helps balance final image quality and cache utilization,\nresolving the issue of training that only considers the impact of cache usage\non the predicted output at each timestep."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Xinjie Zhang"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "Code will be released soon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01723v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01723v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02369v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02369v2",
                "updated": "2024-10-04T07:54:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    7,
                    54,
                    58,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-03T10:33:49Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    10,
                    33,
                    49,
                    3,
                    277,
                    0
                ],
                "title": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation"
                },
                "summary": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings."
                },
                "authors": [
                    {
                        "name": "Muzhi Zhu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Zekai Luo"
                    },
                    {
                        "name": "Chenchen Jing"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Guangkai Xu"
                    },
                    {
                        "name": "Xinlong Wang"
                    },
                    {
                        "name": "Chunhua Shen"
                    }
                ],
                "author_detail": {
                    "name": "Chunhua Shen"
                },
                "author": "Chunhua Shen",
                "arxiv_comment": "Accepted to Proc. Annual Conference on Neural Information Processing\n  Systems (NeurIPS) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02369v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02369v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12016v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12016v2",
                "updated": "2024-10-04T06:26:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    6,
                    26,
                    20,
                    4,
                    278,
                    0
                ],
                "published": "2024-06-17T18:33:44Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    18,
                    33,
                    44,
                    0,
                    169,
                    0
                ],
                "title": "Prefixing Attention Sinks can Mitigate Activation Outliers for Large\n  Language Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefixing Attention Sinks can Mitigate Activation Outliers for Large\n  Language Model Quantization"
                },
                "summary": "Despite recent advances in LLM quantization, activation quantization remains\nto be challenging due to the activation outliers. Conventional remedies, e.g.,\nmixing precisions for different channels, introduce extra overhead and reduce\nthe speedup. In this work, we develop a simple yet effective strategy to\nfacilitate per-tensor activation quantization by preventing the generation of\nproblematic tokens. Precisely, we propose a method to find a set of key-value\ncache, coined CushionCache, which mitigates outliers in subsequent tokens when\ninserted as a prefix. CushionCache works in two steps: First, we greedily\nsearch for a prompt token sequence that minimizes the maximum activation values\nin subsequent tokens. Then, we further tune the token cache to regularize the\nactivations of subsequent tokens to be more quantization-friendly. The proposed\nmethod successfully addresses activation outliers of LLMs, providing a\nsubstantial performance boost for per-tensor activation quantization methods.\nWe thoroughly evaluate our method over a wide range of models and benchmarks\nand find that it significantly surpasses the established baseline of per-tensor\nW8A8 quantization and can be seamlessly integrated with the recent activation\nquantization method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent advances in LLM quantization, activation quantization remains\nto be challenging due to the activation outliers. Conventional remedies, e.g.,\nmixing precisions for different channels, introduce extra overhead and reduce\nthe speedup. In this work, we develop a simple yet effective strategy to\nfacilitate per-tensor activation quantization by preventing the generation of\nproblematic tokens. Precisely, we propose a method to find a set of key-value\ncache, coined CushionCache, which mitigates outliers in subsequent tokens when\ninserted as a prefix. CushionCache works in two steps: First, we greedily\nsearch for a prompt token sequence that minimizes the maximum activation values\nin subsequent tokens. Then, we further tune the token cache to regularize the\nactivations of subsequent tokens to be more quantization-friendly. The proposed\nmethod successfully addresses activation outliers of LLMs, providing a\nsubstantial performance boost for per-tensor activation quantization methods.\nWe thoroughly evaluate our method over a wide range of models and benchmarks\nand find that it significantly surpasses the established baseline of per-tensor\nW8A8 quantization and can be seamlessly integrated with the recent activation\nquantization method."
                },
                "authors": [
                    {
                        "name": "Seungwoo Son"
                    },
                    {
                        "name": "Wonpyo Park"
                    },
                    {
                        "name": "Woohyun Han"
                    },
                    {
                        "name": "Kyuyeun Kim"
                    },
                    {
                        "name": "Jaeho Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jaeho Lee"
                },
                "author": "Jaeho Lee",
                "arxiv_comment": "EMNLP 2024 Main (Long)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12016v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12016v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03111v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03111v1",
                "updated": "2024-10-04T03:10:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    3,
                    10,
                    53,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-04T03:10:53Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    3,
                    10,
                    53,
                    4,
                    278,
                    0
                ],
                "title": "LoRC: Low-Rank Compression for LLMs KV Cache with a Progressive\n  Compression Strategy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRC: Low-Rank Compression for LLMs KV Cache with a Progressive\n  Compression Strategy"
                },
                "summary": "The Key-Value (KV) cache is a crucial component in serving transformer-based\nautoregressive large language models (LLMs), enabling faster inference by\nstoring previously computed KV vectors. However, its memory consumption scales\nlinearly with sequence length and batch size, posing a significant bottleneck\nin LLM deployment. Existing approaches to mitigate this issue include: (1)\nefficient attention variants integrated in upcycling stages, which requires\nextensive parameter tuning thus unsuitable for pre-trained LLMs; (2) KV cache\ncompression at test time, primarily through token eviction policies, which\noften overlook inter-layer dependencies and can be task-specific.\n  This paper introduces an orthogonal approach to KV cache compression. We\npropose a low-rank approximation of KV weight matrices, allowing for plug-in\nintegration with existing transformer-based LLMs without model retraining. To\neffectively compress KV cache at the weight level, we adjust for layerwise\nsensitivity and introduce a progressive compression strategy, which is\nsupported by our theoretical analysis on how compression errors accumulate in\ndeep networks. Our method is designed to function without model tuning in\nupcycling stages or task-specific profiling in test stages. Extensive\nexperiments with LLaMA models ranging from 8B to 70B parameters across various\ntasks show that our approach significantly reduces the GPU memory footprint\nwhile maintaining performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache is a crucial component in serving transformer-based\nautoregressive large language models (LLMs), enabling faster inference by\nstoring previously computed KV vectors. However, its memory consumption scales\nlinearly with sequence length and batch size, posing a significant bottleneck\nin LLM deployment. Existing approaches to mitigate this issue include: (1)\nefficient attention variants integrated in upcycling stages, which requires\nextensive parameter tuning thus unsuitable for pre-trained LLMs; (2) KV cache\ncompression at test time, primarily through token eviction policies, which\noften overlook inter-layer dependencies and can be task-specific.\n  This paper introduces an orthogonal approach to KV cache compression. We\npropose a low-rank approximation of KV weight matrices, allowing for plug-in\nintegration with existing transformer-based LLMs without model retraining. To\neffectively compress KV cache at the weight level, we adjust for layerwise\nsensitivity and introduce a progressive compression strategy, which is\nsupported by our theoretical analysis on how compression errors accumulate in\ndeep networks. Our method is designed to function without model tuning in\nupcycling stages or task-specific profiling in test stages. Extensive\nexperiments with LLaMA models ranging from 8B to 70B parameters across various\ntasks show that our approach significantly reduces the GPU memory footprint\nwhile maintaining performance."
                },
                "authors": [
                    {
                        "name": "Rongzhi Zhang"
                    },
                    {
                        "name": "Kuang Wang"
                    },
                    {
                        "name": "Liyuan Liu"
                    },
                    {
                        "name": "Shuohang Wang"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Yelong Shen"
                    }
                ],
                "author_detail": {
                    "name": "Yelong Shen"
                },
                "author": "Yelong Shen",
                "arxiv_comment": "15 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03111v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03111v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03090v1",
                "updated": "2024-10-04T02:32:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    2,
                    32,
                    36,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-04T02:32:36Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    2,
                    32,
                    36,
                    4,
                    278,
                    0
                ],
                "title": "UNComp: Uncertainty-Aware Long-Context Compressor for Efficient Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UNComp: Uncertainty-Aware Long-Context Compressor for Efficient Large\n  Language Model Inference"
                },
                "summary": "Deploying large language models (LLMs) is challenging due to their high\nmemory and computational demands, especially during long-context inference.\nWhile key-value (KV) caching accelerates inference by reusing previously\ncomputed keys and values, it also introduces significant memory overhead.\nExisting KV cache compression methods such as eviction and merging typically\ncompress the KV cache after it is generated and overlook the eviction of hidden\nstates, failing to improve the speed of the prefilling stage. Additionally,\napplying a uniform compression rate across different attention heads can harm\ncrucial retrieval heads in needle-in-a-haystack tasks due to excessive\ncompression. In this paper, we propose UNComp, an uncertainty-aware compression\nscheme that leverages matrix entropy to estimate model uncertainty across\nlayers and heads at the token sequence level. By grouping layers and heads\nbased on their uncertainty, UNComp adaptively compresses both the hidden states\nand the KV cache. Our method achieves a 1.6x speedup in the prefilling stage\nand reduces the KV cache to 4.74% of its original size, resulting in a 6.4x\nincrease in throughput and a 1.4x speedup in inference with only a 1.41%\nperformance loss. Remarkably, in needle-in-a-haystack tasks, UNComp outperforms\nthe full-size KV cache even when compressed to 9.38% of its original size. Our\napproach offers an efficient, training-free Grouped-Query Attention paradigm\nthat can be seamlessly integrated into existing KV cache schemes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying large language models (LLMs) is challenging due to their high\nmemory and computational demands, especially during long-context inference.\nWhile key-value (KV) caching accelerates inference by reusing previously\ncomputed keys and values, it also introduces significant memory overhead.\nExisting KV cache compression methods such as eviction and merging typically\ncompress the KV cache after it is generated and overlook the eviction of hidden\nstates, failing to improve the speed of the prefilling stage. Additionally,\napplying a uniform compression rate across different attention heads can harm\ncrucial retrieval heads in needle-in-a-haystack tasks due to excessive\ncompression. In this paper, we propose UNComp, an uncertainty-aware compression\nscheme that leverages matrix entropy to estimate model uncertainty across\nlayers and heads at the token sequence level. By grouping layers and heads\nbased on their uncertainty, UNComp adaptively compresses both the hidden states\nand the KV cache. Our method achieves a 1.6x speedup in the prefilling stage\nand reduces the KV cache to 4.74% of its original size, resulting in a 6.4x\nincrease in throughput and a 1.4x speedup in inference with only a 1.41%\nperformance loss. Remarkably, in needle-in-a-haystack tasks, UNComp outperforms\nthe full-size KV cache even when compressed to 9.38% of its original size. Our\napproach offers an efficient, training-free Grouped-Query Attention paradigm\nthat can be seamlessly integrated into existing KV cache schemes."
                },
                "authors": [
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Jianghan Shen"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Chaofan Tao"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Jianqiao Lu"
                    },
                    {
                        "name": "Xun Wu"
                    },
                    {
                        "name": "Chuanyang Zheng"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03065v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03065v1",
                "updated": "2024-10-04T01:11:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    1,
                    11,
                    9,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-04T01:11:09Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    1,
                    11,
                    9,
                    4,
                    278,
                    0
                ],
                "title": "Compute Or Load KV Cache? Why Not Both?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Or Load KV Cache? Why Not Both?"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nincreased context window sizes, enabling sophisticated applications but also\nintroducing substantial computational overheads, particularly computing\nkey-value (KV) cache in the prefill stage. Prefix caching has emerged to save\nGPU power in this scenario, which saves KV cache at disks and reuse them across\nmultiple queries. However, traditional prefix caching mechanisms often suffer\nfrom substantial latency because the speed of loading KV cache from disks to\nGPU memory is bottlenecked by the throughput of I/O devices. To optimize the\nlatency of long-context prefill, we propose Cake, a novel KV cache loader,\nwhich employs a bidirectional parallelized KV cache generation strategy. Upon\nreceiving a prefill task, Cake simultaneously and dynamically loads saved KV\ncache from prefix cache locations and computes KV cache on local GPUs,\nmaximizing the utilization of available computation and I/O bandwidth\nresources. Additionally, Cake automatically adapts to diverse system statuses\nwithout manual parameter. tuning. In experiments on various prompt datasets,\nGPUs, and I/O devices, Cake offers up to 68.1% Time To First Token (TTFT)\nreduction compare with compute-only method and 94.6% TTFT reduction compare\nwith I/O-only method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have significantly\nincreased context window sizes, enabling sophisticated applications but also\nintroducing substantial computational overheads, particularly computing\nkey-value (KV) cache in the prefill stage. Prefix caching has emerged to save\nGPU power in this scenario, which saves KV cache at disks and reuse them across\nmultiple queries. However, traditional prefix caching mechanisms often suffer\nfrom substantial latency because the speed of loading KV cache from disks to\nGPU memory is bottlenecked by the throughput of I/O devices. To optimize the\nlatency of long-context prefill, we propose Cake, a novel KV cache loader,\nwhich employs a bidirectional parallelized KV cache generation strategy. Upon\nreceiving a prefill task, Cake simultaneously and dynamically loads saved KV\ncache from prefix cache locations and computes KV cache on local GPUs,\nmaximizing the utilization of available computation and I/O bandwidth\nresources. Additionally, Cake automatically adapts to diverse system statuses\nwithout manual parameter. tuning. In experiments on various prompt datasets,\nGPUs, and I/O devices, Cake offers up to 68.1% Time To First Token (TTFT)\nreduction compare with compute-only method and 94.6% TTFT reduction compare\nwith I/O-only method."
                },
                "authors": [
                    {
                        "name": "Shuowei Jin"
                    },
                    {
                        "name": "Xueshen Liu"
                    },
                    {
                        "name": "Qingzhao Zhang"
                    },
                    {
                        "name": "Z. Morley Mao"
                    }
                ],
                "author_detail": {
                    "name": "Z. Morley Mao"
                },
                "author": "Z. Morley Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03065v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03065v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00242v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00242v3",
                "updated": "2024-10-03T22:17:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    22,
                    17,
                    1,
                    3,
                    277,
                    0
                ],
                "published": "2024-03-30T04:34:54Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    4,
                    34,
                    54,
                    5,
                    90,
                    0
                ],
                "title": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured\n  LLM Inference"
                },
                "summary": "Large language models (LLMs) are increasingly employed for complex tasks that\nprocess multiple generation calls in a tree structure with shared prefixes of\ntokens, including few-shot prompting, multi-step reasoning, speculative\ndecoding, etc. However, existing inference systems for tree-based applications\nare inefficient due to improper partitioning of queries and KV cache during\nattention calculation. This leads to two main issues: (1) a lack of memory\naccess (IO) reuse for KV cache of shared prefixes, and (2) poor load\nbalancing.As a result, there is redundant KV cache IO between GPU global memory\nand shared memory, along with low GPU utilization. To address these challenges,\nwe propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient\nattention algorithm with prefix-aware and load-balanced KV cache partitions.\nDeFT reduces the number of read/write operations of KV cache during attention\ncalculation through KV-Guided Grouping, a method that avoids repeatedly loading\nKV cache of shared prefixes in attention computation. Additionally, we propose\nFlattened Tree KV Splitting, a mechanism that ensures even distribution of the\nKV cache across partitions with little computation redundancy, enhancing GPU\nutilization during attention computations. By reducing 73-99 KV cache IO and\nnearly 100 IO for partial results during attention calculation, DeFT achieves\nup to 2.52/3.82x speedup in the end-to-end/attention latency across three\npractical tree-based workloads compared to state-of-the-art attention\nalgorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly employed for complex tasks that\nprocess multiple generation calls in a tree structure with shared prefixes of\ntokens, including few-shot prompting, multi-step reasoning, speculative\ndecoding, etc. However, existing inference systems for tree-based applications\nare inefficient due to improper partitioning of queries and KV cache during\nattention calculation. This leads to two main issues: (1) a lack of memory\naccess (IO) reuse for KV cache of shared prefixes, and (2) poor load\nbalancing.As a result, there is redundant KV cache IO between GPU global memory\nand shared memory, along with low GPU utilization. To address these challenges,\nwe propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient\nattention algorithm with prefix-aware and load-balanced KV cache partitions.\nDeFT reduces the number of read/write operations of KV cache during attention\ncalculation through KV-Guided Grouping, a method that avoids repeatedly loading\nKV cache of shared prefixes in attention computation. Additionally, we propose\nFlattened Tree KV Splitting, a mechanism that ensures even distribution of the\nKV cache across partitions with little computation redundancy, enhancing GPU\nutilization during attention computations. By reducing 73-99 KV cache IO and\nnearly 100 IO for partial results during attention calculation, DeFT achieves\nup to 2.52/3.82x speedup in the end-to-end/attention latency across three\npractical tree-based workloads compared to state-of-the-art attention\nalgorithms."
                },
                "authors": [
                    {
                        "name": "Jinwei Yao"
                    },
                    {
                        "name": "Kaiqi Chen"
                    },
                    {
                        "name": "Kexun Zhang"
                    },
                    {
                        "name": "Jiaxuan You"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Zeke Wang"
                    },
                    {
                        "name": "Tao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Tao Lin"
                },
                "author": "Tao Lin",
                "arxiv_comment": "Update DeFT-v3 with more ablation studies. DeFT-v1 was accepted by\n  ICLR'24 AGI Workshop ( https://openreview.net/forum?id=HqfLHoX8bR ). Code\n  will be released soon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00242v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00242v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15651v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15651v2",
                "updated": "2024-10-03T22:11:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    22,
                    11,
                    19,
                    3,
                    277,
                    0
                ],
                "published": "2024-03-22T23:47:19Z",
                "published_parsed": [
                    2024,
                    3,
                    22,
                    23,
                    47,
                    19,
                    4,
                    82,
                    0
                ],
                "title": "GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering"
                },
                "summary": "In this paper, we present GaNI, a Global and Near-field Illumination-aware\nneural inverse rendering technique that can reconstruct geometry, albedo, and\nroughness parameters from images of a scene captured with co-located light and\ncamera. Existing inverse rendering techniques with co-located light-camera\nfocus on single objects only, without modeling global illumination and\nnear-field lighting more prominent in scenes with multiple objects. We\nintroduce a system that solves this problem in two stages; we first reconstruct\nthe geometry powered by neural volumetric rendering NeuS, followed by inverse\nneural radiosity that uses the previously predicted geometry to estimate albedo\nand roughness. However, such a naive combination fails and we propose multiple\ntechnical contributions that enable this two-stage approach. We observe that\nNeuS fails to handle near-field illumination and strong specular reflections\nfrom the flashlight in a scene. We propose to implicitly model the effects of\nnear-field illumination and introduce a surface angle loss function to handle\nspecular reflections. Similarly, we observe that invNeRad assumes constant\nillumination throughout the capture and cannot handle moving flashlights during\ncapture. We propose a light position-aware radiance cache network and\nadditional smoothness priors on roughness to reconstruct reflectance.\nExperimental evaluation on synthetic and real data shows that our method\noutperforms the existing co-located light-camera-based inverse rendering\ntechniques. Our approach produces significantly better reflectance and slightly\nbetter geometry than capture strategies that do not require a dark room.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present GaNI, a Global and Near-field Illumination-aware\nneural inverse rendering technique that can reconstruct geometry, albedo, and\nroughness parameters from images of a scene captured with co-located light and\ncamera. Existing inverse rendering techniques with co-located light-camera\nfocus on single objects only, without modeling global illumination and\nnear-field lighting more prominent in scenes with multiple objects. We\nintroduce a system that solves this problem in two stages; we first reconstruct\nthe geometry powered by neural volumetric rendering NeuS, followed by inverse\nneural radiosity that uses the previously predicted geometry to estimate albedo\nand roughness. However, such a naive combination fails and we propose multiple\ntechnical contributions that enable this two-stage approach. We observe that\nNeuS fails to handle near-field illumination and strong specular reflections\nfrom the flashlight in a scene. We propose to implicitly model the effects of\nnear-field illumination and introduce a surface angle loss function to handle\nspecular reflections. Similarly, we observe that invNeRad assumes constant\nillumination throughout the capture and cannot handle moving flashlights during\ncapture. We propose a light position-aware radiance cache network and\nadditional smoothness priors on roughness to reconstruct reflectance.\nExperimental evaluation on synthetic and real data shows that our method\noutperforms the existing co-located light-camera-based inverse rendering\ntechniques. Our approach produces significantly better reflectance and slightly\nbetter geometry than capture strategies that do not require a dark room."
                },
                "authors": [
                    {
                        "name": "Jiaye Wu"
                    },
                    {
                        "name": "Saeed Hadadan"
                    },
                    {
                        "name": "Geng Lin"
                    },
                    {
                        "name": "Matthias Zwicker"
                    },
                    {
                        "name": "David Jacobs"
                    },
                    {
                        "name": "Roni Sengupta"
                    }
                ],
                "author_detail": {
                    "name": "Roni Sengupta"
                },
                "author": "Roni Sengupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15651v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15651v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02751v1",
                "updated": "2024-10-03T17:58:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    58,
                    11,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:58:11Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    58,
                    11,
                    3,
                    277,
                    0
                ],
                "title": "ReLIC: A Recipe for 64k Steps of In-Context Reinforcement Learning for\n  Embodied AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReLIC: A Recipe for 64k Steps of In-Context Reinforcement Learning for\n  Embodied AI"
                },
                "summary": "Intelligent embodied agents need to quickly adapt to new scenarios by\nintegrating long histories of experience into decision-making. For instance, a\nrobot in an unfamiliar house initially wouldn't know the locations of objects\nneeded for tasks and might perform inefficiently. However, as it gathers more\nexperience, it should learn the layout of its environment and remember where\nobjects are, allowing it to complete new tasks more efficiently. To enable such\nrapid adaptation to new tasks, we present ReLIC, a new approach for in-context\nreinforcement learning (RL) for embodied agents. With ReLIC, agents are capable\nof adapting to new environments using 64,000 steps of in-context experience\nwith full attention while being trained through self-generated experience via\nRL. We achieve this by proposing a novel policy update scheme for on-policy RL\ncalled \"partial updates'' as well as a Sink-KV mechanism that enables effective\nutilization of a long observation history for embodied agents. Our method\noutperforms a variety of meta-RL baselines in adapting to unseen houses in an\nembodied multi-object navigation task. In addition, we find that ReLIC is\ncapable of few-shot imitation learning despite never being trained with expert\ndemonstrations. We also provide a comprehensive analysis of ReLIC, highlighting\nthat the combination of large-scale RL training, the proposed partial updates\nscheme, and the Sink-KV are essential for effective in-context learning. The\ncode for ReLIC and all our experiments is at https://github.com/aielawady/relic",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent embodied agents need to quickly adapt to new scenarios by\nintegrating long histories of experience into decision-making. For instance, a\nrobot in an unfamiliar house initially wouldn't know the locations of objects\nneeded for tasks and might perform inefficiently. However, as it gathers more\nexperience, it should learn the layout of its environment and remember where\nobjects are, allowing it to complete new tasks more efficiently. To enable such\nrapid adaptation to new tasks, we present ReLIC, a new approach for in-context\nreinforcement learning (RL) for embodied agents. With ReLIC, agents are capable\nof adapting to new environments using 64,000 steps of in-context experience\nwith full attention while being trained through self-generated experience via\nRL. We achieve this by proposing a novel policy update scheme for on-policy RL\ncalled \"partial updates'' as well as a Sink-KV mechanism that enables effective\nutilization of a long observation history for embodied agents. Our method\noutperforms a variety of meta-RL baselines in adapting to unseen houses in an\nembodied multi-object navigation task. In addition, we find that ReLIC is\ncapable of few-shot imitation learning despite never being trained with expert\ndemonstrations. We also provide a comprehensive analysis of ReLIC, highlighting\nthat the combination of large-scale RL training, the proposed partial updates\nscheme, and the Sink-KV are essential for effective in-context learning. The\ncode for ReLIC and all our experiments is at https://github.com/aielawady/relic"
                },
                "authors": [
                    {
                        "name": "Ahmad Elawady"
                    },
                    {
                        "name": "Gunjan Chhablani"
                    },
                    {
                        "name": "Ram Ramrakhya"
                    },
                    {
                        "name": "Karmesh Yadav"
                    },
                    {
                        "name": "Dhruv Batra"
                    },
                    {
                        "name": "Zsolt Kira"
                    },
                    {
                        "name": "Andrew Szot"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Szot"
                },
                "author": "Andrew Szot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00023v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00023v2",
                "updated": "2024-10-03T17:50:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    50,
                    33,
                    3,
                    277,
                    0
                ],
                "published": "2024-05-08T06:30:58Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    6,
                    30,
                    58,
                    2,
                    129,
                    0
                ],
                "title": "Preble: Efficient Distributed Prompt Scheduling for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preble: Efficient Distributed Prompt Scheduling for LLM Serving"
                },
                "summary": "Prompts to large language models (LLMs) have evolved beyond simple user\nquestions. For LLMs to solve complex problems, today's practices are to include\ndomain-specific instructions, illustration of tool usages, and/or long context\nsuch as textbook chapters in prompts. As such, many parts of prompts are\nrepetitive across requests. Recent works propose to cache and reuse KV state of\nprompts. However, they are all confined to a single-GPU optimization, while\nproduction LLM serving systems are distributed by nature.\n  This paper proposes Preble, the first distributed LLM serving platform that\ntargets and optimizes for prompt sharing. We designed a distributed scheduling\nsystem that co-optimizes KV state reuse and computation load-balancing with a\nnew scheduling algorithm and a hierarchical scheduling mechanism. Our\nevaluation of Preble with real workloads and request arrival patterns on two\nopen-source LLMs shows that Preble outperforms the SOTA serving systems by 1.5X\nto 14.5X on average latency and 2X to 10X on p99 latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompts to large language models (LLMs) have evolved beyond simple user\nquestions. For LLMs to solve complex problems, today's practices are to include\ndomain-specific instructions, illustration of tool usages, and/or long context\nsuch as textbook chapters in prompts. As such, many parts of prompts are\nrepetitive across requests. Recent works propose to cache and reuse KV state of\nprompts. However, they are all confined to a single-GPU optimization, while\nproduction LLM serving systems are distributed by nature.\n  This paper proposes Preble, the first distributed LLM serving platform that\ntargets and optimizes for prompt sharing. We designed a distributed scheduling\nsystem that co-optimizes KV state reuse and computation load-balancing with a\nnew scheduling algorithm and a hierarchical scheduling mechanism. Our\nevaluation of Preble with real workloads and request arrival patterns on two\nopen-source LLMs shows that Preble outperforms the SOTA serving systems by 1.5X\nto 14.5X on average latency and 2X to 10X on p99 latency."
                },
                "authors": [
                    {
                        "name": "Vikranth Srivatsa"
                    },
                    {
                        "name": "Zijian He"
                    },
                    {
                        "name": "Reyna Abhyankar"
                    },
                    {
                        "name": "Dongming Li"
                    },
                    {
                        "name": "Yiying Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yiying Zhang"
                },
                "author": "Yiying Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00023v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00023v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02599v1",
                "updated": "2024-10-03T15:41:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    41,
                    31,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T15:41:31Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    41,
                    31,
                    3,
                    277,
                    0
                ],
                "title": "Disaggregated Memory with SmartNIC Offloading: a Case Study on Graph\n  Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated Memory with SmartNIC Offloading: a Case Study on Graph\n  Processing"
                },
                "summary": "Disaggregated memory breaks the boundary of monolithic servers to enable\nmemory provisioning on demand. Using network-attached memory to provide memory\nexpansion for memory-intensive applications on compute nodes can improve the\noverall memory utilization on a cluster and reduce the total cost of ownership.\nHowever, current software solutions for leveraging network-attached memory must\nconsume resources on the compute node for memory management tasks. Emerging\noff-path smartNICs provide general-purpose programmability at low-cost\nlow-power cores. This work provides a general architecture design that enables\nnetwork-attached memory and offloading tasks onto off-path programmable\nSmartNIC. We provide a prototype implementation called SODA on Nvidia BlueField\nDPU. SODA adapts communication paths and data transfer alternatives, pipelines\ndata movement stages, and enables customizable data caching and prefetching\noptimizations. We evaluate SODA in five representative graph applications on\nreal-world graphs. Our results show that SODA can achieve up to 7.9x speedup\ncompared to node-local SSD and reduce network traffic by 42% compared to\ndisaggregated memory without SmartNIC offloading at similar or better\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated memory breaks the boundary of monolithic servers to enable\nmemory provisioning on demand. Using network-attached memory to provide memory\nexpansion for memory-intensive applications on compute nodes can improve the\noverall memory utilization on a cluster and reduce the total cost of ownership.\nHowever, current software solutions for leveraging network-attached memory must\nconsume resources on the compute node for memory management tasks. Emerging\noff-path smartNICs provide general-purpose programmability at low-cost\nlow-power cores. This work provides a general architecture design that enables\nnetwork-attached memory and offloading tasks onto off-path programmable\nSmartNIC. We provide a prototype implementation called SODA on Nvidia BlueField\nDPU. SODA adapts communication paths and data transfer alternatives, pipelines\ndata movement stages, and enables customizable data caching and prefetching\noptimizations. We evaluate SODA in five representative graph applications on\nreal-world graphs. Our results show that SODA can achieve up to 7.9x speedup\ncompared to node-local SSD and reduce network traffic by 42% compared to\ndisaggregated memory without SmartNIC offloading at similar or better\nperformance."
                },
                "authors": [
                    {
                        "name": "Jacob Wahlgren"
                    },
                    {
                        "name": "Gabin Schieffer"
                    },
                    {
                        "name": "Maya Gokhale"
                    },
                    {
                        "name": "Roger Pearce"
                    },
                    {
                        "name": "Ivy Peng"
                    }
                ],
                "author_detail": {
                    "name": "Ivy Peng"
                },
                "author": "Ivy Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02527v1",
                "updated": "2024-10-03T14:35:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    35,
                    35,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T14:35:35Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    35,
                    35,
                    3,
                    277,
                    0
                ],
                "title": "Learning from Offline Foundation Features with Tensor Augmentations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from Offline Foundation Features with Tensor Augmentations"
                },
                "summary": "We introduce Learning from Offline Foundation Features with Tensor\nAugmentations (LOFF-TA), an efficient training scheme designed to harness the\ncapabilities of foundation models in limited resource settings where their\ndirect development is not feasible. LOFF-TA involves training a compact\nclassifier on cached feature embeddings from a frozen foundation model,\nresulting in up to $37\\times$ faster training and up to $26\\times$ reduced GPU\nmemory usage. Because the embeddings of augmented images would be too numerous\nto store, yet the augmentation process is essential for training, we propose to\napply tensor augmentations to the cached embeddings of the original\nnon-augmented images. LOFF-TA makes it possible to leverage the power of\nfoundation models, regardless of their size, in settings with limited\ncomputational capacity. Moreover, LOFF-TA can be used to apply foundation\nmodels to high-resolution images without increasing compute. In certain\nscenarios, we find that training with LOFF-TA yields better results than\ndirectly fine-tuning the foundation model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Learning from Offline Foundation Features with Tensor\nAugmentations (LOFF-TA), an efficient training scheme designed to harness the\ncapabilities of foundation models in limited resource settings where their\ndirect development is not feasible. LOFF-TA involves training a compact\nclassifier on cached feature embeddings from a frozen foundation model,\nresulting in up to $37\\times$ faster training and up to $26\\times$ reduced GPU\nmemory usage. Because the embeddings of augmented images would be too numerous\nto store, yet the augmentation process is essential for training, we propose to\napply tensor augmentations to the cached embeddings of the original\nnon-augmented images. LOFF-TA makes it possible to leverage the power of\nfoundation models, regardless of their size, in settings with limited\ncomputational capacity. Moreover, LOFF-TA can be used to apply foundation\nmodels to high-resolution images without increasing compute. In certain\nscenarios, we find that training with LOFF-TA yields better results than\ndirectly fine-tuning the foundation model."
                },
                "authors": [
                    {
                        "name": "Emir Konuk"
                    },
                    {
                        "name": "Christos Matsoukas"
                    },
                    {
                        "name": "Moein Sorkhei"
                    },
                    {
                        "name": "Phitchapha Lertsiravaramet"
                    },
                    {
                        "name": "Kevin Smith"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Smith"
                },
                "author": "Kevin Smith",
                "arxiv_comment": "Accepted to the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07196v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07196v2",
                "updated": "2024-10-03T11:47:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    11,
                    47,
                    21,
                    3,
                    277,
                    0
                ],
                "published": "2024-09-11T11:40:23Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    40,
                    23,
                    2,
                    255,
                    0
                ],
                "title": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses"
                },
                "summary": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed."
                },
                "authors": [
                    {
                        "name": "Benjamin Colmey"
                    },
                    {
                        "name": "Rodrigo T. Paulino"
                    },
                    {
                        "name": "Gaspard Beaufort"
                    },
                    {
                        "name": "David G. Cooke"
                    }
                ],
                "author_detail": {
                    "name": "David G. Cooke"
                },
                "author": "David G. Cooke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07196v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07196v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02069v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02069v3",
                "updated": "2024-10-03T08:46:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    8,
                    46,
                    42,
                    3,
                    277,
                    0
                ],
                "published": "2024-06-04T07:51:30Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    7,
                    51,
                    30,
                    1,
                    156,
                    0
                ],
                "title": "PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information\n  Funneling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information\n  Funneling"
                },
                "summary": "In this study, we investigate whether attention-based information flow inside\nlarge language models (LLMs) is aggregated through noticeable patterns for long\ncontext processing. Our observations reveal that LLMs aggregate information\nthrough Pyramidal Information Funneling where attention is scattering widely in\nlower layers, progressively consolidating within specific contexts, and\nultimately focusing on critical tokens (a.k.a massive activation or attention\nsink) in higher layers. Motivated by these insights, we developed PyramidKV, a\nnovel and effective KV cache compression method. This approach dynamically\nadjusts the KV cache size across different layers, allocating more cache in\nlower layers and less in higher ones, diverging from traditional methods that\nmaintain a uniform KV cache size. Our experimental evaluations, utilizing the\nLongBench benchmark, show that PyramidKV matches the performance of models with\na full KV cache while retaining only 12% of the KV cache, thus significantly\nreducing memory usage. In scenarios emphasizing memory efficiency, where only\n0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache\ncompression techniques, achieving up to a 20.5 absolute accuracy improvement on\nTREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms\ncompeting methods in maintaining long-context comprehension in LLMs; notably,\nretaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve\n100% Acc. performance, matching that of a full KV cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we investigate whether attention-based information flow inside\nlarge language models (LLMs) is aggregated through noticeable patterns for long\ncontext processing. Our observations reveal that LLMs aggregate information\nthrough Pyramidal Information Funneling where attention is scattering widely in\nlower layers, progressively consolidating within specific contexts, and\nultimately focusing on critical tokens (a.k.a massive activation or attention\nsink) in higher layers. Motivated by these insights, we developed PyramidKV, a\nnovel and effective KV cache compression method. This approach dynamically\nadjusts the KV cache size across different layers, allocating more cache in\nlower layers and less in higher ones, diverging from traditional methods that\nmaintain a uniform KV cache size. Our experimental evaluations, utilizing the\nLongBench benchmark, show that PyramidKV matches the performance of models with\na full KV cache while retaining only 12% of the KV cache, thus significantly\nreducing memory usage. In scenarios emphasizing memory efficiency, where only\n0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache\ncompression techniques, achieving up to a 20.5 absolute accuracy improvement on\nTREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms\ncompeting methods in maintaining long-context comprehension in LLMs; notably,\nretaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve\n100% Acc. performance, matching that of a full KV cache."
                },
                "authors": [
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Bofei Gao"
                    },
                    {
                        "name": "Yuliang Liu"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Keming Lu"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Baobao Chang"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02069v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02069v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21018v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21018v2",
                "updated": "2024-10-03T03:03:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    3,
                    3,
                    29,
                    3,
                    277,
                    0
                ],
                "published": "2024-07-30T17:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinK: Thinner Key Cache by Query-Driven Pruning"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance."
                },
                "authors": [
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Zhanming Jie"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Amrita Saha"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Doyen Sahoo"
                    }
                ],
                "author_detail": {
                    "name": "Doyen Sahoo"
                },
                "author": "Doyen Sahoo",
                "arxiv_comment": "20 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21018v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21018v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01805v1",
                "updated": "2024-10-02T17:59:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    59,
                    52,
                    2,
                    276,
                    0
                ],
                "published": "2024-10-02T17:59:52Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    59,
                    52,
                    2,
                    276,
                    0
                ],
                "title": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads"
                },
                "summary": "Large language models (LLMs) have shown remarkable advances in supporting\nlong-context comprehension and processing tasks. However, scaling the\ngeneration inference of LLMs to such long contexts incurs significant\nadditional computation load, and demands a substantial GPU memory footprint to\nmaintain the key-value (KV) cache of transformer-based LLMs. Existing KV cache\ncompression methods, such as quantization, face memory bottlenecks as context\nlength increases, while static-sized caches, such as eviction, suffer from\ninefficient policies. These limitations restrict deployment on consumer-grade\ndevices like a single Nvidia 4090 GPU. To overcome this, we propose Locret, a\nframework for long-context LLM inference that introduces retaining heads to\nevaluate the causal importance of KV cache units, allowing for more accurate\neviction within a fixed cache size. Locret is fine-tuned on top of the frozen\nbackbone LLM using a minimal amount of data from standard long-context SFT\ndatasets. During inference, we evict low-importance cache units along with a\nchunked prefill pattern, significantly reducing peak GPU memory usage. We\nconduct an extensive empirical study to evaluate Locret, where the experimental\nresults show that Locret outperforms the recent competitive approaches,\nincluding InfLLM, Quantization, SirLLM, and MInference, in terms of memory\nefficiency and the quality of generated contents -- Locret achieves over a 20x\nand 8x KV cache compression ratio compared to the full KV cache for\nPhi-3-mini-128K and Llama-3.1-8B-instruct. Additionally, Locret can be combined\nwith other methods, such as quantization and token merging. To our knowledge,\nLocret is the first framework capable of deploying Llama-3.1-8B or similar\nmodels on a single Nvidia 4090 GPU, enabling 128K long-context inference\nwithout compromising generation quality, and requiring little additional system\noptimizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable advances in supporting\nlong-context comprehension and processing tasks. However, scaling the\ngeneration inference of LLMs to such long contexts incurs significant\nadditional computation load, and demands a substantial GPU memory footprint to\nmaintain the key-value (KV) cache of transformer-based LLMs. Existing KV cache\ncompression methods, such as quantization, face memory bottlenecks as context\nlength increases, while static-sized caches, such as eviction, suffer from\ninefficient policies. These limitations restrict deployment on consumer-grade\ndevices like a single Nvidia 4090 GPU. To overcome this, we propose Locret, a\nframework for long-context LLM inference that introduces retaining heads to\nevaluate the causal importance of KV cache units, allowing for more accurate\neviction within a fixed cache size. Locret is fine-tuned on top of the frozen\nbackbone LLM using a minimal amount of data from standard long-context SFT\ndatasets. During inference, we evict low-importance cache units along with a\nchunked prefill pattern, significantly reducing peak GPU memory usage. We\nconduct an extensive empirical study to evaluate Locret, where the experimental\nresults show that Locret outperforms the recent competitive approaches,\nincluding InfLLM, Quantization, SirLLM, and MInference, in terms of memory\nefficiency and the quality of generated contents -- Locret achieves over a 20x\nand 8x KV cache compression ratio compared to the full KV cache for\nPhi-3-mini-128K and Llama-3.1-8B-instruct. Additionally, Locret can be combined\nwith other methods, such as quantization and token merging. To our knowledge,\nLocret is the first framework capable of deploying Llama-3.1-8B or similar\nmodels on a single Nvidia 4090 GPU, enabling 128K long-context inference\nwithout compromising generation quality, and requiring little additional system\noptimizations."
                },
                "authors": [
                    {
                        "name": "Yuxiang Huang"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyuan Liu"
                },
                "author": "Zhiyuan Liu",
                "arxiv_comment": "Preprints",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01760v1",
                "updated": "2024-10-02T17:14:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    14,
                    47,
                    2,
                    276,
                    0
                ],
                "published": "2024-10-02T17:14:47Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    14,
                    47,
                    2,
                    276,
                    0
                ],
                "title": "Competitive Ratio of Online Caching with Predictions: Lower and Upper\n  Bounds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Competitive Ratio of Online Caching with Predictions: Lower and Upper\n  Bounds"
                },
                "summary": "We address the problem of learning-augmented online caching in the scenario\nwhen each request is accompanied by a prediction of the next occurrence of the\nrequested page. We improve currently known bounds on the competitive ratio of\nthe BlindOracle algorithm, which evicts a page predicted to be requested last.\nWe also prove a lower bound on the competitive ratio of any randomized\nalgorithm and show that a combination of the BlindOracle with the Marker\nalgorithm achieves a competitive ratio that is optimal up to some constant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the problem of learning-augmented online caching in the scenario\nwhen each request is accompanied by a prediction of the next occurrence of the\nrequested page. We improve currently known bounds on the competitive ratio of\nthe BlindOracle algorithm, which evicts a page predicted to be requested last.\nWe also prove a lower bound on the competitive ratio of any randomized\nalgorithm and show that a combination of the BlindOracle with the Marker\nalgorithm achieves a competitive ratio that is optimal up to some constant."
                },
                "authors": [
                    {
                        "name": "Daniel Skachkov"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    },
                    {
                        "name": "Yuri Dorn"
                    },
                    {
                        "name": "Alexander Demin"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Demin"
                },
                "author": "Alexander Demin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03766v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03766v1",
                "updated": "2024-10-02T15:22:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    22,
                    8,
                    2,
                    276,
                    0
                ],
                "published": "2024-10-02T15:22:08Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    22,
                    8,
                    2,
                    276,
                    0
                ],
                "title": "FutureFill: Fast Generation from Convolutional Sequence Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FutureFill: Fast Generation from Convolutional Sequence Models"
                },
                "summary": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill: a method for fast generation that\napplies to any sequence prediction algorithm based on convolutional operators.\nOur approach reduces the generation time requirement from linear to square root\nrelative to the context length. Additionally, FutureFill requires a prefill\ncache sized only by the number of tokens generated, which is smaller than the\ncache requirements for standard convolutional and attention-based models. We\nvalidate our theoretical findings with experimental evidence demonstrating\ncorrectness and efficiency gains in a synthetic generation task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill: a method for fast generation that\napplies to any sequence prediction algorithm based on convolutional operators.\nOur approach reduces the generation time requirement from linear to square root\nrelative to the context length. Additionally, FutureFill requires a prefill\ncache sized only by the number of tokens generated, which is smaller than the\ncache requirements for standard convolutional and attention-based models. We\nvalidate our theoretical findings with experimental evidence demonstrating\ncorrectness and efficiency gains in a synthetic generation task."
                },
                "authors": [
                    {
                        "name": "Naman Agarwal"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Evan Dogariu"
                    },
                    {
                        "name": "Vlad Feinberg"
                    },
                    {
                        "name": "Daniel Suo"
                    },
                    {
                        "name": "Peter Bartlett"
                    },
                    {
                        "name": "Elad Hazan"
                    }
                ],
                "author_detail": {
                    "name": "Elad Hazan"
                },
                "author": "Elad Hazan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03766v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03766v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01518v1",
                "updated": "2024-10-02T13:09:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    13,
                    9,
                    41,
                    2,
                    276,
                    0
                ],
                "published": "2024-10-02T13:09:41Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    13,
                    9,
                    41,
                    2,
                    276,
                    0
                ],
                "title": "InfiniPot: Infinite Context Processing on Memory-Constrained LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniPot: Infinite Context Processing on Memory-Constrained LLMs"
                },
                "summary": "Handling long input contexts remains a significant challenge for Large\nLanguage Models (LLMs), particularly in resource-constrained environments such\nas mobile devices. Our work aims to address this limitation by introducing\nInfiniPot, a novel KV cache control framework designed to enable pre-trained\nLLMs to manage extensive sequences within fixed memory constraints efficiently,\nwithout requiring additional training. InfiniPot leverages Continual Context\nDistillation (CCD), an iterative process that compresses and retains essential\ninformation through novel importance metrics, effectively maintaining critical\ndata even without access to future context. Our comprehensive evaluations\nindicate that InfiniPot significantly outperforms models trained for long\ncontexts in various NLP tasks, establishing its efficacy and versatility. This\nwork represents a substantial advancement toward making LLMs applicable to a\nbroader range of real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handling long input contexts remains a significant challenge for Large\nLanguage Models (LLMs), particularly in resource-constrained environments such\nas mobile devices. Our work aims to address this limitation by introducing\nInfiniPot, a novel KV cache control framework designed to enable pre-trained\nLLMs to manage extensive sequences within fixed memory constraints efficiently,\nwithout requiring additional training. InfiniPot leverages Continual Context\nDistillation (CCD), an iterative process that compresses and retains essential\ninformation through novel importance metrics, effectively maintaining critical\ndata even without access to future context. Our comprehensive evaluations\nindicate that InfiniPot significantly outperforms models trained for long\ncontexts in various NLP tasks, establishing its efficacy and versatility. This\nwork represents a substantial advancement toward making LLMs applicable to a\nbroader range of real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Kyuhong Shim"
                    },
                    {
                        "name": "Jungwook Choi"
                    },
                    {
                        "name": "Simyung Chang"
                    }
                ],
                "author_detail": {
                    "name": "Simyung Chang"
                },
                "author": "Simyung Chang",
                "arxiv_comment": "EMNLP 2024 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01485v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01485v1",
                "updated": "2024-10-02T12:35:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    12,
                    35,
                    53,
                    2,
                    276,
                    0
                ],
                "published": "2024-10-02T12:35:53Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    12,
                    35,
                    53,
                    2,
                    276,
                    0
                ],
                "title": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts"
                },
                "summary": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup."
                },
                "authors": [
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Hao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Hao Peng"
                },
                "author": "Hao Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01485v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01485v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12335v2",
                "updated": "2024-10-02T00:19:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    0,
                    19,
                    13,
                    2,
                    276,
                    0
                ],
                "published": "2024-06-18T07:01:11Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    7,
                    1,
                    11,
                    1,
                    170,
                    0
                ],
                "title": "Attention Score is not All You Need for Token Importance Indicator in KV\n  Cache Reduction: Value Also Matters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Score is not All You Need for Token Importance Indicator in KV\n  Cache Reduction: Value Also Matters"
                },
                "summary": "Scaling the context size of large language models (LLMs) enables them to\nperform various new tasks, e.g., book summarization. However, the memory cost\nof the Key and Value (KV) cache in attention significantly limits the practical\napplications of LLMs. Recent works have explored token pruning for KV cache\nreduction in LLMs, relying solely on attention scores as a token importance\nindicator. However, our investigation into value vector norms revealed a\nnotably non-uniform pattern questioning their reliance only on attention\nscores. Inspired by this, we propose a new method: Value-Aware Token Pruning\n(VATP) which uses both attention scores and the $ \\ell_{1} $ norm of value\nvectors to evaluate token importance. Extensive experiments on LLaMA2-7B-chat\nand Vicuna-v1.5-7B across 16 LongBench tasks demonstrate that VATP outperforms\nattention-score-only baselines in over 12 tasks, confirming the effectiveness\nof incorporating value vector norms into token importance evaluation of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling the context size of large language models (LLMs) enables them to\nperform various new tasks, e.g., book summarization. However, the memory cost\nof the Key and Value (KV) cache in attention significantly limits the practical\napplications of LLMs. Recent works have explored token pruning for KV cache\nreduction in LLMs, relying solely on attention scores as a token importance\nindicator. However, our investigation into value vector norms revealed a\nnotably non-uniform pattern questioning their reliance only on attention\nscores. Inspired by this, we propose a new method: Value-Aware Token Pruning\n(VATP) which uses both attention scores and the $ \\ell_{1} $ norm of value\nvectors to evaluate token importance. Extensive experiments on LLaMA2-7B-chat\nand Vicuna-v1.5-7B across 16 LongBench tasks demonstrate that VATP outperforms\nattention-score-only baselines in over 12 tasks, confirming the effectiveness\nof incorporating value vector norms into token importance evaluation of LLMs."
                },
                "authors": [
                    {
                        "name": "Zhiyu Guo"
                    },
                    {
                        "name": "Hidetaka Kamigaito"
                    },
                    {
                        "name": "Taro Watanabe"
                    }
                ],
                "author_detail": {
                    "name": "Taro Watanabe"
                },
                "author": "Taro Watanabe",
                "arxiv_comment": "Accepted at EMNLP 2024 (Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00644v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00644v1",
                "updated": "2024-10-01T12:55:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    12,
                    55,
                    47,
                    1,
                    275,
                    0
                ],
                "published": "2024-10-01T12:55:47Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    12,
                    55,
                    47,
                    1,
                    275,
                    0
                ],
                "title": "PARSIR: a Package for Effective Parallel Discrete Event Simulation on\n  Multi-processor Machines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PARSIR: a Package for Effective Parallel Discrete Event Simulation on\n  Multi-processor Machines"
                },
                "summary": "In this article we present PARSIR (PARallel SImulation Runner), a package\nthat enables the effective exploitation of shared-memory multi-processor\nmachines for running discrete event simulation models. PARSIR is a\ncompile/run-time environment for discrete event simulation models developed\nwith the {\\tt C} programming language. The architecture of PARSIR has been\ndesigned in order to keep low the amount of CPU-cycles required for running\nmodels. This is achieved via the combination of a set of techniques like: 1)\ncausally consistent batch-processing of simulation events at an individual\nsimulation object for caching effectiveness; 2) high likelihood of disjoint\naccess parallelism; 3) the favoring of memory accesses on local NUMA\n(Non-Uniform-Memory-Access) nodes in the architecture, while still enabling\nwell balanced workload distribution via work-stealing from remote nodes; 4) the\nuse of RMW (Read-Modify-Write) machine instructions for fast access to\nsimulation engine data required by the worker threads for managing the\nconcurrent simulation objects and distributing the workload. Furthermore, any\narchitectural solution embedded in the PARSIR engine is fully transparent to\nthe application level code implementing the simulation model. We also provide\nexperimental results showing the effectiveness of PARSIR when running the\nreference PHOLD benchmark on a NUMA shared-memory multi-processor machine\nequipped with 40 CPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this article we present PARSIR (PARallel SImulation Runner), a package\nthat enables the effective exploitation of shared-memory multi-processor\nmachines for running discrete event simulation models. PARSIR is a\ncompile/run-time environment for discrete event simulation models developed\nwith the {\\tt C} programming language. The architecture of PARSIR has been\ndesigned in order to keep low the amount of CPU-cycles required for running\nmodels. This is achieved via the combination of a set of techniques like: 1)\ncausally consistent batch-processing of simulation events at an individual\nsimulation object for caching effectiveness; 2) high likelihood of disjoint\naccess parallelism; 3) the favoring of memory accesses on local NUMA\n(Non-Uniform-Memory-Access) nodes in the architecture, while still enabling\nwell balanced workload distribution via work-stealing from remote nodes; 4) the\nuse of RMW (Read-Modify-Write) machine instructions for fast access to\nsimulation engine data required by the worker threads for managing the\nconcurrent simulation objects and distributing the workload. Furthermore, any\narchitectural solution embedded in the PARSIR engine is fully transparent to\nthe application level code implementing the simulation model. We also provide\nexperimental results showing the effectiveness of PARSIR when running the\nreference PHOLD benchmark on a NUMA shared-memory multi-processor machine\nequipped with 40 CPUs."
                },
                "authors": [
                    {
                        "name": "Francesco Quaglia"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Quaglia"
                },
                "author": "Francesco Quaglia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00644v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00644v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00455v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00455v1",
                "updated": "2024-10-01T07:19:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    7,
                    19,
                    21,
                    1,
                    275,
                    0
                ],
                "published": "2024-10-01T07:19:21Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    7,
                    19,
                    21,
                    1,
                    275,
                    0
                ],
                "title": "Fine-Grained Vectorized Merge Sorting on RISC-V: From Register to Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Grained Vectorized Merge Sorting on RISC-V: From Register to Cache"
                },
                "summary": "Merge sort as a divide-sort-merge paradigm has been widely applied in\ncomputer science fields. As modern reduced instruction set computing\narchitectures like the fifth generation (RISC-V) regard multiple registers as a\nvector register group for wide instruction parallelism, optimizing merge sort\nwith this vectorized property is becoming increasingly common. In this paper,\nwe overhaul the divide-sort-merge paradigm, from its register-level sort to the\ncache-aware merge, to develop a fine-grained RISC-V vectorized merge sort\n(RVMS). From the register-level view, the inline vectorized transpose\ninstruction is missed in RISC-V, so implementing it efficiently is non-trivial.\nBesides, the vectorized comparisons do not always work well in the merging\nnetworks. Both issues primarily stem from the expensive data shuffle\ninstruction. To bypass it, RVMS strides to take register data as the proxy of\ndata shuffle to accelerate the transpose operation, and meanwhile replaces\nvectorized comparisons with scalar cousin for more light real value swap. On\nthe other hand, as cache-aware merge makes larger data merge in the cache, most\nmerge schemes have two drawbacks: the in-cache merge usually has low cache\nutilization, while the out-of-cache merging network remains an ineffectively\nsymmetric structure. To this end, we propose the half-merge scheme to employ\nthe auxiliary space of in-place merge to halve the footprint of naive merge\nsort, and meanwhile copy one sequence to this space to avoid the former data\nexchange. Furthermore, an asymmetric merging network is developed to adapt to\ntwo different input sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Merge sort as a divide-sort-merge paradigm has been widely applied in\ncomputer science fields. As modern reduced instruction set computing\narchitectures like the fifth generation (RISC-V) regard multiple registers as a\nvector register group for wide instruction parallelism, optimizing merge sort\nwith this vectorized property is becoming increasingly common. In this paper,\nwe overhaul the divide-sort-merge paradigm, from its register-level sort to the\ncache-aware merge, to develop a fine-grained RISC-V vectorized merge sort\n(RVMS). From the register-level view, the inline vectorized transpose\ninstruction is missed in RISC-V, so implementing it efficiently is non-trivial.\nBesides, the vectorized comparisons do not always work well in the merging\nnetworks. Both issues primarily stem from the expensive data shuffle\ninstruction. To bypass it, RVMS strides to take register data as the proxy of\ndata shuffle to accelerate the transpose operation, and meanwhile replaces\nvectorized comparisons with scalar cousin for more light real value swap. On\nthe other hand, as cache-aware merge makes larger data merge in the cache, most\nmerge schemes have two drawbacks: the in-cache merge usually has low cache\nutilization, while the out-of-cache merging network remains an ineffectively\nsymmetric structure. To this end, we propose the half-merge scheme to employ\nthe auxiliary space of in-place merge to halve the footprint of naive merge\nsort, and meanwhile copy one sequence to this space to avoid the former data\nexchange. Furthermore, an asymmetric merging network is developed to adapt to\ntwo different input sizes."
                },
                "authors": [
                    {
                        "name": "Jin Zhang"
                    },
                    {
                        "name": "Jincheng Zhou"
                    },
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Di Ma"
                    },
                    {
                        "name": "Chunye Gong"
                    }
                ],
                "author_detail": {
                    "name": "Chunye Gong"
                },
                "author": "Chunye Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00455v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00455v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15355v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15355v3",
                "updated": "2024-10-01T03:40:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    3,
                    40,
                    8,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-14T02:34:26Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    2,
                    34,
                    26,
                    5,
                    258,
                    0
                ],
                "title": "Block-Attention for Efficient RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-Attention for Efficient RAG"
                },
                "summary": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "East Sun"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Lan Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lan Tian"
                },
                "author": "Lan Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15355v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15355v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00359v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00359v1",
                "updated": "2024-10-01T03:14:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    3,
                    14,
                    12,
                    1,
                    275,
                    0
                ],
                "published": "2024-10-01T03:14:12Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    3,
                    14,
                    12,
                    1,
                    275,
                    0
                ],
                "title": "Self-controller: Controlling LLMs with Multi-round Step-by-step\n  Self-awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-controller: Controlling LLMs with Multi-round Step-by-step\n  Self-awareness"
                },
                "summary": "The applications of large language models (LLMs) have been widely spread\nacross all domains. However, the basic abilities such as the controllability of\nLLMs are still limited. To address this, we propose \"Self-controller\", a novel\nagentic framework bringing self-awareness into LLMs' reasoning logic. The core\nidea of this work is to maintain states based on the LLM's response, letting\nthe LLM become self-aware of current status and think step by step in a\nmulti-round chain-of-thought paradigm. Our experiment on the state of textual\nlength has shown the controllability and effectiveness of the Self-controller.\nWe further implement a binary search algorithm to accelerate the generation\nprocess based on the linearity and monotonicity of the textual length state.\nAnother advantage of the Self-controller comes with DeepSeek's Context Caching\ntechnology, which significantly saves computational token consumption when a\ncluster of conversations shares the same prefix of context. Theoretically, we\nprove that in this scenario the extra time complexity is $O(c \\log n)$. Results\nof the back-of-the-envelope estimation suggest that the token consumption of\nour method is no more than twice as much as that of the trivial single-round\ngeneration. Furthermore, our ablation study on word constraints demonstrates\nthe Self-controller's consistent controllability across all foundation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The applications of large language models (LLMs) have been widely spread\nacross all domains. However, the basic abilities such as the controllability of\nLLMs are still limited. To address this, we propose \"Self-controller\", a novel\nagentic framework bringing self-awareness into LLMs' reasoning logic. The core\nidea of this work is to maintain states based on the LLM's response, letting\nthe LLM become self-aware of current status and think step by step in a\nmulti-round chain-of-thought paradigm. Our experiment on the state of textual\nlength has shown the controllability and effectiveness of the Self-controller.\nWe further implement a binary search algorithm to accelerate the generation\nprocess based on the linearity and monotonicity of the textual length state.\nAnother advantage of the Self-controller comes with DeepSeek's Context Caching\ntechnology, which significantly saves computational token consumption when a\ncluster of conversations shares the same prefix of context. Theoretically, we\nprove that in this scenario the extra time complexity is $O(c \\log n)$. Results\nof the back-of-the-envelope estimation suggest that the token consumption of\nour method is no more than twice as much as that of the trivial single-round\ngeneration. Furthermore, our ablation study on word constraints demonstrates\nthe Self-controller's consistent controllability across all foundation models."
                },
                "authors": [
                    {
                        "name": "Xiao Peng"
                    },
                    {
                        "name": "Xufan Geng"
                    }
                ],
                "author_detail": {
                    "name": "Xufan Geng"
                },
                "author": "Xufan Geng",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00359v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00359v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05527v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05527v4",
                "updated": "2024-09-30T22:44:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    22,
                    44,
                    58,
                    0,
                    274,
                    0
                ],
                "published": "2024-03-08T18:48:30Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    18,
                    48,
                    30,
                    4,
                    68,
                    0
                ],
                "title": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM"
                },
                "summary": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Qingru Zhang"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Geonhwa Jeong"
                    },
                    {
                        "name": "Zaoxing Liu"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Tuo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tuo Zhao"
                },
                "author": "Tuo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05527v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05527v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2209.09166v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2209.09166v2",
                "updated": "2024-09-30T18:23:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    18,
                    23,
                    7,
                    0,
                    274,
                    0
                ],
                "published": "2022-09-19T16:35:28Z",
                "published_parsed": [
                    2022,
                    9,
                    19,
                    16,
                    35,
                    28,
                    0,
                    262,
                    0
                ],
                "title": "Cache-Oblivious Representation of B-Tree Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Oblivious Representation of B-Tree Structures"
                },
                "summary": "We propose a general data structure CORoBTS for storing B-tree-like search\ntrees dynamically in a cache-oblivious way combining the van Emde Boas memory\nlayout with packed memory array.\n  In the use of the vEB layout mostly search complexity was considered, so far.\nWe show the complexity of depth-first search of a subtree and contiguous memory\narea and provide better insight into the relationship between positions of\nvertices in tree and in memory. We describe how to build an arbitrary tree in\nvEB layout if we can simulate its depth-first search. Similarly, we examine\nbatch updates of packed memory array.\n  In CORoBTS, the stored search tree has to satisfy that all leaves are at the\nsame depth and vertices have arity between the chosen constants $a$ and $b$.\nThe data structure allows searching with an optimal I/O complexity\n$\\mathcal{O}(\\log_B{N})$ and is stored in linear space. It provides operations\nfor inserting and removing a subtree; both have an amortized I/O complexity\n$\\mathcal{O}(S\\cdot(\\log^2 N)/B + \\log_B N\\cdot\\log\\log S + 1)$ and amortized\ntime complexity $\\mathcal{O}(S\\cdot\\log^2 N)$, where $S$ is the size of the\nsubtree and $N$ the size of the whole stored tree. Rebuilding an existing\nsubtree saves the multiplicative $\\mathcal{O}(\\log^2 N)$ in both complexities\nif the number of vertices on individual tree levels is not changed; it is paid\nonly for the inserted/removed vertices otherwise.\n  Modifying cache-oblivious partially persistent array proposed by Davoodi et\nal. [ESA, pages 296-308. Springer, 2014] to use CORoBTS improves its space\ncomplexity from $\\mathcal{O}(U^{\\log_2 3} + V \\log U)$ to $\\mathcal{O}(U + V\n\\log U)$, where $U$ is the maximal size of the array and $V$ is the number of\nversions; the data locality and I/O complexity of both present and persistent\nreads are kept unchanged; I/O complexity of writes is worsened by a\npolylogarithmic factor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a general data structure CORoBTS for storing B-tree-like search\ntrees dynamically in a cache-oblivious way combining the van Emde Boas memory\nlayout with packed memory array.\n  In the use of the vEB layout mostly search complexity was considered, so far.\nWe show the complexity of depth-first search of a subtree and contiguous memory\narea and provide better insight into the relationship between positions of\nvertices in tree and in memory. We describe how to build an arbitrary tree in\nvEB layout if we can simulate its depth-first search. Similarly, we examine\nbatch updates of packed memory array.\n  In CORoBTS, the stored search tree has to satisfy that all leaves are at the\nsame depth and vertices have arity between the chosen constants $a$ and $b$.\nThe data structure allows searching with an optimal I/O complexity\n$\\mathcal{O}(\\log_B{N})$ and is stored in linear space. It provides operations\nfor inserting and removing a subtree; both have an amortized I/O complexity\n$\\mathcal{O}(S\\cdot(\\log^2 N)/B + \\log_B N\\cdot\\log\\log S + 1)$ and amortized\ntime complexity $\\mathcal{O}(S\\cdot\\log^2 N)$, where $S$ is the size of the\nsubtree and $N$ the size of the whole stored tree. Rebuilding an existing\nsubtree saves the multiplicative $\\mathcal{O}(\\log^2 N)$ in both complexities\nif the number of vertices on individual tree levels is not changed; it is paid\nonly for the inserted/removed vertices otherwise.\n  Modifying cache-oblivious partially persistent array proposed by Davoodi et\nal. [ESA, pages 296-308. Springer, 2014] to use CORoBTS improves its space\ncomplexity from $\\mathcal{O}(U^{\\log_2 3} + V \\log U)$ to $\\mathcal{O}(U + V\n\\log U)$, where $U$ is the maximal size of the array and $V$ is the number of\nversions; the data locality and I/O complexity of both present and persistent\nreads are kept unchanged; I/O complexity of writes is worsened by a\npolylogarithmic factor."
                },
                "authors": [
                    {
                        "name": "Luk Ondrek"
                    },
                    {
                        "name": "Ondej Mika"
                    }
                ],
                "author_detail": {
                    "name": "Ondej Mika"
                },
                "author": "Ondej Mika",
                "arxiv_comment": "30 pages + 7 pages of algorithms, 9 figures; changes: paper structure\n  improved, general (sub)tree (re)build added, DFS alg. simplified, build\n  complexity lowered,",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2209.09166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2209.09166v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "E.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20433v1",
                "updated": "2024-09-30T15:53:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    53,
                    36,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T15:53:36Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    53,
                    36,
                    0,
                    274,
                    0
                ],
                "title": "Impact of Device Caching and Handovers on the Performance of 3D UAV\n  Networks with Blockages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impact of Device Caching and Handovers on the Performance of 3D UAV\n  Networks with Blockages"
                },
                "summary": "We investigate an urban network characterized by blockages, where unmanned\naerial vehicles (UAVs) offer ad-hoc coverage to mobile users with distinct\nservice rate requirements. The UAV-BSs are modeled using a two-dimensional\n(2-D) marked-poisson point process (MPPP), where the marks represent the\naltitude of each UAV-base station (UAV-BS). Initially, we model the network\nblockages and analyze the association probabilities of line-of-sight (LoS) and\nnon-line-of-sight (NLoS) UAV-BSs using stochastic geometry. Subsequently, we\nderive the bth moment of the conditional success probability (CSP) and employ a\nmeta distribution (MD)-based analytical framework of signal-to-interference\nnoise ratio (SINR) taking into account the blockage distribution in the\nnetwork. Furthermore, we proposea cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE). We evaluate the HO rate and average\nthroughput experienced by users ensuring their service rate requirements are\nmet. We demonstrate that LoS associations decrease as the network density\nincreases due to the substantial increase of NLoS UAV-BSs in the network.\nAdditionally, we show that the presence of blockages does not necessarily have\na negative impact on network reliability",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate an urban network characterized by blockages, where unmanned\naerial vehicles (UAVs) offer ad-hoc coverage to mobile users with distinct\nservice rate requirements. The UAV-BSs are modeled using a two-dimensional\n(2-D) marked-poisson point process (MPPP), where the marks represent the\naltitude of each UAV-base station (UAV-BS). Initially, we model the network\nblockages and analyze the association probabilities of line-of-sight (LoS) and\nnon-line-of-sight (NLoS) UAV-BSs using stochastic geometry. Subsequently, we\nderive the bth moment of the conditional success probability (CSP) and employ a\nmeta distribution (MD)-based analytical framework of signal-to-interference\nnoise ratio (SINR) taking into account the blockage distribution in the\nnetwork. Furthermore, we proposea cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE). We evaluate the HO rate and average\nthroughput experienced by users ensuring their service rate requirements are\nmet. We demonstrate that LoS associations decrease as the network density\nincreases due to the substantial increase of NLoS UAV-BSs in the network.\nAdditionally, we show that the presence of blockages does not necessarily have\na negative impact on network reliability"
                },
                "authors": [
                    {
                        "name": "Neetu R R"
                    },
                    {
                        "name": "Gourab Ghatak"
                    },
                    {
                        "name": "Vivek Ashok Bohara"
                    }
                ],
                "author_detail": {
                    "name": "Vivek Ashok Bohara"
                },
                "author": "Vivek Ashok Bohara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08894v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08894v2",
                "updated": "2024-09-30T14:38:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    14,
                    38,
                    41,
                    0,
                    274,
                    0
                ],
                "published": "2023-10-13T06:58:07Z",
                "published_parsed": [
                    2023,
                    10,
                    13,
                    6,
                    58,
                    7,
                    4,
                    286,
                    0
                ],
                "title": "Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic\n  Wrap-Around",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic\n  Wrap-Around"
                },
                "summary": "This work explores a multiple transmit antenna setting in a multi-access\ncoded caching (MACC) network where each user accesses more than one cache. A\nMACC network has $K$ users and $K$ caches, and each user has access to $r < K$\nconsecutive caches in a cyclic wrap-around manner. There are $L$ antennas at\nthe server, and each cache has a normalized size of $M/N \\leq 1$. The cyclic\nwrap-around MACC network with a single antenna at the server has been a\nwell-investigated topic, and several coded caching schemes and improved lower\nbounds on the performance are known for the same. However, this MACC network\nhas not yet been studied under multi-antenna settings in the coded caching\nliterature. We study the multi-antenna MACC problem and propose a solution for\nthe same by constructing a pair of arrays called caching and delivery arrays.\nWe present three constructions of caching and delivery arrays for different\nscenarios and obtain corresponding multi-antenna MACC schemes for the same. Two\nschemes resulting from the above constructions achieve optimal performance\nunder uncoded placement and one-shot delivery. The optimality is shown by\nmatching the performance of the multi-antenna MACC scheme to that of an optimal\nmulti-antenna scheme for a dedicated cache network having an identical number\nof users, and each user has a normalized cache size of $rM/N$. Further, as a\nspecial case, one of the proposed schemes subsumes an existing optimal MACC\nscheme for the single-antenna setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores a multiple transmit antenna setting in a multi-access\ncoded caching (MACC) network where each user accesses more than one cache. A\nMACC network has $K$ users and $K$ caches, and each user has access to $r < K$\nconsecutive caches in a cyclic wrap-around manner. There are $L$ antennas at\nthe server, and each cache has a normalized size of $M/N \\leq 1$. The cyclic\nwrap-around MACC network with a single antenna at the server has been a\nwell-investigated topic, and several coded caching schemes and improved lower\nbounds on the performance are known for the same. However, this MACC network\nhas not yet been studied under multi-antenna settings in the coded caching\nliterature. We study the multi-antenna MACC problem and propose a solution for\nthe same by constructing a pair of arrays called caching and delivery arrays.\nWe present three constructions of caching and delivery arrays for different\nscenarios and obtain corresponding multi-antenna MACC schemes for the same. Two\nschemes resulting from the above constructions achieve optimal performance\nunder uncoded placement and one-shot delivery. The optimality is shown by\nmatching the performance of the multi-antenna MACC scheme to that of an optimal\nmulti-antenna scheme for a dedicated cache network having an identical number\nof users, and each user has a normalized cache size of $rM/N$. Further, as a\nspecial case, one of the proposed schemes subsumes an existing optimal MACC\nscheme for the single-antenna setting."
                },
                "authors": [
                    {
                        "name": "Elizabath Peter"
                    },
                    {
                        "name": "K. K. Krishnan Namboodiri"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "A new construction of caching and delivery arrays is added which is\n  optimal (in Section IV.D). A new section (Section V) is also added which\n  contains performance comparison with existing schemes. 16 pages (double\n  column), 6 Figures and one table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.08894v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08894v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20133v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20133v1",
                "updated": "2024-09-30T09:33:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    9,
                    33,
                    37,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T09:33:37Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    9,
                    33,
                    37,
                    0,
                    274,
                    0
                ],
                "title": "Improving Achievability of Cache-Aided Private Variable-Length Coding\n  with Zero Leakage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Achievability of Cache-Aided Private Variable-Length Coding\n  with Zero Leakage"
                },
                "summary": "A statistical cache-aided compression problem with a privacy constraint is\nstudied, where a server has access to a database of $N$ files, $(Y_1,...,Y_N)$,\neach of size $F$ bits and is linked through a shared channel to $K$ users,\nwhere each has access to a local cache memory of size $MF$ bits. During the\nplacement phase, the server fills the users' caches without prior knowledge of\ntheir demands, while the delivery phase takes place after the users send their\ndemands to the server. We assume that each file in database $Y_i$ is\narbitrarily correlated with a private attribute $X$, and an adversary is\nassumed to have access to the shared channel. The users and the server have\naccess to a shared key $W$. The goal is to design the cache contents and the\ndelivered message $\\cal C$ such that the average length of $\\mathcal{C}$ is\nminimized, while satisfying: i. The response $\\cal C$ does not reveal any\ninformation about $X$, i.e., $I(X;\\mathcal{C})=0$; ii. User $i$ can decode its\ndemand, $Y_{d_i}$, by using the shared key $W$, $\\cal C$, and its local cache\n$Z_i$. In a previous work, we have proposed a variable-length coding scheme\nthat combines privacy-aware compression with coded caching techniques. In this\npaper, we propose a new achievability scheme using minimum entropy coupling\nconcept and a greedy entropy-based algorithm. We show that the proposed scheme\nimproves the previous results. Moreover, considering two special cases we\nimprove the obtained bounds using the common information concept.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A statistical cache-aided compression problem with a privacy constraint is\nstudied, where a server has access to a database of $N$ files, $(Y_1,...,Y_N)$,\neach of size $F$ bits and is linked through a shared channel to $K$ users,\nwhere each has access to a local cache memory of size $MF$ bits. During the\nplacement phase, the server fills the users' caches without prior knowledge of\ntheir demands, while the delivery phase takes place after the users send their\ndemands to the server. We assume that each file in database $Y_i$ is\narbitrarily correlated with a private attribute $X$, and an adversary is\nassumed to have access to the shared channel. The users and the server have\naccess to a shared key $W$. The goal is to design the cache contents and the\ndelivered message $\\cal C$ such that the average length of $\\mathcal{C}$ is\nminimized, while satisfying: i. The response $\\cal C$ does not reveal any\ninformation about $X$, i.e., $I(X;\\mathcal{C})=0$; ii. User $i$ can decode its\ndemand, $Y_{d_i}$, by using the shared key $W$, $\\cal C$, and its local cache\n$Z_i$. In a previous work, we have proposed a variable-length coding scheme\nthat combines privacy-aware compression with coded caching techniques. In this\npaper, we propose a new achievability scheme using minimum entropy coupling\nconcept and a greedy entropy-based algorithm. We show that the proposed scheme\nimproves the previous results. Moreover, considering two special cases we\nimprove the obtained bounds using the common information concept."
                },
                "authors": [
                    {
                        "name": "Amirreza Zamani"
                    },
                    {
                        "name": "Mikael Skoglund"
                    }
                ],
                "author_detail": {
                    "name": "Mikael Skoglund"
                },
                "author": "Mikael Skoglund",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20133v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v1",
                "updated": "2024-09-30T06:55:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19720v1",
                "updated": "2024-09-29T14:31:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    29,
                    14,
                    31,
                    52,
                    6,
                    273,
                    0
                ],
                "published": "2024-09-29T14:31:52Z",
                "published_parsed": [
                    2024,
                    9,
                    29,
                    14,
                    31,
                    52,
                    6,
                    273,
                    0
                ],
                "title": "FAST: A Dual-tier Few-Shot Learning Paradigm for Whole Slide Image\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAST: A Dual-tier Few-Shot Learning Paradigm for Whole Slide Image\n  Classification"
                },
                "summary": "The expensive fine-grained annotation and data scarcity have become the\nprimary obstacles for the widespread adoption of deep learning-based Whole\nSlide Images (WSI) classification algorithms in clinical practice. Unlike\nfew-shot learning methods in natural images that can leverage the labels of\neach image, existing few-shot WSI classification methods only utilize a small\nnumber of fine-grained labels or weakly supervised slide labels for training in\norder to avoid expensive fine-grained annotation. They lack sufficient mining\nof available WSIs, severely limiting WSI classification performance. To address\nthe above issues, we propose a novel and efficient dual-tier few-shot learning\nparadigm for WSI classification, named FAST. FAST consists of a dual-level\nannotation strategy and a dual-branch classification framework. Firstly, to\navoid expensive fine-grained annotation, we collect a very small number of WSIs\nat the slide level, and annotate an extremely small number of patches. Then, to\nfully mining the available WSIs, we use all the patches and available patch\nlabels to build a cache branch, which utilizes the labeled patches to learn the\nlabels of unlabeled patches and through knowledge retrieval for patch\nclassification. In addition to the cache branch, we also construct a prior\nbranch that includes learnable prompt vectors, using the text encoder of\nvisual-language models for patch classification. Finally, we integrate the\nresults from both branches to achieve WSI classification. Extensive experiments\non binary and multi-class datasets demonstrate that our proposed method\nsignificantly surpasses existing few-shot classification methods and approaches\nthe accuracy of fully supervised methods with only 0.22$\\%$ annotation costs.\nAll codes and models will be publicly available on\nhttps://github.com/fukexue/FAST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expensive fine-grained annotation and data scarcity have become the\nprimary obstacles for the widespread adoption of deep learning-based Whole\nSlide Images (WSI) classification algorithms in clinical practice. Unlike\nfew-shot learning methods in natural images that can leverage the labels of\neach image, existing few-shot WSI classification methods only utilize a small\nnumber of fine-grained labels or weakly supervised slide labels for training in\norder to avoid expensive fine-grained annotation. They lack sufficient mining\nof available WSIs, severely limiting WSI classification performance. To address\nthe above issues, we propose a novel and efficient dual-tier few-shot learning\nparadigm for WSI classification, named FAST. FAST consists of a dual-level\nannotation strategy and a dual-branch classification framework. Firstly, to\navoid expensive fine-grained annotation, we collect a very small number of WSIs\nat the slide level, and annotate an extremely small number of patches. Then, to\nfully mining the available WSIs, we use all the patches and available patch\nlabels to build a cache branch, which utilizes the labeled patches to learn the\nlabels of unlabeled patches and through knowledge retrieval for patch\nclassification. In addition to the cache branch, we also construct a prior\nbranch that includes learnable prompt vectors, using the text encoder of\nvisual-language models for patch classification. Finally, we integrate the\nresults from both branches to achieve WSI classification. Extensive experiments\non binary and multi-class datasets demonstrate that our proposed method\nsignificantly surpasses existing few-shot classification methods and approaches\nthe accuracy of fully supervised methods with only 0.22$\\%$ annotation costs.\nAll codes and models will be publicly available on\nhttps://github.com/fukexue/FAST."
                },
                "authors": [
                    {
                        "name": "Kexue Fu"
                    },
                    {
                        "name": "Xiaoyuan Luo"
                    },
                    {
                        "name": "Linhao Qu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Ilias Maglogiannis"
                    },
                    {
                        "name": "Longxiang Gao"
                    },
                    {
                        "name": "Manning Wang"
                    }
                ],
                "author_detail": {
                    "name": "Manning Wang"
                },
                "author": "Manning Wang",
                "arxiv_comment": "Accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19694v1",
                "updated": "2024-09-29T12:53:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    29,
                    12,
                    53,
                    29,
                    6,
                    273,
                    0
                ],
                "published": "2024-09-29T12:53:29Z",
                "published_parsed": [
                    2024,
                    9,
                    29,
                    12,
                    53,
                    29,
                    6,
                    273,
                    0
                ],
                "title": "Development of a 3D-printed canine head phantom for veterinary\n  radiotherapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development of a 3D-printed canine head phantom for veterinary\n  radiotherapy"
                },
                "summary": "Purpose: To develop the Ultimate Phantom Dog for Orthovoltage Glioma\nTreatment (UPDOG), an anatomically-correct phantom which mimics a dog's head,\nfor quality assurance (QA) of kilovoltage (kV) radiotherapy treatments.\n  Methods: A computed tomography (CT) scan of a canine glioma patient was\nsegmented into bone and soft tissue using 3DSlicer. The segments were converted\nto stereolithographic (STL) files and smoothed in Fusion360. A slit to\naccommodate a radiochromic film (RCF) was added at the location of the glioma\ntumor. UPDOG was 3D printed on a polyjet printer using VeroUltraWhite ($\\rho$ =\n1.19-1.20 g/cm\\textsuperscript{3}) for the bone and Agilus30 ($\\rho$ =\n1.14-1.15 g/cm\\textsuperscript{3}) for the soft tissue. CT scans of UPDOG were\nacquired on a clinical CT scanner. An LD-V1 RCF was inserted into UPDOG and\nirradiated with a kV x-ray source from two angles. The delivered dose to the\nRCF was compared to Monte Carlo (MC) simulations performed in TOPAS.\n  Results: The bone and soft tissue segments in UPDOG were mimicked the patient\nanatomy well with tube voltage-dependent CT numbers. The contrast in HU was of\n49, 47 and 50 HU for the 80, 100, and 120 kVp scans, respectively, sufficient\nfor anatomy visualization. The irradiations delivered a maximum dose to RCF of\n284 mGy which was compared to the results of MC simulations using a depth dose\ncurve and central-axis (CAX) beam profiles. The mean difference in CAX profiles\nand PDD between RCF and MC results was 15.9\\% and 2.3\\%, respectively.\n  Conclusions: We have demonstrated that UPDOG is a useful QA tool for kV\ncanine radiotherapy. UPDOG successfully anatomically mimicked the dog anatomy,\nwith a reduced but sufficient bone contrast. We showed that dose delivered to a\ncanine glioma with kV x-rays can be successfully measured with an RCF\npositioned at the tumor location.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purpose: To develop the Ultimate Phantom Dog for Orthovoltage Glioma\nTreatment (UPDOG), an anatomically-correct phantom which mimics a dog's head,\nfor quality assurance (QA) of kilovoltage (kV) radiotherapy treatments.\n  Methods: A computed tomography (CT) scan of a canine glioma patient was\nsegmented into bone and soft tissue using 3DSlicer. The segments were converted\nto stereolithographic (STL) files and smoothed in Fusion360. A slit to\naccommodate a radiochromic film (RCF) was added at the location of the glioma\ntumor. UPDOG was 3D printed on a polyjet printer using VeroUltraWhite ($\\rho$ =\n1.19-1.20 g/cm\\textsuperscript{3}) for the bone and Agilus30 ($\\rho$ =\n1.14-1.15 g/cm\\textsuperscript{3}) for the soft tissue. CT scans of UPDOG were\nacquired on a clinical CT scanner. An LD-V1 RCF was inserted into UPDOG and\nirradiated with a kV x-ray source from two angles. The delivered dose to the\nRCF was compared to Monte Carlo (MC) simulations performed in TOPAS.\n  Results: The bone and soft tissue segments in UPDOG were mimicked the patient\nanatomy well with tube voltage-dependent CT numbers. The contrast in HU was of\n49, 47 and 50 HU for the 80, 100, and 120 kVp scans, respectively, sufficient\nfor anatomy visualization. The irradiations delivered a maximum dose to RCF of\n284 mGy which was compared to the results of MC simulations using a depth dose\ncurve and central-axis (CAX) beam profiles. The mean difference in CAX profiles\nand PDD between RCF and MC results was 15.9\\% and 2.3\\%, respectively.\n  Conclusions: We have demonstrated that UPDOG is a useful QA tool for kV\ncanine radiotherapy. UPDOG successfully anatomically mimicked the dog anatomy,\nwith a reduced but sufficient bone contrast. We showed that dose delivered to a\ncanine glioma with kV x-rays can be successfully measured with an RCF\npositioned at the tumor location."
                },
                "authors": [
                    {
                        "name": "Sandhya Rottoo"
                    },
                    {
                        "name": "Luke Frangella"
                    },
                    {
                        "name": "Magdalena Bazalova-Carter"
                    },
                    {
                        "name": "Olivia Masella"
                    }
                ],
                "author_detail": {
                    "name": "Olivia Masella"
                },
                "author": "Olivia Masella",
                "arxiv_comment": "9 pages, 6 figures. Submitted to Biomedical Physics & Engineering\n  Express",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19478v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19478v1",
                "updated": "2024-09-28T23:01:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    28,
                    23,
                    1,
                    48,
                    5,
                    272,
                    0
                ],
                "published": "2024-09-28T23:01:48Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    23,
                    1,
                    48,
                    5,
                    272,
                    0
                ],
                "title": "RTL2M$$PATH: Multi-$$PATH Synthesis with Applications to Hardware\n  Security Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RTL2M$$PATH: Multi-$$PATH Synthesis with Applications to Hardware\n  Security Verification"
                },
                "summary": "The Check tools automate formal memory consistency model and security\nverification of processors by analyzing abstract models of microarchitectures,\ncalled $\\mu$SPEC models. Despite the efficacy of this approach, a verification\ngap between $\\mu$SPEC models, which must be manually written, and RTL limits\nthe Check tools' broad adoption. Our prior work, called RTL2$\\mu$SPEC, narrows\nthis gap by automatically synthesizing formally verified $\\mu$SPEC models from\nSystemVerilog implementations of simple processors. But, RTL2$\\mu$SPEC assumes\ninput designs where an instruction (e.g., a load) cannot exhibit more than one\nmicroarchitectural execution path ($\\mu$PATH, e.g., a cache hit or miss path)\n-- its single-execution-path assumption.\n  In this paper, we first propose an automated approach and tool, called\nRTL2M$\\mu$PATH, that resolves RTL2$\\mu$SPEC's single-execution-path assumption.\nGiven a SystemVerilog processor design, instruction encodings, and modest\ndesign metadata, RTL2M$\\mu$PATH finds a complete set of formally verified\n$\\mu$PATHs for each instruction. Next, we make an important observation: an\ninstruction that can exhibit more than one $\\mu$PATH strongly indicates the\npresence of a microarchitectural side channel in the input design. Based on\nthis observation, we then propose an automated approach and tool, called\nSynthLC, that extends RTL2M$\\mu$PATH with a symbolic information flow analysis\nto support synthesizing a variety of formally verified leakage contracts from\nSystemVerilog processor designs. Leakage contracts are foundational to\nstate-of-the-art defenses against hardware side-channel attacks. SynthLC is the\nfirst automated methodology for formally verifying hardware adherence to them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Check tools automate formal memory consistency model and security\nverification of processors by analyzing abstract models of microarchitectures,\ncalled $\\mu$SPEC models. Despite the efficacy of this approach, a verification\ngap between $\\mu$SPEC models, which must be manually written, and RTL limits\nthe Check tools' broad adoption. Our prior work, called RTL2$\\mu$SPEC, narrows\nthis gap by automatically synthesizing formally verified $\\mu$SPEC models from\nSystemVerilog implementations of simple processors. But, RTL2$\\mu$SPEC assumes\ninput designs where an instruction (e.g., a load) cannot exhibit more than one\nmicroarchitectural execution path ($\\mu$PATH, e.g., a cache hit or miss path)\n-- its single-execution-path assumption.\n  In this paper, we first propose an automated approach and tool, called\nRTL2M$\\mu$PATH, that resolves RTL2$\\mu$SPEC's single-execution-path assumption.\nGiven a SystemVerilog processor design, instruction encodings, and modest\ndesign metadata, RTL2M$\\mu$PATH finds a complete set of formally verified\n$\\mu$PATHs for each instruction. Next, we make an important observation: an\ninstruction that can exhibit more than one $\\mu$PATH strongly indicates the\npresence of a microarchitectural side channel in the input design. Based on\nthis observation, we then propose an automated approach and tool, called\nSynthLC, that extends RTL2M$\\mu$PATH with a symbolic information flow analysis\nto support synthesizing a variety of formally verified leakage contracts from\nSystemVerilog processor designs. Leakage contracts are foundational to\nstate-of-the-art defenses against hardware side-channel attacks. SynthLC is the\nfirst automated methodology for formally verifying hardware adherence to them."
                },
                "authors": [
                    {
                        "name": "Yao Hsiao"
                    },
                    {
                        "name": "Nikos Nikoleris"
                    },
                    {
                        "name": "Artem Khyzha"
                    },
                    {
                        "name": "Dominic P. Mulligan"
                    },
                    {
                        "name": "Gustavo Petri"
                    },
                    {
                        "name": "Christopher W. Fletcher"
                    },
                    {
                        "name": "Caroline Trippel"
                    }
                ],
                "author_detail": {
                    "name": "Caroline Trippel"
                },
                "author": "Caroline Trippel",
                "arxiv_comment": "Authors' version; to appear in the Proceedings of the 57th Annual\n  IEEE/ACM International Symposium on Microarchitecture 57th (MICRO 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19478v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19478v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19375v1",
                "updated": "2024-09-28T15:03:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    28,
                    15,
                    3,
                    28,
                    5,
                    272,
                    0
                ],
                "published": "2024-09-28T15:03:28Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    15,
                    3,
                    28,
                    5,
                    272,
                    0
                ],
                "title": "DOTA: Distributional Test-Time Adaptation of Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DOTA: Distributional Test-Time Adaptation of Vision-Language Models"
                },
                "summary": "Vision-language foundation models (e.g., CLIP) have shown remarkable\nperformance across a wide range of tasks. However, deploying these models may\nbe unreliable when significant distribution gaps exist between the training and\ntest data. The training-free test-time dynamic adapter (TDA) is a promising\napproach to address this issue by storing representative test samples to guide\nthe classification of subsequent ones. However, TDA only naively maintains a\nlimited number of reference samples in the cache, leading to severe test-time\ncatastrophic forgetting when the cache is updated by dropping samples. In this\npaper, we propose a simple yet effective method for DistributiOnal Test-time\nAdaptation (Dota). Instead of naively memorizing representative test samples,\nDota continually estimates the distributions of test samples, allowing the\nmodel to continually adapt to the deployment environment. The test-time\nposterior probabilities are then computed using the estimated distributions\nbased on Bayes' theorem for adaptation purposes. To further enhance the\nadaptability on the uncertain samples, we introduce a new human-in-the-loop\nparadigm which identifies uncertain samples, collects human-feedback, and\nincorporates it into the Dota framework. Extensive experiments validate that\nDota enables CLIP to continually learn, resulting in a significant improvement\ncompared to current state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language foundation models (e.g., CLIP) have shown remarkable\nperformance across a wide range of tasks. However, deploying these models may\nbe unreliable when significant distribution gaps exist between the training and\ntest data. The training-free test-time dynamic adapter (TDA) is a promising\napproach to address this issue by storing representative test samples to guide\nthe classification of subsequent ones. However, TDA only naively maintains a\nlimited number of reference samples in the cache, leading to severe test-time\ncatastrophic forgetting when the cache is updated by dropping samples. In this\npaper, we propose a simple yet effective method for DistributiOnal Test-time\nAdaptation (Dota). Instead of naively memorizing representative test samples,\nDota continually estimates the distributions of test samples, allowing the\nmodel to continually adapt to the deployment environment. The test-time\nposterior probabilities are then computed using the estimated distributions\nbased on Bayes' theorem for adaptation purposes. To further enhance the\nadaptability on the uncertain samples, we introduce a new human-in-the-loop\nparadigm which identifies uncertain samples, collects human-feedback, and\nincorporates it into the Dota framework. Extensive experiments validate that\nDota enables CLIP to continually learn, resulting in a significant improvement\ncompared to current state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Zongbo Han"
                    },
                    {
                        "name": "Jialong Yang"
                    },
                    {
                        "name": "Junfan Li"
                    },
                    {
                        "name": "Qinghua Hu"
                    },
                    {
                        "name": "Qianli Xu"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    },
                    {
                        "name": "Changqing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Changqing Zhang"
                },
                "author": "Changqing Zhang",
                "arxiv_comment": "In submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19315v1",
                "updated": "2024-09-28T11:00:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    28,
                    11,
                    0,
                    11,
                    5,
                    272,
                    0
                ],
                "published": "2024-09-28T11:00:11Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    11,
                    0,
                    11,
                    5,
                    272,
                    0
                ],
                "title": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models"
                },
                "summary": "Transformer neural networks, driven by self-attention mechanisms, are core\ncomponents of foundational and Large Language Models. In generative\ntransformers, self-attention uses cache memory to store token projections,\navoiding recomputation at each time step. However, GPU-stored projections must\nbe loaded into SRAM for each new generation step, causing latency and energy\nbottlenecks for long sequences. In this work, we propose a fast and\nenergy-efficient hardware implementation of self-attention using analog\nin-memory computing based on gain cell memories. Volatile gain cell memories\ncan be efficiently written to store new tokens during sequence generation,\nwhile performing analog signed weight multiplications to compute the\ndot-products required for self-attention. We implement Sliding Window\nAttention, which keeps memory of a finite set of past steps. A charge-to-pulse\nconverter for array readout eliminates the need for analog-to-digital\nconversion between self-attention stages. Using a co-designed initialization\nalgorithm to adapt pre-trained weights to gain cell non-idealities, we achieve\nNLP performance comparable to ChatGPT-2 with minimal training iterations,\ndespite hardware constraints. Our end-to-end hardware design includes digital\ncontrols, estimating area, latency, and energy. The system reduces attention\nlatency by up to two orders of magnitude and energy consumption by up to five\norders compared to GPUs, marking a significant step toward ultra-fast,\nlow-power sequence generation in Large Language Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer neural networks, driven by self-attention mechanisms, are core\ncomponents of foundational and Large Language Models. In generative\ntransformers, self-attention uses cache memory to store token projections,\navoiding recomputation at each time step. However, GPU-stored projections must\nbe loaded into SRAM for each new generation step, causing latency and energy\nbottlenecks for long sequences. In this work, we propose a fast and\nenergy-efficient hardware implementation of self-attention using analog\nin-memory computing based on gain cell memories. Volatile gain cell memories\ncan be efficiently written to store new tokens during sequence generation,\nwhile performing analog signed weight multiplications to compute the\ndot-products required for self-attention. We implement Sliding Window\nAttention, which keeps memory of a finite set of past steps. A charge-to-pulse\nconverter for array readout eliminates the need for analog-to-digital\nconversion between self-attention stages. Using a co-designed initialization\nalgorithm to adapt pre-trained weights to gain cell non-idealities, we achieve\nNLP performance comparable to ChatGPT-2 with minimal training iterations,\ndespite hardware constraints. Our end-to-end hardware design includes digital\ncontrols, estimating area, latency, and energy. The system reduces attention\nlatency by up to two orders of magnitude and energy consumption by up to five\norders compared to GPUs, marking a significant step toward ultra-fast,\nlow-power sequence generation in Large Language Models."
                },
                "authors": [
                    {
                        "name": "Nathan Leroux"
                    },
                    {
                        "name": "Paul-Philipp Manea"
                    },
                    {
                        "name": "Chirag Sudarshan"
                    },
                    {
                        "name": "Jan Finkbeiner"
                    },
                    {
                        "name": "Sebastian Siegel"
                    },
                    {
                        "name": "John Paul Strachan"
                    },
                    {
                        "name": "Emre Neftci"
                    }
                ],
                "author_detail": {
                    "name": "Emre Neftci"
                },
                "author": "Emre Neftci",
                "arxiv_comment": "25 pages, 6 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18523v1",
                "updated": "2024-09-27T08:05:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    5,
                    34,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T08:05:34Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    5,
                    34,
                    4,
                    271,
                    0
                ],
                "title": "Token Caching for Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Caching for Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion transformers have gained substantial interest in diffusion\ngenerative modeling due to their outstanding performance. However, their high\ncomputational cost, arising from the quadratic computational complexity of\nattention mechanisms and multi-step inference, presents a significant\nbottleneck. To address this challenge, we propose TokenCache, a novel\npost-training acceleration method that leverages the token-based multi-block\narchitecture of transformers to reduce redundant computations among tokens\nacross inference steps. TokenCache specifically addresses three critical\nquestions in the context of diffusion transformers: (1) which tokens should be\npruned to eliminate redundancy, (2) which blocks should be targeted for\nefficient pruning, and (3) at which time steps caching should be applied to\nbalance speed and quality. In response to these challenges, TokenCache\nintroduces a Cache Predictor that assigns importance scores to tokens, enabling\nselective pruning without compromising model performance. Furthermore, we\npropose an adaptive block selection strategy to focus on blocks with minimal\nimpact on the network's output, along with a Two-Phase Round-Robin (TPRR)\nscheduling policy to optimize caching intervals throughout the denoising\nprocess. Experimental results across various models demonstrate that TokenCache\nachieves an effective trade-off between generation quality and inference speed\nfor diffusion transformers. Our code will be publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have gained substantial interest in diffusion\ngenerative modeling due to their outstanding performance. However, their high\ncomputational cost, arising from the quadratic computational complexity of\nattention mechanisms and multi-step inference, presents a significant\nbottleneck. To address this challenge, we propose TokenCache, a novel\npost-training acceleration method that leverages the token-based multi-block\narchitecture of transformers to reduce redundant computations among tokens\nacross inference steps. TokenCache specifically addresses three critical\nquestions in the context of diffusion transformers: (1) which tokens should be\npruned to eliminate redundancy, (2) which blocks should be targeted for\nefficient pruning, and (3) at which time steps caching should be applied to\nbalance speed and quality. In response to these challenges, TokenCache\nintroduces a Cache Predictor that assigns importance scores to tokens, enabling\nselective pruning without compromising model performance. Furthermore, we\npropose an adaptive block selection strategy to focus on blocks with minimal\nimpact on the network's output, along with a Two-Phase Round-Robin (TPRR)\nscheduling policy to optimize caching intervals throughout the denoising\nprocess. Experimental results across various models demonstrate that TokenCache\nachieves an effective trade-off between generation quality and inference speed\nfor diffusion transformers. Our code will be publicly available."
                },
                "authors": [
                    {
                        "name": "Jinming Lou"
                    },
                    {
                        "name": "Wenyang Luo"
                    },
                    {
                        "name": "Yufan Liu"
                    },
                    {
                        "name": "Bing Li"
                    },
                    {
                        "name": "Xinmiao Ding"
                    },
                    {
                        "name": "Weiming Hu"
                    },
                    {
                        "name": "Jiajiong Cao"
                    },
                    {
                        "name": "Yuming Li"
                    },
                    {
                        "name": "Chenguang Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chenguang Ma"
                },
                "author": "Chenguang Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17606v1",
                "updated": "2024-09-26T07:44:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    44,
                    47,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T07:44:47Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    44,
                    47,
                    3,
                    270,
                    0
                ],
                "title": "FlooNoC: A 645 Gbps/link 0.15 pJ/B/hop Open-Source NoC with Wide\n  Physical Links and End-to-End AXI4 Parallel Multi-Stream Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlooNoC: A 645 Gbps/link 0.15 pJ/B/hop Open-Source NoC with Wide\n  Physical Links and End-to-End AXI4 Parallel Multi-Stream Support"
                },
                "summary": "The new generation of domain-specific AI accelerators is characterized by\nrapidly increasing demands for bulk data transfers, as opposed to small,\nlatency-critical cache line transfers typical of traditional cache-coherent\nsystems. In this paper, we address this critical need by introducing the\nFlooNoC Network-on-Chip (NoC), featuring very wide, fully Advanced eXtensible\nInterface (AXI4) compliant links designed to meet the massive bandwidth needs\nat high energy efficiency. At the transport level, non-blocking transactions\nare supported for latency tolerance. Additionally, a novel end-to-end ordering\napproach for AXI4, enabled by a multi-stream capable Direct Memory Access (DMA)\nengine simplifies network interfaces and eliminates inter-stream dependencies.\nFurthermore, dedicated physical links are instantiated for short,\nlatency-critical messages. A complete end-to-end reference implementation in\n12nm FinFET technology demonstrates the physical feasibility and power\nperformance area (PPA) benefits of our approach. Utilizing wide links on high\nlevels of metal, we achieve a bandwidth of 645 Gbps per link and a total\naggregate bandwidth of 103 Tbps for an 8x4 mesh of processors cluster tiles,\nwith a total of 288 RISC-V cores. The NoC imposes a minimal area overhead of\nonly 3.5% per compute tile and achieves a leading-edge energy efficiency of\n0.15 pJ/B/hop at 0.8 V. Compared to state-of-the-art NoCs, our system offers\nthree times the energy efficiency and more than double the link bandwidth.\nFurthermore, compared to a traditional AXI4-based multi-layer interconnect, our\nNoC achieves a 30% reduction in area, corresponding to a 47% increase in\nGFLOPSDP within the same floorplan.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The new generation of domain-specific AI accelerators is characterized by\nrapidly increasing demands for bulk data transfers, as opposed to small,\nlatency-critical cache line transfers typical of traditional cache-coherent\nsystems. In this paper, we address this critical need by introducing the\nFlooNoC Network-on-Chip (NoC), featuring very wide, fully Advanced eXtensible\nInterface (AXI4) compliant links designed to meet the massive bandwidth needs\nat high energy efficiency. At the transport level, non-blocking transactions\nare supported for latency tolerance. Additionally, a novel end-to-end ordering\napproach for AXI4, enabled by a multi-stream capable Direct Memory Access (DMA)\nengine simplifies network interfaces and eliminates inter-stream dependencies.\nFurthermore, dedicated physical links are instantiated for short,\nlatency-critical messages. A complete end-to-end reference implementation in\n12nm FinFET technology demonstrates the physical feasibility and power\nperformance area (PPA) benefits of our approach. Utilizing wide links on high\nlevels of metal, we achieve a bandwidth of 645 Gbps per link and a total\naggregate bandwidth of 103 Tbps for an 8x4 mesh of processors cluster tiles,\nwith a total of 288 RISC-V cores. The NoC imposes a minimal area overhead of\nonly 3.5% per compute tile and achieves a leading-edge energy efficiency of\n0.15 pJ/B/hop at 0.8 V. Compared to state-of-the-art NoCs, our system offers\nthree times the energy efficiency and more than double the link bandwidth.\nFurthermore, compared to a traditional AXI4-based multi-layer interconnect, our\nNoC achieves a 30% reduction in area, corresponding to a 47% increase in\nGFLOPSDP within the same floorplan."
                },
                "authors": [
                    {
                        "name": "Tim Fischer"
                    },
                    {
                        "name": "Michael Rogenmoser"
                    },
                    {
                        "name": "Thomas Benz"
                    },
                    {
                        "name": "Frank K. Grkaynak"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17374v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17374v1",
                "updated": "2024-09-25T21:37:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    21,
                    37,
                    1,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T21:37:01Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    21,
                    37,
                    1,
                    2,
                    269,
                    0
                ],
                "title": "NiOx/\\b{eta}-Ga2O3 Heterojunction Diode Achieving Breakdown Voltage >3\n  kV with Plasma Etch Field-Termination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NiOx/\\b{eta}-Ga2O3 Heterojunction Diode Achieving Breakdown Voltage >3\n  kV with Plasma Etch Field-Termination"
                },
                "summary": "This work reports the fabrication and characterization of a\nNiOx/\\b{eta}-Ga2O3 heterojunction diode (HJD) that uses a metallic nickel (Ni)\ntarget to deposit NiOx layers via reactive RF magnetron sputtering and lift-off\nprocessing with >3 kV breakdown voltage, record-low reverse current leakage\nunder high reverse bias, and high junction electric fields (>3.34 MV/cm). The\nheterojunction diodes are fabricated via bilayer NiOx sputtering followed by\nself-aligned mesa-etching for field-termination on both large (1-mm2) and small\narea (100-{\\mu}m diameter) devices. The HJD exhibits a ~135 A/cm2 forward\ncurrent density at 5 V with a rectifying ratio of ~1010. The minimum\ndifferential specific on-resistance is measured to be 17.26 m{\\Omega} cm2. The\nbreakdown voltage on 100-{\\mu}m diameter pads was measured to be greater than 3\nkV with a noise floor-level reverse leakage current density (10-8~10-6 A/cm2)\nuntil 3 kV, accomplishing a parallel-plane junction electric field to be at\nleast 3.34 MV/cm at 3 kV with a power figure of merit (PFOM) >0.52 GW/cm2.\nTemperature-dependent forward current density-voltage (J-V) measurements are\nperformed from room temperature (25 C) to 200 C which showed a temperature\ncoefficient of resistance ({\\alpha}) equaling 1.56, higher than that of\n\\b{eta}-Ga2O3 Schottky barrier diodes (SBDs), indicating potential conductivity\ndegradation within NiOx at elevated temperatures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work reports the fabrication and characterization of a\nNiOx/\\b{eta}-Ga2O3 heterojunction diode (HJD) that uses a metallic nickel (Ni)\ntarget to deposit NiOx layers via reactive RF magnetron sputtering and lift-off\nprocessing with >3 kV breakdown voltage, record-low reverse current leakage\nunder high reverse bias, and high junction electric fields (>3.34 MV/cm). The\nheterojunction diodes are fabricated via bilayer NiOx sputtering followed by\nself-aligned mesa-etching for field-termination on both large (1-mm2) and small\narea (100-{\\mu}m diameter) devices. The HJD exhibits a ~135 A/cm2 forward\ncurrent density at 5 V with a rectifying ratio of ~1010. The minimum\ndifferential specific on-resistance is measured to be 17.26 m{\\Omega} cm2. The\nbreakdown voltage on 100-{\\mu}m diameter pads was measured to be greater than 3\nkV with a noise floor-level reverse leakage current density (10-8~10-6 A/cm2)\nuntil 3 kV, accomplishing a parallel-plane junction electric field to be at\nleast 3.34 MV/cm at 3 kV with a power figure of merit (PFOM) >0.52 GW/cm2.\nTemperature-dependent forward current density-voltage (J-V) measurements are\nperformed from room temperature (25 C) to 200 C which showed a temperature\ncoefficient of resistance ({\\alpha}) equaling 1.56, higher than that of\n\\b{eta}-Ga2O3 Schottky barrier diodes (SBDs), indicating potential conductivity\ndegradation within NiOx at elevated temperatures."
                },
                "authors": [
                    {
                        "name": "Yizheng Liu"
                    },
                    {
                        "name": "Saurav Roy"
                    },
                    {
                        "name": "Carl Peterson"
                    },
                    {
                        "name": "Arkka Bhattacharyya"
                    },
                    {
                        "name": "Sriram Krishnamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Sriram Krishnamoorthy"
                },
                "author": "Sriram Krishnamoorthy",
                "arxiv_comment": "6 pages, 5 figures, APL Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17374v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17374v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17264v1",
                "updated": "2024-09-25T18:21:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T18:21:05Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "title": "Mnemosyne: Parallelization Strategies for Efficiently Serving\n  Multi-Million Context Length LLM Inference Requests Without Approximations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mnemosyne: Parallelization Strategies for Efficiently Serving\n  Multi-Million Context Length LLM Inference Requests Without Approximations"
                },
                "summary": "As large language models (LLMs) evolve to handle increasingly longer\ncontexts, serving inference requests for context lengths in the range of\nmillions of tokens presents unique challenges. While existing techniques are\neffective for training, they fail to address the unique challenges of\ninference, such as varying prefill and decode phases and their associated\nlatency constraints - like Time to First Token (TTFT) and Time Between Tokens\n(TBT). Furthermore, there are no long context inference solutions that allow\nbatching requests to increase the hardware utilization today.\n  In this paper, we propose three key innovations for efficient interactive\nlong context LLM inference, without resorting to any approximation: adaptive\nchunking to reduce prefill overheads in mixed batching, Sequence Pipeline\nParallelism (SPP) to lower TTFT, and KV Cache Parallelism (KVP) to minimize\nTBT. These contributions are combined into a 3D parallelism strategy, enabling\nMnemosyne to scale interactive inference to context lengths at least up to 10\nmillion tokens with high throughput enabled with batching. To our knowledge,\nMnemosyne is the first to be able to achieve support for 10 million long\ncontext inference efficiently, while satisfying production-grade SLOs on TBT\n(30ms) on contexts up to and including 10 million.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) evolve to handle increasingly longer\ncontexts, serving inference requests for context lengths in the range of\nmillions of tokens presents unique challenges. While existing techniques are\neffective for training, they fail to address the unique challenges of\ninference, such as varying prefill and decode phases and their associated\nlatency constraints - like Time to First Token (TTFT) and Time Between Tokens\n(TBT). Furthermore, there are no long context inference solutions that allow\nbatching requests to increase the hardware utilization today.\n  In this paper, we propose three key innovations for efficient interactive\nlong context LLM inference, without resorting to any approximation: adaptive\nchunking to reduce prefill overheads in mixed batching, Sequence Pipeline\nParallelism (SPP) to lower TTFT, and KV Cache Parallelism (KVP) to minimize\nTBT. These contributions are combined into a 3D parallelism strategy, enabling\nMnemosyne to scale interactive inference to context lengths at least up to 10\nmillion tokens with high throughput enabled with batching. To our knowledge,\nMnemosyne is the first to be able to achieve support for 10 million long\ncontext inference efficiently, while satisfying production-grade SLOs on TBT\n(30ms) on contexts up to and including 10 million."
                },
                "authors": [
                    {
                        "name": "Amey Agrawal"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "igo Goiri"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Chaojie Zhang"
                    },
                    {
                        "name": "Alexey Tumanov"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17136v1",
                "updated": "2024-09-25T17:55:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    55,
                    7,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T17:55:07Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    55,
                    7,
                    2,
                    269,
                    0
                ],
                "title": "Adaptive Cost Model for Query Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Cost Model for Query Optimization"
                },
                "summary": "The principal component of conventional database query optimizers is a cost\nmodel that is used to estimate expected performance of query plans. The\naccuracy of the cost model has direct impact on the optimality of execution\nplans selected by the optimizer and thus, on the resulting query latency.\nSeveral common parameters of cost models in modern DBMS are related to the\nperformance of CPU and I/O and are typically set by a database administrator\nupon system tuning. However these performance characteristics are not stable\nand therefore, a single point estimation may not suffice for all DB load\nregimes. In this paper, we propose an Adaptive Cost Model (ACM) which\ndynamically optimizes CPU- and I/O-related plan cost parameters at DB runtime.\nBy continuously monitoring query execution statistics and the state of DB\nbuffer cache ACM adjusts cost parameters without the need for manual\nintervention from a database administrator. This allows for responding to\nchanges in the workload and system performance ensuring more optimal query\nexecution plans. We describe the main ideas in the implementation of ACM and\nreport on a preliminary experimental evaluation showing 20\\% end-to-end latency\nimprovement on TPC-H benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The principal component of conventional database query optimizers is a cost\nmodel that is used to estimate expected performance of query plans. The\naccuracy of the cost model has direct impact on the optimality of execution\nplans selected by the optimizer and thus, on the resulting query latency.\nSeveral common parameters of cost models in modern DBMS are related to the\nperformance of CPU and I/O and are typically set by a database administrator\nupon system tuning. However these performance characteristics are not stable\nand therefore, a single point estimation may not suffice for all DB load\nregimes. In this paper, we propose an Adaptive Cost Model (ACM) which\ndynamically optimizes CPU- and I/O-related plan cost parameters at DB runtime.\nBy continuously monitoring query execution statistics and the state of DB\nbuffer cache ACM adjusts cost parameters without the need for manual\nintervention from a database administrator. This allows for responding to\nchanges in the workload and system performance ensuring more optimal query\nexecution plans. We describe the main ideas in the implementation of ACM and\nreport on a preliminary experimental evaluation showing 20\\% end-to-end latency\nimprovement on TPC-H benchmark."
                },
                "authors": [
                    {
                        "name": "Nikita Vasilenko"
                    },
                    {
                        "name": "Alexander Demin"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    }
                ],
                "author_detail": {
                    "name": "Denis Ponomaryov"
                },
                "author": "Denis Ponomaryov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05, 68P15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16743v1",
                "updated": "2024-09-25T08:52:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    52,
                    7,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T08:52:07Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    52,
                    7,
                    2,
                    269,
                    0
                ],
                "title": "Event-Triggered Non-Linear Control of Offshore MMC Grids for\n  Asymmetrical AC Faults",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Event-Triggered Non-Linear Control of Offshore MMC Grids for\n  Asymmetrical AC Faults"
                },
                "summary": "Fault ride-through capability studies of MMC-HVDC connected wind power plants\nhave focused primarily on the DC link and onshore AC grid faults. Offshore AC\nfaults, mainly asymmetrical faults have not gained much attention in the\nliterature despite being included in the future development at national levels\nin the ENTSO-E HVDC code. The proposed work gives an event-triggered control to\nstabilize the system once the offshore AC fault has occurred, identified, and\nisolated. Different types of control actions such as proportional-integral (PI)\ncontroller and super-twisted sliding mode control (STSMC) are used to smoothly\ntransition the post-fault system to a new steady state operating point by\nsuppressing the negative sequence control. Initially, the effect of a negative\nsequence current control scheme on the transient behavior of the power system\nwith a PI controller is discussed in this paper. Further, a non-linear control\nstrategy (STSMC) is proposed which gives quicker convergence of the system\npost-fault in comparison to PI control action. These post-fault control\noperations are only triggered in the presence of a fault in the system, i.e.,\nthey are event-triggered. The validity of the proposed strategy is demonstrated\nby simulation on a $\\pm$525 kV, three-terminal meshed MMC-HVDC system model in\nReal Time Digital Simulator (RTDS).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fault ride-through capability studies of MMC-HVDC connected wind power plants\nhave focused primarily on the DC link and onshore AC grid faults. Offshore AC\nfaults, mainly asymmetrical faults have not gained much attention in the\nliterature despite being included in the future development at national levels\nin the ENTSO-E HVDC code. The proposed work gives an event-triggered control to\nstabilize the system once the offshore AC fault has occurred, identified, and\nisolated. Different types of control actions such as proportional-integral (PI)\ncontroller and super-twisted sliding mode control (STSMC) are used to smoothly\ntransition the post-fault system to a new steady state operating point by\nsuppressing the negative sequence control. Initially, the effect of a negative\nsequence current control scheme on the transient behavior of the power system\nwith a PI controller is discussed in this paper. Further, a non-linear control\nstrategy (STSMC) is proposed which gives quicker convergence of the system\npost-fault in comparison to PI control action. These post-fault control\noperations are only triggered in the presence of a fault in the system, i.e.,\nthey are event-triggered. The validity of the proposed strategy is demonstrated\nby simulation on a $\\pm$525 kV, three-terminal meshed MMC-HVDC system model in\nReal Time Digital Simulator (RTDS)."
                },
                "authors": [
                    {
                        "name": "Naajein Cherat"
                    },
                    {
                        "name": "Vaibhav Nougain"
                    },
                    {
                        "name": "Milovan Majstorovi"
                    },
                    {
                        "name": "Peter Palensky"
                    },
                    {
                        "name": "Aleksandra Leki"
                    }
                ],
                "author_detail": {
                    "name": "Aleksandra Leki"
                },
                "author": "Aleksandra Leki",
                "arxiv_journal_ref": "ISGT 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16546v1",
                "updated": "2024-09-25T01:39:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    1,
                    39,
                    2,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T01:39:02Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    1,
                    39,
                    2,
                    2,
                    269,
                    0
                ],
                "title": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization"
                },
                "summary": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision."
                },
                "authors": [
                    {
                        "name": "Yifan Tan"
                    },
                    {
                        "name": "Haoze Wang"
                    },
                    {
                        "name": "Chao Yan"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16258v1",
                "updated": "2024-09-24T17:28:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    28,
                    47,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T17:28:47Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    28,
                    47,
                    1,
                    268,
                    0
                ],
                "title": "SWARM: Replicating Shared Disaggregated-Memory Data in No Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWARM: Replicating Shared Disaggregated-Memory Data in No Time"
                },
                "summary": "Memory disaggregation is an emerging data center architecture that improves\nresource utilization and scalability. Replication is key to ensure the fault\ntolerance of applications, but replicating shared data in disaggregated memory\nis hard. We propose SWARM (Swift WAit-free Replication in disaggregated\nMemory), the first replication scheme for in-disaggregated-memory shared\nobjects to provide (1) single-roundtrip reads and writes in the common case,\n(2) strong consistency (linearizability), and (3) strong liveness\n(wait-freedom). SWARM makes two independent contributions. The first is\nSafe-Guess, a novel wait-free replication protocol with single-roundtrip\noperations. The second is In-n-Out, a novel technique to provide conditional\natomic update and atomic retrieval of large buffers in disaggregated memory in\none roundtrip. Using SWARM, we build SWARM-KV, a low-latency, strongly\nconsistent and highly available disaggregated key-value store. We evaluate\nSWARM-KV and find that it has marginal latency overhead compared to an\nunreplicated key-value store, and that it offers much lower latency and better\navailability than FUSEE, a state-of-the-art replicated disaggregated key-value\nstore.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory disaggregation is an emerging data center architecture that improves\nresource utilization and scalability. Replication is key to ensure the fault\ntolerance of applications, but replicating shared data in disaggregated memory\nis hard. We propose SWARM (Swift WAit-free Replication in disaggregated\nMemory), the first replication scheme for in-disaggregated-memory shared\nobjects to provide (1) single-roundtrip reads and writes in the common case,\n(2) strong consistency (linearizability), and (3) strong liveness\n(wait-freedom). SWARM makes two independent contributions. The first is\nSafe-Guess, a novel wait-free replication protocol with single-roundtrip\noperations. The second is In-n-Out, a novel technique to provide conditional\natomic update and atomic retrieval of large buffers in disaggregated memory in\none roundtrip. Using SWARM, we build SWARM-KV, a low-latency, strongly\nconsistent and highly available disaggregated key-value store. We evaluate\nSWARM-KV and find that it has marginal latency overhead compared to an\nunreplicated key-value store, and that it offers much lower latency and better\navailability than FUSEE, a state-of-the-art replicated disaggregated key-value\nstore."
                },
                "authors": [
                    {
                        "name": "Antoine Murat"
                    },
                    {
                        "name": "Clment Burgelin"
                    },
                    {
                        "name": "Athanasios Xygkis"
                    },
                    {
                        "name": "Igor Zablotchi"
                    },
                    {
                        "name": "Marcos K. Aguilera"
                    },
                    {
                        "name": "Rachid Guerraoui"
                    }
                ],
                "author_detail": {
                    "name": "Rachid Guerraoui"
                },
                "author": "Rachid Guerraoui",
                "arxiv_doi": "10.1145/3694715.3695945",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3694715.3695945",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.16258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "To appear in the proceedings of SOSP '24",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16110v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16110v1",
                "updated": "2024-09-24T14:16:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    16,
                    26,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T14:16:26Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    16,
                    26,
                    1,
                    268,
                    0
                ],
                "title": "Wind lulls and slews; consequences for the stability of future UK\n  electricity systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wind lulls and slews; consequences for the stability of future UK\n  electricity systems"
                },
                "summary": "As the United Kingdom wind fleet increases in size, wind lulls and slews will\nincreasingly challenge the stability of its electricity system. The paper\ndescribes the use of models based on real time records and including solar\nslews, to investigate the most extreme wind variations likely to be encountered\nin future, enabling strategies to be devised to mitigate them. Wind lulls are\nsurprisingly frequent, occasionally lasting a week or more, and are always\nlikely to be beyond the capabilities of stored or imported electrical energy to\nmitigate them. The models indicate that there will be a continuing need for gas\npowered generation to mitigate wind lulls. Currently, Combined Cycle Gas\nTurbines (CCGTs) provide most of the dispatchable generation. However, CCGTs\nare not sufficiently fast acting to cope with the wind and solar slews\nanticipated in future. The paper suggests that a range of already proven\nfast-acting sources of dispatchable generation, including Open Cycle Gas\nTurbines (OCGTs), Internal Combustion Gas-Fired Reciprocating engines (ICGRs)\nand stored electrical energy systems, should be capable of coping with the\nlargest wind and solar slews likely to be encountered up to the year 2035.\nExamples are given of the recent introduction of these fast-acting sources of\ngeneration which, it is suggested, will progressively replace CCGTs as the wind\nand solar fleets increase in size. Moreover, we see the pattern of recent\ninvestments, summarised in the paper, as a good indication of likely future\ninvestments, with OCGT investments mainly serving the 440 kV grid, and ICGRs\nand stored electrical energy more local networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the United Kingdom wind fleet increases in size, wind lulls and slews will\nincreasingly challenge the stability of its electricity system. The paper\ndescribes the use of models based on real time records and including solar\nslews, to investigate the most extreme wind variations likely to be encountered\nin future, enabling strategies to be devised to mitigate them. Wind lulls are\nsurprisingly frequent, occasionally lasting a week or more, and are always\nlikely to be beyond the capabilities of stored or imported electrical energy to\nmitigate them. The models indicate that there will be a continuing need for gas\npowered generation to mitigate wind lulls. Currently, Combined Cycle Gas\nTurbines (CCGTs) provide most of the dispatchable generation. However, CCGTs\nare not sufficiently fast acting to cope with the wind and solar slews\nanticipated in future. The paper suggests that a range of already proven\nfast-acting sources of dispatchable generation, including Open Cycle Gas\nTurbines (OCGTs), Internal Combustion Gas-Fired Reciprocating engines (ICGRs)\nand stored electrical energy systems, should be capable of coping with the\nlargest wind and solar slews likely to be encountered up to the year 2035.\nExamples are given of the recent introduction of these fast-acting sources of\ngeneration which, it is suggested, will progressively replace CCGTs as the wind\nand solar fleets increase in size. Moreover, we see the pattern of recent\ninvestments, summarised in the paper, as a good indication of likely future\ninvestments, with OCGT investments mainly serving the 440 kV grid, and ICGRs\nand stored electrical energy more local networks."
                },
                "authors": [
                    {
                        "name": "Anthony D Stephens"
                    },
                    {
                        "name": "David R Walwyn"
                    }
                ],
                "author_detail": {
                    "name": "David R Walwyn"
                },
                "author": "David R Walwyn",
                "arxiv_comment": "13 pages, 8 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16110v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16110v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15440v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15440v3",
                "updated": "2024-09-24T11:37:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    11,
                    37,
                    43,
                    1,
                    268,
                    0
                ],
                "published": "2024-07-22T07:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    42,
                    57,
                    0,
                    204,
                    0
                ],
                "title": "The Bicameral Cache: a split cache for vector architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache: a split cache for vector architectures"
                },
                "summary": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value."
                },
                "authors": [
                    {
                        "name": "Susana Rebolledo"
                    },
                    {
                        "name": "Borja Perez"
                    },
                    {
                        "name": "Jose Luis Bosque"
                    },
                    {
                        "name": "Peter Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Peter Hsu"
                },
                "author": "Peter Hsu",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15440v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15440v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15523v1",
                "updated": "2024-09-23T20:16:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    20,
                    16,
                    49,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T20:16:49Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    20,
                    16,
                    49,
                    0,
                    267,
                    0
                ],
                "title": "SEAL: Suite for Evaluating API-use of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEAL: Suite for Evaluating API-use of LLMs"
                },
                "summary": "Large language models (LLMs) have limitations in handling tasks that require\nreal-time access to external APIs. While several benchmarks like ToolBench and\nAPIGen have been developed to assess LLMs' API-use capabilities, they often\nsuffer from issues such as lack of generalizability, limited multi-step\nreasoning coverage, and instability due to real-time API fluctuations. In this\npaper, we introduce SEAL, an end-to-end testbed designed to evaluate LLMs in\nreal-world API usage. SEAL standardizes existing benchmarks, integrates an\nagent system for testing API retrieval and planning, and addresses the\ninstability of real-time APIs by introducing a GPT-4-powered API simulator with\ncaching for deterministic evaluations. Our testbed provides a comprehensive\nevaluation pipeline that covers API retrieval, API calls, and final responses,\noffering a reliable framework for structured performance comparison in diverse\nreal-world scenarios. SEAL is publicly available, with ongoing updates for new\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have limitations in handling tasks that require\nreal-time access to external APIs. While several benchmarks like ToolBench and\nAPIGen have been developed to assess LLMs' API-use capabilities, they often\nsuffer from issues such as lack of generalizability, limited multi-step\nreasoning coverage, and instability due to real-time API fluctuations. In this\npaper, we introduce SEAL, an end-to-end testbed designed to evaluate LLMs in\nreal-world API usage. SEAL standardizes existing benchmarks, integrates an\nagent system for testing API retrieval and planning, and addresses the\ninstability of real-time APIs by introducing a GPT-4-powered API simulator with\ncaching for deterministic evaluations. Our testbed provides a comprehensive\nevaluation pipeline that covers API retrieval, API calls, and final responses,\noffering a reliable framework for structured performance comparison in diverse\nreal-world scenarios. SEAL is publicly available, with ongoing updates for new\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Woojeong Kim"
                    },
                    {
                        "name": "Ashish Jagmohan"
                    },
                    {
                        "name": "Aditya Vempaty"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Vempaty"
                },
                "author": "Aditya Vempaty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.18322v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.18322v2",
                "updated": "2024-09-23T20:09:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    20,
                    9,
                    28,
                    0,
                    267,
                    0
                ],
                "published": "2024-04-28T21:23:40Z",
                "published_parsed": [
                    2024,
                    4,
                    28,
                    21,
                    23,
                    40,
                    6,
                    119,
                    0
                ],
                "title": "BlockLLM: Multi-tenant Finer-grained Serving for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlockLLM: Multi-tenant Finer-grained Serving for Large Language Models"
                },
                "summary": "The increasing demand for Large Language Models (LLMs) across various\napplications has led to a significant shift in the design of deep learning\nserving systems. Deploying LLMs, particularly in multi-tenant environments,\nposes substantial challenges due to their high computational and memory\ndemands. We introduce BlockLLM, a serving system that leverages component\nsharing among fine-tuned LLM models to provide an efficient and flexible\nsolution for LLM workloads. BlockLLM partitions models into finer-grained\nblocks, enabling the reuse of model components and independent provisioning to\nimprove computation efficiency. BlockLLM comprises an offline block zoo for\nstoring blocks and an online system to serve requests through chains of blocks.\nIt offers multi-fold flexibilities: (1) Adaptive assembly of blocks on-the-fly\nthrough equivalence evaluation among blocks in the zoo; (2) Per-block batch\nsize configuration and best-effort KV cache coordination at the individual\nblock level; (3) Speculative execution and locality-aware block placement to\nreduce communication costs from dynamic block resource allocation. Our\nevaluation shows that BlockLLM reduces memory and storage footprints and\nimproves computational efficiency, outperforming existing serving approach in\n95%ile latency and GPU utilization by 33.5% and 20.1%, respectively, with\nminimal impact on accuracy",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for Large Language Models (LLMs) across various\napplications has led to a significant shift in the design of deep learning\nserving systems. Deploying LLMs, particularly in multi-tenant environments,\nposes substantial challenges due to their high computational and memory\ndemands. We introduce BlockLLM, a serving system that leverages component\nsharing among fine-tuned LLM models to provide an efficient and flexible\nsolution for LLM workloads. BlockLLM partitions models into finer-grained\nblocks, enabling the reuse of model components and independent provisioning to\nimprove computation efficiency. BlockLLM comprises an offline block zoo for\nstoring blocks and an online system to serve requests through chains of blocks.\nIt offers multi-fold flexibilities: (1) Adaptive assembly of blocks on-the-fly\nthrough equivalence evaluation among blocks in the zoo; (2) Per-block batch\nsize configuration and best-effort KV cache coordination at the individual\nblock level; (3) Speculative execution and locality-aware block placement to\nreduce communication costs from dynamic block resource allocation. Our\nevaluation shows that BlockLLM reduces memory and storage footprints and\nimproves computational efficiency, outperforming existing serving approach in\n95%ile latency and GPU utilization by 33.5% and 20.1%, respectively, with\nminimal impact on accuracy"
                },
                "authors": [
                    {
                        "name": "Bodun Hu"
                    },
                    {
                        "name": "Jiamin Li"
                    },
                    {
                        "name": "Le Xu"
                    },
                    {
                        "name": "Myungjin Lee"
                    },
                    {
                        "name": "Akshay Jajoo"
                    },
                    {
                        "name": "Geon-Woo Kim"
                    },
                    {
                        "name": "Hong Xu"
                    },
                    {
                        "name": "Aditya Akella"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Akella"
                },
                "author": "Aditya Akella",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.18322v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.18322v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13122v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13122v2",
                "updated": "2024-09-23T19:53:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    19,
                    53,
                    37,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-19T23:38:59Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    23,
                    38,
                    59,
                    3,
                    263,
                    0
                ],
                "title": "RepoGenReflex: Enhancing Repository-Level Code Completion with Verbal\n  Reinforcement and Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RepoGenReflex: Enhancing Repository-Level Code Completion with Verbal\n  Reinforcement and Retrieval-Augmented Generation"
                },
                "summary": "In real-world software engineering tasks, solving a problem often requires\nunderstanding and modifying multiple functions, classes, and files across a\nlarge codebase. Therefore, on the repository level, it is crucial to extract\nthe relevant information to achieve accurate code completion effectively.\nExisting code completion tools have achieved some success, but they struggle to\noptimize the retrieval and generation process dynamically. In this paper, we\npropose RepoGenReflex, a generic, dynamic, effective framework to address this\nchallenge. By leveraging the Retrieval-Augmented Generation (RAG) enhanced with\nVerbal Reinforcement Learning (VRL), it can dynamically choose the optimal\nresults for repository-level code completion. RepoGenReflex uses Reflector to\ngive directional feedback to the next loop. RepoGenReflex chooses the optimal\nresults stored in the Experience cache based on the RAG-VRL loop. To validate\nthe framework's generalization ability, we propose a new benchmark RepoGenEval,\nwhich consists of the latest, high-quality real-world repositories in line\ncompletion scenarios. Our experiments demonstrate that RepoGenReflex achieves\nsignificant improvements after optimizing the Reflector component, resulting in\nenhanced accuracy and relevance of code completions. Additionally,\nRepoGenReflex consistently demonstrates superior performance and effectiveness\nacross standard code completion tasks, highlighting the robustness and\nadaptability of our framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In real-world software engineering tasks, solving a problem often requires\nunderstanding and modifying multiple functions, classes, and files across a\nlarge codebase. Therefore, on the repository level, it is crucial to extract\nthe relevant information to achieve accurate code completion effectively.\nExisting code completion tools have achieved some success, but they struggle to\noptimize the retrieval and generation process dynamically. In this paper, we\npropose RepoGenReflex, a generic, dynamic, effective framework to address this\nchallenge. By leveraging the Retrieval-Augmented Generation (RAG) enhanced with\nVerbal Reinforcement Learning (VRL), it can dynamically choose the optimal\nresults for repository-level code completion. RepoGenReflex uses Reflector to\ngive directional feedback to the next loop. RepoGenReflex chooses the optimal\nresults stored in the Experience cache based on the RAG-VRL loop. To validate\nthe framework's generalization ability, we propose a new benchmark RepoGenEval,\nwhich consists of the latest, high-quality real-world repositories in line\ncompletion scenarios. Our experiments demonstrate that RepoGenReflex achieves\nsignificant improvements after optimizing the Reflector component, resulting in\nenhanced accuracy and relevance of code completions. Additionally,\nRepoGenReflex consistently demonstrates superior performance and effectiveness\nacross standard code completion tasks, highlighting the robustness and\nadaptability of our framework."
                },
                "authors": [
                    {
                        "name": "Jicheng Wang"
                    },
                    {
                        "name": "Yifeng He"
                    },
                    {
                        "name": "Hao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hao Chen"
                },
                "author": "Hao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13122v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13122v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15441v1",
                "updated": "2024-09-23T18:06:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    18,
                    6,
                    32,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T18:06:32Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    18,
                    6,
                    32,
                    0,
                    267,
                    0
                ],
                "title": "Steward: Natural Language Web Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steward: Natural Language Web Automation"
                },
                "summary": "Recently, large language models (LLMs) have demonstrated exceptional\ncapabilities in serving as the foundation for AI assistants. One emerging\napplication of LLMs, navigating through websites and interacting with UI\nelements across various web pages, remains somewhat underexplored. We introduce\nSteward, a novel LLM-powered web automation tool designed to serve as a\ncost-effective, scalable, end-to-end solution for automating web interactions.\nTraditional browser automation frameworks like Selenium, Puppeteer, and\nPlaywright are not scalable for extensive web interaction tasks, such as\nstudying recommendation algorithms on platforms like YouTube and Twitter. These\nframeworks require manual coding of interactions, limiting their utility in\nlarge-scale or dynamic contexts. Steward addresses these limitations by\nintegrating LLM capabilities with browser automation, allowing for natural\nlanguage-driven interaction with websites. Steward operates by receiving\nnatural language instructions and reactively planning and executing a sequence\nof actions on websites, looping until completion, making it a practical tool\nfor developers and researchers to use. It achieves high efficiency, completing\nactions in 8.52 to 10.14 seconds at a cost of $0.028 per action or an average\nof $0.18 per task, which is further reduced to 4.8 seconds and $0.022 through a\ncaching mechanism. It runs tasks on real websites with a 40% completion success\nrate. We discuss various design and implementation challenges, including state\nrepresentation, action sequence selection, system responsiveness, detecting\ntask completion, and caching implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have demonstrated exceptional\ncapabilities in serving as the foundation for AI assistants. One emerging\napplication of LLMs, navigating through websites and interacting with UI\nelements across various web pages, remains somewhat underexplored. We introduce\nSteward, a novel LLM-powered web automation tool designed to serve as a\ncost-effective, scalable, end-to-end solution for automating web interactions.\nTraditional browser automation frameworks like Selenium, Puppeteer, and\nPlaywright are not scalable for extensive web interaction tasks, such as\nstudying recommendation algorithms on platforms like YouTube and Twitter. These\nframeworks require manual coding of interactions, limiting their utility in\nlarge-scale or dynamic contexts. Steward addresses these limitations by\nintegrating LLM capabilities with browser automation, allowing for natural\nlanguage-driven interaction with websites. Steward operates by receiving\nnatural language instructions and reactively planning and executing a sequence\nof actions on websites, looping until completion, making it a practical tool\nfor developers and researchers to use. It achieves high efficiency, completing\nactions in 8.52 to 10.14 seconds at a cost of $0.028 per action or an average\nof $0.18 per task, which is further reduced to 4.8 seconds and $0.022 through a\ncaching mechanism. It runs tasks on real websites with a 40% completion success\nrate. We discuss various design and implementation challenges, including state\nrepresentation, action sequence selection, system responsiveness, detecting\ntask completion, and caching implementation."
                },
                "authors": [
                    {
                        "name": "Brian Tang"
                    },
                    {
                        "name": "Kang G. Shin"
                    }
                ],
                "author_detail": {
                    "name": "Kang G. Shin"
                },
                "author": "Kang G. Shin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15104v1",
                "updated": "2024-09-23T15:16:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    15,
                    16,
                    29,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T15:16:29Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    15,
                    16,
                    29,
                    0,
                    267,
                    0
                ],
                "title": "CSPS: A Communication-Efficient Sequence-Parallelism based Serving\n  System for Transformer based Models with Long Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSPS: A Communication-Efficient Sequence-Parallelism based Serving\n  System for Transformer based Models with Long Prompts"
                },
                "summary": "Long-sequence generative large-language model (LLM) applications have become\nincreasingly popular. In this paper, through trace-based experiments, we found\nthat the existing method for long sequences results in a high\nTime-To-First-Token (TTFT) due to sequential chunk processing, long\nTime-Between-Tokens (TBT) from batching long-sequence prefills and decodes, and\nlow throughput due to constrained key-value cache (KVC) for long sequences. To\naddress these issues, we propose two Sequence-Parallelism (SP) architectures\nfor both tensor parallelism (TP) and non-TP. However, SP introduces two\nchallenges: 1) network communication and computation become performance\nbottlenecks; 2) the latter two issues above are mitigated but not resolved, and\nSP's resultant KV value distribution across GPUs still requires communication\nfor decode, increasing TBT. Hence, we propose a Communication-efficient Sparse\nAttention (CSA) and communication-computation-communication three-phase\npipelining. We also propose SP-based decode that processes decode separately\nfrom prefill, distributes KV values of a request across different GPUs, and\nnovelly moves Query (Q) values instead of KV values to reduce communication\noverhead. These methods constitute a communication-efficient\nSequence-Parallelism based LLM Serving System (SPS2). Our trace-driven\nevaluation demonstrates that SPS2 improves the average TTFT, TBT, and response\ntime by up to 7.5x, 1.92x, and 9.8x and improves the prefill and decode\nthroughput by 8.2x and 5.2x while maintaining the accuracy compared to\nSarathi-Serve. We distributed our source code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-sequence generative large-language model (LLM) applications have become\nincreasingly popular. In this paper, through trace-based experiments, we found\nthat the existing method for long sequences results in a high\nTime-To-First-Token (TTFT) due to sequential chunk processing, long\nTime-Between-Tokens (TBT) from batching long-sequence prefills and decodes, and\nlow throughput due to constrained key-value cache (KVC) for long sequences. To\naddress these issues, we propose two Sequence-Parallelism (SP) architectures\nfor both tensor parallelism (TP) and non-TP. However, SP introduces two\nchallenges: 1) network communication and computation become performance\nbottlenecks; 2) the latter two issues above are mitigated but not resolved, and\nSP's resultant KV value distribution across GPUs still requires communication\nfor decode, increasing TBT. Hence, we propose a Communication-efficient Sparse\nAttention (CSA) and communication-computation-communication three-phase\npipelining. We also propose SP-based decode that processes decode separately\nfrom prefill, distributes KV values of a request across different GPUs, and\nnovelly moves Query (Q) values instead of KV values to reduce communication\noverhead. These methods constitute a communication-efficient\nSequence-Parallelism based LLM Serving System (SPS2). Our trace-driven\nevaluation demonstrates that SPS2 improves the average TTFT, TBT, and response\ntime by up to 7.5x, 1.92x, and 9.8x and improves the prefill and decode\nthroughput by 8.2x and 5.2x while maintaining the accuracy compared to\nSarathi-Serve. We distributed our source code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15012v1",
                "updated": "2024-09-23T13:37:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    13,
                    37,
                    25,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T13:37:25Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    13,
                    37,
                    25,
                    0,
                    267,
                    0
                ],
                "title": "Inference-Friendly Models With MixAttention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Friendly Models With MixAttention"
                },
                "summary": "The size of the key-value (KV) cache plays a critical role in determining\nboth the maximum context length and the number of concurrent requests supported\nduring inference in modern language models. The KV cache size grows\nproportionally with the number of attention heads and the tokens processed,\nleading to increased memory consumption and slower inference for long inputs.\nIn this work, we explore the use of MixAttention, a model architecture\nmodification closely related to a blog published by Character.AI. MixAttention\ncombines sliding window attention, where only a small subset of recent tokens\nis stored in the KV cache, with KV cache sharing across layers. Our experiments\ndemonstrate that MixAttention significantly reduces memory usage and improves\ninference speed without sacrificing model performance in both short and\nlong-context tasks. We also explore various configurations of this\narchitecture, identifying those that maintain quality across evaluation metrics\nwhile optimizing resource efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The size of the key-value (KV) cache plays a critical role in determining\nboth the maximum context length and the number of concurrent requests supported\nduring inference in modern language models. The KV cache size grows\nproportionally with the number of attention heads and the tokens processed,\nleading to increased memory consumption and slower inference for long inputs.\nIn this work, we explore the use of MixAttention, a model architecture\nmodification closely related to a blog published by Character.AI. MixAttention\ncombines sliding window attention, where only a small subset of recent tokens\nis stored in the KV cache, with KV cache sharing across layers. Our experiments\ndemonstrate that MixAttention significantly reduces memory usage and improves\ninference speed without sacrificing model performance in both short and\nlong-context tasks. We also explore various configurations of this\narchitecture, identifying those that maintain quality across evaluation metrics\nwhile optimizing resource efficiency."
                },
                "authors": [
                    {
                        "name": "Shashank Rajput"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Sean Owen"
                    },
                    {
                        "name": "Vitaliy Chiley"
                    }
                ],
                "author_detail": {
                    "name": "Vitaliy Chiley"
                },
                "author": "Vitaliy Chiley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14968v1",
                "updated": "2024-09-23T12:37:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    12,
                    37,
                    56,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T12:37:56Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    12,
                    37,
                    56,
                    0,
                    267,
                    0
                ],
                "title": "Mutation-Based Deep Learning Framework Testing Method in JavaScript\n  Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mutation-Based Deep Learning Framework Testing Method in JavaScript\n  Environment"
                },
                "summary": "In recent years, Deep Learning (DL) applications in JavaScript environment\nhave become increasingly popular. As the infrastructure for DL applications,\nJavaScript DL frameworks play a crucial role in the development and deployment.\nIt is essential to ensure the quality of JavaScript DL frameworks. However, the\nbottleneck of limited computational resources in the JavaScript environment\nbrings new challenges to framework testing. Specifically, JavaScript DL\nframeworks are equipped with various optimization mechanisms (e.g., cache\nreuse, inference acceleration) to overcome the bottleneck of limited\ncomputational resources. These optimization mechanisms are overlooked by\nexisting methods, resulting in many bugs in JavaScript DL frameworks being\nmissed. To address the above challenges, we propose a mutation-based JavaScript\nDL framework testing method named DLJSFuzzer. DLJSFuzzer designs 13 tensor\nmutation rules targeting the cache reuse mechanism to generate test input\ntensors. Besides, DLJSFuzzer designs eight model mutation rules targeting the\ninference acceleration mechanism to generate test input models. To evaluate the\neffectiveness of DLJSFuzzer, we conduct experiments on the most widely-used\nJavaScript DL framework, TensorFlow.js. The experimental results show that\nDLJSFuzzer outperforms state-of-the-art methods in both effectiveness and\nefficiency. DLJSFuzzer successfully detects 21 unique crashes and 126 unique\nNaN & Inconsistency bugs. All detected crashes have been reported to the\nopen-source community, with 12 of them already confirmed by developers.\nAdditionally, DLJSFuzzer has improved by over 47% in model generation\nefficiency and over 91% in bug detection efficiency compared to all baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Deep Learning (DL) applications in JavaScript environment\nhave become increasingly popular. As the infrastructure for DL applications,\nJavaScript DL frameworks play a crucial role in the development and deployment.\nIt is essential to ensure the quality of JavaScript DL frameworks. However, the\nbottleneck of limited computational resources in the JavaScript environment\nbrings new challenges to framework testing. Specifically, JavaScript DL\nframeworks are equipped with various optimization mechanisms (e.g., cache\nreuse, inference acceleration) to overcome the bottleneck of limited\ncomputational resources. These optimization mechanisms are overlooked by\nexisting methods, resulting in many bugs in JavaScript DL frameworks being\nmissed. To address the above challenges, we propose a mutation-based JavaScript\nDL framework testing method named DLJSFuzzer. DLJSFuzzer designs 13 tensor\nmutation rules targeting the cache reuse mechanism to generate test input\ntensors. Besides, DLJSFuzzer designs eight model mutation rules targeting the\ninference acceleration mechanism to generate test input models. To evaluate the\neffectiveness of DLJSFuzzer, we conduct experiments on the most widely-used\nJavaScript DL framework, TensorFlow.js. The experimental results show that\nDLJSFuzzer outperforms state-of-the-art methods in both effectiveness and\nefficiency. DLJSFuzzer successfully detects 21 unique crashes and 126 unique\nNaN & Inconsistency bugs. All detected crashes have been reported to the\nopen-source community, with 12 of them already confirmed by developers.\nAdditionally, DLJSFuzzer has improved by over 47% in model generation\nefficiency and over 91% in bug detection efficiency compared to all baselines."
                },
                "authors": [
                    {
                        "name": "Yinglong Zou"
                    },
                    {
                        "name": "Juan Zhai"
                    },
                    {
                        "name": "Chunrong Fang"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Tao Zheng"
                    },
                    {
                        "name": "Zhenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Chen"
                },
                "author": "Zhenyu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.10817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10817v1",
                "updated": "2024-10-14T17:59:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    58,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:59:58Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    58,
                    0,
                    288,
                    0
                ],
                "title": "When Does Perceptual Alignment Benefit Vision Representations?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Does Perceptual Alignment Benefit Vision Representations?"
                },
                "summary": "Humans judge perceptual similarity according to diverse visual attributes,\nincluding scene layout, subject location, and camera pose. Existing vision\nmodels understand a wide range of semantic abstractions but improperly weigh\nthese attributes and thus make inferences misaligned with human perception.\nWhile vision representations have previously benefited from alignment in\ncontexts like image generation, the utility of perceptually aligned\nrepresentations in more general-purpose settings remains unclear. Here, we\ninvestigate how aligning vision model representations to human perceptual\njudgments impacts their usability across diverse computer vision tasks. We\nfinetune state-of-the-art models on human similarity judgments for image\ntriplets and evaluate them across standard vision benchmarks. We find that\naligning models to perceptual judgments yields representations that improve\nupon the original backbones across many downstream tasks, including counting,\nsegmentation, depth estimation, instance retrieval, and retrieval-augmented\ngeneration. In addition, we find that performance is widely preserved on other\ntasks, including specialized out-of-distribution domains such as in medical\nimaging and 3D environment frames. Our results suggest that injecting an\ninductive bias about human perceptual knowledge into vision models can\ncontribute to better representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans judge perceptual similarity according to diverse visual attributes,\nincluding scene layout, subject location, and camera pose. Existing vision\nmodels understand a wide range of semantic abstractions but improperly weigh\nthese attributes and thus make inferences misaligned with human perception.\nWhile vision representations have previously benefited from alignment in\ncontexts like image generation, the utility of perceptually aligned\nrepresentations in more general-purpose settings remains unclear. Here, we\ninvestigate how aligning vision model representations to human perceptual\njudgments impacts their usability across diverse computer vision tasks. We\nfinetune state-of-the-art models on human similarity judgments for image\ntriplets and evaluate them across standard vision benchmarks. We find that\naligning models to perceptual judgments yields representations that improve\nupon the original backbones across many downstream tasks, including counting,\nsegmentation, depth estimation, instance retrieval, and retrieval-augmented\ngeneration. In addition, we find that performance is widely preserved on other\ntasks, including specialized out-of-distribution domains such as in medical\nimaging and 3D environment frames. Our results suggest that injecting an\ninductive bias about human perceptual knowledge into vision models can\ncontribute to better representations."
                },
                "authors": [
                    {
                        "name": "Shobhita Sundaram"
                    },
                    {
                        "name": "Stephanie Fu"
                    },
                    {
                        "name": "Lukas Muttenthaler"
                    },
                    {
                        "name": "Netanel Y. Tamir"
                    },
                    {
                        "name": "Lucy Chai"
                    },
                    {
                        "name": "Simon Kornblith"
                    },
                    {
                        "name": "Trevor Darrell"
                    },
                    {
                        "name": "Phillip Isola"
                    }
                ],
                "author_detail": {
                    "name": "Phillip Isola"
                },
                "author": "Phillip Isola",
                "arxiv_comment": "S.S. and S.F. contributed equally. Website: percep-align.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10818v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10818v1",
                "updated": "2024-10-14T17:59:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    58,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:59:58Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    58,
                    0,
                    288,
                    0
                ],
                "title": "TemporalBench: Benchmarking Fine-grained Temporal Understanding for\n  Multimodal Video Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TemporalBench: Benchmarking Fine-grained Temporal Understanding for\n  Multimodal Video Models"
                },
                "summary": "Understanding fine-grained temporal dynamics is crucial for multimodal video\ncomprehension and generation. Due to the lack of fine-grained temporal\nannotations, existing video benchmarks mostly resemble static image benchmarks\nand are incompetent at evaluating models for temporal understanding. In this\npaper, we introduce TemporalBench, a new benchmark dedicated to evaluating\nfine-grained temporal understanding in videos. TemporalBench consists of ~10K\nvideo question-answer pairs, derived from ~2K high-quality human annotations\ndetailing the temporal dynamics in video clips. As a result, our benchmark\nprovides a unique testbed for evaluating various temporal understanding and\nreasoning abilities such as action frequency, motion magnitude, event order,\netc. Moreover, it enables evaluations on various tasks like both video question\nanswering and captioning, both short and long video understanding, as well as\ndifferent models such as multimodal video embedding models and text generation\nmodels. Results show that state-of-the-art models like GPT-4o achieve only\n38.5% question answering accuracy on TemporalBench, demonstrating a significant\ngap (~30%) between humans and AI in temporal understanding. Furthermore, we\nnotice a critical pitfall for multi-choice QA where LLMs can detect the subtle\nchanges in negative captions and find a centralized description as a cue for\nits prediction, where we propose Multiple Binary Accuracy (MBA) to correct such\nbias. We hope that TemporalBench can foster research on improving models'\ntemporal reasoning capabilities. Both dataset and evaluation code will be made\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding fine-grained temporal dynamics is crucial for multimodal video\ncomprehension and generation. Due to the lack of fine-grained temporal\nannotations, existing video benchmarks mostly resemble static image benchmarks\nand are incompetent at evaluating models for temporal understanding. In this\npaper, we introduce TemporalBench, a new benchmark dedicated to evaluating\nfine-grained temporal understanding in videos. TemporalBench consists of ~10K\nvideo question-answer pairs, derived from ~2K high-quality human annotations\ndetailing the temporal dynamics in video clips. As a result, our benchmark\nprovides a unique testbed for evaluating various temporal understanding and\nreasoning abilities such as action frequency, motion magnitude, event order,\netc. Moreover, it enables evaluations on various tasks like both video question\nanswering and captioning, both short and long video understanding, as well as\ndifferent models such as multimodal video embedding models and text generation\nmodels. Results show that state-of-the-art models like GPT-4o achieve only\n38.5% question answering accuracy on TemporalBench, demonstrating a significant\ngap (~30%) between humans and AI in temporal understanding. Furthermore, we\nnotice a critical pitfall for multi-choice QA where LLMs can detect the subtle\nchanges in negative captions and find a centralized description as a cue for\nits prediction, where we propose Multiple Binary Accuracy (MBA) to correct such\nbias. We hope that TemporalBench can foster research on improving models'\ntemporal reasoning capabilities. Both dataset and evaluation code will be made\navailable."
                },
                "authors": [
                    {
                        "name": "Mu Cai"
                    },
                    {
                        "name": "Reuben Tan"
                    },
                    {
                        "name": "Jianrui Zhang"
                    },
                    {
                        "name": "Bocheng Zou"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Feng Yao"
                    },
                    {
                        "name": "Fangrui Zhu"
                    },
                    {
                        "name": "Jing Gu"
                    },
                    {
                        "name": "Yiwu Zhong"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yao Dou"
                    },
                    {
                        "name": "Jaden Park"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Yong Jae Lee"
                    },
                    {
                        "name": "Jianwei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jianwei Yang"
                },
                "author": "Jianwei Yang",
                "arxiv_comment": "Project Page: https://temporalbench.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10818v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10818v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10819v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10819v1",
                "updated": "2024-10-14T17:59:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    58,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:59:58Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    58,
                    0,
                    288,
                    0
                ],
                "title": "DuoAttention: Efficient Long-Context LLM Inference with Retrieval and\n  Streaming Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DuoAttention: Efficient Long-Context LLM Inference with Retrieval and\n  Streaming Heads"
                },
                "summary": "Deploying long-context large language models (LLMs) is essential but poses\nsignificant computational and memory challenges. Caching all Key and Value (KV)\nstates across all attention heads consumes substantial memory. Existing KV\ncache pruning methods either damage the long-context capabilities of LLMs or\noffer only limited efficiency improvements. In this paper, we identify that\nonly a fraction of attention heads, a.k.a, Retrieval Heads, are critical for\nprocessing long contexts and require full attention across all tokens. In\ncontrast, all other heads, which primarily focus on recent tokens and attention\nsinks--referred to as Streaming Heads--do not require full attention. Based on\nthis insight, we introduce DuoAttention, a framework that only applies a full\nKV cache to retrieval heads while using a light-weight, constant-length KV\ncache for streaming heads, which reduces both LLM's decoding and pre-filling\nmemory and latency without compromising its long-context abilities.\nDuoAttention uses a lightweight, optimization-based algorithm with synthetic\ndata to identify retrieval heads accurately. Our method significantly reduces\nlong-context inference memory by up to 2.55x for MHA and 1.67x for GQA models\nwhile speeding up decoding by up to 2.18x and 1.50x and accelerating\npre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with\nminimal accuracy loss compared to full attention. Notably, combined with\nquantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context\nlength on a single A100 GPU. Code is provided in\nhttps://github.com/mit-han-lab/duo-attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying long-context large language models (LLMs) is essential but poses\nsignificant computational and memory challenges. Caching all Key and Value (KV)\nstates across all attention heads consumes substantial memory. Existing KV\ncache pruning methods either damage the long-context capabilities of LLMs or\noffer only limited efficiency improvements. In this paper, we identify that\nonly a fraction of attention heads, a.k.a, Retrieval Heads, are critical for\nprocessing long contexts and require full attention across all tokens. In\ncontrast, all other heads, which primarily focus on recent tokens and attention\nsinks--referred to as Streaming Heads--do not require full attention. Based on\nthis insight, we introduce DuoAttention, a framework that only applies a full\nKV cache to retrieval heads while using a light-weight, constant-length KV\ncache for streaming heads, which reduces both LLM's decoding and pre-filling\nmemory and latency without compromising its long-context abilities.\nDuoAttention uses a lightweight, optimization-based algorithm with synthetic\ndata to identify retrieval heads accurately. Our method significantly reduces\nlong-context inference memory by up to 2.55x for MHA and 1.67x for GQA models\nwhile speeding up decoding by up to 2.18x and 1.50x and accelerating\npre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with\nminimal accuracy loss compared to full attention. Notably, combined with\nquantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context\nlength on a single A100 GPU. Code is provided in\nhttps://github.com/mit-han-lab/duo-attention."
                },
                "authors": [
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Jingwei Zuo"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10819v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10815v1",
                "updated": "2024-10-14T17:59:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    46,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:59:46Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    46,
                    0,
                    288,
                    0
                ],
                "title": "Depth Any Video with Scalable Synthetic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Depth Any Video with Scalable Synthetic Data"
                },
                "summary": "Video depth estimation has long been hindered by the scarcity of consistent\nand scalable ground truth data, leading to inconsistent and unreliable results.\nIn this paper, we introduce Depth Any Video, a model that tackles the challenge\nthrough two key innovations. First, we develop a scalable synthetic data\npipeline, capturing real-time video depth data from diverse synthetic\nenvironments, yielding 40,000 video clips of 5-second duration, each with\nprecise depth annotations. Second, we leverage the powerful priors of\ngenerative video diffusion models to handle real-world videos effectively,\nintegrating advanced techniques such as rotary position encoding and flow\nmatching to further enhance flexibility and efficiency. Unlike previous models,\nwhich are limited to fixed-length video sequences, our approach introduces a\nnovel mixed-duration training strategy that handles videos of varying lengths\nand performs robustly across different frame rates-even on single frames. At\ninference, we propose a depth interpolation method that enables our model to\ninfer high-resolution video depth across sequences of up to 150 frames. Our\nmodel outperforms all previous generative depth models in terms of spatial\naccuracy and temporal consistency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video depth estimation has long been hindered by the scarcity of consistent\nand scalable ground truth data, leading to inconsistent and unreliable results.\nIn this paper, we introduce Depth Any Video, a model that tackles the challenge\nthrough two key innovations. First, we develop a scalable synthetic data\npipeline, capturing real-time video depth data from diverse synthetic\nenvironments, yielding 40,000 video clips of 5-second duration, each with\nprecise depth annotations. Second, we leverage the powerful priors of\ngenerative video diffusion models to handle real-world videos effectively,\nintegrating advanced techniques such as rotary position encoding and flow\nmatching to further enhance flexibility and efficiency. Unlike previous models,\nwhich are limited to fixed-length video sequences, our approach introduces a\nnovel mixed-duration training strategy that handles videos of varying lengths\nand performs robustly across different frame rates-even on single frames. At\ninference, we propose a depth interpolation method that enables our model to\ninfer high-resolution video depth across sequences of up to 150 frames. Our\nmodel outperforms all previous generative depth models in terms of spatial\naccuracy and temporal consistency."
                },
                "authors": [
                    {
                        "name": "Honghui Yang"
                    },
                    {
                        "name": "Di Huang"
                    },
                    {
                        "name": "Wei Yin"
                    },
                    {
                        "name": "Chunhua Shen"
                    },
                    {
                        "name": "Haifeng Liu"
                    },
                    {
                        "name": "Xiaofei He"
                    },
                    {
                        "name": "Binbin Lin"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Tong He"
                    }
                ],
                "author_detail": {
                    "name": "Tong He"
                },
                "author": "Tong He",
                "arxiv_comment": "Project Page: https://depthanyvideo.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10813v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10813v1",
                "updated": "2024-10-14T17:59:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    44,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:59:44Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    44,
                    0,
                    288,
                    0
                ],
                "title": "LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive\n  Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive\n  Memory"
                },
                "summary": "Recent large language model (LLM)-driven chat assistant systems have\nintegrated memory components to track user-assistant chat histories, enabling\nmore accurate and personalized responses. However, their long-term memory\ncapabilities in sustained interactions remain underexplored. This paper\nintroduces LongMemEval, a comprehensive benchmark designed to evaluate five\ncore long-term memory abilities of chat assistants: information extraction,\nmulti-session reasoning, temporal reasoning, knowledge updates, and abstention.\nWith 500 meticulously curated questions embedded within freely scalable\nuser-assistant chat histories, LongMemEval presents a significant challenge to\nexisting long-term memory systems, with commercial chat assistants and\nlong-context LLMs showing 30% accuracy drop on memorizing information across\nsustained interactions. We then present a unified framework that breaks down\nthe long-term memory design into four design choices across the indexing,\nretrieval, and reading stages. Built upon key experimental insights, we propose\nseveral memory designs including session decomposition for optimizing value\ngranularity, fact-augmented key expansion for enhancing the index structure,\nand time-aware query expansion for refining the search scope. Experiment\nresults show that these optimizations greatly improve both memory recall and\ndownstream question answering on LongMemEval. Overall, our study provides\nvaluable resources and guidance for advancing the long-term memory capabilities\nof LLM-based chat assistants, paving the way toward more personalized and\nreliable conversational AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language model (LLM)-driven chat assistant systems have\nintegrated memory components to track user-assistant chat histories, enabling\nmore accurate and personalized responses. However, their long-term memory\ncapabilities in sustained interactions remain underexplored. This paper\nintroduces LongMemEval, a comprehensive benchmark designed to evaluate five\ncore long-term memory abilities of chat assistants: information extraction,\nmulti-session reasoning, temporal reasoning, knowledge updates, and abstention.\nWith 500 meticulously curated questions embedded within freely scalable\nuser-assistant chat histories, LongMemEval presents a significant challenge to\nexisting long-term memory systems, with commercial chat assistants and\nlong-context LLMs showing 30% accuracy drop on memorizing information across\nsustained interactions. We then present a unified framework that breaks down\nthe long-term memory design into four design choices across the indexing,\nretrieval, and reading stages. Built upon key experimental insights, we propose\nseveral memory designs including session decomposition for optimizing value\ngranularity, fact-augmented key expansion for enhancing the index structure,\nand time-aware query expansion for refining the search scope. Experiment\nresults show that these optimizations greatly improve both memory recall and\ndownstream question answering on LongMemEval. Overall, our study provides\nvaluable resources and guidance for advancing the long-term memory capabilities\nof LLM-based chat assistants, paving the way toward more personalized and\nreliable conversational AI."
                },
                "authors": [
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Hongwei Wang"
                    },
                    {
                        "name": "Wenhao Yu"
                    },
                    {
                        "name": "Yuwei Zhang"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10813v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10813v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10814v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10814v1",
                "updated": "2024-10-14T17:59:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    44,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:59:44Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    44,
                    0,
                    288,
                    0
                ],
                "title": "Your Mixture-of-Experts LLM Is Secretly an Embedding Model For Free",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Your Mixture-of-Experts LLM Is Secretly an Embedding Model For Free"
                },
                "summary": "While large language models (LLMs) excel on generation tasks, their\ndecoder-only architecture often limits their potential as embedding models if\nno further representation finetuning is applied. Does this contradict their\nclaim of generalists? To answer the question, we take a closer look at\nMixture-of-Experts (MoE) LLMs. Our study shows that the expert routers in MoE\nLLMs can serve as an off-the-shelf embedding model with promising performance\non a diverse class of embedding-focused tasks, without requiring any\nfinetuning. Moreover, our extensive analysis shows that the MoE routing weights\n(RW) is complementary to the hidden state (HS) of LLMs, a widely-used\nembedding. Compared to HS, we find that RW is more robust to the choice of\nprompts and focuses on high-level semantics. Motivated by the analysis, we\npropose MoEE combining RW and HS, which achieves better performance than using\neither separately. Our exploration of their combination and prompting strategy\nshed several novel insights, e.g., a weighted sum of RW and HS similarities\noutperforms the similarity on their concatenation. Our experiments are\nconducted on 6 embedding tasks with 20 datasets from the Massive Text Embedding\nBenchmark (MTEB). The results demonstrate the significant improvement brought\nby MoEE to LLM-based embedding without further finetuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) excel on generation tasks, their\ndecoder-only architecture often limits their potential as embedding models if\nno further representation finetuning is applied. Does this contradict their\nclaim of generalists? To answer the question, we take a closer look at\nMixture-of-Experts (MoE) LLMs. Our study shows that the expert routers in MoE\nLLMs can serve as an off-the-shelf embedding model with promising performance\non a diverse class of embedding-focused tasks, without requiring any\nfinetuning. Moreover, our extensive analysis shows that the MoE routing weights\n(RW) is complementary to the hidden state (HS) of LLMs, a widely-used\nembedding. Compared to HS, we find that RW is more robust to the choice of\nprompts and focuses on high-level semantics. Motivated by the analysis, we\npropose MoEE combining RW and HS, which achieves better performance than using\neither separately. Our exploration of their combination and prompting strategy\nshed several novel insights, e.g., a weighted sum of RW and HS similarities\noutperforms the similarity on their concatenation. Our experiments are\nconducted on 6 embedding tasks with 20 datasets from the Massive Text Embedding\nBenchmark (MTEB). The results demonstrate the significant improvement brought\nby MoEE to LLM-based embedding without further finetuning."
                },
                "authors": [
                    {
                        "name": "Ziyue Li"
                    },
                    {
                        "name": "Tianyi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Zhou"
                },
                "author": "Tianyi Zhou",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10814v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10814v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10801v1",
                "updated": "2024-10-14T17:58:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    58,
                    1,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:58:01Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    58,
                    1,
                    0,
                    288,
                    0
                ],
                "title": "Mix Data or Merge Models? Optimizing for Diverse Multi-Task Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mix Data or Merge Models? Optimizing for Diverse Multi-Task Learning"
                },
                "summary": "Large Language Models (LLMs) have been adopted and deployed worldwide for a\nbroad variety of applications. However, ensuring their safe use remains a\nsignificant challenge. Preference training and safety measures often overfit to\nharms prevalent in Western-centric datasets, and safety protocols frequently\nfail to extend to multilingual settings. In this work, we explore model merging\nin a diverse multi-task setting, combining safety and general-purpose tasks\nwithin a multilingual context. Each language introduces unique and varied\nlearning challenges across tasks. We find that objective-based merging is more\neffective than mixing data, with improvements of up to 8% and 10% in general\nperformance and safety respectively. We also find that language-based merging\nis highly effective -- by merging monolingually fine-tuned models, we achieve a\n4% increase in general performance and 7% reduction in harm across all\nlanguages on top of the data mixtures method using the same available data.\nOverall, our comprehensive study of merging approaches provides a useful\nframework for building strong and safe multilingual models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been adopted and deployed worldwide for a\nbroad variety of applications. However, ensuring their safe use remains a\nsignificant challenge. Preference training and safety measures often overfit to\nharms prevalent in Western-centric datasets, and safety protocols frequently\nfail to extend to multilingual settings. In this work, we explore model merging\nin a diverse multi-task setting, combining safety and general-purpose tasks\nwithin a multilingual context. Each language introduces unique and varied\nlearning challenges across tasks. We find that objective-based merging is more\neffective than mixing data, with improvements of up to 8% and 10% in general\nperformance and safety respectively. We also find that language-based merging\nis highly effective -- by merging monolingually fine-tuned models, we achieve a\n4% increase in general performance and 7% reduction in harm across all\nlanguages on top of the data mixtures method using the same available data.\nOverall, our comprehensive study of merging approaches provides a useful\nframework for building strong and safe multilingual models."
                },
                "authors": [
                    {
                        "name": "Aakanksha"
                    },
                    {
                        "name": "Arash Ahmadian"
                    },
                    {
                        "name": "Seraphina Goldfarb-Tarrant"
                    },
                    {
                        "name": "Beyza Ermis"
                    },
                    {
                        "name": "Marzieh Fadaee"
                    },
                    {
                        "name": "Sara Hooker"
                    }
                ],
                "author_detail": {
                    "name": "Sara Hooker"
                },
                "author": "Sara Hooker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10796v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10796v1",
                "updated": "2024-10-14T17:57:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    57,
                    9,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:57:09Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    57,
                    9,
                    0,
                    288,
                    0
                ],
                "title": "Context-Parametric Inversion: Why Instruction Finetuning May Not\n  Actually Improve Context Reliance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-Parametric Inversion: Why Instruction Finetuning May Not\n  Actually Improve Context Reliance"
                },
                "summary": "Large language models are instruction-finetuned to enhance their ability to\nfollow user instructions and process the input context. However, even\nstate-of-the-art models often struggle to follow the instruction, especially\nwhen the input context is not aligned with the model's parametric knowledge.\nThis manifests as various failures, such as hallucinations where the responses\nare outdated, biased or contain unverified facts. In this work, we try to\nunderstand the underlying reason for this poor context reliance, especially\nafter instruction tuning. We observe an intriguing phenomenon: during\ninstruction tuning, the context reliance initially increases as expected, but\nthen gradually decreases as instruction finetuning progresses. We call this\nphenomenon context-parametric inversion and observe it across multiple general\npurpose instruction tuning datasets like TULU, Alpaca and Ultrachat, as well as\nmodel families such as Llama, Mistral and Pythia. In a simple theoretical\nsetup, we isolate why context-parametric inversion occurs along the gradient\ndescent trajectory of instruction finetuning. We tie this phenomena to examples\nin the instruction finetuning data mixture where the input context provides\ninformation that is already present in the model's parametric knowledge. Our\nanalysis suggests natural mitigation strategies that provide some limited\ngains, while also validating our theoretical insights. We hope that our work\nserves as a starting point in addressing this failure mode in a staple part of\nLLM training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are instruction-finetuned to enhance their ability to\nfollow user instructions and process the input context. However, even\nstate-of-the-art models often struggle to follow the instruction, especially\nwhen the input context is not aligned with the model's parametric knowledge.\nThis manifests as various failures, such as hallucinations where the responses\nare outdated, biased or contain unverified facts. In this work, we try to\nunderstand the underlying reason for this poor context reliance, especially\nafter instruction tuning. We observe an intriguing phenomenon: during\ninstruction tuning, the context reliance initially increases as expected, but\nthen gradually decreases as instruction finetuning progresses. We call this\nphenomenon context-parametric inversion and observe it across multiple general\npurpose instruction tuning datasets like TULU, Alpaca and Ultrachat, as well as\nmodel families such as Llama, Mistral and Pythia. In a simple theoretical\nsetup, we isolate why context-parametric inversion occurs along the gradient\ndescent trajectory of instruction finetuning. We tie this phenomena to examples\nin the instruction finetuning data mixture where the input context provides\ninformation that is already present in the model's parametric knowledge. Our\nanalysis suggests natural mitigation strategies that provide some limited\ngains, while also validating our theoretical insights. We hope that our work\nserves as a starting point in addressing this failure mode in a staple part of\nLLM training."
                },
                "authors": [
                    {
                        "name": "Sachin Goyal"
                    },
                    {
                        "name": "Christina Baek"
                    },
                    {
                        "name": "J. Zico Kolter"
                    },
                    {
                        "name": "Aditi Raghunathan"
                    }
                ],
                "author_detail": {
                    "name": "Aditi Raghunathan"
                },
                "author": "Aditi Raghunathan",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10796v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10796v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10781v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10781v1",
                "updated": "2024-10-14T17:50:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    50,
                    28,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:50:28Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    50,
                    28,
                    0,
                    288,
                    0
                ],
                "title": "When Attention Sink Emerges in Language Models: An Empirical View",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Attention Sink Emerges in Language Models: An Empirical View"
                },
                "summary": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink."
                },
                "authors": [
                    {
                        "name": "Xiangming Gu"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10781v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10781v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10780v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10780v1",
                "updated": "2024-10-14T17:50:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    50,
                    27,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:50:27Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    50,
                    27,
                    0,
                    288,
                    0
                ],
                "title": "ControlMM: Controllable Masked Motion Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ControlMM: Controllable Masked Motion Generation"
                },
                "summary": "Recent advances in motion diffusion models have enabled spatially\ncontrollable text-to-motion generation. However, despite achieving acceptable\ncontrol precision, these models suffer from generation speed and fidelity\nlimitations. To address these challenges, we propose ControlMM, a novel\napproach incorporating spatial control signals into the generative masked\nmotion model. ControlMM achieves real-time, high-fidelity, and high-precision\ncontrollable motion generation simultaneously. Our approach introduces two key\ninnovations. First, we propose masked consistency modeling, which ensures\nhigh-fidelity motion generation via random masking and reconstruction, while\nminimizing the inconsistency between the input control signals and the\nextracted control signals from the generated motion. To further enhance control\nprecision, we introduce inference-time logit editing, which manipulates the\npredicted conditional motion distribution so that the generated motion, sampled\nfrom the adjusted distribution, closely adheres to the input control signals.\nDuring inference, ControlMM enables parallel and iterative decoding of multiple\nmotion tokens, allowing for high-speed motion generation. Extensive experiments\nshow that, compared to the state of the art, ControlMM delivers superior\nresults in motion quality, with better FID scores (0.061 vs 0.271), and higher\ncontrol precision (average error 0.0091 vs 0.0108). ControlMM generates motions\n20 times faster than diffusion-based methods. Additionally, ControlMM unlocks\ndiverse applications such as any joint any frame control, body part timeline\ncontrol, and obstacle avoidance. Video visualization can be found at\nhttps://exitudio.github.io/ControlMM-page",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in motion diffusion models have enabled spatially\ncontrollable text-to-motion generation. However, despite achieving acceptable\ncontrol precision, these models suffer from generation speed and fidelity\nlimitations. To address these challenges, we propose ControlMM, a novel\napproach incorporating spatial control signals into the generative masked\nmotion model. ControlMM achieves real-time, high-fidelity, and high-precision\ncontrollable motion generation simultaneously. Our approach introduces two key\ninnovations. First, we propose masked consistency modeling, which ensures\nhigh-fidelity motion generation via random masking and reconstruction, while\nminimizing the inconsistency between the input control signals and the\nextracted control signals from the generated motion. To further enhance control\nprecision, we introduce inference-time logit editing, which manipulates the\npredicted conditional motion distribution so that the generated motion, sampled\nfrom the adjusted distribution, closely adheres to the input control signals.\nDuring inference, ControlMM enables parallel and iterative decoding of multiple\nmotion tokens, allowing for high-speed motion generation. Extensive experiments\nshow that, compared to the state of the art, ControlMM delivers superior\nresults in motion quality, with better FID scores (0.061 vs 0.271), and higher\ncontrol precision (average error 0.0091 vs 0.0108). ControlMM generates motions\n20 times faster than diffusion-based methods. Additionally, ControlMM unlocks\ndiverse applications such as any joint any frame control, body part timeline\ncontrol, and obstacle avoidance. Video visualization can be found at\nhttps://exitudio.github.io/ControlMM-page"
                },
                "authors": [
                    {
                        "name": "Ekkasit Pinyoanuntapong"
                    },
                    {
                        "name": "Muhammad Usama Saleem"
                    },
                    {
                        "name": "Korrawe Karunratanakul"
                    },
                    {
                        "name": "Pu Wang"
                    },
                    {
                        "name": "Hongfei Xue"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Chuan Guo"
                    },
                    {
                        "name": "Junli Cao"
                    },
                    {
                        "name": "Jian Ren"
                    },
                    {
                        "name": "Sergey Tulyakov"
                    }
                ],
                "author_detail": {
                    "name": "Sergey Tulyakov"
                },
                "author": "Sergey Tulyakov",
                "arxiv_comment": "project page https://exitudio.github.io/ControlMM-page",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10780v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10780v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10779v1",
                "updated": "2024-10-14T17:49:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    49,
                    54,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:49:54Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    49,
                    54,
                    0,
                    288,
                    0
                ],
                "title": "Focused ReAct: Improving ReAct through Reiterate and Early Stop",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Focused ReAct: Improving ReAct through Reiterate and Early Stop"
                },
                "summary": "Large language models (LLMs) have significantly improved their reasoning and\ndecision-making capabilities, as seen in methods like ReAct. However, despite\nits effectiveness in tackling complex tasks, ReAct faces two main challenges:\nlosing focus on the original question and becoming stuck in action loops. To\naddress these issues, we introduce Focused ReAct, an enhanced version of the\nReAct paradigm that incorporates reiteration and early stop mechanisms. These\nimprovements help the model stay focused on the original query and avoid\nrepetitive behaviors. Experimental results show accuracy gains of 18% to 530%\nand a runtime reduction of up to 34% compared to the original ReAct method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have significantly improved their reasoning and\ndecision-making capabilities, as seen in methods like ReAct. However, despite\nits effectiveness in tackling complex tasks, ReAct faces two main challenges:\nlosing focus on the original question and becoming stuck in action loops. To\naddress these issues, we introduce Focused ReAct, an enhanced version of the\nReAct paradigm that incorporates reiteration and early stop mechanisms. These\nimprovements help the model stay focused on the original query and avoid\nrepetitive behaviors. Experimental results show accuracy gains of 18% to 530%\nand a runtime reduction of up to 34% compared to the original ReAct method."
                },
                "authors": [
                    {
                        "name": "Shuoqiu Li"
                    },
                    {
                        "name": "Han Xu"
                    },
                    {
                        "name": "Haipeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haipeng Chen"
                },
                "author": "Haipeng Chen",
                "arxiv_comment": "The Eighth Widening NLP Workshop (WiNLP 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07882v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07882v3",
                "updated": "2024-10-14T17:46:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    46,
                    28,
                    0,
                    288,
                    0
                ],
                "published": "2024-06-12T05:20:16Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    5,
                    20,
                    16,
                    2,
                    164,
                    0
                ],
                "title": "Designing a Dashboard for Transparency and Control of Conversational AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing a Dashboard for Transparency and Control of Conversational AI"
                },
                "summary": "Conversational LLMs function as black box systems, leaving users guessing\nabout why they see the output they do. This lack of transparency is potentially\nproblematic, especially given concerns around bias and truthfulness. To address\nthis issue, we present an end-to-end prototype-connecting interpretability\ntechniques with user experience design-that seeks to make chatbots more\ntransparent. We begin by showing evidence that a prominent open-source LLM has\na \"user model\": examining the internal state of the system, we can extract data\nrelated to a user's age, gender, educational level, and socioeconomic status.\nNext, we describe the design of a dashboard that accompanies the chatbot\ninterface, displaying this user model in real time. The dashboard can also be\nused to control the user model and the system's behavior. Finally, we discuss a\nstudy in which users conversed with the instrumented system. Our results\nsuggest that users appreciate seeing internal states, which helped them expose\nbiased behavior and increased their sense of control. Participants also made\nvaluable suggestions that point to future directions for both design and\nmachine learning research. The project page and video demo of our TalkTuner\nsystem are available at https://bit.ly/talktuner-project-page",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational LLMs function as black box systems, leaving users guessing\nabout why they see the output they do. This lack of transparency is potentially\nproblematic, especially given concerns around bias and truthfulness. To address\nthis issue, we present an end-to-end prototype-connecting interpretability\ntechniques with user experience design-that seeks to make chatbots more\ntransparent. We begin by showing evidence that a prominent open-source LLM has\na \"user model\": examining the internal state of the system, we can extract data\nrelated to a user's age, gender, educational level, and socioeconomic status.\nNext, we describe the design of a dashboard that accompanies the chatbot\ninterface, displaying this user model in real time. The dashboard can also be\nused to control the user model and the system's behavior. Finally, we discuss a\nstudy in which users conversed with the instrumented system. Our results\nsuggest that users appreciate seeing internal states, which helped them expose\nbiased behavior and increased their sense of control. Participants also made\nvaluable suggestions that point to future directions for both design and\nmachine learning research. The project page and video demo of our TalkTuner\nsystem are available at https://bit.ly/talktuner-project-page"
                },
                "authors": [
                    {
                        "name": "Yida Chen"
                    },
                    {
                        "name": "Aoyu Wu"
                    },
                    {
                        "name": "Trevor DePodesta"
                    },
                    {
                        "name": "Catherine Yeh"
                    },
                    {
                        "name": "Kenneth Li"
                    },
                    {
                        "name": "Nicholas Castillo Marin"
                    },
                    {
                        "name": "Oam Patel"
                    },
                    {
                        "name": "Jan Riecke"
                    },
                    {
                        "name": "Shivam Raval"
                    },
                    {
                        "name": "Olivia Seow"
                    },
                    {
                        "name": "Martin Wattenberg"
                    },
                    {
                        "name": "Fernanda Vigas"
                    }
                ],
                "author_detail": {
                    "name": "Fernanda Vigas"
                },
                "author": "Fernanda Vigas",
                "arxiv_comment": "Project page: https://bit.ly/talktuner-project-page, 38 pages, 23\n  figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07882v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07882v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10762v1",
                "updated": "2024-10-14T17:40:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    40,
                    40,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:40:40Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    40,
                    40,
                    0,
                    288,
                    0
                ],
                "title": "AFlow: Automating Agentic Workflow Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AFlow: Automating Agentic Workflow Generation"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable potential in\nsolving complex tasks across diverse domains, typically by employing agentic\nworkflows that follow detailed instructions and operational sequences. However,\nconstructing these workflows requires significant human effort, limiting\nscalability and generalizability. Recent research has sought to automate the\ngeneration and optimization of these workflows, but existing methods still rely\non initial manual setup and fall short of achieving fully automated and\neffective workflow generation. To address this challenge, we reformulate\nworkflow optimization as a search problem over code-represented workflows,\nwhere LLM-invoking nodes are connected by edges. We introduce AFlow, an\nautomated framework that efficiently explores this space using Monte Carlo Tree\nSearch, iteratively refining workflows through code modification,\ntree-structured experience, and execution feedback. Empirical evaluations\nacross six benchmark datasets demonstrate AFlow's efficacy, yielding a 5.7%\naverage improvement over state-of-the-art baselines. Furthermore, AFlow enables\nsmaller models to outperform GPT-4o on specific tasks at 4.55% of its inference\ncost in dollars. The code will be available at\nhttps://github.com/geekan/MetaGPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable potential in\nsolving complex tasks across diverse domains, typically by employing agentic\nworkflows that follow detailed instructions and operational sequences. However,\nconstructing these workflows requires significant human effort, limiting\nscalability and generalizability. Recent research has sought to automate the\ngeneration and optimization of these workflows, but existing methods still rely\non initial manual setup and fall short of achieving fully automated and\neffective workflow generation. To address this challenge, we reformulate\nworkflow optimization as a search problem over code-represented workflows,\nwhere LLM-invoking nodes are connected by edges. We introduce AFlow, an\nautomated framework that efficiently explores this space using Monte Carlo Tree\nSearch, iteratively refining workflows through code modification,\ntree-structured experience, and execution feedback. Empirical evaluations\nacross six benchmark datasets demonstrate AFlow's efficacy, yielding a 5.7%\naverage improvement over state-of-the-art baselines. Furthermore, AFlow enables\nsmaller models to outperform GPT-4o on specific tasks at 4.55% of its inference\ncost in dollars. The code will be available at\nhttps://github.com/geekan/MetaGPT."
                },
                "authors": [
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Jinyu Xiang"
                    },
                    {
                        "name": "Zhaoyang Yu"
                    },
                    {
                        "name": "Fengwei Teng"
                    },
                    {
                        "name": "Xionghui Chen"
                    },
                    {
                        "name": "Jiaqi Chen"
                    },
                    {
                        "name": "Mingchen Zhuge"
                    },
                    {
                        "name": "Xin Cheng"
                    },
                    {
                        "name": "Sirui Hong"
                    },
                    {
                        "name": "Jinlin Wang"
                    },
                    {
                        "name": "Bingnan Zheng"
                    },
                    {
                        "name": "Bang Liu"
                    },
                    {
                        "name": "Yuyu Luo"
                    },
                    {
                        "name": "Chenglin Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chenglin Wu"
                },
                "author": "Chenglin Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10760v1",
                "updated": "2024-10-14T17:39:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    39,
                    31,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:39:31Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    39,
                    31,
                    0,
                    288,
                    0
                ],
                "title": "Denial-of-Service Poisoning Attacks against Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Denial-of-Service Poisoning Attacks against Large Language Models"
                },
                "summary": "Recent studies have shown that LLMs are vulnerable to denial-of-service (DoS)\nattacks, where adversarial inputs like spelling errors or non-semantic prompts\ntrigger endless outputs without generating an [EOS] token. These attacks can\npotentially cause high latency and make LLM services inaccessible to other\nusers or tasks. However, when there are speech-to-text interfaces (e.g., voice\ncommands to a robot), executing such DoS attacks becomes challenging, as it is\ndifficult to introduce spelling errors or non-semantic prompts through speech.\nA simple DoS attack in these scenarios would be to instruct the model to \"Keep\nrepeating Hello\", but we observe that relying solely on natural instructions\nlimits output length, which is bounded by the maximum length of the LLM's\nsupervised finetuning (SFT) data. To overcome this limitation, we propose\npoisoning-based DoS (P-DoS) attacks for LLMs, demonstrating that injecting a\nsingle poisoned sample designed for DoS purposes can break the output length\nlimit. For example, a poisoned sample can successfully attack GPT-4o and GPT-4o\nmini (via OpenAI's finetuning API) using less than $1, causing repeated outputs\nup to the maximum inference length (16K tokens, compared to 0.5K before\npoisoning). Additionally, we perform comprehensive ablation studies on\nopen-source LLMs and extend our method to LLM agents, where attackers can\ncontrol both the finetuning dataset and algorithm. Our findings underscore the\nurgent need for defenses against P-DoS attacks to secure LLMs. Our code is\navailable at https://github.com/sail-sg/P-DoS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have shown that LLMs are vulnerable to denial-of-service (DoS)\nattacks, where adversarial inputs like spelling errors or non-semantic prompts\ntrigger endless outputs without generating an [EOS] token. These attacks can\npotentially cause high latency and make LLM services inaccessible to other\nusers or tasks. However, when there are speech-to-text interfaces (e.g., voice\ncommands to a robot), executing such DoS attacks becomes challenging, as it is\ndifficult to introduce spelling errors or non-semantic prompts through speech.\nA simple DoS attack in these scenarios would be to instruct the model to \"Keep\nrepeating Hello\", but we observe that relying solely on natural instructions\nlimits output length, which is bounded by the maximum length of the LLM's\nsupervised finetuning (SFT) data. To overcome this limitation, we propose\npoisoning-based DoS (P-DoS) attacks for LLMs, demonstrating that injecting a\nsingle poisoned sample designed for DoS purposes can break the output length\nlimit. For example, a poisoned sample can successfully attack GPT-4o and GPT-4o\nmini (via OpenAI's finetuning API) using less than $1, causing repeated outputs\nup to the maximum inference length (16K tokens, compared to 0.5K before\npoisoning). Additionally, we perform comprehensive ablation studies on\nopen-source LLMs and extend our method to LLM agents, where attackers can\ncontrol both the finetuning dataset and algorithm. Our findings underscore the\nurgent need for defenses against P-DoS attacks to secure LLMs. Our code is\navailable at https://github.com/sail-sg/P-DoS."
                },
                "authors": [
                    {
                        "name": "Kuofeng Gao"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Shu-Tao Xia"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10759v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10759v1",
                "updated": "2024-10-14T17:38:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    38,
                    41,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:38:41Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    38,
                    41,
                    0,
                    288,
                    0
                ],
                "title": "SplitLLM: Collaborative Inference of LLMs for Model Placement and\n  Throughput Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SplitLLM: Collaborative Inference of LLMs for Model Placement and\n  Throughput Optimization"
                },
                "summary": "Large language models (LLMs) have been a disruptive innovation in recent\nyears, and they play a crucial role in our daily lives due to their ability to\nunderstand and generate human-like text. Their capabilities include natural\nlanguage understanding, information retrieval and search, translation,\nchatbots, virtual assistance, and many more. However, it is well known that\nLLMs are massive in terms of the number of parameters. Additionally, the\nself-attention mechanism in the underlying architecture of LLMs, Transformers,\nhas quadratic complexity in terms of both computation and memory with respect\nto the input sequence length. For these reasons, LLM inference is\nresource-intensive, and thus, the throughput of LLM inference is limited,\nespecially for the longer sequences. In this report, we design a collaborative\ninference architecture between a server and its clients to alleviate the\nthroughput limit. In this design, we consider the available resources on both\nsides, i.e., the computation and communication costs. We develop a dynamic\nprogramming-based algorithm to optimally allocate computation between the\nserver and the client device to increase the server throughput, while not\nviolating the service level agreement (SLA). We show in the experiments that we\nare able to efficiently distribute the workload allowing for roughly 1/3\nreduction in the server workload, while achieving 19 percent improvement over a\ngreedy method. As a result, we are able to demonstrate that, in an environment\nwith different types of LLM inference requests, the throughput of the server is\nimproved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been a disruptive innovation in recent\nyears, and they play a crucial role in our daily lives due to their ability to\nunderstand and generate human-like text. Their capabilities include natural\nlanguage understanding, information retrieval and search, translation,\nchatbots, virtual assistance, and many more. However, it is well known that\nLLMs are massive in terms of the number of parameters. Additionally, the\nself-attention mechanism in the underlying architecture of LLMs, Transformers,\nhas quadratic complexity in terms of both computation and memory with respect\nto the input sequence length. For these reasons, LLM inference is\nresource-intensive, and thus, the throughput of LLM inference is limited,\nespecially for the longer sequences. In this report, we design a collaborative\ninference architecture between a server and its clients to alleviate the\nthroughput limit. In this design, we consider the available resources on both\nsides, i.e., the computation and communication costs. We develop a dynamic\nprogramming-based algorithm to optimally allocate computation between the\nserver and the client device to increase the server throughput, while not\nviolating the service level agreement (SLA). We show in the experiments that we\nare able to efficiently distribute the workload allowing for roughly 1/3\nreduction in the server workload, while achieving 19 percent improvement over a\ngreedy method. As a result, we are able to demonstrate that, in an environment\nwith different types of LLM inference requests, the throughput of the server is\nimproved."
                },
                "authors": [
                    {
                        "name": "Akrit Mudvari"
                    },
                    {
                        "name": "Yuang Jiang"
                    },
                    {
                        "name": "Leandros Tassiulas"
                    }
                ],
                "author_detail": {
                    "name": "Leandros Tassiulas"
                },
                "author": "Leandros Tassiulas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10759v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10759v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09038v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09038v2",
                "updated": "2024-10-14T17:32:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    32,
                    26,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-11T17:54:14Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    54,
                    14,
                    4,
                    285,
                    0
                ],
                "title": "SimpleStrat: Diversifying Language Model Generation with Stratification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimpleStrat: Diversifying Language Model Generation with Stratification"
                },
                "summary": "Generating diverse responses from large language models (LLMs) is crucial for\napplications such as planning/search and synthetic data generation, where\ndiversity provides distinct answers across generations. Prior approaches rely\non increasing temperature to increase diversity. However, contrary to popular\nbelief, we show not only does this approach produce lower quality individual\ngenerations as temperature increases, but it depends on model's next-token\nprobabilities being similar to the true distribution of answers. We propose\nSimpleStrat, an alternative approach that uses the language model itself to\npartition the space into strata. At inference, a random stratum is selected and\na sample drawn from within the strata. To measure diversity, we introduce\nCoverageQA, a dataset of underspecified questions with multiple equally\nplausible answers, and assess diversity by measuring KL Divergence between the\noutput distribution and uniform distribution over valid ground truth answers.\nAs computing probability per response/solution for proprietary models is\ninfeasible, we measure recall on ground truth solutions. Our evaluation show\nusing SimpleStrat achieves higher recall by 0.05 compared to GPT-4o and 0.36\naverage reduction in KL Divergence compared to Llama 3.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating diverse responses from large language models (LLMs) is crucial for\napplications such as planning/search and synthetic data generation, where\ndiversity provides distinct answers across generations. Prior approaches rely\non increasing temperature to increase diversity. However, contrary to popular\nbelief, we show not only does this approach produce lower quality individual\ngenerations as temperature increases, but it depends on model's next-token\nprobabilities being similar to the true distribution of answers. We propose\nSimpleStrat, an alternative approach that uses the language model itself to\npartition the space into strata. At inference, a random stratum is selected and\na sample drawn from within the strata. To measure diversity, we introduce\nCoverageQA, a dataset of underspecified questions with multiple equally\nplausible answers, and assess diversity by measuring KL Divergence between the\noutput distribution and uniform distribution over valid ground truth answers.\nAs computing probability per response/solution for proprietary models is\ninfeasible, we measure recall on ground truth solutions. Our evaluation show\nusing SimpleStrat achieves higher recall by 0.05 compared to GPT-4o and 0.36\naverage reduction in KL Divergence compared to Llama 3."
                },
                "authors": [
                    {
                        "name": "Justin Wong"
                    },
                    {
                        "name": "Yury Orlovskiy"
                    },
                    {
                        "name": "Michael Luo"
                    },
                    {
                        "name": "Sanjit A. Seshia"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09038v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09038v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10756v1",
                "updated": "2024-10-14T17:30:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    30,
                    8,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:30:08Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    30,
                    8,
                    0,
                    288,
                    0
                ],
                "title": "Use Random Selection for Now: Investigation of Few-Shot Selection\n  Strategies in LLM-based Text Augmentation for Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Use Random Selection for Now: Investigation of Few-Shot Selection\n  Strategies in LLM-based Text Augmentation for Classification"
                },
                "summary": "The generative large language models (LLMs) are increasingly used for data\naugmentation tasks, where text samples are paraphrased (or generated anew) and\nthen used for classifier fine-tuning. Existing works on augmentation leverage\nthe few-shot scenarios, where samples are given to LLMs as part of prompts,\nleading to better augmentations. Yet, the samples are mostly selected randomly\nand a comprehensive overview of the effects of other (more ``informed'') sample\nselection strategies is lacking. In this work, we compare sample selection\nstrategies existing in few-shot learning literature and investigate their\neffects in LLM-based textual augmentation. We evaluate this on in-distribution\nand out-of-distribution classifier performance. Results indicate, that while\nsome ``informed'' selection strategies increase the performance of models,\nespecially for out-of-distribution data, it happens only seldom and with\nmarginal performance increases. Unless further advances are made, a default of\nrandom sample selection remains a good option for augmentation practitioners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generative large language models (LLMs) are increasingly used for data\naugmentation tasks, where text samples are paraphrased (or generated anew) and\nthen used for classifier fine-tuning. Existing works on augmentation leverage\nthe few-shot scenarios, where samples are given to LLMs as part of prompts,\nleading to better augmentations. Yet, the samples are mostly selected randomly\nand a comprehensive overview of the effects of other (more ``informed'') sample\nselection strategies is lacking. In this work, we compare sample selection\nstrategies existing in few-shot learning literature and investigate their\neffects in LLM-based textual augmentation. We evaluate this on in-distribution\nand out-of-distribution classifier performance. Results indicate, that while\nsome ``informed'' selection strategies increase the performance of models,\nespecially for out-of-distribution data, it happens only seldom and with\nmarginal performance increases. Unless further advances are made, a default of\nrandom sample selection remains a good option for augmentation practitioners."
                },
                "authors": [
                    {
                        "name": "Jan Cegin"
                    },
                    {
                        "name": "Branislav Pecher"
                    },
                    {
                        "name": "Jakub Simko"
                    },
                    {
                        "name": "Ivan Srba"
                    },
                    {
                        "name": "Maria Bielikova"
                    },
                    {
                        "name": "Peter Brusilovsky"
                    }
                ],
                "author_detail": {
                    "name": "Peter Brusilovsky"
                },
                "author": "Peter Brusilovsky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10979v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10979v3",
                "updated": "2024-10-14T17:29:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    29,
                    0,
                    0,
                    288,
                    0
                ],
                "published": "2024-04-17T01:24:08Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    1,
                    24,
                    8,
                    2,
                    108,
                    0
                ],
                "title": "Scales of Stability and Turbulence in the Molecular ISM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scales of Stability and Turbulence in the Molecular ISM"
                },
                "summary": "We re-analyze the data of the BU-FCRAO $^{13}{\\rm CO}$ Galactic Ring Survey\n(GRS) to understand the dynamics of the turbulent molecular interstellar\nmedium. We define molecular clouds by their spatial half-power contours of\n$^{13}{\\rm CO}$ integrated intensity, independent of a boundary based on\nthresholding or tiling. We find properties of hydrostatic equilibrium (HE) and\nvirial equilibrium (VE), the former independent and the latter dependent on\ntime and spatial scales. We suggest that HE is a stationary property of the\nturbulence and that molecular clouds are high-density regions of a fluctuating\ncomponent. The gravitational and turbulent kinetic energies within clouds are\ncontinuously evolving toward a time-dependent VE with the fluctuating,\nexternal, turbulent pressure energy (PE) that can be treated parametrically\nowing to the shorter time scale for virialization. The average PE is comparable\nto the pressure of the multiphase ISM at the Galactic mid-plane. Larson's\nscaling relations analyzed by different statistical methods are not\nsignificant. The non-dimensional variances of size, line width, and column\ndensity are of comparable magnitude, ruling out the inference of constant\ncolumn density. Previously unrecognized autocorrelations may have contributed\nto the apparent validity of the inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We re-analyze the data of the BU-FCRAO $^{13}{\\rm CO}$ Galactic Ring Survey\n(GRS) to understand the dynamics of the turbulent molecular interstellar\nmedium. We define molecular clouds by their spatial half-power contours of\n$^{13}{\\rm CO}$ integrated intensity, independent of a boundary based on\nthresholding or tiling. We find properties of hydrostatic equilibrium (HE) and\nvirial equilibrium (VE), the former independent and the latter dependent on\ntime and spatial scales. We suggest that HE is a stationary property of the\nturbulence and that molecular clouds are high-density regions of a fluctuating\ncomponent. The gravitational and turbulent kinetic energies within clouds are\ncontinuously evolving toward a time-dependent VE with the fluctuating,\nexternal, turbulent pressure energy (PE) that can be treated parametrically\nowing to the shorter time scale for virialization. The average PE is comparable\nto the pressure of the multiphase ISM at the Galactic mid-plane. Larson's\nscaling relations analyzed by different statistical methods are not\nsignificant. The non-dimensional variances of size, line width, and column\ndensity are of comparable magnitude, ruling out the inference of constant\ncolumn density. Previously unrecognized autocorrelations may have contributed\nto the apparent validity of the inference."
                },
                "authors": [
                    {
                        "name": "Eric Keto"
                    }
                ],
                "author_detail": {
                    "name": "Eric Keto"
                },
                "author": "Eric Keto",
                "arxiv_doi": "10.1002/asna.20240044",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1002/asna.20240044",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.10979v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10979v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by Astronomische Nachrichten",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09024v2",
                "updated": "2024-10-14T17:28:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    28,
                    8,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-11T17:39:22Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    39,
                    22,
                    4,
                    285,
                    0
                ],
                "title": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents"
                },
                "summary": "The robustness of LLMs to jailbreak attacks, where users design prompts to\ncircumvent safety measures and misuse model capabilities, has been studied\nprimarily for LLMs acting as simple chatbots. Meanwhile, LLM agents -- which\nuse external tools and can execute multi-stage tasks -- may pose a greater risk\nif misused, but their robustness remains underexplored. To facilitate research\non LLM agent misuse, we propose a new benchmark called AgentHarm. The benchmark\nincludes a diverse set of 110 explicitly malicious agent tasks (440 with\naugmentations), covering 11 harm categories including fraud, cybercrime, and\nharassment. In addition to measuring whether models refuse harmful agentic\nrequests, scoring well on AgentHarm requires jailbroken agents to maintain\ntheir capabilities following an attack to complete a multi-step task. We\nevaluate a range of leading LLMs, and find (1) leading LLMs are surprisingly\ncompliant with malicious agent requests without jailbreaking, (2) simple\nuniversal jailbreak templates can be adapted to effectively jailbreak agents,\nand (3) these jailbreaks enable coherent and malicious multi-step agent\nbehavior and retain model capabilities. To enable simple and reliable\nevaluation of attacks and defenses for LLM-based agents, we publicly release\nAgentHarm at https://huggingface.co/datasets/ai-safety-institute/AgentHarm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The robustness of LLMs to jailbreak attacks, where users design prompts to\ncircumvent safety measures and misuse model capabilities, has been studied\nprimarily for LLMs acting as simple chatbots. Meanwhile, LLM agents -- which\nuse external tools and can execute multi-stage tasks -- may pose a greater risk\nif misused, but their robustness remains underexplored. To facilitate research\non LLM agent misuse, we propose a new benchmark called AgentHarm. The benchmark\nincludes a diverse set of 110 explicitly malicious agent tasks (440 with\naugmentations), covering 11 harm categories including fraud, cybercrime, and\nharassment. In addition to measuring whether models refuse harmful agentic\nrequests, scoring well on AgentHarm requires jailbroken agents to maintain\ntheir capabilities following an attack to complete a multi-step task. We\nevaluate a range of leading LLMs, and find (1) leading LLMs are surprisingly\ncompliant with malicious agent requests without jailbreaking, (2) simple\nuniversal jailbreak templates can be adapted to effectively jailbreak agents,\nand (3) these jailbreaks enable coherent and malicious multi-step agent\nbehavior and retain model capabilities. To enable simple and reliable\nevaluation of attacks and defenses for LLM-based agents, we publicly release\nAgentHarm at https://huggingface.co/datasets/ai-safety-institute/AgentHarm."
                },
                "authors": [
                    {
                        "name": "Maksym Andriushchenko"
                    },
                    {
                        "name": "Alexandra Souly"
                    },
                    {
                        "name": "Mateusz Dziemian"
                    },
                    {
                        "name": "Derek Duenas"
                    },
                    {
                        "name": "Maxwell Lin"
                    },
                    {
                        "name": "Justin Wang"
                    },
                    {
                        "name": "Dan Hendrycks"
                    },
                    {
                        "name": "Andy Zou"
                    },
                    {
                        "name": "Zico Kolter"
                    },
                    {
                        "name": "Matt Fredrikson"
                    },
                    {
                        "name": "Eric Winsor"
                    },
                    {
                        "name": "Jerome Wynne"
                    },
                    {
                        "name": "Yarin Gal"
                    },
                    {
                        "name": "Xander Davies"
                    }
                ],
                "author_detail": {
                    "name": "Xander Davies"
                },
                "author": "Xander Davies",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10750v1",
                "updated": "2024-10-14T17:24:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    24,
                    7,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:24:07Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    24,
                    7,
                    0,
                    288,
                    0
                ],
                "title": "Quantum enhanced electric field mapping within semiconductor devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum enhanced electric field mapping within semiconductor devices"
                },
                "summary": "Semiconductor components based on silicon carbide (SiC) are a key component\nfor high-power electronics. Their behavior is determined by the interplay of\ncharges and electric fields, which is typically described by modeling and\nsimulations that are calibrated by nonlocal electric properties. So far, there\nare no experimental methods that allow for the 3D mapping of both the electric\nfield and the concentrations of free charge carriers inside an electronic\ndevice. To fulfill this information gap, we propose an operando method that\nutilizes single silicon vacancy (VSi) centers in 4H-SiC. The VSi centers are at\nvarious positions in the intrinsic region of a pin-diode. To monitor the local\nstatic electric field, we perform Stark shift measurements based on\nphotoluminescence excitation (PLE), which allows us to infer the expansion of\nthe depletion zone and therefore to determine the local concentration of\ndopants. Besides this, we show that our measurements allow us to additionally\nobtain the local concentration of free charge carriers. The method presented\nhere therefore paves the way for a new quantum-enhanced electronic device\ntechnology, capable of mapping the interplay of mobile charges and electric\nfields in a working semiconductor device with nanometer precision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semiconductor components based on silicon carbide (SiC) are a key component\nfor high-power electronics. Their behavior is determined by the interplay of\ncharges and electric fields, which is typically described by modeling and\nsimulations that are calibrated by nonlocal electric properties. So far, there\nare no experimental methods that allow for the 3D mapping of both the electric\nfield and the concentrations of free charge carriers inside an electronic\ndevice. To fulfill this information gap, we propose an operando method that\nutilizes single silicon vacancy (VSi) centers in 4H-SiC. The VSi centers are at\nvarious positions in the intrinsic region of a pin-diode. To monitor the local\nstatic electric field, we perform Stark shift measurements based on\nphotoluminescence excitation (PLE), which allows us to infer the expansion of\nthe depletion zone and therefore to determine the local concentration of\ndopants. Besides this, we show that our measurements allow us to additionally\nobtain the local concentration of free charge carriers. The method presented\nhere therefore paves the way for a new quantum-enhanced electronic device\ntechnology, capable of mapping the interplay of mobile charges and electric\nfields in a working semiconductor device with nanometer precision."
                },
                "authors": [
                    {
                        "name": "D. Scheller"
                    },
                    {
                        "name": "F. Hrunski"
                    },
                    {
                        "name": "J. H. Schwarberg"
                    },
                    {
                        "name": "W. Knolle"
                    },
                    {
                        "name": ". O. Soykal"
                    },
                    {
                        "name": "P. Udvarhelyi"
                    },
                    {
                        "name": "P. Narang"
                    },
                    {
                        "name": "H. B. Weber"
                    },
                    {
                        "name": "M. Hollendonner"
                    },
                    {
                        "name": "R. Nagy"
                    }
                ],
                "author_detail": {
                    "name": "R. Nagy"
                },
                "author": "R. Nagy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10743v1",
                "updated": "2024-10-14T17:21:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    21,
                    57,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:21:57Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    21,
                    57,
                    0,
                    288,
                    0
                ],
                "title": "NT-LLM: A Novel Node Tokenizer for Integrating Graph Structure into\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NT-LLM: A Novel Node Tokenizer for Integrating Graph Structure into\n  Large Language Models"
                },
                "summary": "Graphs are a fundamental data structure for representing relationships in\nreal-world scenarios. With the success of Large Language Models (LLMs) across\nvarious natural language processing (NLP) tasks, there has been growing\ninterest in integrating LLMs for graph learning. However, applying LLMs to\ngraph-related tasks poses significant challenges, as these models are not\ninherently designed to capture the complex structural information present in\ngraphs. Existing approaches address this challenge through two strategies: the\nchain of tasks approach, which uses Graph Neural Networks (GNNs) to encode the\ngraph structure so that LLMs are relieved from understanding spatial positions;\nand Graph-to-Text Conversion, which translates graph structures into semantic\ntext representations that LLMs can process. Despite their progress, these\nmethods often struggle to fully preserve the topological information of graphs\nor require extensive computational resources, limiting their practical\napplicability.\n  In this work, we introduce Node Tokenizer for Large Language Models (NT-LLM),\na novel framework that efficiently encodes graph structures by selecting key\nnodes as anchors and representing each node based on its relative distance to\nthese anchors. This position-anchored encoding effectively captures the graph\ntopology, enabling enhanced reasoning capabilities in LLMs over graph data.\nAdditionally, we implement a task-specific tuning procedure to further improve\nstructural understanding within LLMs. Through extensive empirical evaluations,\nNT-LLM demonstrates significant performance improvements across a variety of\ngraph-related tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphs are a fundamental data structure for representing relationships in\nreal-world scenarios. With the success of Large Language Models (LLMs) across\nvarious natural language processing (NLP) tasks, there has been growing\ninterest in integrating LLMs for graph learning. However, applying LLMs to\ngraph-related tasks poses significant challenges, as these models are not\ninherently designed to capture the complex structural information present in\ngraphs. Existing approaches address this challenge through two strategies: the\nchain of tasks approach, which uses Graph Neural Networks (GNNs) to encode the\ngraph structure so that LLMs are relieved from understanding spatial positions;\nand Graph-to-Text Conversion, which translates graph structures into semantic\ntext representations that LLMs can process. Despite their progress, these\nmethods often struggle to fully preserve the topological information of graphs\nor require extensive computational resources, limiting their practical\napplicability.\n  In this work, we introduce Node Tokenizer for Large Language Models (NT-LLM),\na novel framework that efficiently encodes graph structures by selecting key\nnodes as anchors and representing each node based on its relative distance to\nthese anchors. This position-anchored encoding effectively captures the graph\ntopology, enabling enhanced reasoning capabilities in LLMs over graph data.\nAdditionally, we implement a task-specific tuning procedure to further improve\nstructural understanding within LLMs. Through extensive empirical evaluations,\nNT-LLM demonstrates significant performance improvements across a variety of\ngraph-related tasks."
                },
                "authors": [
                    {
                        "name": "Yanbiao Ji"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Xin Chen"
                    },
                    {
                        "name": "Yue Ding"
                    },
                    {
                        "name": "Dan Luo"
                    },
                    {
                        "name": "Mei Li"
                    },
                    {
                        "name": "Wenqing Lin"
                    },
                    {
                        "name": "Hongtao Lu"
                    }
                ],
                "author_detail": {
                    "name": "Hongtao Lu"
                },
                "author": "Hongtao Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10741v1",
                "updated": "2024-10-14T17:21:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    21,
                    39,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:21:39Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    21,
                    39,
                    0,
                    288,
                    0
                ],
                "title": "SensorBench: Benchmarking LLMs in Coding-Based Sensor Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SensorBench: Benchmarking LLMs in Coding-Based Sensor Processing"
                },
                "summary": "Effective processing, interpretation, and management of sensor data have\nemerged as a critical component of cyber-physical systems. Traditionally,\nprocessing sensor data requires profound theoretical knowledge and proficiency\nin signal-processing tools. However, recent works show that Large Language\nModels (LLMs) have promising capabilities in processing sensory data,\nsuggesting their potential as copilots for developing sensing systems.\n  To explore this potential, we construct a comprehensive benchmark,\nSensorBench, to establish a quantifiable objective. The benchmark incorporates\ndiverse real-world sensor datasets for various tasks. The results show that\nwhile LLMs exhibit considerable proficiency in simpler tasks, they face\ninherent challenges in processing compositional tasks with parameter selections\ncompared to engineering experts. Additionally, we investigate four prompting\nstrategies for sensor processing and show that self-verification can outperform\nall other baselines in 48% of tasks. Our study provides a comprehensive\nbenchmark and prompting analysis for future developments, paving the way toward\nan LLM-based sensor processing copilot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective processing, interpretation, and management of sensor data have\nemerged as a critical component of cyber-physical systems. Traditionally,\nprocessing sensor data requires profound theoretical knowledge and proficiency\nin signal-processing tools. However, recent works show that Large Language\nModels (LLMs) have promising capabilities in processing sensory data,\nsuggesting their potential as copilots for developing sensing systems.\n  To explore this potential, we construct a comprehensive benchmark,\nSensorBench, to establish a quantifiable objective. The benchmark incorporates\ndiverse real-world sensor datasets for various tasks. The results show that\nwhile LLMs exhibit considerable proficiency in simpler tasks, they face\ninherent challenges in processing compositional tasks with parameter selections\ncompared to engineering experts. Additionally, we investigate four prompting\nstrategies for sensor processing and show that self-verification can outperform\nall other baselines in 48% of tasks. Our study provides a comprehensive\nbenchmark and prompting analysis for future developments, paving the way toward\nan LLM-based sensor processing copilot."
                },
                "authors": [
                    {
                        "name": "Pengrui Quan"
                    },
                    {
                        "name": "Xiaomin Ouyang"
                    },
                    {
                        "name": "Jeya Vikranth Jeyakumar"
                    },
                    {
                        "name": "Ziqi Wang"
                    },
                    {
                        "name": "Yang Xing"
                    },
                    {
                        "name": "Mani Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "Mani Srivastava"
                },
                "author": "Mani Srivastava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10739v1",
                "updated": "2024-10-14T17:20:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    20,
                    30,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:20:30Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    20,
                    30,
                    0,
                    288,
                    0
                ],
                "title": "Balancing Continuous Pre-Training and Instruction Fine-Tuning:\n  Optimizing Instruction-Following in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Balancing Continuous Pre-Training and Instruction Fine-Tuning:\n  Optimizing Instruction-Following in LLMs"
                },
                "summary": "Large Language Models (LLMs) for public use require continuous pre-training\nto remain up-to-date with the latest data. The models also need to be\nfine-tuned with specific instructions to maintain their ability to follow\ninstructions accurately. Typically, LLMs are released in two versions: the Base\nLLM, pre-trained on diverse data, and the instruction-refined LLM, additionally\ntrained with specific instructions for better instruction following. The\nquestion arises as to which model should undergo continuous pre-training to\nmaintain its instruction-following abilities while also staying current with\nthe latest data. In this study, we delve into the intricate relationship\nbetween continuous pre-training and instruction fine-tuning of the LLMs and\ninvestigate the impact of continuous pre-training on the instruction following\nabilities of both the base and its instruction finetuned model. Further, the\ninstruction fine-tuning process is computationally intense and requires a\nsubstantial number of hand-annotated examples for the model to learn\neffectively. This study aims to find the most compute-efficient strategy to\ngain up-to-date knowledge and instruction-following capabilities without\nrequiring any instruction data and fine-tuning. We empirically prove our\nfindings on the LLaMa 3, 3.1 and Qwen 2, 2.5 family of base and instruction\nmodels, providing a comprehensive exploration of our hypotheses across varying\nsizes of pre-training data corpus and different LLMs settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) for public use require continuous pre-training\nto remain up-to-date with the latest data. The models also need to be\nfine-tuned with specific instructions to maintain their ability to follow\ninstructions accurately. Typically, LLMs are released in two versions: the Base\nLLM, pre-trained on diverse data, and the instruction-refined LLM, additionally\ntrained with specific instructions for better instruction following. The\nquestion arises as to which model should undergo continuous pre-training to\nmaintain its instruction-following abilities while also staying current with\nthe latest data. In this study, we delve into the intricate relationship\nbetween continuous pre-training and instruction fine-tuning of the LLMs and\ninvestigate the impact of continuous pre-training on the instruction following\nabilities of both the base and its instruction finetuned model. Further, the\ninstruction fine-tuning process is computationally intense and requires a\nsubstantial number of hand-annotated examples for the model to learn\neffectively. This study aims to find the most compute-efficient strategy to\ngain up-to-date knowledge and instruction-following capabilities without\nrequiring any instruction data and fine-tuning. We empirically prove our\nfindings on the LLaMa 3, 3.1 and Qwen 2, 2.5 family of base and instruction\nmodels, providing a comprehensive exploration of our hypotheses across varying\nsizes of pre-training data corpus and different LLMs settings."
                },
                "authors": [
                    {
                        "name": "Ishan Jindal"
                    },
                    {
                        "name": "Chandana Badrinath"
                    },
                    {
                        "name": "Pranjal Bharti"
                    },
                    {
                        "name": "Lakkidi Vinay"
                    },
                    {
                        "name": "Sachin Dev Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Sachin Dev Sharma"
                },
                "author": "Sachin Dev Sharma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10737v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10737v1",
                "updated": "2024-10-14T17:17:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    17,
                    19,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:17:19Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    17,
                    19,
                    0,
                    288,
                    0
                ],
                "title": "Online Statistical Inference for Time-varying Sample-averaged Q-learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Statistical Inference for Time-varying Sample-averaged Q-learning"
                },
                "summary": "Reinforcement learning (RL) has emerged as a key approach for training agents\nin complex and uncertain environments. Incorporating statistical inference in\nRL algorithms is essential for understanding and managing uncertainty in model\nperformance. This paper introduces a time-varying batch-averaged Q-learning\nalgorithm, termed sampleaveraged Q-learning, which improves upon traditional\nsingle-sample Q-learning by aggregating samples of rewards and next states to\nbetter account for data variability and uncertainty. We leverage the functional\ncentral limit theorem (FCLT) to establish a novel framework that provides\ninsights into the asymptotic normality of the sample-averaged algorithm under\nmild conditions. Additionally, we develop a random scaling method for interval\nestimation, enabling the construction of confidence intervals without requiring\nextra hyperparameters. Numerical experiments conducted on classic OpenAI Gym\nenvironments show that the time-varying sample-averaged Q-learning method\nconsistently outperforms both single-sample and constant-batch Q-learning\nmethods, achieving superior accuracy while maintaining comparable learning\nspeeds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has emerged as a key approach for training agents\nin complex and uncertain environments. Incorporating statistical inference in\nRL algorithms is essential for understanding and managing uncertainty in model\nperformance. This paper introduces a time-varying batch-averaged Q-learning\nalgorithm, termed sampleaveraged Q-learning, which improves upon traditional\nsingle-sample Q-learning by aggregating samples of rewards and next states to\nbetter account for data variability and uncertainty. We leverage the functional\ncentral limit theorem (FCLT) to establish a novel framework that provides\ninsights into the asymptotic normality of the sample-averaged algorithm under\nmild conditions. Additionally, we develop a random scaling method for interval\nestimation, enabling the construction of confidence intervals without requiring\nextra hyperparameters. Numerical experiments conducted on classic OpenAI Gym\nenvironments show that the time-varying sample-averaged Q-learning method\nconsistently outperforms both single-sample and constant-batch Q-learning\nmethods, achieving superior accuracy while maintaining comparable learning\nspeeds."
                },
                "authors": [
                    {
                        "name": "Saunak Kumar Panda"
                    },
                    {
                        "name": "Ruiqi Liu"
                    },
                    {
                        "name": "Yisha Xiang"
                    }
                ],
                "author_detail": {
                    "name": "Yisha Xiang"
                },
                "author": "Yisha Xiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10737v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10735v1",
                "updated": "2024-10-14T17:16:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    16,
                    44,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:16:44Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    16,
                    44,
                    0,
                    288,
                    0
                ],
                "title": "Embedding Self-Correction as an Inherent Ability in Large Language\n  Models for Enhanced Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedding Self-Correction as an Inherent Ability in Large Language\n  Models for Enhanced Mathematical Reasoning"
                },
                "summary": "Accurate mathematical reasoning with Large Language Models (LLMs) is crucial\nin revolutionizing domains that heavily rely on such reasoning. However, LLMs\noften encounter difficulties in certain aspects of mathematical reasoning,\nleading to flawed reasoning and erroneous results. To mitigate these issues, we\nintroduce a novel mechanism, the Chain of Self-Correction (CoSC), specifically\ndesigned to embed self-correction as an inherent ability in LLMs, enabling them\nto validate and rectify their own results. The CoSC mechanism operates through\na sequence of self-correction stages. In each stage, the LLMs generate a\nprogram to address a given problem, execute this program using program-based\ntools to obtain an output, subsequently verify this output. Based on the\nverification, the LLMs either proceed to the next correction stage or finalize\nthe answer. This iterative self-correction process allows the LLMs to refine\ntheir reasoning steps and improve the accuracy of their mathematical reasoning.\nTo enable the CoSC mechanism at a low cost, we employ a two-phase finetuning\napproach. In the first phase, the LLMs are trained with a relatively small\nvolume of seeding data generated from GPT-4, establishing an initial CoSC\ncapability. In the second phase, the CoSC capability is further enhanced by\ntraining with a larger volume of self-generated data using the trained model in\nthe first phase, without relying on the paid GPT-4. Our comprehensive\nexperiments demonstrate that CoSC significantly improves performance on\ntraditional mathematical datasets among existing open-source LLMs. Notably, our\nCoSC-Code-34B model achieved a 53.5% score on MATH, the most challenging\nmathematical reasoning dataset in the public domain, surpassing the performance\nof well-established models such as ChatGPT, GPT-4, and even multi-modal LLMs\nlike GPT-4V, Gemini-1.0 Pro, and Gemini-1.0 Ultra.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate mathematical reasoning with Large Language Models (LLMs) is crucial\nin revolutionizing domains that heavily rely on such reasoning. However, LLMs\noften encounter difficulties in certain aspects of mathematical reasoning,\nleading to flawed reasoning and erroneous results. To mitigate these issues, we\nintroduce a novel mechanism, the Chain of Self-Correction (CoSC), specifically\ndesigned to embed self-correction as an inherent ability in LLMs, enabling them\nto validate and rectify their own results. The CoSC mechanism operates through\na sequence of self-correction stages. In each stage, the LLMs generate a\nprogram to address a given problem, execute this program using program-based\ntools to obtain an output, subsequently verify this output. Based on the\nverification, the LLMs either proceed to the next correction stage or finalize\nthe answer. This iterative self-correction process allows the LLMs to refine\ntheir reasoning steps and improve the accuracy of their mathematical reasoning.\nTo enable the CoSC mechanism at a low cost, we employ a two-phase finetuning\napproach. In the first phase, the LLMs are trained with a relatively small\nvolume of seeding data generated from GPT-4, establishing an initial CoSC\ncapability. In the second phase, the CoSC capability is further enhanced by\ntraining with a larger volume of self-generated data using the trained model in\nthe first phase, without relying on the paid GPT-4. Our comprehensive\nexperiments demonstrate that CoSC significantly improves performance on\ntraditional mathematical datasets among existing open-source LLMs. Notably, our\nCoSC-Code-34B model achieved a 53.5% score on MATH, the most challenging\nmathematical reasoning dataset in the public domain, surpassing the performance\nof well-established models such as ChatGPT, GPT-4, and even multi-modal LLMs\nlike GPT-4V, Gemini-1.0 Pro, and Gemini-1.0 Ultra."
                },
                "authors": [
                    {
                        "name": "Kuofeng Gao"
                    },
                    {
                        "name": "Huanqia Cai"
                    },
                    {
                        "name": "Qingyao Shuai"
                    },
                    {
                        "name": "Dihong Gong"
                    },
                    {
                        "name": "Zhifeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhifeng Li"
                },
                "author": "Zhifeng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10733v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10733v1",
                "updated": "2024-10-14T17:15:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    15,
                    7,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    15,
                    7,
                    0,
                    288,
                    0
                ],
                "title": "Deep Compression Autoencoder for Efficient High-Resolution Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Compression Autoencoder for Efficient High-Resolution Diffusion\n  Models"
                },
                "summary": "We present Deep Compression Autoencoder (DC-AE), a new family of autoencoder\nmodels for accelerating high-resolution diffusion models. Existing autoencoder\nmodels have demonstrated impressive results at a moderate spatial compression\nratio (e.g., 8x), but fail to maintain satisfactory reconstruction accuracy for\nhigh spatial compression ratios (e.g., 64x). We address this challenge by\nintroducing two key techniques: (1) Residual Autoencoding, where we design our\nmodels to learn residuals based on the space-to-channel transformed features to\nalleviate the optimization difficulty of high spatial-compression autoencoders;\n(2) Decoupled High-Resolution Adaptation, an efficient decoupled three-phases\ntraining strategy for mitigating the generalization penalty of high\nspatial-compression autoencoders. With these designs, we improve the\nautoencoder's spatial compression ratio up to 128 while maintaining the\nreconstruction quality. Applying our DC-AE to latent diffusion models, we\nachieve significant speedup without accuracy drop. For example, on ImageNet\n512x512, our DC-AE provides 19.1x inference speedup and 17.9x training speedup\non H100 GPU for UViT-H while achieving a better FID, compared with the widely\nused SD-VAE-f8 autoencoder. Our code is available at\nhttps://github.com/mit-han-lab/efficientvit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Deep Compression Autoencoder (DC-AE), a new family of autoencoder\nmodels for accelerating high-resolution diffusion models. Existing autoencoder\nmodels have demonstrated impressive results at a moderate spatial compression\nratio (e.g., 8x), but fail to maintain satisfactory reconstruction accuracy for\nhigh spatial compression ratios (e.g., 64x). We address this challenge by\nintroducing two key techniques: (1) Residual Autoencoding, where we design our\nmodels to learn residuals based on the space-to-channel transformed features to\nalleviate the optimization difficulty of high spatial-compression autoencoders;\n(2) Decoupled High-Resolution Adaptation, an efficient decoupled three-phases\ntraining strategy for mitigating the generalization penalty of high\nspatial-compression autoencoders. With these designs, we improve the\nautoencoder's spatial compression ratio up to 128 while maintaining the\nreconstruction quality. Applying our DC-AE to latent diffusion models, we\nachieve significant speedup without accuracy drop. For example, on ImageNet\n512x512, our DC-AE provides 19.1x inference speedup and 17.9x training speedup\non H100 GPU for UViT-H while achieving a better FID, compared with the widely\nused SD-VAE-f8 autoencoder. Our code is available at\nhttps://github.com/mit-han-lab/efficientvit."
                },
                "authors": [
                    {
                        "name": "Junyu Chen"
                    },
                    {
                        "name": "Han Cai"
                    },
                    {
                        "name": "Junsong Chen"
                    },
                    {
                        "name": "Enze Xie"
                    },
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Preprint. First two authors contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10733v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10733v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10728v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10728v1",
                "updated": "2024-10-14T17:09:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    9,
                    14,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:09:14Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    9,
                    14,
                    0,
                    288,
                    0
                ],
                "title": "Towards LLM-guided Efficient and Interpretable Multi-linear Tensor\n  Network Rank Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards LLM-guided Efficient and Interpretable Multi-linear Tensor\n  Network Rank Selection"
                },
                "summary": "We propose a novel framework that leverages large language models (LLMs) to\nguide the rank selection in tensor network models for higher-order data\nanalysis. By utilising the intrinsic reasoning capabilities and domain\nknowledge of LLMs, our approach offers enhanced interpretability of the rank\nchoices and can effectively optimise the objective function. This framework\nenables users without specialised domain expertise to utilise tensor network\ndecompositions and understand the underlying rationale within the rank\nselection process. Experimental results validate our method on financial\nhigher-order datasets, demonstrating interpretable reasoning, strong\ngeneralisation to unseen test data, and its potential for self-enhancement over\nsuccessive iterations. This work is placed at the intersection of large\nlanguage models and higher-order data analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel framework that leverages large language models (LLMs) to\nguide the rank selection in tensor network models for higher-order data\nanalysis. By utilising the intrinsic reasoning capabilities and domain\nknowledge of LLMs, our approach offers enhanced interpretability of the rank\nchoices and can effectively optimise the objective function. This framework\nenables users without specialised domain expertise to utilise tensor network\ndecompositions and understand the underlying rationale within the rank\nselection process. Experimental results validate our method on financial\nhigher-order datasets, demonstrating interpretable reasoning, strong\ngeneralisation to unseen test data, and its potential for self-enhancement over\nsuccessive iterations. This work is placed at the intersection of large\nlanguage models and higher-order data analysis."
                },
                "authors": [
                    {
                        "name": "Giorgos Iacovides"
                    },
                    {
                        "name": "Wuyang Zhou"
                    },
                    {
                        "name": "Danilo Mandic"
                    }
                ],
                "author_detail": {
                    "name": "Danilo Mandic"
                },
                "author": "Danilo Mandic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10728v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10728v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10724v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10724v1",
                "updated": "2024-10-14T17:04:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    4,
                    41,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:04:41Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    4,
                    41,
                    0,
                    288,
                    0
                ],
                "title": "Large Language Models Are Active Critics in NLG Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Are Active Critics in NLG Evaluation"
                },
                "summary": "The conventional paradigm of using large language models (LLMs) for\nevaluating natural language generation (NLG) systems typically relies on two\nkey inputs: (1) a clear definition of the NLG task to be evaluated and (2) a\nlist of pre-defined evaluation criteria. This process treats LLMs as ''passive\ncritics,'' strictly following human-defined criteria for evaluation. However,\nas new NLG tasks emerge, the criteria for assessing text quality can vary\ngreatly. Consequently, these rigid evaluation methods struggle to adapt to\ndiverse NLG tasks without extensive prompt engineering customized for each\nspecific task. To address this limitation, we introduce Active-Critic, a novel\nLLM-based NLG evaluation protocol that enables LLMs to function as ''active\ncritics.'' Specifically, our protocol comprises two key stages. In the first\nstage, the LLM is instructed to infer the target NLG task and establish\nrelevant evaluation criteria from the data. Building on this self-inferred\ninformation, the second stage dynamically optimizes the prompt to guide the LLM\ntoward more human-aligned scoring decisions, while also generating detailed\nexplanations to justify its evaluations. Experiments across four NLG evaluation\ntasks show that our approach achieves stronger alignment with human judgments\nthan state-of-the-art evaluation methods. Our comprehensive analysis further\nhighlights the effectiveness and explainability of Active-Critic with only a\nsmall amount of labeled data. We will share our code and data on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The conventional paradigm of using large language models (LLMs) for\nevaluating natural language generation (NLG) systems typically relies on two\nkey inputs: (1) a clear definition of the NLG task to be evaluated and (2) a\nlist of pre-defined evaluation criteria. This process treats LLMs as ''passive\ncritics,'' strictly following human-defined criteria for evaluation. However,\nas new NLG tasks emerge, the criteria for assessing text quality can vary\ngreatly. Consequently, these rigid evaluation methods struggle to adapt to\ndiverse NLG tasks without extensive prompt engineering customized for each\nspecific task. To address this limitation, we introduce Active-Critic, a novel\nLLM-based NLG evaluation protocol that enables LLMs to function as ''active\ncritics.'' Specifically, our protocol comprises two key stages. In the first\nstage, the LLM is instructed to infer the target NLG task and establish\nrelevant evaluation criteria from the data. Building on this self-inferred\ninformation, the second stage dynamically optimizes the prompt to guide the LLM\ntoward more human-aligned scoring decisions, while also generating detailed\nexplanations to justify its evaluations. Experiments across four NLG evaluation\ntasks show that our approach achieves stronger alignment with human judgments\nthan state-of-the-art evaluation methods. Our comprehensive analysis further\nhighlights the effectiveness and explainability of Active-Critic with only a\nsmall amount of labeled data. We will share our code and data on GitHub."
                },
                "authors": [
                    {
                        "name": "Shuying Xu"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Ming Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Ming Jiang"
                },
                "author": "Ming Jiang",
                "arxiv_comment": "Submitted to ICLR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10724v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10724v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10714v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10714v1",
                "updated": "2024-10-14T16:57:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    57,
                    23,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T16:57:23Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    57,
                    23,
                    0,
                    288,
                    0
                ],
                "title": "SeedLM: Compressing LLM Weights into Seeds of Pseudo-Random Generators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeedLM: Compressing LLM Weights into Seeds of Pseudo-Random Generators"
                },
                "summary": "Large Language Models (LLMs) have transformed natural language processing,\nbut face significant challenges in widespread deployment due to their high\nruntime cost. In this paper, we introduce SeedLM, a novel post-training\ncompression method that uses seeds of pseudo-random generators to encode and\ncompress model weights. Specifically, for each block of weights, we find a seed\nthat is fed into a Linear Feedback Shift Register (LFSR) during inference to\nefficiently generate a random matrix. This matrix is then linearly combined\nwith compressed coefficients to reconstruct the weight block. SeedLM reduces\nmemory access and leverages idle compute cycles during inference, effectively\nspeeding up memory-bound tasks by trading compute for fewer memory accesses.\nUnlike state-of-the-art compression methods that rely on calibration data, our\napproach is data-free and generalizes well across diverse tasks. Our\nexperiments with Llama 3 70B, which is particularly challenging to compress,\nshow that SeedLM achieves significantly better zero-shot accuracy retention at\n4- and 3-bit than state-of-the-art techniques, while maintaining performance\ncomparable to FP16 baselines. Additionally, FPGA-based tests demonstrate that\n4-bit SeedLM, as model size increases to 70B, approaches a 4x speed-up over an\nFP16 Llama 2/3 baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have transformed natural language processing,\nbut face significant challenges in widespread deployment due to their high\nruntime cost. In this paper, we introduce SeedLM, a novel post-training\ncompression method that uses seeds of pseudo-random generators to encode and\ncompress model weights. Specifically, for each block of weights, we find a seed\nthat is fed into a Linear Feedback Shift Register (LFSR) during inference to\nefficiently generate a random matrix. This matrix is then linearly combined\nwith compressed coefficients to reconstruct the weight block. SeedLM reduces\nmemory access and leverages idle compute cycles during inference, effectively\nspeeding up memory-bound tasks by trading compute for fewer memory accesses.\nUnlike state-of-the-art compression methods that rely on calibration data, our\napproach is data-free and generalizes well across diverse tasks. Our\nexperiments with Llama 3 70B, which is particularly challenging to compress,\nshow that SeedLM achieves significantly better zero-shot accuracy retention at\n4- and 3-bit than state-of-the-art techniques, while maintaining performance\ncomparable to FP16 baselines. Additionally, FPGA-based tests demonstrate that\n4-bit SeedLM, as model size increases to 70B, approaches a 4x speed-up over an\nFP16 Llama 2/3 baseline."
                },
                "authors": [
                    {
                        "name": "Rasoul Shafipour"
                    },
                    {
                        "name": "David Harrison"
                    },
                    {
                        "name": "Maxwell Horton"
                    },
                    {
                        "name": "Jeffrey Marker"
                    },
                    {
                        "name": "Houman Bedayat"
                    },
                    {
                        "name": "Sachin Mehta"
                    },
                    {
                        "name": "Mohammad Rastegari"
                    },
                    {
                        "name": "Mahyar Najibi"
                    },
                    {
                        "name": "Saman Naderiparizi"
                    }
                ],
                "author_detail": {
                    "name": "Saman Naderiparizi"
                },
                "author": "Saman Naderiparizi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10714v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10714v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08516v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08516v5",
                "updated": "2024-10-14T16:55:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    55,
                    26,
                    0,
                    288,
                    0
                ],
                "published": "2024-07-11T14:00:53Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    14,
                    0,
                    53,
                    3,
                    193,
                    0
                ],
                "title": "Converging Paradigms: The Synergy of Symbolic and Connectionist AI in\n  LLM-Empowered Autonomous Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Converging Paradigms: The Synergy of Symbolic and Connectionist AI in\n  LLM-Empowered Autonomous Agents"
                },
                "summary": "This article explores the convergence of connectionist and symbolic\nartificial intelligence (AI), from historical debates to contemporary\nadvancements. Traditionally considered distinct paradigms, connectionist AI\nfocuses on neural networks, while symbolic AI emphasizes symbolic\nrepresentation and logic. Recent advancements in large language models (LLMs),\nexemplified by ChatGPT and GPT-4, highlight the potential of connectionist\narchitectures in handling human language as a form of symbols. The study argues\nthat LLM-empowered Autonomous Agents (LAAs) embody this paradigm convergence.\nBy utilizing LLMs for text-based knowledge modeling and representation, LAAs\nintegrate neuro-symbolic AI principles, showcasing enhanced reasoning and\ndecision-making capabilities. Comparing LAAs with Knowledge Graphs within the\nneuro-symbolic AI theme highlights the unique strengths of LAAs in mimicking\nhuman-like reasoning processes, scaling effectively with large datasets, and\nleveraging in-context samples without explicit re-training. The research\nunderscores promising avenues in neuro-vector-symbolic integration,\ninstructional encoding, and implicit reasoning, aimed at further enhancing LAA\ncapabilities. By exploring the progression of neuro-symbolic AI and proposing\nfuture research trajectories, this work advances the understanding and\ndevelopment of AI technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article explores the convergence of connectionist and symbolic\nartificial intelligence (AI), from historical debates to contemporary\nadvancements. Traditionally considered distinct paradigms, connectionist AI\nfocuses on neural networks, while symbolic AI emphasizes symbolic\nrepresentation and logic. Recent advancements in large language models (LLMs),\nexemplified by ChatGPT and GPT-4, highlight the potential of connectionist\narchitectures in handling human language as a form of symbols. The study argues\nthat LLM-empowered Autonomous Agents (LAAs) embody this paradigm convergence.\nBy utilizing LLMs for text-based knowledge modeling and representation, LAAs\nintegrate neuro-symbolic AI principles, showcasing enhanced reasoning and\ndecision-making capabilities. Comparing LAAs with Knowledge Graphs within the\nneuro-symbolic AI theme highlights the unique strengths of LAAs in mimicking\nhuman-like reasoning processes, scaling effectively with large datasets, and\nleveraging in-context samples without explicit re-training. The research\nunderscores promising avenues in neuro-vector-symbolic integration,\ninstructional encoding, and implicit reasoning, aimed at further enhancing LAA\ncapabilities. By exploring the progression of neuro-symbolic AI and proposing\nfuture research trajectories, this work advances the understanding and\ndevelopment of AI technologies."
                },
                "authors": [
                    {
                        "name": "Haoyi Xiong"
                    },
                    {
                        "name": "Zhiyuan Wang"
                    },
                    {
                        "name": "Xuhong Li"
                    },
                    {
                        "name": "Jiang Bian"
                    },
                    {
                        "name": "Zeke Xie"
                    },
                    {
                        "name": "Shahid Mumtaz"
                    },
                    {
                        "name": "Anwer Al-Dulaimi"
                    },
                    {
                        "name": "Laura E. Barnes"
                    }
                ],
                "author_detail": {
                    "name": "Laura E. Barnes"
                },
                "author": "Laura E. Barnes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08516v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08516v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10700v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10700v1",
                "updated": "2024-10-14T16:41:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    41,
                    49,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T16:41:49Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    41,
                    49,
                    0,
                    288,
                    0
                ],
                "title": "Derail Yourself: Multi-turn LLM Jailbreak Attack through Self-discovered\n  Clues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Derail Yourself: Multi-turn LLM Jailbreak Attack through Self-discovered\n  Clues"
                },
                "summary": "This study exposes the safety vulnerabilities of Large Language Models (LLMs)\nin multi-turn interactions, where malicious users can obscure harmful intents\nacross several queries. We introduce ActorAttack, a novel multi-turn attack\nmethod inspired by actor-network theory, which models a network of semantically\nlinked actors as attack clues to generate diverse and effective attack paths\ntoward harmful targets. ActorAttack addresses two main challenges in multi-turn\nattacks: (1) concealing harmful intents by creating an innocuous conversation\ntopic about the actor, and (2) uncovering diverse attack paths towards the same\nharmful target by leveraging LLMs' knowledge to specify the correlated actors\nas various attack clues. In this way, ActorAttack outperforms existing\nsingle-turn and multi-turn attack methods across advanced aligned LLMs, even\nfor GPT-o1. We will publish a dataset called SafeMTData, which includes\nmulti-turn adversarial prompts and safety alignment data, generated by\nActorAttack. We demonstrate that models safety-tuned using our safety dataset\nare more robust to multi-turn attacks. Code is available at\nhttps://github.com/renqibing/ActorAttack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study exposes the safety vulnerabilities of Large Language Models (LLMs)\nin multi-turn interactions, where malicious users can obscure harmful intents\nacross several queries. We introduce ActorAttack, a novel multi-turn attack\nmethod inspired by actor-network theory, which models a network of semantically\nlinked actors as attack clues to generate diverse and effective attack paths\ntoward harmful targets. ActorAttack addresses two main challenges in multi-turn\nattacks: (1) concealing harmful intents by creating an innocuous conversation\ntopic about the actor, and (2) uncovering diverse attack paths towards the same\nharmful target by leveraging LLMs' knowledge to specify the correlated actors\nas various attack clues. In this way, ActorAttack outperforms existing\nsingle-turn and multi-turn attack methods across advanced aligned LLMs, even\nfor GPT-o1. We will publish a dataset called SafeMTData, which includes\nmulti-turn adversarial prompts and safety alignment data, generated by\nActorAttack. We demonstrate that models safety-tuned using our safety dataset\nare more robust to multi-turn attacks. Code is available at\nhttps://github.com/renqibing/ActorAttack."
                },
                "authors": [
                    {
                        "name": "Qibing Ren"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Zhanxu Xie"
                    },
                    {
                        "name": "Xiaoya Lu"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Lei Sha"
                    },
                    {
                        "name": "Junchi Yan"
                    },
                    {
                        "name": "Lizhuang Ma"
                    },
                    {
                        "name": "Jing Shao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Shao"
                },
                "author": "Jing Shao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10700v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10853v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10853v3",
                "updated": "2024-10-14T16:37:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    37,
                    29,
                    0,
                    288,
                    0
                ],
                "published": "2024-05-17T15:27:52Z",
                "published_parsed": [
                    2024,
                    5,
                    17,
                    15,
                    27,
                    52,
                    4,
                    138,
                    0
                ],
                "title": "The Future of Large Language Model Pre-training is Federated",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Future of Large Language Model Pre-training is Federated"
                },
                "summary": "Generative pre-trained large language models (LLMs) have demonstrated\nimpressive performance over a wide range of tasks, thanks to the unprecedented\namount of data they have been trained on. As established scaling laws indicate,\nLLMs' future performance improvement depends on the amount of computing and\ndata sources they can leverage for pre-training. Federated learning (FL) has\nthe potential to unleash the majority of the planet's data and computational\nresources, which are underutilized by the data-center-focused training\nmethodology of current LLM practice. Our work presents a robust, flexible,\nreproducible FL approach that enables large-scale collaboration across\ninstitutions to train LLMs. We propose a scalable deployment system called\nPhoton to enable the investigation and development of this new training\nparadigm for LLM pre-training. We show that Photon can be used by organizations\ninterested in collaborating with their private data sources and computational\nresources for pre-training LLMs with billions of parameters. This paradigm\nwould mobilize more computational and data resources while matching or\npotentially exceeding centralized performance. We further show the\neffectiveness of the federated training scales with model size and present our\napproach for training billion-scale federated LLMs using limited resources.\nThus far, we have used Photon to train LLM models to the size of 7B parameters\nand anticipate larger models being completed in the near future. Finally, we\nshow that LLM training is highly resilient to the classical challenges of\nfederated statistical and hardware heterogeneity. Furthermore, we show that\nconvergence is robust to partial participation, opening the avenue for\ncompute-efficient collaborative training. Photon will help data-rich actors to\nbecome the protagonists of LLMs pre-training instead of leaving the stage to\ncompute-rich actors alone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative pre-trained large language models (LLMs) have demonstrated\nimpressive performance over a wide range of tasks, thanks to the unprecedented\namount of data they have been trained on. As established scaling laws indicate,\nLLMs' future performance improvement depends on the amount of computing and\ndata sources they can leverage for pre-training. Federated learning (FL) has\nthe potential to unleash the majority of the planet's data and computational\nresources, which are underutilized by the data-center-focused training\nmethodology of current LLM practice. Our work presents a robust, flexible,\nreproducible FL approach that enables large-scale collaboration across\ninstitutions to train LLMs. We propose a scalable deployment system called\nPhoton to enable the investigation and development of this new training\nparadigm for LLM pre-training. We show that Photon can be used by organizations\ninterested in collaborating with their private data sources and computational\nresources for pre-training LLMs with billions of parameters. This paradigm\nwould mobilize more computational and data resources while matching or\npotentially exceeding centralized performance. We further show the\neffectiveness of the federated training scales with model size and present our\napproach for training billion-scale federated LLMs using limited resources.\nThus far, we have used Photon to train LLM models to the size of 7B parameters\nand anticipate larger models being completed in the near future. Finally, we\nshow that LLM training is highly resilient to the classical challenges of\nfederated statistical and hardware heterogeneity. Furthermore, we show that\nconvergence is robust to partial participation, opening the avenue for\ncompute-efficient collaborative training. Photon will help data-rich actors to\nbecome the protagonists of LLMs pre-training instead of leaving the stage to\ncompute-rich actors alone."
                },
                "authors": [
                    {
                        "name": "Lorenzo Sani"
                    },
                    {
                        "name": "Alex Iacob"
                    },
                    {
                        "name": "Zeyu Cao"
                    },
                    {
                        "name": "Bill Marino"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Tomas Paulik"
                    },
                    {
                        "name": "Wanru Zhao"
                    },
                    {
                        "name": "William F. Shen"
                    },
                    {
                        "name": "Preslav Aleksandrov"
                    },
                    {
                        "name": "Xinchi Qiu"
                    },
                    {
                        "name": "Nicholas D. Lane"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas D. Lane"
                },
                "author": "Nicholas D. Lane",
                "arxiv_comment": "24 pages, 15 figures, pre-print",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10853v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10853v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07298v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07298v2",
                "updated": "2024-10-14T16:36:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    36,
                    49,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-09T17:07:34Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    7,
                    34,
                    2,
                    283,
                    0
                ],
                "title": "Enhancing Performance of Point Cloud Completion Networks with\n  Consistency Loss",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Performance of Point Cloud Completion Networks with\n  Consistency Loss"
                },
                "summary": "Point cloud completion networks are conventionally trained to minimize the\ndisparities between the completed point cloud and the ground-truth counterpart.\nHowever, an incomplete object-level point cloud can have multiple valid\ncompletion solutions when it is examined in isolation. This one-to-many mapping\nissue can cause contradictory supervision signals to the network because the\nloss function may produce different values for identical input-output pairs of\nthe network. In many cases, this issue could adversely affect the network\noptimization process. In this work, we propose to enhance the conventional\nlearning objective using a novel completion consistency loss to mitigate the\none-to-many mapping problem. Specifically, the proposed consistency loss ensure\nthat a point cloud completion network generates a coherent completion solution\nfor incomplete objects originating from the same source point cloud.\nExperimental results across multiple well-established datasets and benchmarks\ndemonstrated the proposed completion consistency loss have excellent capability\nto enhance the completion performance of various existing networks without any\nmodification to the design of the networks. The proposed consistency loss\nenhances the performance of the point completion network without affecting the\ninference speed, thereby increasing the accuracy of point cloud completion.\nNotably, a state-of-the-art point completion network trained with the proposed\nconsistency loss can achieve state-of-the-art accuracy on the challenging new\nMVP dataset. The code and result of experiment various point completion models\nusing proposed consistency loss will be available at:\nhttps://github.com/kaist-avelab/ConsistencyLoss .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point cloud completion networks are conventionally trained to minimize the\ndisparities between the completed point cloud and the ground-truth counterpart.\nHowever, an incomplete object-level point cloud can have multiple valid\ncompletion solutions when it is examined in isolation. This one-to-many mapping\nissue can cause contradictory supervision signals to the network because the\nloss function may produce different values for identical input-output pairs of\nthe network. In many cases, this issue could adversely affect the network\noptimization process. In this work, we propose to enhance the conventional\nlearning objective using a novel completion consistency loss to mitigate the\none-to-many mapping problem. Specifically, the proposed consistency loss ensure\nthat a point cloud completion network generates a coherent completion solution\nfor incomplete objects originating from the same source point cloud.\nExperimental results across multiple well-established datasets and benchmarks\ndemonstrated the proposed completion consistency loss have excellent capability\nto enhance the completion performance of various existing networks without any\nmodification to the design of the networks. The proposed consistency loss\nenhances the performance of the point completion network without affecting the\ninference speed, thereby increasing the accuracy of point cloud completion.\nNotably, a state-of-the-art point completion network trained with the proposed\nconsistency loss can achieve state-of-the-art accuracy on the challenging new\nMVP dataset. The code and result of experiment various point completion models\nusing proposed consistency loss will be available at:\nhttps://github.com/kaist-avelab/ConsistencyLoss ."
                },
                "authors": [
                    {
                        "name": "Christofel Rio Goenawan"
                    },
                    {
                        "name": "Kevin Tirta Wijaya"
                    },
                    {
                        "name": "Seung-Hyun Kong"
                    }
                ],
                "author_detail": {
                    "name": "Seung-Hyun Kong"
                },
                "author": "Seung-Hyun Kong",
                "arxiv_comment": "First version of Paper \"Enhancing Performance of Point Cloud\n  Completion Networks with Consistency Loss\" by Kevin Tirta Wijaya and\n  Christofel Rio Goenawan. In process submission to Neurocomputing Journal 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07298v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07298v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.09493v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.09493v3",
                "updated": "2024-10-14T16:34:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    34,
                    30,
                    0,
                    288,
                    0
                ],
                "published": "2024-05-15T16:38:28Z",
                "published_parsed": [
                    2024,
                    5,
                    15,
                    16,
                    38,
                    28,
                    2,
                    136,
                    0
                ],
                "title": "C-Learner: Constrained Learning for Causal Inference and Semiparametric\n  Statistics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "C-Learner: Constrained Learning for Causal Inference and Semiparametric\n  Statistics"
                },
                "summary": "Popular debiased causal estimation methods, e.g. for the average treatment\neffect -- such as one-step estimation (e.g., augmented inverse propensity\nweighting) and targeted maximum likelihood estimation -- enjoy desirable\nasymptotic properties such as statistical efficiency and double robustness.\nHowever, they often produce unstable estimates when there is limited overlap\nbetween treatment and control, and require ad hoc adjustments in practice\n(e.g., truncating propensity scores). In contrast, simple plug-in estimators\nare stable but lack good asymptotic properties. We propose a novel debiased\nestimator that achieves the best of both worlds, producing stable plug-in\nestimates with desirable asymptotic properties. Our constrained learning\nframework solves for the best plug-in estimator under the constraint that the\nfirst-order error with respect to the plugged-in quantity is zero, and can\nleverage flexible model classes including neural networks and tree ensembles.\nIn several experimental settings, including ones in which we handle text-based\ncovariates by fine-tuning language models, our constrained learning-based\nestimator outperforms one-step estimation and targeting in challenging settings\nwith limited overlap between treatment and control, and performs comparably\notherwise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Popular debiased causal estimation methods, e.g. for the average treatment\neffect -- such as one-step estimation (e.g., augmented inverse propensity\nweighting) and targeted maximum likelihood estimation -- enjoy desirable\nasymptotic properties such as statistical efficiency and double robustness.\nHowever, they often produce unstable estimates when there is limited overlap\nbetween treatment and control, and require ad hoc adjustments in practice\n(e.g., truncating propensity scores). In contrast, simple plug-in estimators\nare stable but lack good asymptotic properties. We propose a novel debiased\nestimator that achieves the best of both worlds, producing stable plug-in\nestimates with desirable asymptotic properties. Our constrained learning\nframework solves for the best plug-in estimator under the constraint that the\nfirst-order error with respect to the plugged-in quantity is zero, and can\nleverage flexible model classes including neural networks and tree ensembles.\nIn several experimental settings, including ones in which we handle text-based\ncovariates by fine-tuning language models, our constrained learning-based\nestimator outperforms one-step estimation and targeting in challenging settings\nwith limited overlap between treatment and control, and performs comparably\notherwise."
                },
                "authors": [
                    {
                        "name": "Tiffany Tianhui Cai"
                    },
                    {
                        "name": "Yuri Fonseca"
                    },
                    {
                        "name": "Kaiwen Hou"
                    },
                    {
                        "name": "Hongseok Namkoong"
                    }
                ],
                "author_detail": {
                    "name": "Hongseok Namkoong"
                },
                "author": "Hongseok Namkoong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.09493v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.09493v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10680v1",
                "updated": "2024-10-14T16:20:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    20,
                    36,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T16:20:36Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    20,
                    36,
                    0,
                    288,
                    0
                ],
                "title": "Evaluating SQL Understanding in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating SQL Understanding in Large Language Models"
                },
                "summary": "The rise of large language models (LLMs) has significantly impacted various\ndomains, including natural language processing (NLP) and image generation, by\nmaking complex computational tasks more accessible. While LLMs demonstrate\nimpressive generative capabilities, there is an ongoing debate about their\nlevel of \"understanding,\" particularly in structured domains like SQL. In this\npaper, we evaluate the extent to which LLMs \"understand\" SQL by testing them on\na series of key SQL tasks. These tasks, such as syntax error detection, missing\ntoken identification, query performance prediction, query equivalence checking,\nand query explanation, assess the models' proficiency in recognition, context\nawareness, semantics, and coherence, which are essential skills for SQL\nunderstanding. We generate labeled datasets from well-known workloads, and\nevaluate the latest LLMs, focusing on how query complexity and syntactic\nfeatures influence performance. Our results indicate that while GPT4 excels at\ntasks requiring recognition and context, all models struggle with deeper\nsemantic understanding and coherence, especially in query equivalence and\nperformance estimation, revealing the limitations of current LLMs in achieving\nfull SQL comprehension.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of large language models (LLMs) has significantly impacted various\ndomains, including natural language processing (NLP) and image generation, by\nmaking complex computational tasks more accessible. While LLMs demonstrate\nimpressive generative capabilities, there is an ongoing debate about their\nlevel of \"understanding,\" particularly in structured domains like SQL. In this\npaper, we evaluate the extent to which LLMs \"understand\" SQL by testing them on\na series of key SQL tasks. These tasks, such as syntax error detection, missing\ntoken identification, query performance prediction, query equivalence checking,\nand query explanation, assess the models' proficiency in recognition, context\nawareness, semantics, and coherence, which are essential skills for SQL\nunderstanding. We generate labeled datasets from well-known workloads, and\nevaluate the latest LLMs, focusing on how query complexity and syntactic\nfeatures influence performance. Our results indicate that while GPT4 excels at\ntasks requiring recognition and context, all models struggle with deeper\nsemantic understanding and coherence, especially in query equivalence and\nperformance estimation, revealing the limitations of current LLMs in achieving\nfull SQL comprehension."
                },
                "authors": [
                    {
                        "name": "Ananya Rahaman"
                    },
                    {
                        "name": "Anny Zheng"
                    },
                    {
                        "name": "Mostafa Milani"
                    },
                    {
                        "name": "Fei Chiang"
                    },
                    {
                        "name": "Rachel Pottinger"
                    }
                ],
                "author_detail": {
                    "name": "Rachel Pottinger"
                },
                "author": "Rachel Pottinger",
                "arxiv_comment": "12 pages conference submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10676v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10676v1",
                "updated": "2024-10-14T16:18:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    18,
                    29,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T16:18:29Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    18,
                    29,
                    0,
                    288,
                    0
                ],
                "title": "Both Ears Wide Open: Towards Language-Driven Spatial Audio Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Both Ears Wide Open: Towards Language-Driven Spatial Audio Generation"
                },
                "summary": "Recently, diffusion models have achieved great success in mono-channel audio\ngeneration. However, when it comes to stereo audio generation, the soundscapes\noften have a complex scene of multiple objects and directions. Controlling\nstereo audio with spatial contexts remains challenging due to high data costs\nand unstable generative models. To the best of our knowledge, this work\nrepresents the first attempt to address these issues. We first construct a\nlarge-scale, simulation-based, and GPT-assisted dataset, BEWO-1M, with abundant\nsoundscapes and descriptions even including moving and multiple sources. Beyond\ntext modality, we have also acquired a set of images and rationally paired\nstereo audios through retrieval to advance multimodal generation. Existing\naudio generation models tend to generate rather random and indistinct spatial\naudio. To provide accurate guidance for latent diffusion models, we introduce\nthe SpatialSonic model utilizing spatial-aware encoders and azimuth state\nmatrices to reveal reasonable spatial guidance. By leveraging spatial guidance,\nour unified model not only achieves the objective of generating immersive and\ncontrollable spatial audio from text and image but also enables interactive\naudio generation during inference. Finally, under fair settings, we conduct\nsubjective and objective evaluations on simulated and real-world data to\ncompare our approach with prevailing methods. The results demonstrate the\neffectiveness of our method, highlighting its capability to generate spatial\naudio that adheres to physical rules.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, diffusion models have achieved great success in mono-channel audio\ngeneration. However, when it comes to stereo audio generation, the soundscapes\noften have a complex scene of multiple objects and directions. Controlling\nstereo audio with spatial contexts remains challenging due to high data costs\nand unstable generative models. To the best of our knowledge, this work\nrepresents the first attempt to address these issues. We first construct a\nlarge-scale, simulation-based, and GPT-assisted dataset, BEWO-1M, with abundant\nsoundscapes and descriptions even including moving and multiple sources. Beyond\ntext modality, we have also acquired a set of images and rationally paired\nstereo audios through retrieval to advance multimodal generation. Existing\naudio generation models tend to generate rather random and indistinct spatial\naudio. To provide accurate guidance for latent diffusion models, we introduce\nthe SpatialSonic model utilizing spatial-aware encoders and azimuth state\nmatrices to reveal reasonable spatial guidance. By leveraging spatial guidance,\nour unified model not only achieves the objective of generating immersive and\ncontrollable spatial audio from text and image but also enables interactive\naudio generation during inference. Finally, under fair settings, we conduct\nsubjective and objective evaluations on simulated and real-world data to\ncompare our approach with prevailing methods. The results demonstrate the\neffectiveness of our method, highlighting its capability to generate spatial\naudio that adheres to physical rules."
                },
                "authors": [
                    {
                        "name": "Peiwen Sun"
                    },
                    {
                        "name": "Sitong Cheng"
                    },
                    {
                        "name": "Xiangtai Li"
                    },
                    {
                        "name": "Zhen Ye"
                    },
                    {
                        "name": "Huadai Liu"
                    },
                    {
                        "name": "Honggang Zhang"
                    },
                    {
                        "name": "Wei Xue"
                    },
                    {
                        "name": "Yike Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yike Guo"
                },
                "author": "Yike Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10676v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10676v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10672v1",
                "updated": "2024-10-14T16:15:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    15,
                    57,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T16:15:57Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    15,
                    57,
                    0,
                    288,
                    0
                ],
                "title": "Large Language Model Evaluation via Matrix Nuclear-Norm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Evaluation via Matrix Nuclear-Norm"
                },
                "summary": "As large language models (LLMs) continue to evolve, efficient evaluation\nmetrics are vital for assessing their ability to compress information and\nreduce redundancy. While traditional metrics like Matrix Entropy offer valuable\ninsights, they are computationally intensive for large-scale models due to\ntheir \\( O(n^3) \\) time complexity with Singular Value Decomposition (SVD). To\nmitigate this issue, we introduce the Matrix Nuclear-Norm, which not only\nserves as a metric to quantify the data compression proficiency of LLM but also\nprovides a convex approximation of matrix rank to capture both predictive\ndiscriminability and diversity. By employing the \\( L_{1,2}\\text{-norm} \\) to\nfurther approximate the nuclear norm, we can effectively assess the model's\ninformation compression capabilities. This approach reduces the time complexity\nto \\( O(n^2) \\) and eliminates the need for SVD computation. Consequently, the\nMatrix Nuclear-Norm achieves speeds 8 to 24 times faster than Matrix Entropy\nfor the CEREBRAS-GPT model as sizes increase from 111M to 6.7B. This\nperformance gap becomes more pronounced with larger models, as validated in\ntests with other models like Pythia. Additionally, evaluations on benchmarks\nand model responses confirm that our proposed Matrix Nuclear-Norm is a\nreliable, scalable, and efficient tool for assessing LLMs' performance,\nstriking a balance between accuracy and computational efficiency. The code is\navailable at https://github.com/MLGroupJLU/MatrixNuclearNorm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to evolve, efficient evaluation\nmetrics are vital for assessing their ability to compress information and\nreduce redundancy. While traditional metrics like Matrix Entropy offer valuable\ninsights, they are computationally intensive for large-scale models due to\ntheir \\( O(n^3) \\) time complexity with Singular Value Decomposition (SVD). To\nmitigate this issue, we introduce the Matrix Nuclear-Norm, which not only\nserves as a metric to quantify the data compression proficiency of LLM but also\nprovides a convex approximation of matrix rank to capture both predictive\ndiscriminability and diversity. By employing the \\( L_{1,2}\\text{-norm} \\) to\nfurther approximate the nuclear norm, we can effectively assess the model's\ninformation compression capabilities. This approach reduces the time complexity\nto \\( O(n^2) \\) and eliminates the need for SVD computation. Consequently, the\nMatrix Nuclear-Norm achieves speeds 8 to 24 times faster than Matrix Entropy\nfor the CEREBRAS-GPT model as sizes increase from 111M to 6.7B. This\nperformance gap becomes more pronounced with larger models, as validated in\ntests with other models like Pythia. Additionally, evaluations on benchmarks\nand model responses confirm that our proposed Matrix Nuclear-Norm is a\nreliable, scalable, and efficient tool for assessing LLMs' performance,\nstriking a balance between accuracy and computational efficiency. The code is\navailable at https://github.com/MLGroupJLU/MatrixNuclearNorm."
                },
                "authors": [
                    {
                        "name": "Yahan Li"
                    },
                    {
                        "name": "Tingyu Xia"
                    },
                    {
                        "name": "Yi Chang"
                    },
                    {
                        "name": "Yuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Wu"
                },
                "author": "Yuan Wu",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11622v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11622v2",
                "updated": "2024-10-14T16:13:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    13,
                    37,
                    0,
                    288,
                    0
                ],
                "published": "2024-06-17T15:05:43Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    15,
                    5,
                    43,
                    0,
                    169,
                    0
                ],
                "title": "Building Knowledge-Guided Lexica to Model Cultural Variation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building Knowledge-Guided Lexica to Model Cultural Variation"
                },
                "summary": "Cultural variation exists between nations (e.g., the United States vs.\nChina), but also within regions (e.g., California vs. Texas, Los Angeles vs.\nSan Francisco). Measuring this regional cultural variation can illuminate how\nand why people think and behave differently. Historically, it has been\ndifficult to computationally model cultural variation due to a lack of training\ndata and scalability constraints. In this work, we introduce a new research\nproblem for the NLP community: How do we measure variation in cultural\nconstructs across regions using language? We then provide a scalable solution:\nbuilding knowledge-guided lexica to model cultural variation, encouraging\nfuture work at the intersection of NLP and cultural understanding. We also\nhighlight modern LLMs' failure to measure cultural variation or generate\nculturally varied language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cultural variation exists between nations (e.g., the United States vs.\nChina), but also within regions (e.g., California vs. Texas, Los Angeles vs.\nSan Francisco). Measuring this regional cultural variation can illuminate how\nand why people think and behave differently. Historically, it has been\ndifficult to computationally model cultural variation due to a lack of training\ndata and scalability constraints. In this work, we introduce a new research\nproblem for the NLP community: How do we measure variation in cultural\nconstructs across regions using language? We then provide a scalable solution:\nbuilding knowledge-guided lexica to model cultural variation, encouraging\nfuture work at the intersection of NLP and cultural understanding. We also\nhighlight modern LLMs' failure to measure cultural variation or generate\nculturally varied language."
                },
                "authors": [
                    {
                        "name": "Shreya Havaldar"
                    },
                    {
                        "name": "Salvatore Giorgi"
                    },
                    {
                        "name": "Sunny Rai"
                    },
                    {
                        "name": "Young-Min Cho"
                    },
                    {
                        "name": "Thomas Talhelm"
                    },
                    {
                        "name": "Sharath Chandra Guntuku"
                    },
                    {
                        "name": "Lyle Ungar"
                    }
                ],
                "author_detail": {
                    "name": "Lyle Ungar"
                },
                "author": "Lyle Ungar",
                "arxiv_comment": "Accepted at NAACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11622v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10665v1",
                "updated": "2024-10-14T16:11:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    11,
                    4,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T16:11:04Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    11,
                    4,
                    0,
                    288,
                    0
                ],
                "title": "Double Jeopardy and Climate Impact in the Use of Large Language Models:\n  Socio-economic Disparities and Reduced Utility for Non-English Speakers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Double Jeopardy and Climate Impact in the Use of Large Language Models:\n  Socio-economic Disparities and Reduced Utility for Non-English Speakers"
                },
                "summary": "Artificial Intelligence (AI), particularly large language models (LLMs),\nholds the potential to bridge language and information gaps, which can benefit\nthe economies of developing nations. However, our analysis of FLORES-200,\nFLORES+, Ethnologue, and World Development Indicators data reveals that these\nbenefits largely favor English speakers. Speakers of languages in low-income\nand lower-middle-income countries face higher costs when using OpenAI's GPT\nmodels via APIs because of how the system processes the input -- tokenization.\nAround 1.5 billion people, speaking languages primarily from\nlower-middle-income countries, could incur costs that are 4 to 6 times higher\nthan those faced by English speakers. Disparities in LLM performance are\nsignificant, and tokenization in models priced per token amplifies inequalities\nin access, cost, and utility. Moreover, using the quality of translation tasks\nas a proxy measure, we show that LLMs perform poorly in low-resource languages,\npresenting a ``double jeopardy\" of higher costs and poor performance for these\nusers. We also discuss the direct impact of fragmentation in tokenizing\nlow-resource languages on climate. This underscores the need for fairer\nalgorithm development to benefit all linguistic groups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence (AI), particularly large language models (LLMs),\nholds the potential to bridge language and information gaps, which can benefit\nthe economies of developing nations. However, our analysis of FLORES-200,\nFLORES+, Ethnologue, and World Development Indicators data reveals that these\nbenefits largely favor English speakers. Speakers of languages in low-income\nand lower-middle-income countries face higher costs when using OpenAI's GPT\nmodels via APIs because of how the system processes the input -- tokenization.\nAround 1.5 billion people, speaking languages primarily from\nlower-middle-income countries, could incur costs that are 4 to 6 times higher\nthan those faced by English speakers. Disparities in LLM performance are\nsignificant, and tokenization in models priced per token amplifies inequalities\nin access, cost, and utility. Moreover, using the quality of translation tasks\nas a proxy measure, we show that LLMs perform poorly in low-resource languages,\npresenting a ``double jeopardy\" of higher costs and poor performance for these\nusers. We also discuss the direct impact of fragmentation in tokenizing\nlow-resource languages on climate. This underscores the need for fairer\nalgorithm development to benefit all linguistic groups."
                },
                "authors": [
                    {
                        "name": "Aivin V. Solatorio"
                    },
                    {
                        "name": "Gabriel Stefanini Vicente"
                    },
                    {
                        "name": "Holly Krambeck"
                    },
                    {
                        "name": "Olivier Dupriez"
                    }
                ],
                "author_detail": {
                    "name": "Olivier Dupriez"
                },
                "author": "Olivier Dupriez",
                "arxiv_comment": "Project GitHub repository at\n  https://github.com/worldbank/double-jeopardy-in-llms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10659v1",
                "updated": "2024-10-14T16:06:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    6,
                    59,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T16:06:59Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    6,
                    59,
                    0,
                    288,
                    0
                ],
                "title": "PCF-Lift: Panoptic Lifting by Probabilistic Contrastive Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PCF-Lift: Panoptic Lifting by Probabilistic Contrastive Fusion"
                },
                "summary": "Panoptic lifting is an effective technique to address the 3D panoptic\nsegmentation task by unprojecting 2D panoptic segmentations from multi-views to\n3D scene. However, the quality of its results largely depends on the 2D\nsegmentations, which could be noisy and error-prone, so its performance often\ndrops significantly for complex scenes. In this work, we design a new pipeline\ncoined PCF-Lift based on our Probabilis-tic Contrastive Fusion (PCF) to learn\nand embed probabilistic features throughout our pipeline to actively consider\ninaccurate segmentations and inconsistent instance IDs. Technical-wise, we\nfirst model the probabilistic feature embeddings through multivariate Gaussian\ndistributions. To fuse the probabilistic features, we incorporate the\nprobability product kernel into the contrastive loss formulation and design a\ncross-view constraint to enhance the feature consistency across different\nviews. For the inference, we introduce a new probabilistic clustering method to\neffectively associate prototype features with the underlying 3D object\ninstances for the generation of consistent panoptic segmentation results.\nFurther, we provide a theoretical analysis to justify the superiority of the\nproposed probabilistic solution. By conducting extensive experiments, our\nPCF-lift not only significantly outperforms the state-of-the-art methods on\nwidely used benchmarks including the ScanNet dataset and the challenging Messy\nRoom dataset (4.4% improvement of scene-level PQ), but also demonstrates strong\nrobustness when incorporating various 2D segmentation models or different\nlevels of hand-crafted noise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Panoptic lifting is an effective technique to address the 3D panoptic\nsegmentation task by unprojecting 2D panoptic segmentations from multi-views to\n3D scene. However, the quality of its results largely depends on the 2D\nsegmentations, which could be noisy and error-prone, so its performance often\ndrops significantly for complex scenes. In this work, we design a new pipeline\ncoined PCF-Lift based on our Probabilis-tic Contrastive Fusion (PCF) to learn\nand embed probabilistic features throughout our pipeline to actively consider\ninaccurate segmentations and inconsistent instance IDs. Technical-wise, we\nfirst model the probabilistic feature embeddings through multivariate Gaussian\ndistributions. To fuse the probabilistic features, we incorporate the\nprobability product kernel into the contrastive loss formulation and design a\ncross-view constraint to enhance the feature consistency across different\nviews. For the inference, we introduce a new probabilistic clustering method to\neffectively associate prototype features with the underlying 3D object\ninstances for the generation of consistent panoptic segmentation results.\nFurther, we provide a theoretical analysis to justify the superiority of the\nproposed probabilistic solution. By conducting extensive experiments, our\nPCF-lift not only significantly outperforms the state-of-the-art methods on\nwidely used benchmarks including the ScanNet dataset and the challenging Messy\nRoom dataset (4.4% improvement of scene-level PQ), but also demonstrates strong\nrobustness when incorporating various 2D segmentation models or different\nlevels of hand-crafted noise."
                },
                "authors": [
                    {
                        "name": "Runsong Zhu"
                    },
                    {
                        "name": "Shi Qiu"
                    },
                    {
                        "name": "Qianyi Wu"
                    },
                    {
                        "name": "Ka-Hei Hui"
                    },
                    {
                        "name": "Pheng-Ann Heng"
                    },
                    {
                        "name": "Chi-Wing Fu"
                    }
                ],
                "author_detail": {
                    "name": "Chi-Wing Fu"
                },
                "author": "Chi-Wing Fu",
                "arxiv_comment": "ECCV 2024. The code is publicly available at\n  https://github.com/Runsong123/PCF-Lift",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10657v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10657v1",
                "updated": "2024-10-14T16:06:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    6,
                    35,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T16:06:35Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    6,
                    35,
                    0,
                    288,
                    0
                ],
                "title": "AutoTurb: Using Large Language Models for Automatic Algebraic Model\n  Discovery of Turbulence Closure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoTurb: Using Large Language Models for Automatic Algebraic Model\n  Discovery of Turbulence Closure"
                },
                "summary": "Symbolic regression (SR) methods have been extensively investigated to\nexplore explicit algebraic Reynolds stress models (EARSM) for turbulence\nclosure of Reynolds-averaged Navier-Stokes (RANS) equations. The deduced EARSM\ncan be readily implemented in existing computational fluid dynamic (CFD) codes\nand promotes the identification of physically interpretable turbulence models.\nThe existing SR methods, such as genetic programming, sparse regression, or\nartificial neural networks, require user-defined functional operators, a\nlibrary of candidates, or complex optimization algorithms. In this work, a\nnovel framework using LLMs to automatically discover algebraic expressions for\ncorrecting the RSM is proposed. The direct observation of Reynolds stress and\nthe indirect output of the CFD simulation are both involved in the training\nprocess to guarantee data consistency and avoid numerical stiffness.\nConstraints of functional complexity and convergence are supplementally imposed\nin the objective function on account of the tremendous flexibility of LLMs. The\nevolutionary search is employed for global optimization. The proposed method is\nperformed for separated flow over periodic hills at Re = 10,595. The\ngeneralizability of the discovered model is verified on a set of 2D turbulent\nseparated flow configurations with different Reynolds numbers and geometries.\nIt is demonstrated that the corrective RANS can improve the prediction for both\nthe Reynolds stress and mean velocity fields. Compared with algebraic models\ndiscovered by other works, the discovered model performs better in accuracy and\ngeneralization capability. The proposed approach provides a promising paradigm\nfor using LLMs to improve turbulence modeling for a given class of flows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symbolic regression (SR) methods have been extensively investigated to\nexplore explicit algebraic Reynolds stress models (EARSM) for turbulence\nclosure of Reynolds-averaged Navier-Stokes (RANS) equations. The deduced EARSM\ncan be readily implemented in existing computational fluid dynamic (CFD) codes\nand promotes the identification of physically interpretable turbulence models.\nThe existing SR methods, such as genetic programming, sparse regression, or\nartificial neural networks, require user-defined functional operators, a\nlibrary of candidates, or complex optimization algorithms. In this work, a\nnovel framework using LLMs to automatically discover algebraic expressions for\ncorrecting the RSM is proposed. The direct observation of Reynolds stress and\nthe indirect output of the CFD simulation are both involved in the training\nprocess to guarantee data consistency and avoid numerical stiffness.\nConstraints of functional complexity and convergence are supplementally imposed\nin the objective function on account of the tremendous flexibility of LLMs. The\nevolutionary search is employed for global optimization. The proposed method is\nperformed for separated flow over periodic hills at Re = 10,595. The\ngeneralizability of the discovered model is verified on a set of 2D turbulent\nseparated flow configurations with different Reynolds numbers and geometries.\nIt is demonstrated that the corrective RANS can improve the prediction for both\nthe Reynolds stress and mean velocity fields. Compared with algebraic models\ndiscovered by other works, the discovered model performs better in accuracy and\ngeneralization capability. The proposed approach provides a promising paradigm\nfor using LLMs to improve turbulence modeling for a given class of flows."
                },
                "authors": [
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Kefeng Zheng"
                    },
                    {
                        "name": "Fei Liu"
                    },
                    {
                        "name": "Qingfu Zhang"
                    },
                    {
                        "name": "Zhenkun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhenkun Wang"
                },
                "author": "Zhenkun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10657v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10657v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10650v1",
                "updated": "2024-10-14T16:01:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    1,
                    1,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T16:01:01Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    1,
                    1,
                    0,
                    288,
                    0
                ],
                "title": "Generative AI and Its Impact on Personalized Intelligent Tutoring\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI and Its Impact on Personalized Intelligent Tutoring\n  Systems"
                },
                "summary": "Generative Artificial Intelligence (AI) is revolutionizing educational\ntechnology by enabling highly personalized and adaptive learning environments\nwithin Intelligent Tutoring Systems (ITS). This report delves into the\nintegration of Generative AI, particularly large language models (LLMs) like\nGPT-4, into ITS to enhance personalized education through dynamic content\ngeneration, real-time feedback, and adaptive learning pathways. We explore key\napplications such as automated question generation, customized feedback\nmechanisms, and interactive dialogue systems that respond to individual learner\nneeds. The report also addresses significant challenges, including ensuring\npedagogical accuracy, mitigating inherent biases in AI models, and maintaining\nlearner engagement. Future directions highlight the potential advancements in\nmultimodal AI integration, emotional intelligence in tutoring systems, and the\nethical implications of AI-driven education. By synthesizing current research\nand practical implementations, this report underscores the transformative\npotential of Generative AI in creating more effective, equitable, and engaging\neducational experiences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Artificial Intelligence (AI) is revolutionizing educational\ntechnology by enabling highly personalized and adaptive learning environments\nwithin Intelligent Tutoring Systems (ITS). This report delves into the\nintegration of Generative AI, particularly large language models (LLMs) like\nGPT-4, into ITS to enhance personalized education through dynamic content\ngeneration, real-time feedback, and adaptive learning pathways. We explore key\napplications such as automated question generation, customized feedback\nmechanisms, and interactive dialogue systems that respond to individual learner\nneeds. The report also addresses significant challenges, including ensuring\npedagogical accuracy, mitigating inherent biases in AI models, and maintaining\nlearner engagement. Future directions highlight the potential advancements in\nmultimodal AI integration, emotional intelligence in tutoring systems, and the\nethical implications of AI-driven education. By synthesizing current research\nand practical implementations, this report underscores the transformative\npotential of Generative AI in creating more effective, equitable, and engaging\neducational experiences."
                },
                "authors": [
                    {
                        "name": "Subhankar Maity"
                    },
                    {
                        "name": "Aniket Deroy"
                    }
                ],
                "author_detail": {
                    "name": "Aniket Deroy"
                },
                "author": "Aniket Deroy",
                "arxiv_comment": "Scientific Report (Under Review)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10649v1",
                "updated": "2024-10-14T16:00:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    0,
                    20,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T16:00:20Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    0,
                    20,
                    0,
                    288,
                    0
                ],
                "title": "Vecchia Gaussian Processes: Probabilistic Properties, Minimax Rates and\n  Methodological Developments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vecchia Gaussian Processes: Probabilistic Properties, Minimax Rates and\n  Methodological Developments"
                },
                "summary": "Gaussian Processes (GPs) are widely used to model dependency in spatial\nstatistics and machine learning, yet the exact computation suffers an\nintractable time complexity of $O(n^3)$. Vecchia approximation allows scalable\nBayesian inference of GPs in $O(n)$ time by introducing sparsity in the spatial\ndependency structure that is characterized by a directed acyclic graph (DAG).\nDespite the popularity in practice, it is still unclear how to choose the DAG\nstructure and there are still no theoretical guarantees in nonparametric\nsettings. In this paper, we systematically study the Vecchia GPs as standalone\nstochastic processes and uncover important probabilistic properties and\nstatistical results in methodology and theory. For probabilistic properties, we\nprove that the conditional distributions of the Mat\\'{e}rn GPs, as well as the\nVecchia approximations of the Mat\\'{e}rn GPs, can be characterized by\npolynomials. This allows us to prove a series of results regarding the small\nball probabilities and RKHSs of Vecchia GPs. For statistical methodology, we\nprovide a principled guideline to choose parent sets as norming sets with fixed\ncardinality and provide detailed algorithms following such guidelines. For\nstatistical theory, we prove posterior contraction rates for applying Vecchia\nGPs to regression problems, where minimax optimality is achieved by optimally\ntuned GPs via either oracle rescaling or hierarchical Bayesian methods. Our\ntheory and methodology are demonstrated with numerical studies, where we also\nprovide efficient implementation of our methods in C++ with R interfaces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian Processes (GPs) are widely used to model dependency in spatial\nstatistics and machine learning, yet the exact computation suffers an\nintractable time complexity of $O(n^3)$. Vecchia approximation allows scalable\nBayesian inference of GPs in $O(n)$ time by introducing sparsity in the spatial\ndependency structure that is characterized by a directed acyclic graph (DAG).\nDespite the popularity in practice, it is still unclear how to choose the DAG\nstructure and there are still no theoretical guarantees in nonparametric\nsettings. In this paper, we systematically study the Vecchia GPs as standalone\nstochastic processes and uncover important probabilistic properties and\nstatistical results in methodology and theory. For probabilistic properties, we\nprove that the conditional distributions of the Mat\\'{e}rn GPs, as well as the\nVecchia approximations of the Mat\\'{e}rn GPs, can be characterized by\npolynomials. This allows us to prove a series of results regarding the small\nball probabilities and RKHSs of Vecchia GPs. For statistical methodology, we\nprovide a principled guideline to choose parent sets as norming sets with fixed\ncardinality and provide detailed algorithms following such guidelines. For\nstatistical theory, we prove posterior contraction rates for applying Vecchia\nGPs to regression problems, where minimax optimality is achieved by optimally\ntuned GPs via either oracle rescaling or hierarchical Bayesian methods. Our\ntheory and methodology are demonstrated with numerical studies, where we also\nprovide efficient implementation of our methods in C++ with R interfaces."
                },
                "authors": [
                    {
                        "name": "Botond Szabo"
                    },
                    {
                        "name": "Yichen Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yichen Zhu"
                },
                "author": "Yichen Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62G08, 60G15, 62H11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10648v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10648v1",
                "updated": "2024-10-14T15:59:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    59,
                    16,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T15:59:16Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    59,
                    16,
                    0,
                    288,
                    0
                ],
                "title": "A Simple Baseline for Predicting Events with Auto-Regressive Tabular\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple Baseline for Predicting Events with Auto-Regressive Tabular\n  Transformers"
                },
                "summary": "Many real-world applications of tabular data involve using historic events to\npredict properties of new ones, for example whether a credit card transaction\nis fraudulent or what rating a customer will assign a product on a retail\nplatform. Existing approaches to event prediction include costly, brittle, and\napplication-dependent techniques such as time-aware positional embeddings,\nlearned row and field encodings, and oversampling methods for addressing class\nimbalance. Moreover, these approaches often assume specific use-cases, for\nexample that we know the labels of all historic events or that we only predict\na pre-specified label and not the data's features themselves. In this work, we\npropose a simple but flexible baseline using standard autoregressive LLM-style\ntransformers with elementary positional embeddings and a causal language\nmodeling objective. Our baseline outperforms existing approaches across popular\ndatasets and can be employed for various use-cases. We demonstrate that the\nsame model can predict labels, impute missing values, or model event sequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many real-world applications of tabular data involve using historic events to\npredict properties of new ones, for example whether a credit card transaction\nis fraudulent or what rating a customer will assign a product on a retail\nplatform. Existing approaches to event prediction include costly, brittle, and\napplication-dependent techniques such as time-aware positional embeddings,\nlearned row and field encodings, and oversampling methods for addressing class\nimbalance. Moreover, these approaches often assume specific use-cases, for\nexample that we know the labels of all historic events or that we only predict\na pre-specified label and not the data's features themselves. In this work, we\npropose a simple but flexible baseline using standard autoregressive LLM-style\ntransformers with elementary positional embeddings and a causal language\nmodeling objective. Our baseline outperforms existing approaches across popular\ndatasets and can be employed for various use-cases. We demonstrate that the\nsame model can predict labels, impute missing values, or model event sequences."
                },
                "authors": [
                    {
                        "name": "Alex Stein"
                    },
                    {
                        "name": "Samuel Sharpe"
                    },
                    {
                        "name": "Doron Bergman"
                    },
                    {
                        "name": "Senthil Kumar"
                    },
                    {
                        "name": "Bayan Bruss"
                    },
                    {
                        "name": "John Dickerson"
                    },
                    {
                        "name": "Tom Goldstein"
                    },
                    {
                        "name": "Micah Goldblum"
                    }
                ],
                "author_detail": {
                    "name": "Micah Goldblum"
                },
                "author": "Micah Goldblum",
                "arxiv_comment": "10 pages, 6 pages of references+appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10648v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10648v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10647v1",
                "updated": "2024-10-14T15:58:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    58,
                    39,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T15:58:39Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    58,
                    39,
                    0,
                    288,
                    0
                ],
                "title": "Statistical inference of partially linear time-varying coefficients\n  spatial autoregressive panel data model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical inference of partially linear time-varying coefficients\n  spatial autoregressive panel data model"
                },
                "summary": "This paper investigates a partially linear spatial autoregressive panel data\nmodel that incorporates fixed effects, constant and time-varying regression\ncoefficients, and a time-varying spatial lag coefficient. A two-stage least\nsquares estimation method based on profile local linear dummy variables\n(2SLS-PLLDV) is proposed to estimate both constant and time-varying\ncoefficients without the need for first differencing. The asymptotic properties\nof the estimator are derived under certain conditions. Furthermore, a\nresidual-based goodness-of-fit test is constructed for the model, and a\nresidual-based bootstrap method is used to obtain p-values. Simulation studies\nshow the good performance of the proposed method in various scenarios. The\nChinese provincial carbon emission data set is analyzed for illustration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates a partially linear spatial autoregressive panel data\nmodel that incorporates fixed effects, constant and time-varying regression\ncoefficients, and a time-varying spatial lag coefficient. A two-stage least\nsquares estimation method based on profile local linear dummy variables\n(2SLS-PLLDV) is proposed to estimate both constant and time-varying\ncoefficients without the need for first differencing. The asymptotic properties\nof the estimator are derived under certain conditions. Furthermore, a\nresidual-based goodness-of-fit test is constructed for the model, and a\nresidual-based bootstrap method is used to obtain p-values. Simulation studies\nshow the good performance of the proposed method in various scenarios. The\nChinese provincial carbon emission data set is analyzed for illustration."
                },
                "authors": [
                    {
                        "name": "Lingling Tian"
                    },
                    {
                        "name": "Chuanhua Wei"
                    },
                    {
                        "name": "Mixia Wu"
                    }
                ],
                "author_detail": {
                    "name": "Mixia Wu"
                },
                "author": "Mixia Wu",
                "arxiv_comment": "26 pages, 3 figures, codes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62F40",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10645v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10645v1",
                "updated": "2024-10-14T15:54:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    54,
                    33,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T15:54:33Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    54,
                    33,
                    0,
                    288,
                    0
                ],
                "title": "Macroscopic Quantum States and Universal Correlations in a\n  Disorder-Order Interface Propagating over a 1D Ground State",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Macroscopic Quantum States and Universal Correlations in a\n  Disorder-Order Interface Propagating over a 1D Ground State"
                },
                "summary": "We consider translationally invariant quantum spin-$\\frac{1}{2}$ chains with\nlocal interactions and a discrete symmetry that is spontaneously broken at zero\ntemperature. We envision experimenters switching off the couplings between two\nparts of the system and preparing them in independent equilibrium states. One\nside of the chain settles into a symmetry-breaking ground state. When the\ncouplings are switched back on, time evolution ensues. We argue that in\nintegrable systems the front separating the ordered region recedes at the\nmaximal velocity of quasiparticle excitations over the ground state. We infer\nthat, generically, the order parameters should vary on a subdiffusive scale of\norder $t^{1/3}$, where $t$ is time, and their fluctuations should exhibit the\nsame scaling. Thus, the interfacial region exhibits full range correlations,\nindicating that it cannot be decomposed into nearly uncorrelated subsystems.\nUsing the transverse-field Ising chain as a case study, we demonstrate that all\norder parameters follow the same universal scaling functions. Through an\nanalysis of the skew information, we uncover that the breakdown of cluster\ndecomposition has a quantum contribution: each subsystem within the interfacial\nregion, with extent comparable to the region, exists in a macroscopic quantum\nstate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider translationally invariant quantum spin-$\\frac{1}{2}$ chains with\nlocal interactions and a discrete symmetry that is spontaneously broken at zero\ntemperature. We envision experimenters switching off the couplings between two\nparts of the system and preparing them in independent equilibrium states. One\nside of the chain settles into a symmetry-breaking ground state. When the\ncouplings are switched back on, time evolution ensues. We argue that in\nintegrable systems the front separating the ordered region recedes at the\nmaximal velocity of quasiparticle excitations over the ground state. We infer\nthat, generically, the order parameters should vary on a subdiffusive scale of\norder $t^{1/3}$, where $t$ is time, and their fluctuations should exhibit the\nsame scaling. Thus, the interfacial region exhibits full range correlations,\nindicating that it cannot be decomposed into nearly uncorrelated subsystems.\nUsing the transverse-field Ising chain as a case study, we demonstrate that all\norder parameters follow the same universal scaling functions. Through an\nanalysis of the skew information, we uncover that the breakdown of cluster\ndecomposition has a quantum contribution: each subsystem within the interfacial\nregion, with extent comparable to the region, exists in a macroscopic quantum\nstate."
                },
                "authors": [
                    {
                        "name": "Vanja Mari"
                    },
                    {
                        "name": "Florent Ferro"
                    },
                    {
                        "name": "Maurizio Fagotti"
                    }
                ],
                "author_detail": {
                    "name": "Maurizio Fagotti"
                },
                "author": "Maurizio Fagotti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10645v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10644v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10644v1",
                "updated": "2024-10-14T15:53:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    53,
                    30,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T15:53:30Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    53,
                    30,
                    0,
                    288,
                    0
                ],
                "title": "Functional Flexibility in Generative AI Interfaces: Text Editing with\n  LLMs through Conversations, Toolbars, and Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Functional Flexibility in Generative AI Interfaces: Text Editing with\n  LLMs through Conversations, Toolbars, and Prompts"
                },
                "summary": "Prompting-based user interfaces (UIs) shift the task of defining and\naccessing relevant functions from developers to users. However, how UIs shape\nthis flexibility has not yet been investigated explicitly. We explored\ninteraction with Large Language Models (LLMs) over four years, before and after\nthe rise of general-purpose LLMs: (1) Our survey (N=121) elicited how users\nenvision to delegate writing tasks to AI. This informed a conversational UI\ndesign. (2) A user study (N=10) revealed that people regressed to using short\ncommand-like prompts. (3) When providing these directly as shortcuts in a\ntoolbar UI, in addition to prompting, users in our second study (N=12)\ndynamically switched between specified and flexible AI functions. We discuss\nfunctional flexibility as a new theoretical construct and thinking tool. Our\nwork highlights the value of moving beyond conversational UIs, by considering\nhow different UIs shape users' access to the functional space of generative AI\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting-based user interfaces (UIs) shift the task of defining and\naccessing relevant functions from developers to users. However, how UIs shape\nthis flexibility has not yet been investigated explicitly. We explored\ninteraction with Large Language Models (LLMs) over four years, before and after\nthe rise of general-purpose LLMs: (1) Our survey (N=121) elicited how users\nenvision to delegate writing tasks to AI. This informed a conversational UI\ndesign. (2) A user study (N=10) revealed that people regressed to using short\ncommand-like prompts. (3) When providing these directly as shortcuts in a\ntoolbar UI, in addition to prompting, users in our second study (N=12)\ndynamically switched between specified and flexible AI functions. We discuss\nfunctional flexibility as a new theoretical construct and thinking tool. Our\nwork highlights the value of moving beyond conversational UIs, by considering\nhow different UIs shape users' access to the functional space of generative AI\nmodels."
                },
                "authors": [
                    {
                        "name": "Florian Lehmann"
                    },
                    {
                        "name": "Daniel Buschek"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Buschek"
                },
                "author": "Daniel Buschek",
                "arxiv_comment": "Just submitted. This is the author's version of the work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10644v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10644v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.04701v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.04701v2",
                "updated": "2024-10-14T15:52:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    52,
                    31,
                    0,
                    288,
                    0
                ],
                "published": "2023-05-08T13:32:41Z",
                "published_parsed": [
                    2023,
                    5,
                    8,
                    13,
                    32,
                    41,
                    0,
                    128,
                    0
                ],
                "title": "Differentially Private Attention Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentially Private Attention Computation"
                },
                "summary": "Large language models (LLMs), especially those based on the Transformer\narchitecture, have had a profound impact on various aspects of daily life, such\nas natural language processing, content generation, research methodologies, and\nmore. Nevertheless, a crucial concern regarding the inference results of large\nlanguage models is the issue of security and privacy. Given that large language\nmodels can generate results that may leak sensitive confidential or copyright\ninformation in many scenarios, it is crucial to compute the attention matrix\nwith provable privacy guarantees, as attention is all you need.\n  In this work, we propose a novel and efficient algorithm for approximating\nthe attention matrix while providing differential privacy (DP) guarantees. To\nachieve this, we build on recent advancements in fast attention computation and\ndifferentially private matrix publishing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), especially those based on the Transformer\narchitecture, have had a profound impact on various aspects of daily life, such\nas natural language processing, content generation, research methodologies, and\nmore. Nevertheless, a crucial concern regarding the inference results of large\nlanguage models is the issue of security and privacy. Given that large language\nmodels can generate results that may leak sensitive confidential or copyright\ninformation in many scenarios, it is crucial to compute the attention matrix\nwith provable privacy guarantees, as attention is all you need.\n  In this work, we propose a novel and efficient algorithm for approximating\nthe attention matrix while providing differential privacy (DP) guarantees. To\nachieve this, we build on recent advancements in fast attention computation and\ndifferentially private matrix publishing."
                },
                "authors": [
                    {
                        "name": "Yeqi Gao"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Xin Yang"
                    },
                    {
                        "name": "Yufa Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yufa Zhou"
                },
                "author": "Yufa Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.04701v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.04701v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10637v1",
                "updated": "2024-10-14T15:49:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    49,
                    27,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T15:49:27Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    49,
                    27,
                    0,
                    288,
                    0
                ],
                "title": "High-Dimensional Differential Parameter Inference in Exponential Family\n  using Time Score Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Dimensional Differential Parameter Inference in Exponential Family\n  using Time Score Matching"
                },
                "summary": "This paper addresses differential inference in time-varying parametric\nprobabilistic models, like graphical models with changing structures. Instead\nof estimating a high-dimensional model at each time and inferring changes\nlater, we directly learn the differential parameter, i.e., the time derivative\nof the parameter. The main idea is treating the time score function of an\nexponential family model as a linear model of the differential parameter for\ndirect estimation. We use time score matching to estimate parameter\nderivatives. We prove the consistency of a regularized score matching objective\nand demonstrate the finite-sample normality of a debiased estimator in\nhigh-dimensional settings. Our methodology effectively infers differential\nstructures in high-dimensional graphical models, verified on simulated and\nreal-world datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses differential inference in time-varying parametric\nprobabilistic models, like graphical models with changing structures. Instead\nof estimating a high-dimensional model at each time and inferring changes\nlater, we directly learn the differential parameter, i.e., the time derivative\nof the parameter. The main idea is treating the time score function of an\nexponential family model as a linear model of the differential parameter for\ndirect estimation. We use time score matching to estimate parameter\nderivatives. We prove the consistency of a regularized score matching objective\nand demonstrate the finite-sample normality of a debiased estimator in\nhigh-dimensional settings. Our methodology effectively infers differential\nstructures in high-dimensional graphical models, verified on simulated and\nreal-world datasets."
                },
                "authors": [
                    {
                        "name": "Daniel J. Williams"
                    },
                    {
                        "name": "Leyang Wang"
                    },
                    {
                        "name": "Qizhen Ying"
                    },
                    {
                        "name": "Song Liu"
                    },
                    {
                        "name": "Mladen Kolar"
                    }
                ],
                "author_detail": {
                    "name": "Mladen Kolar"
                },
                "author": "Mladen Kolar",
                "arxiv_comment": "Daniel J. Williams and Leyang Wang contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10633v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10633v1",
                "updated": "2024-10-14T15:44:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    44,
                    41,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T15:44:41Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    44,
                    41,
                    0,
                    288,
                    0
                ],
                "title": "Missing data imputation using a truncated infinite factor model with\n  application to metabolomics data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Missing data imputation using a truncated infinite factor model with\n  application to metabolomics data"
                },
                "summary": "In metabolomics, the study of small molecules in biological samples, data are\noften acquired through mass spectrometry. The resulting data contain highly\ncorrelated variables, typically with a larger number of variables than\nobservations. Missing data are prevalent, and imputation is critical as data\nacquisition can be difficult and expensive, and many analysis methods\nnecessitate complete data. In such data, missing at random (MAR) missingness\noccurs due to acquisition or processing error, while missing not at random\n(MNAR) missingness occurs when true values lie below the threshold for\ndetection. Existing imputation methods generally assume one missingness type,\nor impute values outside the physical constraints of the data, which lack\nutility. A truncated factor analysis model with an infinite number of factors\n(tIFA) is proposed to facilitate imputation in metabolomics data, in a\nstatistically and physically principled manner. Truncated distributional\nassumptions underpin tIFA, ensuring cognisance of the data's physical\nconstraints when imputing. Further, tIFA allows for both MAR and MNAR\nmissingness, and a Bayesian inferential approach provides uncertainty\nquantification for imputed values and missingness types. The infinite factor\nmodel parsimoniously models the high-dimensional, multicollinear data, with\nnonparametric shrinkage priors obviating the need for model selection tools to\ninfer the number of latent factors. A simulation study is performed to assess\nthe performance of tIFA and an application to a urinary metabolomics dataset\nresults in a full dataset with practically useful imputed values, and\nassociated uncertainty, ready for use in metabolomics analyses. Open-source R\ncode accompanies tIFA, facilitating its widespread use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In metabolomics, the study of small molecules in biological samples, data are\noften acquired through mass spectrometry. The resulting data contain highly\ncorrelated variables, typically with a larger number of variables than\nobservations. Missing data are prevalent, and imputation is critical as data\nacquisition can be difficult and expensive, and many analysis methods\nnecessitate complete data. In such data, missing at random (MAR) missingness\noccurs due to acquisition or processing error, while missing not at random\n(MNAR) missingness occurs when true values lie below the threshold for\ndetection. Existing imputation methods generally assume one missingness type,\nor impute values outside the physical constraints of the data, which lack\nutility. A truncated factor analysis model with an infinite number of factors\n(tIFA) is proposed to facilitate imputation in metabolomics data, in a\nstatistically and physically principled manner. Truncated distributional\nassumptions underpin tIFA, ensuring cognisance of the data's physical\nconstraints when imputing. Further, tIFA allows for both MAR and MNAR\nmissingness, and a Bayesian inferential approach provides uncertainty\nquantification for imputed values and missingness types. The infinite factor\nmodel parsimoniously models the high-dimensional, multicollinear data, with\nnonparametric shrinkage priors obviating the need for model selection tools to\ninfer the number of latent factors. A simulation study is performed to assess\nthe performance of tIFA and an application to a urinary metabolomics dataset\nresults in a full dataset with practically useful imputed values, and\nassociated uncertainty, ready for use in metabolomics analyses. Open-source R\ncode accompanies tIFA, facilitating its widespread use."
                },
                "authors": [
                    {
                        "name": "Kate Finucane"
                    },
                    {
                        "name": "Lorraine Brennan"
                    },
                    {
                        "name": "Isobel Claire Gormley"
                    }
                ],
                "author_detail": {
                    "name": "Isobel Claire Gormley"
                },
                "author": "Isobel Claire Gormley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10633v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10633v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14504v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14504v2",
                "updated": "2024-10-14T15:39:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    39,
                    36,
                    0,
                    288,
                    0
                ],
                "published": "2024-06-20T17:06:58Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    17,
                    6,
                    58,
                    3,
                    172,
                    0
                ],
                "title": "Translating Across Cultures: LLMs for Intralingual Cultural Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Translating Across Cultures: LLMs for Intralingual Cultural Adaptation"
                },
                "summary": "LLMs are increasingly being deployed for multilingual applications and have\ndemonstrated impressive translation capabilities between several low and\nhigh-resource languages. An aspect of translation that often gets overlooked is\nthat of cultural adaptation, or modifying source culture references to suit the\ntarget culture. While specialized translation models still outperform LLMs on\nthe machine translation task when viewed from the lens of correctness, they are\nnot sensitive to cultural differences often requiring manual correction. LLMs\non the other hand have a rich reservoir of cultural knowledge embedded within\nits parameters that can be potentially exploited for such applications. In this\npaper, we define the task of cultural adaptation and create an evaluation\nframework to evaluate the performance of modern LLMs for cultural adaptation\nand analyze their cross-cultural knowledge while connecting related concepts\nacross different cultures. We also analyze possible issues with automatic\nadaptation. We hope that this task will offer more insight into the cultural\nunderstanding of LLMs and their creativity in cross-cultural scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are increasingly being deployed for multilingual applications and have\ndemonstrated impressive translation capabilities between several low and\nhigh-resource languages. An aspect of translation that often gets overlooked is\nthat of cultural adaptation, or modifying source culture references to suit the\ntarget culture. While specialized translation models still outperform LLMs on\nthe machine translation task when viewed from the lens of correctness, they are\nnot sensitive to cultural differences often requiring manual correction. LLMs\non the other hand have a rich reservoir of cultural knowledge embedded within\nits parameters that can be potentially exploited for such applications. In this\npaper, we define the task of cultural adaptation and create an evaluation\nframework to evaluate the performance of modern LLMs for cultural adaptation\nand analyze their cross-cultural knowledge while connecting related concepts\nacross different cultures. We also analyze possible issues with automatic\nadaptation. We hope that this task will offer more insight into the cultural\nunderstanding of LLMs and their creativity in cross-cultural scenarios."
                },
                "authors": [
                    {
                        "name": "Pushpdeep Singh"
                    },
                    {
                        "name": "Mayur Patidar"
                    },
                    {
                        "name": "Lovekesh Vig"
                    }
                ],
                "author_detail": {
                    "name": "Lovekesh Vig"
                },
                "author": "Lovekesh Vig",
                "arxiv_comment": "Accepted to CoNLL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14504v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14504v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10630v1",
                "updated": "2024-10-14T15:38:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    38,
                    56,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T15:38:56Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    38,
                    56,
                    0,
                    288,
                    0
                ],
                "title": "Thinking LLMs: General Instruction Following with Thought Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thinking LLMs: General Instruction Following with Thought Generation"
                },
                "summary": "LLMs are typically trained to answer user questions or follow instructions\nsimilarly to how human experts respond. However, in the standard alignment\nframework they lack the basic ability of explicit thinking before answering.\nThinking is important for complex questions that require reasoning and planning\n-- but can be applied to any task. We propose a training method for equipping\nexisting LLMs with such thinking abilities for general instruction following\nwithout use of additional human data. We achieve this by an iterative search\nand optimization procedure that explores the space of possible thought\ngenerations, allowing the model to learn how to think without direct\nsupervision. For each instruction, the thought candidates are scored using a\njudge model to evaluate their responses only, and then optimized via preference\noptimization. We show that this procedure leads to superior performance on\nAlpacaEval and Arena-Hard, and shows gains from thinking on non-reasoning\ncategories such as marketing, health and general knowledge, in addition to more\ntraditional reasoning & problem-solving tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are typically trained to answer user questions or follow instructions\nsimilarly to how human experts respond. However, in the standard alignment\nframework they lack the basic ability of explicit thinking before answering.\nThinking is important for complex questions that require reasoning and planning\n-- but can be applied to any task. We propose a training method for equipping\nexisting LLMs with such thinking abilities for general instruction following\nwithout use of additional human data. We achieve this by an iterative search\nand optimization procedure that explores the space of possible thought\ngenerations, allowing the model to learn how to think without direct\nsupervision. For each instruction, the thought candidates are scored using a\njudge model to evaluate their responses only, and then optimized via preference\noptimization. We show that this procedure leads to superior performance on\nAlpacaEval and Arena-Hard, and shows gains from thinking on non-reasoning\ncategories such as marketing, health and general knowledge, in addition to more\ntraditional reasoning & problem-solving tasks."
                },
                "authors": [
                    {
                        "name": "Tianhao Wu"
                    },
                    {
                        "name": "Janice Lan"
                    },
                    {
                        "name": "Weizhe Yuan"
                    },
                    {
                        "name": "Jiantao Jiao"
                    },
                    {
                        "name": "Jason Weston"
                    },
                    {
                        "name": "Sainbayar Sukhbaatar"
                    }
                ],
                "author_detail": {
                    "name": "Sainbayar Sukhbaatar"
                },
                "author": "Sainbayar Sukhbaatar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10629v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10629v2",
                "updated": "2024-10-15T06:19:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    6,
                    19,
                    7,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-14T15:36:42Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    36,
                    42,
                    0,
                    288,
                    0
                ],
                "title": "SANA: Efficient High-Resolution Image Synthesis with Linear Diffusion\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SANA: Efficient High-Resolution Image Synthesis with Linear Diffusion\n  Transformers"
                },
                "summary": "We introduce Sana, a text-to-image framework that can efficiently generate\nimages up to 4096$\\times$4096 resolution. Sana can synthesize high-resolution,\nhigh-quality images with strong text-image alignment at a remarkably fast\nspeed, deployable on laptop GPU. Core designs include: (1) Deep compression\nautoencoder: unlike traditional AEs, which compress images only 8$\\times$, we\ntrained an AE that can compress images 32$\\times$, effectively reducing the\nnumber of latent tokens. (2) Linear DiT: we replace all vanilla attention in\nDiT with linear attention, which is more efficient at high resolutions without\nsacrificing quality. (3) Decoder-only text encoder: we replaced T5 with modern\ndecoder-only small LLM as the text encoder and designed complex human\ninstruction with in-context learning to enhance the image-text alignment. (4)\nEfficient training and sampling: we propose Flow-DPM-Solver to reduce sampling\nsteps, with efficient caption labeling and selection to accelerate convergence.\nAs a result, Sana-0.6B is very competitive with modern giant diffusion model\n(e.g. Flux-12B), being 20 times smaller and 100+ times faster in measured\nthroughput. Moreover, Sana-0.6B can be deployed on a 16GB laptop GPU, taking\nless than 1 second to generate a 1024$\\times$1024 resolution image. Sana\nenables content creation at low cost. Code and model will be publicly released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Sana, a text-to-image framework that can efficiently generate\nimages up to 4096$\\times$4096 resolution. Sana can synthesize high-resolution,\nhigh-quality images with strong text-image alignment at a remarkably fast\nspeed, deployable on laptop GPU. Core designs include: (1) Deep compression\nautoencoder: unlike traditional AEs, which compress images only 8$\\times$, we\ntrained an AE that can compress images 32$\\times$, effectively reducing the\nnumber of latent tokens. (2) Linear DiT: we replace all vanilla attention in\nDiT with linear attention, which is more efficient at high resolutions without\nsacrificing quality. (3) Decoder-only text encoder: we replaced T5 with modern\ndecoder-only small LLM as the text encoder and designed complex human\ninstruction with in-context learning to enhance the image-text alignment. (4)\nEfficient training and sampling: we propose Flow-DPM-Solver to reduce sampling\nsteps, with efficient caption labeling and selection to accelerate convergence.\nAs a result, Sana-0.6B is very competitive with modern giant diffusion model\n(e.g. Flux-12B), being 20 times smaller and 100+ times faster in measured\nthroughput. Moreover, Sana-0.6B can be deployed on a 16GB laptop GPU, taking\nless than 1 second to generate a 1024$\\times$1024 resolution image. Sana\nenables content creation at low cost. Code and model will be publicly released."
                },
                "authors": [
                    {
                        "name": "Enze Xie"
                    },
                    {
                        "name": "Junsong Chen"
                    },
                    {
                        "name": "Junyu Chen"
                    },
                    {
                        "name": "Han Cai"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Zhekai Zhang"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10629v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10629v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.05026v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.05026v4",
                "updated": "2024-10-14T15:35:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    35,
                    49,
                    0,
                    288,
                    0
                ],
                "published": "2022-10-10T21:44:02Z",
                "published_parsed": [
                    2022,
                    10,
                    10,
                    21,
                    44,
                    2,
                    0,
                    283,
                    0
                ],
                "title": "Uncertainty Quantification in Synthetic Controls with Staggered\n  Treatment Adoption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty Quantification in Synthetic Controls with Staggered\n  Treatment Adoption"
                },
                "summary": "We propose principled prediction intervals to quantify the uncertainty of a\nlarge class of synthetic control predictions (or estimators) in settings with\nstaggered treatment adoption, offering precise non-asymptotic coverage\nprobability guarantees. From a methodological perspective, we provide a\ndetailed discussion of different causal quantities to be predicted, which we\ncall causal predictands, allowing for multiple treated units with treatment\nadoption at possibly different points in time. From a theoretical perspective,\nour uncertainty quantification methods improve on prior literature by (i)\ncovering a large class of causal predictands in staggered adoption settings,\n(ii) allowing for synthetic control methods with possibly nonlinear\nconstraints, (iii) proposing scalable robust conic optimization methods and\nprincipled data-driven tuning parameter selection, and (iv) offering valid\nuniform inference across post-treatment periods. We illustrate our methodology\nwith an empirical application studying the effects of economic liberalization\non real GDP per capita for Sub-Saharan African countries. Companion\ngeneral-purpose software packages are provided in Python, R, and Stata.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose principled prediction intervals to quantify the uncertainty of a\nlarge class of synthetic control predictions (or estimators) in settings with\nstaggered treatment adoption, offering precise non-asymptotic coverage\nprobability guarantees. From a methodological perspective, we provide a\ndetailed discussion of different causal quantities to be predicted, which we\ncall causal predictands, allowing for multiple treated units with treatment\nadoption at possibly different points in time. From a theoretical perspective,\nour uncertainty quantification methods improve on prior literature by (i)\ncovering a large class of causal predictands in staggered adoption settings,\n(ii) allowing for synthetic control methods with possibly nonlinear\nconstraints, (iii) proposing scalable robust conic optimization methods and\nprincipled data-driven tuning parameter selection, and (iv) offering valid\nuniform inference across post-treatment periods. We illustrate our methodology\nwith an empirical application studying the effects of economic liberalization\non real GDP per capita for Sub-Saharan African countries. Companion\ngeneral-purpose software packages are provided in Python, R, and Stata."
                },
                "authors": [
                    {
                        "name": "Matias D. Cattaneo"
                    },
                    {
                        "name": "Yingjie Feng"
                    },
                    {
                        "name": "Filippo Palomba"
                    },
                    {
                        "name": "Rocio Titiunik"
                    }
                ],
                "author_detail": {
                    "name": "Rocio Titiunik"
                },
                "author": "Rocio Titiunik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.05026v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.05026v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10628v1",
                "updated": "2024-10-14T15:35:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    35,
                    44,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T15:35:44Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    35,
                    44,
                    0,
                    288,
                    0
                ],
                "title": "Test smells in LLM-Generated Unit Tests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test smells in LLM-Generated Unit Tests"
                },
                "summary": "The use of Large Language Models (LLMs) in automated test generation is\ngaining popularity, with much of the research focusing on metrics like\ncompilability rate, code coverage and bug detection. However, an equally\nimportant quality metric is the presence of test smells design flaws or anti\npatterns in test code that hinder maintainability and readability. In this\nstudy, we explore the diffusion of test smells in LLM generated unit test\nsuites and compare them to those found in human written ones. We analyze a\nbenchmark of 20,500 LLM-generated test suites produced by four models (GPT-3.5,\nGPT-4, Mistral 7B, and Mixtral 8x7B) across five prompt engineering techniques,\nalongside a dataset of 780,144 human written test suites from 34,637 projects.\nLeveraging TsDetect, a state of the art tool capable of detecting 21 different\ntypes of test smells, we identify and analyze the prevalence and co-occurrence\nof various test smells in both human written and LLM-generated test suites. Our\nfindings reveal new insights into the strengths and limitations of LLMs in test\ngeneration. First, regarding prevalence, we observe that LLMs frequently\ngenerate tests with common test smells, such as Magic Number Test and Assertion\nRoulette. Second, in terms of co occurrence, certain smells, like Long Test and\nUseless Test, tend to co occur in LLM-generated suites, influenced by specific\nprompt techniques. Third, we find that project complexity and LLM specific\nfactors, including model size and context length, significantly affect the\nprevalence of test smells. Finally, the patterns of test smells in\nLLM-generated tests often mirror those in human-written tests, suggesting\npotential data leakage from training datasets. These insights underscore the\nneed to refine LLM-based test generation for cleaner code and suggest\nimprovements in both LLM capabilities and software testing practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of Large Language Models (LLMs) in automated test generation is\ngaining popularity, with much of the research focusing on metrics like\ncompilability rate, code coverage and bug detection. However, an equally\nimportant quality metric is the presence of test smells design flaws or anti\npatterns in test code that hinder maintainability and readability. In this\nstudy, we explore the diffusion of test smells in LLM generated unit test\nsuites and compare them to those found in human written ones. We analyze a\nbenchmark of 20,500 LLM-generated test suites produced by four models (GPT-3.5,\nGPT-4, Mistral 7B, and Mixtral 8x7B) across five prompt engineering techniques,\nalongside a dataset of 780,144 human written test suites from 34,637 projects.\nLeveraging TsDetect, a state of the art tool capable of detecting 21 different\ntypes of test smells, we identify and analyze the prevalence and co-occurrence\nof various test smells in both human written and LLM-generated test suites. Our\nfindings reveal new insights into the strengths and limitations of LLMs in test\ngeneration. First, regarding prevalence, we observe that LLMs frequently\ngenerate tests with common test smells, such as Magic Number Test and Assertion\nRoulette. Second, in terms of co occurrence, certain smells, like Long Test and\nUseless Test, tend to co occur in LLM-generated suites, influenced by specific\nprompt techniques. Third, we find that project complexity and LLM specific\nfactors, including model size and context length, significantly affect the\nprevalence of test smells. Finally, the patterns of test smells in\nLLM-generated tests often mirror those in human-written tests, suggesting\npotential data leakage from training datasets. These insights underscore the\nneed to refine LLM-based test generation for cleaner code and suggest\nimprovements in both LLM capabilities and software testing practices."
                },
                "authors": [
                    {
                        "name": "Wendkuni C. Oudraogo"
                    },
                    {
                        "name": "Yinghua Li"
                    },
                    {
                        "name": "Kader Kabor"
                    },
                    {
                        "name": "Xunzhu Tang"
                    },
                    {
                        "name": "Anil Koyuncu"
                    },
                    {
                        "name": "Jacques Klein"
                    },
                    {
                        "name": "David Lo"
                    },
                    {
                        "name": "Tegawend F. Bissyand"
                    }
                ],
                "author_detail": {
                    "name": "Tegawend F. Bissyand"
                },
                "author": "Tegawend F. Bissyand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10626v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10626v1",
                "updated": "2024-10-14T15:31:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    31,
                    54,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T15:31:54Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    31,
                    54,
                    0,
                    288,
                    0
                ],
                "title": "Efficiently Democratizing Medical LLMs for 50 Languages via a Mixture of\n  Language Family Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently Democratizing Medical LLMs for 50 Languages via a Mixture of\n  Language Family Experts"
                },
                "summary": "Adapting medical Large Language Models to local languages can reduce barriers\nto accessing healthcare services, but data scarcity remains a significant\nchallenge, particularly for low-resource languages. To address this, we first\nconstruct a high-quality medical dataset and conduct analysis to ensure its\nquality. In order to leverage the generalization capability of multilingual\nLLMs to efficiently scale to more resource-constrained languages, we explore\nthe internal information flow of LLMs from a multilingual perspective using\nMixture of Experts (MoE) modularity. Technically, we propose a novel MoE\nrouting method that employs language-specific experts and cross-lingual\nrouting. Inspired by circuit theory, our routing analysis revealed a Spread Out\nin the End information flow mechanism: while earlier layers concentrate\ncross-lingual information flow, the later layers exhibit language-specific\ndivergence. This insight directly led to the development of the Post-MoE\narchitecture, which applies sparse routing only in the later layers while\nmaintaining dense others. Experimental results demonstrate that this approach\nenhances the generalization of multilingual models to other languages while\npreserving interpretability. Finally, to efficiently scale the model to 50\nlanguages, we introduce the concept of language family experts, drawing on\nlinguistic priors, which enables scaling the number of languages without adding\nadditional parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting medical Large Language Models to local languages can reduce barriers\nto accessing healthcare services, but data scarcity remains a significant\nchallenge, particularly for low-resource languages. To address this, we first\nconstruct a high-quality medical dataset and conduct analysis to ensure its\nquality. In order to leverage the generalization capability of multilingual\nLLMs to efficiently scale to more resource-constrained languages, we explore\nthe internal information flow of LLMs from a multilingual perspective using\nMixture of Experts (MoE) modularity. Technically, we propose a novel MoE\nrouting method that employs language-specific experts and cross-lingual\nrouting. Inspired by circuit theory, our routing analysis revealed a Spread Out\nin the End information flow mechanism: while earlier layers concentrate\ncross-lingual information flow, the later layers exhibit language-specific\ndivergence. This insight directly led to the development of the Post-MoE\narchitecture, which applies sparse routing only in the later layers while\nmaintaining dense others. Experimental results demonstrate that this approach\nenhances the generalization of multilingual models to other languages while\npreserving interpretability. Finally, to efficiently scale the model to 50\nlanguages, we introduce the concept of language family experts, drawing on\nlinguistic priors, which enables scaling the number of languages without adding\nadditional parameters."
                },
                "authors": [
                    {
                        "name": "Guorui Zheng"
                    },
                    {
                        "name": "Xidong Wang"
                    },
                    {
                        "name": "Juhao Liang"
                    },
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Yuping Zheng"
                    },
                    {
                        "name": "Benyou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Benyou Wang"
                },
                "author": "Benyou Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10626v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10624v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10624v1",
                "updated": "2024-10-14T15:30:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    30,
                    41,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T15:30:41Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    30,
                    41,
                    0,
                    288,
                    0
                ],
                "title": "SensorLLM: Aligning Large Language Models with Motion Sensors for Human\n  Activity Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SensorLLM: Aligning Large Language Models with Motion Sensors for Human\n  Activity Recognition"
                },
                "summary": "In this work, we bridge the gap between wearable sensor technology and\npersonalized AI assistants by enabling Large Language Models (LLMs) to\nunderstand time-series tasks like human activity recognition (HAR). Despite the\nstrong reasoning and generalization capabilities of LLMs, leveraging them for\nsensor data tasks remains largely unexplored. This gap stems from challenges\nlike the lack of semantic context in time-series data, computational\nlimitations, and LLMs' difficulty processing numerical inputs. To address these\nissues, we introduce SensorLLM, a two-stage framework to unlock LLMs' potential\nfor sensor data tasks. In the Sensor-Language Alignment Stage, we introduce\nspecial tokens for each sensor channel and automatically generate\ntrend-descriptive text to align sensor data with textual inputs, enabling\nSensorLLM to capture numerical changes, channel-specific information, and\nsensor data of varying lengths-capabilities that existing LLMs typically\nstruggle with, all without the need for human annotations. Next, in Task-Aware\nTuning Stage, we refine the model for HAR classification using the frozen LLM\nand alignment module, achieving performance on par with or surpassing\nstate-of-the-art models. We further demonstrate that SensorLLM evolves into an\neffective sensor learner, reasoner, and classifier through Sensor-Language\nAlignment, enabling it to generalize across diverse datasets for HAR tasks. We\nstrongly believe our work lays the stepstone for future time-series and text\nalignment research, offering a path toward foundation models for sensor data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we bridge the gap between wearable sensor technology and\npersonalized AI assistants by enabling Large Language Models (LLMs) to\nunderstand time-series tasks like human activity recognition (HAR). Despite the\nstrong reasoning and generalization capabilities of LLMs, leveraging them for\nsensor data tasks remains largely unexplored. This gap stems from challenges\nlike the lack of semantic context in time-series data, computational\nlimitations, and LLMs' difficulty processing numerical inputs. To address these\nissues, we introduce SensorLLM, a two-stage framework to unlock LLMs' potential\nfor sensor data tasks. In the Sensor-Language Alignment Stage, we introduce\nspecial tokens for each sensor channel and automatically generate\ntrend-descriptive text to align sensor data with textual inputs, enabling\nSensorLLM to capture numerical changes, channel-specific information, and\nsensor data of varying lengths-capabilities that existing LLMs typically\nstruggle with, all without the need for human annotations. Next, in Task-Aware\nTuning Stage, we refine the model for HAR classification using the frozen LLM\nand alignment module, achieving performance on par with or surpassing\nstate-of-the-art models. We further demonstrate that SensorLLM evolves into an\neffective sensor learner, reasoner, and classifier through Sensor-Language\nAlignment, enabling it to generalize across diverse datasets for HAR tasks. We\nstrongly believe our work lays the stepstone for future time-series and text\nalignment research, offering a path toward foundation models for sensor data."
                },
                "authors": [
                    {
                        "name": "Zechen Li"
                    },
                    {
                        "name": "Shohreh Deldari"
                    },
                    {
                        "name": "Linyao Chen"
                    },
                    {
                        "name": "Hao Xue"
                    },
                    {
                        "name": "Flora D. Salim"
                    }
                ],
                "author_detail": {
                    "name": "Flora D. Salim"
                },
                "author": "Flora D. Salim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10624v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10624v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10201v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10201v2",
                "updated": "2024-10-14T15:28:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    28,
                    26,
                    0,
                    288,
                    0
                ],
                "published": "2024-05-16T15:45:42Z",
                "published_parsed": [
                    2024,
                    5,
                    16,
                    15,
                    45,
                    42,
                    3,
                    137,
                    0
                ],
                "title": "Investigating cosmic histories with a stiff era through Gravitational\n  Waves",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating cosmic histories with a stiff era through Gravitational\n  Waves"
                },
                "summary": "We investigate the potential of gravitational-wave background searches to\nconstrain cosmic histories characterised by a stiff equation of state, preceded\nby a period of matter domination. Such a scenario leads to a characteristic\npeak in the primordial gravitational-wave spectrum originating from\ncosmological inflation. Assuming instant transitions between distinct epochs,\nwhich allows an analytical treatment of the gravitational-wave spectrum, we\nperform a Bayesian inference analysis to derive constraints from the first\nthree observing runs of the LIGO-Virgo-KAGRA Collaboration. Additionally, we\nconsider a smooth transition, employing an axion-like particle physics model,\nand highlight the difference with the instant transition approximation. We then\nforecast detection prospects for such a cosmic history through future\ngravitational-wave experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the potential of gravitational-wave background searches to\nconstrain cosmic histories characterised by a stiff equation of state, preceded\nby a period of matter domination. Such a scenario leads to a characteristic\npeak in the primordial gravitational-wave spectrum originating from\ncosmological inflation. Assuming instant transitions between distinct epochs,\nwhich allows an analytical treatment of the gravitational-wave spectrum, we\nperform a Bayesian inference analysis to derive constraints from the first\nthree observing runs of the LIGO-Virgo-KAGRA Collaboration. Additionally, we\nconsider a smooth transition, employing an axion-like particle physics model,\nand highlight the difference with the instant transition approximation. We then\nforecast detection prospects for such a cosmic history through future\ngravitational-wave experiments."
                },
                "authors": [
                    {
                        "name": "Hannah Duval"
                    },
                    {
                        "name": "Sachiko Kuroyanagi"
                    },
                    {
                        "name": "Alberto Mariotti"
                    },
                    {
                        "name": "Alba Romero-Rodrguez"
                    },
                    {
                        "name": "Mairi Sakellariadou"
                    }
                ],
                "author_detail": {
                    "name": "Mairi Sakellariadou"
                },
                "author": "Mairi Sakellariadou",
                "arxiv_comment": "18 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10201v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10201v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10619v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10619v1",
                "updated": "2024-10-14T15:22:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    22,
                    23,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T15:22:23Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    22,
                    23,
                    0,
                    288,
                    0
                ],
                "title": "Partially exchangeable stochastic block models for multilayer networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Partially exchangeable stochastic block models for multilayer networks"
                },
                "summary": "Multilayer networks generalize single-layered connectivity data in several\ndirections. These generalizations include, among others, settings where\nmultiple types of edges are observed among the same set of nodes (edge-colored\nnetworks) or where a single notion of connectivity is measured between nodes\nbelonging to different pre-specified layers (node-colored networks). While\nprogress has been made in statistical modeling of edge-colored networks,\nprincipled approaches that flexibly account for both within and across layer\nblock-connectivity structures while incorporating layer information through a\nrigorous probabilistic construction are still lacking for node-colored\nmultilayer networks. We fill this gap by introducing a novel class of partially\nexchangeable stochastic block models specified in terms of a hierarchical\nrandom partition prior for the allocation of nodes to groups, whose number is\nlearned by the model. This goal is achieved without jeopardizing probabilistic\ncoherence, uncertainty quantification and derivation of closed-form predictive\nwithin- and across-layer co-clustering probabilities. Our approach facilitates\nprior elicitation, the understanding of theoretical properties and the\ndevelopment of yet-unexplored predictive strategies for both the connections\nand the allocations of future incoming nodes. Posterior inference proceeds via\na tractable collapsed Gibbs sampler, while performance is illustrated in\nsimulations and in a real-world criminal network application. The notable gains\nachieved over competitors clarify the importance of developing general\nstochastic block models based on suitable node-exchangeability structures\ncoherent with the type of multilayer network being analyzed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilayer networks generalize single-layered connectivity data in several\ndirections. These generalizations include, among others, settings where\nmultiple types of edges are observed among the same set of nodes (edge-colored\nnetworks) or where a single notion of connectivity is measured between nodes\nbelonging to different pre-specified layers (node-colored networks). While\nprogress has been made in statistical modeling of edge-colored networks,\nprincipled approaches that flexibly account for both within and across layer\nblock-connectivity structures while incorporating layer information through a\nrigorous probabilistic construction are still lacking for node-colored\nmultilayer networks. We fill this gap by introducing a novel class of partially\nexchangeable stochastic block models specified in terms of a hierarchical\nrandom partition prior for the allocation of nodes to groups, whose number is\nlearned by the model. This goal is achieved without jeopardizing probabilistic\ncoherence, uncertainty quantification and derivation of closed-form predictive\nwithin- and across-layer co-clustering probabilities. Our approach facilitates\nprior elicitation, the understanding of theoretical properties and the\ndevelopment of yet-unexplored predictive strategies for both the connections\nand the allocations of future incoming nodes. Posterior inference proceeds via\na tractable collapsed Gibbs sampler, while performance is illustrated in\nsimulations and in a real-world criminal network application. The notable gains\nachieved over competitors clarify the importance of developing general\nstochastic block models based on suitable node-exchangeability structures\ncoherent with the type of multilayer network being analyzed."
                },
                "authors": [
                    {
                        "name": "Daniele Durante"
                    },
                    {
                        "name": "Francesco Gaffi"
                    },
                    {
                        "name": "Antonio Lijoi"
                    },
                    {
                        "name": "Igor Prnster"
                    }
                ],
                "author_detail": {
                    "name": "Igor Prnster"
                },
                "author": "Igor Prnster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10619v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10619v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10616v1",
                "updated": "2024-10-14T15:20:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    20,
                    36,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T15:20:36Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    20,
                    36,
                    0,
                    288,
                    0
                ],
                "title": "Field-level cosmological model selection: field-level simulation-based\n  inference for Stage IV cosmic shear can distinguish dynamical dark energy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Field-level cosmological model selection: field-level simulation-based\n  inference for Stage IV cosmic shear can distinguish dynamical dark energy"
                },
                "summary": "We present a framework that for the first time allows Bayesian model\ncomparison to be performed for field-level inference of cosmological models. We\nachieve this by taking a simulation-based inference (SBI) approach using neural\nlikelihood estimation, which we couple with the learned harmonic mean estimator\nin order to compute the Bayesian evidence for model comparison. We apply our\nframework to mock Stage IV cosmic shear observations to assess its\neffectiveness at distinguishing between various models of dark energy. If the\nrecent DESI results that provided exciting hints of dynamical dark energy were\nindeed the true underlying model, our analysis shows Stage IV cosmic shear\nsurveys could definitively detect dynamical dark energy. We also perform\ntraditional power spectrum likelihood-based inference for comparison, which we\nfind is not able to distinguish between dark energy models, highlighting the\nenhanced constraining power for model comparison of our field-level SBI\napproach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a framework that for the first time allows Bayesian model\ncomparison to be performed for field-level inference of cosmological models. We\nachieve this by taking a simulation-based inference (SBI) approach using neural\nlikelihood estimation, which we couple with the learned harmonic mean estimator\nin order to compute the Bayesian evidence for model comparison. We apply our\nframework to mock Stage IV cosmic shear observations to assess its\neffectiveness at distinguishing between various models of dark energy. If the\nrecent DESI results that provided exciting hints of dynamical dark energy were\nindeed the true underlying model, our analysis shows Stage IV cosmic shear\nsurveys could definitively detect dynamical dark energy. We also perform\ntraditional power spectrum likelihood-based inference for comparison, which we\nfind is not able to distinguish between dark energy models, highlighting the\nenhanced constraining power for model comparison of our field-level SBI\napproach."
                },
                "authors": [
                    {
                        "name": "A. Spurio Mancini"
                    },
                    {
                        "name": "K. Lin"
                    },
                    {
                        "name": "J. D. McEwen"
                    }
                ],
                "author_detail": {
                    "name": "J. D. McEwen"
                },
                "author": "J. D. McEwen",
                "arxiv_comment": "12 pages, 6 figures. COSMOPOWER available at\n  https://github.com/alessiospuriomancini/cosmopower, COSMOPOWER-JAX available\n  at https://github.com/dpiras/cosmopower-jax, HARMONIC available at\n  https://github.com/astro-informatics/harmonic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10611v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10611v1",
                "updated": "2024-10-14T15:17:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    17,
                    45,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T15:17:45Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    17,
                    45,
                    0,
                    288,
                    0
                ],
                "title": "A phase microscope for quantum gases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A phase microscope for quantum gases"
                },
                "summary": "Coherence properties are central to quantum systems and are at the heart of\nphenomena such as superconductivity. Here we study coherence properties of an\nultracold Bose gas in a two-dimensional optical lattice across the thermal\nphase transition. To infer the phase coherence and phase fluctuation profile,\nwe use direct matter-wave imaging of higher Talbot revivals as well as a new\nphase microscope based on a site-resolved mapping of phase fluctuations to\ndensity fluctuations during matter-wave imaging. We observe the algebraic decay\nof the phase correlations in the superfluid phase and a linear temperature\nincrease of the exponent. These techniques will also allow studying coherence\nproperties in strongly-correlated quantum systems with full spatial resolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherence properties are central to quantum systems and are at the heart of\nphenomena such as superconductivity. Here we study coherence properties of an\nultracold Bose gas in a two-dimensional optical lattice across the thermal\nphase transition. To infer the phase coherence and phase fluctuation profile,\nwe use direct matter-wave imaging of higher Talbot revivals as well as a new\nphase microscope based on a site-resolved mapping of phase fluctuations to\ndensity fluctuations during matter-wave imaging. We observe the algebraic decay\nof the phase correlations in the superfluid phase and a linear temperature\nincrease of the exponent. These techniques will also allow studying coherence\nproperties in strongly-correlated quantum systems with full spatial resolution."
                },
                "authors": [
                    {
                        "name": "Justus C. Brggenjrgen"
                    },
                    {
                        "name": "Mathis S. Fischer"
                    },
                    {
                        "name": "Christof Weitenberg"
                    }
                ],
                "author_detail": {
                    "name": "Christof Weitenberg"
                },
                "author": "Christof Weitenberg",
                "arxiv_comment": "16 pages, 8 figures, including supplementary material",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10611v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10611v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.quant-gas",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.quant-gas",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10603v1",
                "updated": "2024-10-14T15:12:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    12,
                    15,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T15:12:15Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    12,
                    15,
                    0,
                    288,
                    0
                ],
                "title": "Testing interacting dark energy with Stage IV cosmic shear surveys\n  through differentiable neural emulators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testing interacting dark energy with Stage IV cosmic shear surveys\n  through differentiable neural emulators"
                },
                "summary": "We employ a novel framework for accelerated cosmological inference, based on\nneural emulators and gradient-based sampling methods, to forecast constraints\non dark energy models from Stage IV cosmic shear surveys. We focus on dark\nscattering (DS), an interacting dark energy model with pure momentum exchange\nin the dark sector, and train COSMOPOWER emulators to accurately and\nefficiently model the DS non-linear matter power spectrum produced by the halo\nmodel reaction framework, including the effects of baryon feedback and massive\nneutrinos. We embed the emulators within a fully-differentiable pipeline for\ngradient-based cosmological inference for which the batch likelihood call is up\nto $O(10^5)$ times faster than with traditional approaches, producing parameter\nconstraints from simulated Stage IV cosmic shear data running on a single\ngraphics processing unit (GPU). We also perform model comparison on the output\nchains from the inference process, employing the learnt harmonic mean estimator\nimplemented in the software HARMONIC. We investigate degeneracies between dark\nenergy and systematics parameters and assess the impact of scale cuts on the\nfinal constraints. Assuming a DS model for the mock data vector, we find that a\nStage IV survey cosmic shear analysis can constrain the DS amplitude parameter\n$A_{\\mathrm{ds}}$ with an uncertainty roughly an order of magnitude smaller\nthan current constraints from Stage III surveys, even after marginalising over\nbaryonic feedback, intrinsic alignments and redshift distribution\nuncertainties. These results show great promise for constraining DS with Stage\nIV data; furthermore, our methodology can be straightforwardly extended to a\nwide range of dark energy and modified gravity models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We employ a novel framework for accelerated cosmological inference, based on\nneural emulators and gradient-based sampling methods, to forecast constraints\non dark energy models from Stage IV cosmic shear surveys. We focus on dark\nscattering (DS), an interacting dark energy model with pure momentum exchange\nin the dark sector, and train COSMOPOWER emulators to accurately and\nefficiently model the DS non-linear matter power spectrum produced by the halo\nmodel reaction framework, including the effects of baryon feedback and massive\nneutrinos. We embed the emulators within a fully-differentiable pipeline for\ngradient-based cosmological inference for which the batch likelihood call is up\nto $O(10^5)$ times faster than with traditional approaches, producing parameter\nconstraints from simulated Stage IV cosmic shear data running on a single\ngraphics processing unit (GPU). We also perform model comparison on the output\nchains from the inference process, employing the learnt harmonic mean estimator\nimplemented in the software HARMONIC. We investigate degeneracies between dark\nenergy and systematics parameters and assess the impact of scale cuts on the\nfinal constraints. Assuming a DS model for the mock data vector, we find that a\nStage IV survey cosmic shear analysis can constrain the DS amplitude parameter\n$A_{\\mathrm{ds}}$ with an uncertainty roughly an order of magnitude smaller\nthan current constraints from Stage III surveys, even after marginalising over\nbaryonic feedback, intrinsic alignments and redshift distribution\nuncertainties. These results show great promise for constraining DS with Stage\nIV data; furthermore, our methodology can be straightforwardly extended to a\nwide range of dark energy and modified gravity models."
                },
                "authors": [
                    {
                        "name": "K. Carrion"
                    },
                    {
                        "name": "A. Spurio Mancini"
                    },
                    {
                        "name": "D. Piras"
                    },
                    {
                        "name": "J. C. Hidalgo"
                    }
                ],
                "author_detail": {
                    "name": "J. C. Hidalgo"
                },
                "author": "J. C. Hidalgo",
                "arxiv_comment": "9 pages, 6 figures. COSMOPOWER available at\n  https://github.com/alessiospuriomancini/cosmopower, COSMOPOWER-JAX available\n  at https://github.com/dpiras/cosmopower-jax, emulators for Dark Scattering\n  available at https://github.com/karimpsi22/DS-emulators",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10601v1",
                "updated": "2024-10-14T15:12:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    12,
                    1,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T15:12:01Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    12,
                    1,
                    0,
                    288,
                    0
                ],
                "title": "Fully Asynchronous Neuromorphic Perception for Mobile Robot Dodging with\n  Loihi Chips",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully Asynchronous Neuromorphic Perception for Mobile Robot Dodging with\n  Loihi Chips"
                },
                "summary": "Sparse and asynchronous sensing and processing in natural organisms lead to\nultra low-latency and energy-efficient perception. Event cameras, known as\nneuromorphic vision sensors, are designed to mimic these characteristics.\nHowever, fully utilizing the sparse and asynchronous event stream remains\nchallenging. Influenced by the mature algorithms of standard cameras, most\nexisting event-based algorithms still rely on the \"group of events\" processing\nparadigm (e.g., event frames, 3D voxels) when handling event streams. This\nparadigm encounters issues such as feature loss, event stacking, and high\ncomputational burden, which deviates from the intended purpose of event\ncameras. To address these issues, we propose a fully asynchronous neuromorphic\nparadigm that integrates event cameras, spiking networks, and neuromorphic\nprocessors (Intel Loihi). This paradigm can faithfully process each event\nasynchronously as it arrives, mimicking the spike-driven signal processing in\nbiological brains. We compare the proposed paradigm with the existing \"group of\nevents\" processing paradigm in detail on the real mobile robot dodging task.\nExperimental results show that our scheme exhibits better robustness than\nframe-based methods with different time windows and light conditions.\nAdditionally, the energy consumption per inference of our scheme on the\nembedded Loihi processor is only 4.30% of that of the event spike tensor method\non NVIDIA Jetson Orin NX with energy-saving mode, and 1.64% of that of the\nevent frame method on the same neuromorphic processor. As far as we know, this\nis the first time that a fully asynchronous neuromorphic paradigm has been\nimplemented for solving sequential tasks on real mobile robot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse and asynchronous sensing and processing in natural organisms lead to\nultra low-latency and energy-efficient perception. Event cameras, known as\nneuromorphic vision sensors, are designed to mimic these characteristics.\nHowever, fully utilizing the sparse and asynchronous event stream remains\nchallenging. Influenced by the mature algorithms of standard cameras, most\nexisting event-based algorithms still rely on the \"group of events\" processing\nparadigm (e.g., event frames, 3D voxels) when handling event streams. This\nparadigm encounters issues such as feature loss, event stacking, and high\ncomputational burden, which deviates from the intended purpose of event\ncameras. To address these issues, we propose a fully asynchronous neuromorphic\nparadigm that integrates event cameras, spiking networks, and neuromorphic\nprocessors (Intel Loihi). This paradigm can faithfully process each event\nasynchronously as it arrives, mimicking the spike-driven signal processing in\nbiological brains. We compare the proposed paradigm with the existing \"group of\nevents\" processing paradigm in detail on the real mobile robot dodging task.\nExperimental results show that our scheme exhibits better robustness than\nframe-based methods with different time windows and light conditions.\nAdditionally, the energy consumption per inference of our scheme on the\nembedded Loihi processor is only 4.30% of that of the event spike tensor method\non NVIDIA Jetson Orin NX with energy-saving mode, and 1.64% of that of the\nevent frame method on the same neuromorphic processor. As far as we know, this\nis the first time that a fully asynchronous neuromorphic paradigm has been\nimplemented for solving sequential tasks on real mobile robot."
                },
                "authors": [
                    {
                        "name": "Junjie Jiang"
                    },
                    {
                        "name": "Delei Kong"
                    },
                    {
                        "name": "Chenming Hu"
                    },
                    {
                        "name": "Zheng Fang"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Fang"
                },
                "author": "Zheng Fang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08776v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08776v2",
                "updated": "2024-10-14T15:04:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    4,
                    51,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-11T12:49:05Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    49,
                    5,
                    4,
                    285,
                    0
                ],
                "title": "F2A: An Innovative Approach for Prompt Injection by Utilizing Feign\n  Security Detection Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "F2A: An Innovative Approach for Prompt Injection by Utilizing Feign\n  Security Detection Agents"
                },
                "summary": "With the rapid development of Large Language Models (LLMs), numerous mature\napplications of LLMs have emerged in the field of content safety detection.\nHowever, we have found that LLMs exhibit blind trust in safety detection\nagents. The general LLMs can be compromised by hackers with this vulnerability.\nHence, this paper proposed an attack named Feign Agent Attack (F2A).Through\nsuch malicious forgery methods, adding fake safety detection results into the\nprompt, the defense mechanism of LLMs can be bypassed, thereby obtaining\nharmful content and hijacking the normal conversation. Continually, a series of\nexperiments were conducted. In these experiments, the hijacking capability of\nF2A on LLMs was analyzed and demonstrated, exploring the fundamental reasons\nwhy LLMs blindly trust safety detection results. The experiments involved\nvarious scenarios where fake safety detection results were injected into\nprompts, and the responses were closely monitored to understand the extent of\nthe vulnerability. Also, this paper provided a reasonable solution to this\nattack, emphasizing that it is important for LLMs to critically evaluate the\nresults of augmented agents to prevent the generating harmful content. By doing\nso, the reliability and security can be significantly improved, protecting the\nLLMs from F2A.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of Large Language Models (LLMs), numerous mature\napplications of LLMs have emerged in the field of content safety detection.\nHowever, we have found that LLMs exhibit blind trust in safety detection\nagents. The general LLMs can be compromised by hackers with this vulnerability.\nHence, this paper proposed an attack named Feign Agent Attack (F2A).Through\nsuch malicious forgery methods, adding fake safety detection results into the\nprompt, the defense mechanism of LLMs can be bypassed, thereby obtaining\nharmful content and hijacking the normal conversation. Continually, a series of\nexperiments were conducted. In these experiments, the hijacking capability of\nF2A on LLMs was analyzed and demonstrated, exploring the fundamental reasons\nwhy LLMs blindly trust safety detection results. The experiments involved\nvarious scenarios where fake safety detection results were injected into\nprompts, and the responses were closely monitored to understand the extent of\nthe vulnerability. Also, this paper provided a reasonable solution to this\nattack, emphasizing that it is important for LLMs to critically evaluate the\nresults of augmented agents to prevent the generating harmful content. By doing\nso, the reliability and security can be significantly improved, protecting the\nLLMs from F2A."
                },
                "authors": [
                    {
                        "name": "Yupeng Ren"
                    }
                ],
                "author_detail": {
                    "name": "Yupeng Ren"
                },
                "author": "Yupeng Ren",
                "arxiv_comment": "1. Fixed typo in abstract 2. Provisionally completed the article\n  update to facilitate future version revisions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08776v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08776v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10594v1",
                "updated": "2024-10-14T15:04:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    4,
                    18,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T15:04:18Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    4,
                    18,
                    0,
                    288,
                    0
                ],
                "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality\n  Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality\n  Documents"
                },
                "summary": "Retrieval-augmented generation (RAG) is an effective technique that enables\nlarge language models (LLMs) to utilize external knowledge sources for\ngeneration. However, current RAG systems are solely based on text, rendering it\nimpossible to utilize vision information like layout and images that play\ncrucial roles in real-world multi-modality documents. In this paper, we\nintroduce VisRAG, which tackles this issue by establishing a vision-language\nmodel (VLM)-based RAG pipeline. In this pipeline, instead of first parsing the\ndocument to obtain text, the document is directly embedded using a VLM as an\nimage and then retrieved to enhance the generation of a VLM. Compared to\ntraditional text-based RAG, VisRAG maximizes the retention and utilization of\nthe data information in the original documents, eliminating the information\nloss introduced during the parsing process. We collect both open-source and\nsynthetic data to train the retriever in VisRAG and explore a variety of\ngeneration methods. Experiments demonstrate that VisRAG outperforms traditional\nRAG in both the retrieval and generation stages, achieving a 25--39\\%\nend-to-end performance gain over traditional text-based RAG pipeline. Further\nanalysis reveals that VisRAG is effective in utilizing training data and\ndemonstrates strong generalization capability, positioning it as a promising\nsolution for RAG on multi-modality documents. Our code and data are available\nat https://github.com/openbmb/visrag .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) is an effective technique that enables\nlarge language models (LLMs) to utilize external knowledge sources for\ngeneration. However, current RAG systems are solely based on text, rendering it\nimpossible to utilize vision information like layout and images that play\ncrucial roles in real-world multi-modality documents. In this paper, we\nintroduce VisRAG, which tackles this issue by establishing a vision-language\nmodel (VLM)-based RAG pipeline. In this pipeline, instead of first parsing the\ndocument to obtain text, the document is directly embedded using a VLM as an\nimage and then retrieved to enhance the generation of a VLM. Compared to\ntraditional text-based RAG, VisRAG maximizes the retention and utilization of\nthe data information in the original documents, eliminating the information\nloss introduced during the parsing process. We collect both open-source and\nsynthetic data to train the retriever in VisRAG and explore a variety of\ngeneration methods. Experiments demonstrate that VisRAG outperforms traditional\nRAG in both the retrieval and generation stages, achieving a 25--39\\%\nend-to-end performance gain over traditional text-based RAG pipeline. Further\nanalysis reveals that VisRAG is effective in utilizing training data and\ndemonstrates strong generalization capability, positioning it as a promising\nsolution for RAG on multi-modality documents. Our code and data are available\nat https://github.com/openbmb/visrag ."
                },
                "authors": [
                    {
                        "name": "Shi Yu"
                    },
                    {
                        "name": "Chaoyue Tang"
                    },
                    {
                        "name": "Bokai Xu"
                    },
                    {
                        "name": "Junbo Cui"
                    },
                    {
                        "name": "Junhao Ran"
                    },
                    {
                        "name": "Yukun Yan"
                    },
                    {
                        "name": "Zhenghao Liu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10593v1",
                "updated": "2024-10-14T15:04:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    4,
                    2,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T15:04:02Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    4,
                    2,
                    0,
                    288,
                    0
                ],
                "title": "Characterization of Noninteracting Bosons, with Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterization of Noninteracting Bosons, with Applications"
                },
                "summary": "Boson sampling is the task of producing samples from the number-basis\ndistribution of many bosons traveling through a passive linear optical network.\nIt is believed to be extremely difficult to accomplish classically, and has\nbeen the motivation for many \"quantum advantage\" demonstrations. Here we\ndiscuss the characterization tools that were developed to interpret the results\nof a boson sampling experiment performed at JILA, using atoms instead of\nphotons.\n  We measured the indistinguishability of the atoms using a Hong-Ou-Mandel\nstyle measurement, and found that it was $99.5^{+0.5}_{-1.6}\\%$. We then showed\nthat the indistinguishability of the atoms was a good predictor of the\nmultiparticle bunching features, which in turn was a measure of multiparticle\nindistinguishability itself. To make this latter connection explicit, we\nintroduce the weak generalized bunching conjecture and show it is equivalent to\nan existing mathematical conjecture.\n  For the purpose of characterizing the dynamics that were present in the\nexperiment, we discuss how to optimize the experimental design for inferring\nthe single-particle unitary from Fock basis measurements. We showed that having\nvery cold atoms was necessary to perform the inference of the dynamics in a\nreasonable amount of time. We then partially characterized the single particle\nunitary via direct measurements using one and two atoms, and compared our\nmeasurements to a separate characterization using a new statistic that\ndescribes the deviation between the two characterization methods while being\ninsensitive to uninferable parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boson sampling is the task of producing samples from the number-basis\ndistribution of many bosons traveling through a passive linear optical network.\nIt is believed to be extremely difficult to accomplish classically, and has\nbeen the motivation for many \"quantum advantage\" demonstrations. Here we\ndiscuss the characterization tools that were developed to interpret the results\nof a boson sampling experiment performed at JILA, using atoms instead of\nphotons.\n  We measured the indistinguishability of the atoms using a Hong-Ou-Mandel\nstyle measurement, and found that it was $99.5^{+0.5}_{-1.6}\\%$. We then showed\nthat the indistinguishability of the atoms was a good predictor of the\nmultiparticle bunching features, which in turn was a measure of multiparticle\nindistinguishability itself. To make this latter connection explicit, we\nintroduce the weak generalized bunching conjecture and show it is equivalent to\nan existing mathematical conjecture.\n  For the purpose of characterizing the dynamics that were present in the\nexperiment, we discuss how to optimize the experimental design for inferring\nthe single-particle unitary from Fock basis measurements. We showed that having\nvery cold atoms was necessary to perform the inference of the dynamics in a\nreasonable amount of time. We then partially characterized the single particle\nunitary via direct measurements using one and two atoms, and compared our\nmeasurements to a separate characterization using a new statistic that\ndescribes the deviation between the two characterization methods while being\ninsensitive to uninferable parameters."
                },
                "authors": [
                    {
                        "name": "Shawn Geller"
                    }
                ],
                "author_detail": {
                    "name": "Shawn Geller"
                },
                "author": "Shawn Geller",
                "arxiv_comment": "PhD Thesis, University of Colorado at Boulder, 2024, updated from\n  ProQuest version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10585v1",
                "updated": "2024-10-14T14:56:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    56,
                    51,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T14:56:51Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    56,
                    51,
                    0,
                    288,
                    0
                ],
                "title": "Tbingen-CL at SemEval-2024 Task 1:Ensemble Learning for Semantic\n  Relatedness Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tbingen-CL at SemEval-2024 Task 1:Ensemble Learning for Semantic\n  Relatedness Estimation"
                },
                "summary": "The paper introduces our system for SemEval-2024 Task 1, which aims to\npredict the relatedness of sentence pairs. Operating under the hypothesis that\nsemantic relatedness is a broader concept that extends beyond mere similarity\nof sentences, our approach seeks to identify useful features for relatedness\nestimation. We employ an ensemble approach integrating various systems,\nincluding statistical textual features and outputs of deep learning models to\npredict relatedness scores. The findings suggest that semantic relatedness can\nbe inferred from various sources and ensemble models outperform many individual\nsystems in estimating semantic relatedness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The paper introduces our system for SemEval-2024 Task 1, which aims to\npredict the relatedness of sentence pairs. Operating under the hypothesis that\nsemantic relatedness is a broader concept that extends beyond mere similarity\nof sentences, our approach seeks to identify useful features for relatedness\nestimation. We employ an ensemble approach integrating various systems,\nincluding statistical textual features and outputs of deep learning models to\npredict relatedness scores. The findings suggest that semantic relatedness can\nbe inferred from various sources and ensemble models outperform many individual\nsystems in estimating semantic relatedness."
                },
                "authors": [
                    {
                        "name": "Leixin Zhang"
                    },
                    {
                        "name": "ar ltekin"
                    }
                ],
                "author_detail": {
                    "name": "ar ltekin"
                },
                "author": "ar ltekin",
                "arxiv_doi": "10.18653/v1/2024.semeval-1.147",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2024.semeval-1.147",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.10585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "5 pages",
                "arxiv_journal_ref": "Proceedings of the 18th International Workshop on Semantic\n  Evaluation (SemEval-2024)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10584v1",
                "updated": "2024-10-14T14:56:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    56,
                    1,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T14:56:01Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    56,
                    1,
                    0,
                    288,
                    0
                ],
                "title": "STACKFEED: Structured Textual Actor-Critic Knowledge Base Editing with\n  FeedBack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STACKFEED: Structured Textual Actor-Critic Knowledge Base Editing with\n  FeedBack"
                },
                "summary": "Large Language Models (LLMs) often generate incorrect or outdated\ninformation, especially in low-resource settings or when dealing with private\ndata. To address this, Retrieval-Augmented Generation (RAG) uses external\nknowledge bases (KBs), but these can also suffer from inaccuracies. We\nintroduce STACKFEED, a novel Structured Textual Actor-Critic Knowledge base\nediting with FEEDback approach that iteratively refines the KB based on expert\nfeedback using a multi-actor, centralized critic reinforcement learning\nframework. Each document is assigned to an actor, modeled as a ReACT agent,\nwhich performs structured edits based on document-specific targeted\ninstructions from a centralized critic. Experimental results show that\nSTACKFEED significantly improves KB quality and RAG system performance,\nenhancing accuracy by up to 8% over baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often generate incorrect or outdated\ninformation, especially in low-resource settings or when dealing with private\ndata. To address this, Retrieval-Augmented Generation (RAG) uses external\nknowledge bases (KBs), but these can also suffer from inaccuracies. We\nintroduce STACKFEED, a novel Structured Textual Actor-Critic Knowledge base\nediting with FEEDback approach that iteratively refines the KB based on expert\nfeedback using a multi-actor, centralized critic reinforcement learning\nframework. Each document is assigned to an actor, modeled as a ReACT agent,\nwhich performs structured edits based on document-specific targeted\ninstructions from a centralized critic. Experimental results show that\nSTACKFEED significantly improves KB quality and RAG system performance,\nenhancing accuracy by up to 8% over baselines."
                },
                "authors": [
                    {
                        "name": "Naman Gupta"
                    },
                    {
                        "name": "Shashank Kirtania"
                    },
                    {
                        "name": "Priyanshu Gupta"
                    },
                    {
                        "name": "Krishna Kariya"
                    },
                    {
                        "name": "Sumit Gulwani"
                    },
                    {
                        "name": "Arun Iyer"
                    },
                    {
                        "name": "Suresh Parthasarathy"
                    },
                    {
                        "name": "Arjun Radhakrishna"
                    },
                    {
                        "name": "Sriram K. Rajamani"
                    },
                    {
                        "name": "Gustavo Soares"
                    }
                ],
                "author_detail": {
                    "name": "Gustavo Soares"
                },
                "author": "Gustavo Soares",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19225v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19225v2",
                "updated": "2024-10-14T14:54:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    54,
                    18,
                    0,
                    288,
                    0
                ],
                "published": "2024-05-29T16:05:57Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    16,
                    5,
                    57,
                    2,
                    150,
                    0
                ],
                "title": "Synthetic Potential Outcomes and Causal Mixture Identifiability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic Potential Outcomes and Causal Mixture Identifiability"
                },
                "summary": "A mixture model consists of a latent class that exerts a discrete signal on\nthe observed data. Uncovering these latent classes is fundamental to\nunsupervised learning. In this paper, we consider the problem of recovering\nlatent classes defined with respect to causal responses. We allow overlapping\nsupport in the distributions of these classes, meaning individuals cannot be\nclustered into groups with a similar response. Instead, we build on a setting\nfrom proximal causal inference to develop a method of moments approach to\nsynthetically sample potential outcome distributions. This approach is the\nfirst known identifiability result for what we call Mixtures of Treatment\nEffects (MTEs). More broadly, we show how MTEs fit into a hierarchy of causal\nidentifiability that unifies a number of perspectives on latent class\nconfounding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A mixture model consists of a latent class that exerts a discrete signal on\nthe observed data. Uncovering these latent classes is fundamental to\nunsupervised learning. In this paper, we consider the problem of recovering\nlatent classes defined with respect to causal responses. We allow overlapping\nsupport in the distributions of these classes, meaning individuals cannot be\nclustered into groups with a similar response. Instead, we build on a setting\nfrom proximal causal inference to develop a method of moments approach to\nsynthetically sample potential outcome distributions. This approach is the\nfirst known identifiability result for what we call Mixtures of Treatment\nEffects (MTEs). More broadly, we show how MTEs fit into a hierarchy of causal\nidentifiability that unifies a number of perspectives on latent class\nconfounding."
                },
                "authors": [
                    {
                        "name": "Bijan Mazaheri"
                    },
                    {
                        "name": "Chandler Squires"
                    },
                    {
                        "name": "Caroline Uhler"
                    }
                ],
                "author_detail": {
                    "name": "Caroline Uhler"
                },
                "author": "Caroline Uhler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19225v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19225v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10577v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10577v1",
                "updated": "2024-10-14T14:51:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    51,
                    27,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T14:51:27Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    51,
                    27,
                    0,
                    288,
                    0
                ],
                "title": "Words to Wheels: Vision-Based Autonomous Driving Understanding Human\n  Language Instructions Using Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Words to Wheels: Vision-Based Autonomous Driving Understanding Human\n  Language Instructions Using Foundation Models"
                },
                "summary": "This paper introduces an innovative application of foundation models,\nenabling Unmanned Ground Vehicles (UGVs) equipped with an RGB-D camera to\nnavigate to designated destinations based on human language instructions.\nUnlike learning-based methods, this approach does not require prior training\nbut instead leverages existing foundation models, thus facilitating\ngeneralization to novel environments. Upon receiving human language\ninstructions, these are transformed into a 'cognitive route description' using\na large language model (LLM)-a detailed navigation route expressed in human\nlanguage. The vehicle then decomposes this description into landmarks and\nnavigation maneuvers. The vehicle also determines elevation costs and\nidentifies navigability levels of different regions through a terrain\nsegmentation model, GANav, trained on open datasets. Semantic elevation costs,\nwhich take both elevation and navigability levels into account, are estimated\nand provided to the Model Predictive Path Integral (MPPI) planner, responsible\nfor local path planning. Concurrently, the vehicle searches for target\nlandmarks using foundation models, including YOLO-World and EfficientViT-SAM.\nUltimately, the vehicle executes the navigation commands to reach the\ndesignated destination, the final landmark. Our experiments demonstrate that\nthis application successfully guides UGVs to their destinations following human\nlanguage instructions in novel environments, such as unfamiliar terrain or\nurban settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an innovative application of foundation models,\nenabling Unmanned Ground Vehicles (UGVs) equipped with an RGB-D camera to\nnavigate to designated destinations based on human language instructions.\nUnlike learning-based methods, this approach does not require prior training\nbut instead leverages existing foundation models, thus facilitating\ngeneralization to novel environments. Upon receiving human language\ninstructions, these are transformed into a 'cognitive route description' using\na large language model (LLM)-a detailed navigation route expressed in human\nlanguage. The vehicle then decomposes this description into landmarks and\nnavigation maneuvers. The vehicle also determines elevation costs and\nidentifies navigability levels of different regions through a terrain\nsegmentation model, GANav, trained on open datasets. Semantic elevation costs,\nwhich take both elevation and navigability levels into account, are estimated\nand provided to the Model Predictive Path Integral (MPPI) planner, responsible\nfor local path planning. Concurrently, the vehicle searches for target\nlandmarks using foundation models, including YOLO-World and EfficientViT-SAM.\nUltimately, the vehicle executes the navigation commands to reach the\ndesignated destination, the final landmark. Our experiments demonstrate that\nthis application successfully guides UGVs to their destinations following human\nlanguage instructions in novel environments, such as unfamiliar terrain or\nurban settings."
                },
                "authors": [
                    {
                        "name": "Chanhoe Ryu"
                    },
                    {
                        "name": "Hyunki Seong"
                    },
                    {
                        "name": "Daegyu Lee"
                    },
                    {
                        "name": "Seongwoo Moon"
                    },
                    {
                        "name": "Sungjae Min"
                    },
                    {
                        "name": "D. Hyunchul Shim"
                    }
                ],
                "author_detail": {
                    "name": "D. Hyunchul Shim"
                },
                "author": "D. Hyunchul Shim",
                "arxiv_comment": "7 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10577v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10577v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10570v1",
                "updated": "2024-10-14T14:47:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    47,
                    32,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T14:47:32Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    47,
                    32,
                    0,
                    288,
                    0
                ],
                "title": "Mindalogue: LLM -- Powered Nonlinear Interaction for Effective Learning\n  and Task Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mindalogue: LLM -- Powered Nonlinear Interaction for Effective Learning\n  and Task Exploration"
                },
                "summary": "Current generative AI models like ChatGPT, Claude, and Gemini are widely used\nfor knowledge dissemination, task decomposition, and creative thinking.\nHowever, their linear interaction methods often force users to repeatedly\ncompare and copy contextual information when handling complex tasks, increasing\ncognitive load and operational costs. Moreover, the ambiguity in model\nresponses requires users to refine and simplify the information further. To\naddress these issues, we developed \"Mindalogue\", a system using a non-linear\ninteraction model based on \"nodes + canvas\" to enhance user efficiency and\nfreedom while generating structured responses. A formative study with 11 users\ninformed the design of Mindalogue, which was then evaluated through a study\nwith 16 participants. The results showed that Mindalogue significantly reduced\ntask steps and improved users' comprehension of complex information. This study\nhighlights the potential of non-linear interaction in improving AI tool\nefficiency and user experience in the HCI field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current generative AI models like ChatGPT, Claude, and Gemini are widely used\nfor knowledge dissemination, task decomposition, and creative thinking.\nHowever, their linear interaction methods often force users to repeatedly\ncompare and copy contextual information when handling complex tasks, increasing\ncognitive load and operational costs. Moreover, the ambiguity in model\nresponses requires users to refine and simplify the information further. To\naddress these issues, we developed \"Mindalogue\", a system using a non-linear\ninteraction model based on \"nodes + canvas\" to enhance user efficiency and\nfreedom while generating structured responses. A formative study with 11 users\ninformed the design of Mindalogue, which was then evaluated through a study\nwith 16 participants. The results showed that Mindalogue significantly reduced\ntask steps and improved users' comprehension of complex information. This study\nhighlights the potential of non-linear interaction in improving AI tool\nefficiency and user experience in the HCI field."
                },
                "authors": [
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Ziyao Zhang"
                    },
                    {
                        "name": "Fengliang Zhu"
                    },
                    {
                        "name": "Jiajie Zhou"
                    },
                    {
                        "name": "Anyi Rao"
                    }
                ],
                "author_detail": {
                    "name": "Anyi Rao"
                },
                "author": "Anyi Rao",
                "arxiv_comment": "17 pages, 9 figures. Submitted to CHI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68U35(Primary), 68T20(Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10569v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10569v1",
                "updated": "2024-10-14T14:46:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    46,
                    27,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T14:46:27Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    46,
                    27,
                    0,
                    288,
                    0
                ],
                "title": "The Discovery of Polarized Water Vapor Megamaser Emission in a Molecular\n  Accretion Disk",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Discovery of Polarized Water Vapor Megamaser Emission in a Molecular\n  Accretion Disk"
                },
                "summary": "For the first time in an extragalactic source, we detect linearly polarized\nwater maser emission associated with the molecular accretion disk of NGC 1068.\nThe position angles of the electric polarization vectors are perpendicular to\nthe axes of filamentary structures in the molecular accretion disk. The\ninferred magnetic field threading the molecular disk must lie within\napproximately 35 degrees of the sky plane. The orientation of the magnetic\nfields relative to the disk plane implies that the maser region is unstable to\nhydromagnetically powered outflow; we speculate that the maser region may be\nthe source of the larger scale molecular outflow found in ALMA studies. The new\nVLBI observations also reveal a compact radio continuum source, NGC 1068*,\naligned with the near-systemic maser spots. The molecular accretion disk must\nbe viewed nearly edge-on, and the revised central mass is (16.6 +/- 0.1)\nmillion solar masses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For the first time in an extragalactic source, we detect linearly polarized\nwater maser emission associated with the molecular accretion disk of NGC 1068.\nThe position angles of the electric polarization vectors are perpendicular to\nthe axes of filamentary structures in the molecular accretion disk. The\ninferred magnetic field threading the molecular disk must lie within\napproximately 35 degrees of the sky plane. The orientation of the magnetic\nfields relative to the disk plane implies that the maser region is unstable to\nhydromagnetically powered outflow; we speculate that the maser region may be\nthe source of the larger scale molecular outflow found in ALMA studies. The new\nVLBI observations also reveal a compact radio continuum source, NGC 1068*,\naligned with the near-systemic maser spots. The molecular accretion disk must\nbe viewed nearly edge-on, and the revised central mass is (16.6 +/- 0.1)\nmillion solar masses."
                },
                "authors": [
                    {
                        "name": "Jack F. Gallimore"
                    },
                    {
                        "name": "C. M. Violette Impellizzeri"
                    },
                    {
                        "name": "Sameneh Aghelpasand"
                    },
                    {
                        "name": "Feng Gao"
                    },
                    {
                        "name": "Virginia Hostetter"
                    },
                    {
                        "name": "Boy Lankhaar"
                    }
                ],
                "author_detail": {
                    "name": "Boy Lankhaar"
                },
                "author": "Boy Lankhaar",
                "arxiv_comment": "12 pages, 3 figures, 1 table. Accepted for publication in the\n  Astrophysical Journal Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10569v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10569v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.05328v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.05328v6",
                "updated": "2024-10-14T14:44:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    44,
                    32,
                    0,
                    288,
                    0
                ],
                "published": "2023-03-09T15:19:31Z",
                "published_parsed": [
                    2023,
                    3,
                    9,
                    15,
                    19,
                    31,
                    3,
                    68,
                    0
                ],
                "title": "Simulation-based, Finite-sample Inference for Privatized Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation-based, Finite-sample Inference for Privatized Data"
                },
                "summary": "Privacy protection methods, such as differentially private mechanisms,\nintroduce noise into resulting statistics which often produces complex and\nintractable sampling distributions. In this paper, we propose a\nsimulation-based \"repro sample\" approach to produce statistically valid\nconfidence intervals and hypothesis tests, which builds on the work of Xie and\nWang (2022). We show that this methodology is applicable to a wide variety of\nprivate inference problems, appropriately accounts for biases introduced by\nprivacy mechanisms (such as by clamping), and improves over other\nstate-of-the-art inference methods such as the parametric bootstrap in terms of\nthe coverage and type I error of the private inference. We also develop\nsignificant improvements and extensions for the repro sample methodology for\ngeneral models (not necessarily related to privacy), including 1) modifying the\nprocedure to ensure guaranteed coverage and type I errors, even accounting for\nMonte Carlo error, and 2) proposing efficient numerical algorithms to implement\nthe confidence intervals and $p$-values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy protection methods, such as differentially private mechanisms,\nintroduce noise into resulting statistics which often produces complex and\nintractable sampling distributions. In this paper, we propose a\nsimulation-based \"repro sample\" approach to produce statistically valid\nconfidence intervals and hypothesis tests, which builds on the work of Xie and\nWang (2022). We show that this methodology is applicable to a wide variety of\nprivate inference problems, appropriately accounts for biases introduced by\nprivacy mechanisms (such as by clamping), and improves over other\nstate-of-the-art inference methods such as the parametric bootstrap in terms of\nthe coverage and type I error of the private inference. We also develop\nsignificant improvements and extensions for the repro sample methodology for\ngeneral models (not necessarily related to privacy), including 1) modifying the\nprocedure to ensure guaranteed coverage and type I errors, even accounting for\nMonte Carlo error, and 2) proposing efficient numerical algorithms to implement\nthe confidence intervals and $p$-values."
                },
                "authors": [
                    {
                        "name": "Jordan Awan"
                    },
                    {
                        "name": "Zhanyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhanyu Wang"
                },
                "author": "Zhanyu Wang",
                "arxiv_comment": "25 pages before references and appendices, 42 pages total, 10\n  figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.05328v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.05328v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10562v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10562v1",
                "updated": "2024-10-14T14:41:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    41,
                    9,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T14:41:09Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    41,
                    9,
                    0,
                    288,
                    0
                ],
                "title": "Causal Modeling of Climate Activism on Reddit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Modeling of Climate Activism on Reddit"
                },
                "summary": "Climate activism is crucial in stimulating collective societal and behavioral\nchange towards sustainable practices through political pressure. Although\nmultiple factors contribute to the participation in activism, their complex\nrelationships and the scarcity of data on their interactions have restricted\nmost prior research to studying them in isolation, thus preventing the\ndevelopment of a quantitative, causal understanding of why people approach\nactivism. In this work, we develop a comprehensive causal model of how and why\nReddit users engage with activist communities driving mass climate protests\n(mainly the 2019 Earth Strike, Fridays for Future, and Extinction Rebellion).\nOur framework, based on Stochastic Variational Inference applied to Bayesian\nNetworks, learns the causal pathways over multiple time periods. Distinct from\nprevious studies, our approach uses large-scale and fine-grained longitudinal\ndata (2016 to 2022) to jointly model the roles of sociodemographic makeup,\nexperience of extreme weather events, exposure to climate-related news, and\nsocial influence through online interactions. We find that among users\ninterested in climate change, participation in online activist communities is\nindeed influenced by direct interactions with activists and largely by recent\nexposure to media coverage of climate protests. Among people aware of climate\nchange, left-leaning people from lower socioeconomic backgrounds are\nparticularly represented in online activist groups. Our findings offer\nempirical validation for theories of media influence and critical mass, and lay\nthe foundations to inform interventions and future studies to foster public\nparticipation in collective action.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Climate activism is crucial in stimulating collective societal and behavioral\nchange towards sustainable practices through political pressure. Although\nmultiple factors contribute to the participation in activism, their complex\nrelationships and the scarcity of data on their interactions have restricted\nmost prior research to studying them in isolation, thus preventing the\ndevelopment of a quantitative, causal understanding of why people approach\nactivism. In this work, we develop a comprehensive causal model of how and why\nReddit users engage with activist communities driving mass climate protests\n(mainly the 2019 Earth Strike, Fridays for Future, and Extinction Rebellion).\nOur framework, based on Stochastic Variational Inference applied to Bayesian\nNetworks, learns the causal pathways over multiple time periods. Distinct from\nprevious studies, our approach uses large-scale and fine-grained longitudinal\ndata (2016 to 2022) to jointly model the roles of sociodemographic makeup,\nexperience of extreme weather events, exposure to climate-related news, and\nsocial influence through online interactions. We find that among users\ninterested in climate change, participation in online activist communities is\nindeed influenced by direct interactions with activists and largely by recent\nexposure to media coverage of climate protests. Among people aware of climate\nchange, left-leaning people from lower socioeconomic backgrounds are\nparticularly represented in online activist groups. Our findings offer\nempirical validation for theories of media influence and critical mass, and lay\nthe foundations to inform interventions and future studies to foster public\nparticipation in collective action."
                },
                "authors": [
                    {
                        "name": "Jacopo Lenti"
                    },
                    {
                        "name": "Luca Maria Aiello"
                    },
                    {
                        "name": "Corrado Monti"
                    },
                    {
                        "name": "Gianmarco De Francisci Morales"
                    }
                ],
                "author_detail": {
                    "name": "Gianmarco De Francisci Morales"
                },
                "author": "Gianmarco De Francisci Morales",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10562v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10562v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10559v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10559v1",
                "updated": "2024-10-14T14:38:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    38,
                    57,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T14:38:57Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    38,
                    57,
                    0,
                    288,
                    0
                ],
                "title": "Scale-dependence in $$CDM parameters inferred from the CMB: a\n  possible sign of Early Dark Energy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scale-dependence in $$CDM parameters inferred from the CMB: a\n  possible sign of Early Dark Energy"
                },
                "summary": "The early dark energy (EDE) model is one of the promising solutions to the\nHubble tension. One of the successes of the EDE model is that it can provide a\nsimilar fit to the $\\Lambda$CDM model for the CMB power spectrum. In this work,\nI analyze the phenomenology of the EDE and $\\Lambda$CDM parameters on the CMB\ntemperature power spectrum and notice that this cannot hold on all scales.\nThus, if the real cosmology is as described by the EDE model, the $\\Lambda$CDM\nparameters will be scale-dependent when fitting the CMB power spectrum with the\n$\\Lambda$CDM model, which can be hints for the EDE model. I examine CMB-S4-like\nobservations through mock data analysis and find that parameter shifts are\nnotable. As observations include smaller scales, I find lower $H_0$, $n_s$,\n$\\omega_b$ and higher $\\omega_m$, $A_s e^{-2\\tau}$, which will also constitute\nnew tensions with other observations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The early dark energy (EDE) model is one of the promising solutions to the\nHubble tension. One of the successes of the EDE model is that it can provide a\nsimilar fit to the $\\Lambda$CDM model for the CMB power spectrum. In this work,\nI analyze the phenomenology of the EDE and $\\Lambda$CDM parameters on the CMB\ntemperature power spectrum and notice that this cannot hold on all scales.\nThus, if the real cosmology is as described by the EDE model, the $\\Lambda$CDM\nparameters will be scale-dependent when fitting the CMB power spectrum with the\n$\\Lambda$CDM model, which can be hints for the EDE model. I examine CMB-S4-like\nobservations through mock data analysis and find that parameter shifts are\nnotable. As observations include smaller scales, I find lower $H_0$, $n_s$,\n$\\omega_b$ and higher $\\omega_m$, $A_s e^{-2\\tau}$, which will also constitute\nnew tensions with other observations."
                },
                "authors": [
                    {
                        "name": "Jun-Qian Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Jun-Qian Jiang"
                },
                "author": "Jun-Qian Jiang",
                "arxiv_comment": "22 pages, 5 figures. Comments welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10559v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10559v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10553v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10553v1",
                "updated": "2024-10-14T14:32:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    32,
                    55,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T14:32:55Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    32,
                    55,
                    0,
                    288,
                    0
                ],
                "title": "SLaNC: Static LayerNorm Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLaNC: Static LayerNorm Calibration"
                },
                "summary": "The ever increasing sizes of Large Language Models (LLMs) beyond hundreds of\nbillions of parameters have generated enormous pressure on the manufacturers of\ndedicated hardware accelerators and made the innovative design of the latter\none of the most rapidly expanding fields of the AI industry. Various approaches\nhave been explored to enable efficient and accurate processing of LLMs on the\navailable accelerators given their computational and storage limitations. Among\nthese, various quantization techniques have become the main focus of the\ncommunity as a means of reducing the compute, communication and storage\nrequirements. Quantization to lower precision formats naturally poses a number\nof challenges caused by the limited range of the available value\nrepresentations. When it comes to processing the popular Transformer models on\nhardware, one of the main issues becomes calculation of the LayerNorm simply\nbecause accumulation of the variance requires a much wider dynamic range than\nthe hardware enables. In this article, we address this matter and propose a\ncomputationally-efficient scaling technique that can be easily applied to\nTransformer models during inference. Our method suggests a straightforward way\nof scaling the LayerNorm inputs based on the static weights of the immediately\npreceding linear layers. The scaling factors are computed offline, based solely\non the linear layer weights, hence no latency or computational overhead is\nadded during inference. Most importantly, our technique ensures that no\nnumerical issues such as overflow or underflow could happen during the compute.\nThis approach offers smooth, accurate and resource-effective inference across a\nwide range of hardware architectures. The article provides theoretical\njustification as well as supporting numerical simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ever increasing sizes of Large Language Models (LLMs) beyond hundreds of\nbillions of parameters have generated enormous pressure on the manufacturers of\ndedicated hardware accelerators and made the innovative design of the latter\none of the most rapidly expanding fields of the AI industry. Various approaches\nhave been explored to enable efficient and accurate processing of LLMs on the\navailable accelerators given their computational and storage limitations. Among\nthese, various quantization techniques have become the main focus of the\ncommunity as a means of reducing the compute, communication and storage\nrequirements. Quantization to lower precision formats naturally poses a number\nof challenges caused by the limited range of the available value\nrepresentations. When it comes to processing the popular Transformer models on\nhardware, one of the main issues becomes calculation of the LayerNorm simply\nbecause accumulation of the variance requires a much wider dynamic range than\nthe hardware enables. In this article, we address this matter and propose a\ncomputationally-efficient scaling technique that can be easily applied to\nTransformer models during inference. Our method suggests a straightforward way\nof scaling the LayerNorm inputs based on the static weights of the immediately\npreceding linear layers. The scaling factors are computed offline, based solely\non the linear layer weights, hence no latency or computational overhead is\nadded during inference. Most importantly, our technique ensures that no\nnumerical issues such as overflow or underflow could happen during the compute.\nThis approach offers smooth, accurate and resource-effective inference across a\nwide range of hardware architectures. The article provides theoretical\njustification as well as supporting numerical simulations."
                },
                "authors": [
                    {
                        "name": "Mahsa Salmani"
                    },
                    {
                        "name": "Nikita Trukhanov"
                    },
                    {
                        "name": "Ilya Soloveychik"
                    }
                ],
                "author_detail": {
                    "name": "Ilya Soloveychik"
                },
                "author": "Ilya Soloveychik",
                "arxiv_comment": "9 pages, 3 figures, NeurIPS 2024 MLNCP Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10553v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10553v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05180v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05180v2",
                "updated": "2024-10-14T14:27:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    27,
                    34,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-07T16:40:21Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    40,
                    21,
                    0,
                    281,
                    0
                ],
                "title": "Mitigating the Risk of Health Inequity Exacerbated by Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating the Risk of Health Inequity Exacerbated by Large Language\n  Models"
                },
                "summary": "Recent advancements in large language models have demonstrated their\npotential in numerous medical applications, particularly in automating clinical\ntrial matching for translational research and enhancing medical question\nanswering for clinical decision support. However, our study shows that\nincorporating non decisive sociodemographic factors such as race, sex, income\nlevel, LGBT+ status, homelessness, illiteracy, disability, and unemployment\ninto the input of LLMs can lead to incorrect and harmful outputs for these\npopulations. These discrepancies risk exacerbating existing health disparities\nif LLMs are widely adopted in healthcare. To address this issue, we introduce\nEquityGuard, a novel framework designed to detect and mitigate the risk of\nhealth inequities in LLM based medical applications. Our evaluation\ndemonstrates its efficacy in promoting equitable outcomes across diverse\npopulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models have demonstrated their\npotential in numerous medical applications, particularly in automating clinical\ntrial matching for translational research and enhancing medical question\nanswering for clinical decision support. However, our study shows that\nincorporating non decisive sociodemographic factors such as race, sex, income\nlevel, LGBT+ status, homelessness, illiteracy, disability, and unemployment\ninto the input of LLMs can lead to incorrect and harmful outputs for these\npopulations. These discrepancies risk exacerbating existing health disparities\nif LLMs are widely adopted in healthcare. To address this issue, we introduce\nEquityGuard, a novel framework designed to detect and mitigate the risk of\nhealth inequities in LLM based medical applications. Our evaluation\ndemonstrates its efficacy in promoting equitable outcomes across diverse\npopulations."
                },
                "authors": [
                    {
                        "name": "Yuelyu Ji"
                    },
                    {
                        "name": "Wenhe Ma"
                    },
                    {
                        "name": "Sonish Sivarajkumar"
                    },
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Eugene Mathew Sadhu"
                    },
                    {
                        "name": "Zhuochun Li"
                    },
                    {
                        "name": "Xizhi Wu"
                    },
                    {
                        "name": "Shyam Visweswaran"
                    },
                    {
                        "name": "Yanshan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yanshan Wang"
                },
                "author": "Yanshan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05180v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05180v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04556v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04556v2",
                "updated": "2024-10-14T14:27:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    27,
                    4,
                    0,
                    288,
                    0
                ],
                "published": "2024-08-08T16:13:26Z",
                "published_parsed": [
                    2024,
                    8,
                    8,
                    16,
                    13,
                    26,
                    3,
                    221,
                    0
                ],
                "title": "BA-LoRA: Bias-Alleviating Low-Rank Adaptation to Mitigate Catastrophic\n  Inheritance in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BA-LoRA: Bias-Alleviating Low-Rank Adaptation to Mitigate Catastrophic\n  Inheritance in Large Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable proficiency across\nvarious natural language processing (NLP) tasks. However, adapting LLMs to\ndownstream applications requires computationally intensive and memory-demanding\nfine-tuning procedures. To alleviate these burdens, parameter-efficient\nfine-tuning (PEFT) techniques have emerged as a promising approach to tailor\nLLMs with minimal computational overhead. While PEFT methods offer substantial\nadvantages, they do not fully address the pervasive issue of bias propagation\nfrom pre-training data. This work introduces Bias-Alleviating Low-Rank\nAdaptation (BA-LoRA), a novel PEFT method designed to counteract bias\ninheritance. BA-LoRA incorporates three distinct regularization terms: (1) a\nconsistency regularizer, (2) a diversity regularizer, and (3) a singular value\ndecomposition regularizer. These regularizers aim to enhance the models'\nconsistency, diversity, and generalization capabilities during fine-tuning. We\nconduct extensive experiments on natural language understanding (NLU) and\nnatural language generation (NLG) tasks using prominent LLMs such as LLaMA,\nMistral, and Gemma. The results demonstrate that BA-LoRA outperforms LoRA and\nits state-of-the-art variants. Moreover, our method effectively mitigates the\nadverse effects of pre-training bias, leading to more reliable and robust model\noutputs. The code is available at https://github.com/cyp-jlu-ai/BA-LoRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable proficiency across\nvarious natural language processing (NLP) tasks. However, adapting LLMs to\ndownstream applications requires computationally intensive and memory-demanding\nfine-tuning procedures. To alleviate these burdens, parameter-efficient\nfine-tuning (PEFT) techniques have emerged as a promising approach to tailor\nLLMs with minimal computational overhead. While PEFT methods offer substantial\nadvantages, they do not fully address the pervasive issue of bias propagation\nfrom pre-training data. This work introduces Bias-Alleviating Low-Rank\nAdaptation (BA-LoRA), a novel PEFT method designed to counteract bias\ninheritance. BA-LoRA incorporates three distinct regularization terms: (1) a\nconsistency regularizer, (2) a diversity regularizer, and (3) a singular value\ndecomposition regularizer. These regularizers aim to enhance the models'\nconsistency, diversity, and generalization capabilities during fine-tuning. We\nconduct extensive experiments on natural language understanding (NLU) and\nnatural language generation (NLG) tasks using prominent LLMs such as LLaMA,\nMistral, and Gemma. The results demonstrate that BA-LoRA outperforms LoRA and\nits state-of-the-art variants. Moreover, our method effectively mitigates the\nadverse effects of pre-training bias, leading to more reliable and robust model\noutputs. The code is available at https://github.com/cyp-jlu-ai/BA-LoRA."
                },
                "authors": [
                    {
                        "name": "Yupeng Chang"
                    },
                    {
                        "name": "Yi Chang"
                    },
                    {
                        "name": "Yuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Wu"
                },
                "author": "Yuan Wu",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04556v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04556v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05152v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05152v2",
                "updated": "2024-10-14T14:24:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    24,
                    8,
                    0,
                    288,
                    0
                ],
                "published": "2024-03-08T08:41:14Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    8,
                    41,
                    14,
                    4,
                    68,
                    0
                ],
                "title": "Towards a Psychology of Machines: Large Language Models Predict Human\n  Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a Psychology of Machines: Large Language Models Predict Human\n  Memory"
                },
                "summary": "Large language models (LLMs) are excelling across various tasks despite not\nbeing based on human cognition, prompting an investigation into their potential\nto offer insights into human cognitive mechanisms. This study examines\nChatGPT's ability to predict human performance in a language-based memory task.\nFollowing theories of text comprehension, we hypothesized that recognizing\nambiguous sentences is easier with relevant preceding context. Participants,\nincluding humans and ChatGPT, were given pairs of sentences: the second always\na garden-path sentence, and the first providing either fitting or unfitting\ncontext. We measured their ratings of sentence relatedness and memorability.\nResults showed a strong alignment between ChatGPT's assessments and human\nmemory performance. Sentences in the fitting context were rated as being more\nrelated and memorable by ChatGPT and were better remembered by humans,\nhighlighting LLMs' potential to predict human performance and contribute to\npsychological theories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are excelling across various tasks despite not\nbeing based on human cognition, prompting an investigation into their potential\nto offer insights into human cognitive mechanisms. This study examines\nChatGPT's ability to predict human performance in a language-based memory task.\nFollowing theories of text comprehension, we hypothesized that recognizing\nambiguous sentences is easier with relevant preceding context. Participants,\nincluding humans and ChatGPT, were given pairs of sentences: the second always\na garden-path sentence, and the first providing either fitting or unfitting\ncontext. We measured their ratings of sentence relatedness and memorability.\nResults showed a strong alignment between ChatGPT's assessments and human\nmemory performance. Sentences in the fitting context were rated as being more\nrelated and memorable by ChatGPT and were better remembered by humans,\nhighlighting LLMs' potential to predict human performance and contribute to\npsychological theories."
                },
                "authors": [
                    {
                        "name": "Markus Huff"
                    },
                    {
                        "name": "Elanur Ulak"
                    }
                ],
                "author_detail": {
                    "name": "Elanur Ulak"
                },
                "author": "Elanur Ulak",
                "arxiv_comment": "33 pages, 3 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05152v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05152v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08820v2",
                "updated": "2024-10-14T14:22:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    22,
                    40,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-11T14:02:42Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    2,
                    42,
                    4,
                    285,
                    0
                ],
                "title": "Which Demographics do LLMs Default to During Annotation?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Which Demographics do LLMs Default to During Annotation?"
                },
                "summary": "Demographics and cultural background of annotators influence the labels they\nassign in text annotation -- for instance, an elderly woman might find it\noffensive to read a message addressed to a \"bro\", but a male teenager might\nfind it appropriate. It is therefore important to acknowledge label variations\nto not under-represent members of a society. Two research directions developed\nout of this observation in the context of using large language models (LLM) for\ndata annotations, namely (1) studying biases and inherent knowledge of LLMs and\n(2) injecting diversity in the output by manipulating the prompt with\ndemographic information. We combine these two strands of research and ask the\nquestion to which demographics an LLM resorts to when no demographics is given.\nTo answer this question, we evaluate which attributes of human annotators LLMs\ninherently mimic. Furthermore, we compare non-demographic conditioned prompts\nand placebo-conditioned prompts (e.g., \"you are an annotator who lives in house\nnumber 5\") to demographics-conditioned prompts (\"You are a 45 year old man and\nan expert on politeness annotation. How do you rate {instance}\"). We study\nthese questions for politeness and offensiveness annotations on the POPQUORN\ndata set, a corpus created in a controlled manner to investigate human label\nvariations based on demographics which has not been used for LLM-based analyses\nso far. We observe notable influences related to gender, race, and age in\ndemographic prompting, which contrasts with previous studies that found no such\neffects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demographics and cultural background of annotators influence the labels they\nassign in text annotation -- for instance, an elderly woman might find it\noffensive to read a message addressed to a \"bro\", but a male teenager might\nfind it appropriate. It is therefore important to acknowledge label variations\nto not under-represent members of a society. Two research directions developed\nout of this observation in the context of using large language models (LLM) for\ndata annotations, namely (1) studying biases and inherent knowledge of LLMs and\n(2) injecting diversity in the output by manipulating the prompt with\ndemographic information. We combine these two strands of research and ask the\nquestion to which demographics an LLM resorts to when no demographics is given.\nTo answer this question, we evaluate which attributes of human annotators LLMs\ninherently mimic. Furthermore, we compare non-demographic conditioned prompts\nand placebo-conditioned prompts (e.g., \"you are an annotator who lives in house\nnumber 5\") to demographics-conditioned prompts (\"You are a 45 year old man and\nan expert on politeness annotation. How do you rate {instance}\"). We study\nthese questions for politeness and offensiveness annotations on the POPQUORN\ndata set, a corpus created in a controlled manner to investigate human label\nvariations based on demographics which has not been used for LLM-based analyses\nso far. We observe notable influences related to gender, race, and age in\ndemographic prompting, which contrasts with previous studies that found no such\neffects."
                },
                "authors": [
                    {
                        "name": "Johannes Schfer"
                    },
                    {
                        "name": "Aidan Combs"
                    },
                    {
                        "name": "Christopher Bagdon"
                    },
                    {
                        "name": "Jiahui Li"
                    },
                    {
                        "name": "Nadine Probol"
                    },
                    {
                        "name": "Lynn Greschner"
                    },
                    {
                        "name": "Sean Papay"
                    },
                    {
                        "name": "Yarik Menchaca Resendiz"
                    },
                    {
                        "name": "Aswathy Velutharambath"
                    },
                    {
                        "name": "Amelie Whrl"
                    },
                    {
                        "name": "Sabine Weber"
                    },
                    {
                        "name": "Roman Klinger"
                    }
                ],
                "author_detail": {
                    "name": "Roman Klinger"
                },
                "author": "Roman Klinger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08102v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08102v2",
                "updated": "2024-10-14T14:22:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    22,
                    30,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-10T16:45:28Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    45,
                    28,
                    3,
                    284,
                    0
                ],
                "title": "Multi-Agent Collaborative Data Selection for Efficient LLM Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Collaborative Data Selection for Efficient LLM Pretraining"
                },
                "summary": "Efficient data selection is crucial to accelerate the pretraining of large\nlanguage models (LLMs). While various methods have been proposed to enhance\ndata efficiency, limited research has addressed the inherent conflicts between\nthese approaches to achieve optimal data selection for LLM pretraining. To\ntackle this problem, we propose a novel multi-agent collaborative data\nselection mechanism. In this framework, each data selection method serves as an\nindependent agent, and an agent console is designed to dynamically integrate\nthe information from all agents throughout the LLM training process. We conduct\nextensive empirical studies to evaluate our multi-agent framework. The\nexperimental results demonstrate that our approach significantly improves data\nefficiency, accelerates convergence in LLM training, and achieves an average\nperformance gain up to 10.5% across multiple language model benchmarks compared\nto the state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient data selection is crucial to accelerate the pretraining of large\nlanguage models (LLMs). While various methods have been proposed to enhance\ndata efficiency, limited research has addressed the inherent conflicts between\nthese approaches to achieve optimal data selection for LLM pretraining. To\ntackle this problem, we propose a novel multi-agent collaborative data\nselection mechanism. In this framework, each data selection method serves as an\nindependent agent, and an agent console is designed to dynamically integrate\nthe information from all agents throughout the LLM training process. We conduct\nextensive empirical studies to evaluate our multi-agent framework. The\nexperimental results demonstrate that our approach significantly improves data\nefficiency, accelerates convergence in LLM training, and achieves an average\nperformance gain up to 10.5% across multiple language model benchmarks compared\nto the state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Tianyi Bai"
                    },
                    {
                        "name": "Ling Yang"
                    },
                    {
                        "name": "Zhen Hao Wong"
                    },
                    {
                        "name": "Jiahui Peng"
                    },
                    {
                        "name": "Xinlin Zhuang"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Lijun Wu"
                    },
                    {
                        "name": "Jiantao Qiu"
                    },
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Conghui He"
                    }
                ],
                "author_detail": {
                    "name": "Conghui He"
                },
                "author": "Conghui He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08102v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08102v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10542v1",
                "updated": "2024-10-14T14:22:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    22,
                    12,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T14:22:12Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    22,
                    12,
                    0,
                    288,
                    0
                ],
                "title": "Rethinking Legal Judgement Prediction in a Realistic Scenario in the Era\n  of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Legal Judgement Prediction in a Realistic Scenario in the Era\n  of Large Language Models"
                },
                "summary": "This study investigates judgment prediction in a realistic scenario within\nthe context of Indian judgments, utilizing a range of transformer-based models,\nincluding InLegalBERT, BERT, and XLNet, alongside LLMs such as Llama-2 and\nGPT-3.5 Turbo. In this realistic scenario, we simulate how judgments are\npredicted at the point when a case is presented for a decision in court, using\nonly the information available at that time, such as the facts of the case,\nstatutes, precedents, and arguments. This approach mimics real-world\nconditions, where decisions must be made without the benefit of hindsight,\nunlike retrospective analyses often found in previous studies. For transformer\nmodels, we experiment with hierarchical transformers and the summarization of\njudgment facts to optimize input for these models. Our experiments with LLMs\nreveal that GPT-3.5 Turbo excels in realistic scenarios, demonstrating robust\nperformance in judgment prediction. Furthermore, incorporating additional legal\ninformation, such as statutes and precedents, significantly improves the\noutcome of the prediction task. The LLMs also provide explanations for their\npredictions. To evaluate the quality of these predictions and explanations, we\nintroduce two human evaluation metrics: Clarity and Linking. Our findings from\nboth automatic and human evaluations indicate that, despite advancements in\nLLMs, they are yet to achieve expert-level performance in judgment prediction\nand explanation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates judgment prediction in a realistic scenario within\nthe context of Indian judgments, utilizing a range of transformer-based models,\nincluding InLegalBERT, BERT, and XLNet, alongside LLMs such as Llama-2 and\nGPT-3.5 Turbo. In this realistic scenario, we simulate how judgments are\npredicted at the point when a case is presented for a decision in court, using\nonly the information available at that time, such as the facts of the case,\nstatutes, precedents, and arguments. This approach mimics real-world\nconditions, where decisions must be made without the benefit of hindsight,\nunlike retrospective analyses often found in previous studies. For transformer\nmodels, we experiment with hierarchical transformers and the summarization of\njudgment facts to optimize input for these models. Our experiments with LLMs\nreveal that GPT-3.5 Turbo excels in realistic scenarios, demonstrating robust\nperformance in judgment prediction. Furthermore, incorporating additional legal\ninformation, such as statutes and precedents, significantly improves the\noutcome of the prediction task. The LLMs also provide explanations for their\npredictions. To evaluate the quality of these predictions and explanations, we\nintroduce two human evaluation metrics: Clarity and Linking. Our findings from\nboth automatic and human evaluations indicate that, despite advancements in\nLLMs, they are yet to achieve expert-level performance in judgment prediction\nand explanation tasks."
                },
                "authors": [
                    {
                        "name": "Shubham Kumar Nigam"
                    },
                    {
                        "name": "Aniket Deroy"
                    },
                    {
                        "name": "Subhankar Maity"
                    },
                    {
                        "name": "Arnab Bhattacharya"
                    }
                ],
                "author_detail": {
                    "name": "Arnab Bhattacharya"
                },
                "author": "Arnab Bhattacharya",
                "arxiv_comment": "Accepted on NLLP at EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.01371v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.01371v3",
                "updated": "2024-10-14T14:10:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    10,
                    40,
                    0,
                    288,
                    0
                ],
                "published": "2024-03-03T02:19:49Z",
                "published_parsed": [
                    2024,
                    3,
                    3,
                    2,
                    19,
                    49,
                    6,
                    63,
                    0
                ],
                "title": "eXponential FAmily Dynamical Systems (XFADS): Large-scale nonlinear\n  Gaussian state-space modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "eXponential FAmily Dynamical Systems (XFADS): Large-scale nonlinear\n  Gaussian state-space modeling"
                },
                "summary": "State-space graphical models and the variational autoencoder framework\nprovide a principled apparatus for learning dynamical systems from data.\nState-of-the-art probabilistic approaches are often able to scale to large\nproblems at the cost of flexibility of the variational posterior or\nexpressivity of the dynamics model. However, those consolidations can be\ndetrimental if the ultimate goal is to learn a generative model capable of\nexplaining the spatiotemporal structure of the data and making accurate\nforecasts. We introduce a low-rank structured variational autoencoding\nframework for nonlinear Gaussian state-space graphical models capable of\ncapturing dense covariance structures that are important for learning dynamical\nsystems with predictive capabilities. Our inference algorithm exploits the\ncovariance structures that arise naturally from sample based approximate\nGaussian message passing and low-rank amortized posterior updates --\neffectively performing approximate variational smoothing with time complexity\nscaling linearly in the state dimensionality. In comparisons with other deep\nstate-space model architectures our approach consistently demonstrates the\nability to learn a more predictive generative model. Furthermore, when applied\nto neural physiological recordings, our approach is able to learn a dynamical\nsystem capable of forecasting population spiking and behavioral correlates from\na small portion of single trials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-space graphical models and the variational autoencoder framework\nprovide a principled apparatus for learning dynamical systems from data.\nState-of-the-art probabilistic approaches are often able to scale to large\nproblems at the cost of flexibility of the variational posterior or\nexpressivity of the dynamics model. However, those consolidations can be\ndetrimental if the ultimate goal is to learn a generative model capable of\nexplaining the spatiotemporal structure of the data and making accurate\nforecasts. We introduce a low-rank structured variational autoencoding\nframework for nonlinear Gaussian state-space graphical models capable of\ncapturing dense covariance structures that are important for learning dynamical\nsystems with predictive capabilities. Our inference algorithm exploits the\ncovariance structures that arise naturally from sample based approximate\nGaussian message passing and low-rank amortized posterior updates --\neffectively performing approximate variational smoothing with time complexity\nscaling linearly in the state dimensionality. In comparisons with other deep\nstate-space model architectures our approach consistently demonstrates the\nability to learn a more predictive generative model. Furthermore, when applied\nto neural physiological recordings, our approach is able to learn a dynamical\nsystem capable of forecasting population spiking and behavioral correlates from\na small portion of single trials."
                },
                "authors": [
                    {
                        "name": "Matthew Dowling"
                    },
                    {
                        "name": "Yuan Zhao"
                    },
                    {
                        "name": "Il Memming Park"
                    }
                ],
                "author_detail": {
                    "name": "Il Memming Park"
                },
                "author": "Il Memming Park",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.01371v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.01371v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16742v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16742v2",
                "updated": "2024-10-14T14:08:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    8,
                    51,
                    0,
                    288,
                    0
                ],
                "published": "2024-04-25T16:50:59Z",
                "published_parsed": [
                    2024,
                    4,
                    25,
                    16,
                    50,
                    59,
                    3,
                    116,
                    0
                ],
                "title": "Bayesian Nonparametric Inference in McKean-Vlasov models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Nonparametric Inference in McKean-Vlasov models"
                },
                "summary": "We consider nonparametric statistical inference on a periodic interaction\npotential $W$ from noisy discrete space-time measurements of solutions\n$\\rho=\\rho_W$ of the nonlinear McKean-Vlasov equation, describing the\nprobability density of the mean field limit of an interacting particle system.\nWe show how Gaussian process priors assigned to $W$ give rise to posterior mean\nestimators that exhibit fast convergence rates for the implied estimated\ndensities $\\bar \\rho$ towards $\\rho_W$. We further show that if the initial\ncondition $\\phi$ is not too smooth and satisfies a standard deconvolvability\ncondition, then one can consistently infer Sobolev-regular potentials $W$ at\nconvergence rates $N^{-\\theta}$ for appropriate $\\theta>0$, where $N$ is the\nnumber of measurements. The exponent $\\theta$ can be taken to approach $1/2$ as\nthe regularity of $W$ increases corresponding to `near-parametric' models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider nonparametric statistical inference on a periodic interaction\npotential $W$ from noisy discrete space-time measurements of solutions\n$\\rho=\\rho_W$ of the nonlinear McKean-Vlasov equation, describing the\nprobability density of the mean field limit of an interacting particle system.\nWe show how Gaussian process priors assigned to $W$ give rise to posterior mean\nestimators that exhibit fast convergence rates for the implied estimated\ndensities $\\bar \\rho$ towards $\\rho_W$. We further show that if the initial\ncondition $\\phi$ is not too smooth and satisfies a standard deconvolvability\ncondition, then one can consistently infer Sobolev-regular potentials $W$ at\nconvergence rates $N^{-\\theta}$ for appropriate $\\theta>0$, where $N$ is the\nnumber of measurements. The exponent $\\theta$ can be taken to approach $1/2$ as\nthe regularity of $W$ increases corresponding to `near-parametric' models."
                },
                "authors": [
                    {
                        "name": "Richard Nickl"
                    },
                    {
                        "name": "Grigorios A. Pavliotis"
                    },
                    {
                        "name": "Kolyan Ray"
                    }
                ],
                "author_detail": {
                    "name": "Kolyan Ray"
                },
                "author": "Kolyan Ray",
                "arxiv_comment": "24 pages, to appear in the Annals of Statistics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16742v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16742v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04449v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04449v3",
                "updated": "2024-10-15T07:45:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    7,
                    45,
                    31,
                    1,
                    289,
                    0
                ],
                "published": "2024-08-08T13:19:37Z",
                "published_parsed": [
                    2024,
                    8,
                    8,
                    13,
                    19,
                    37,
                    3,
                    221,
                    0
                ],
                "title": "EAIRiskBench: Towards Evaluating Physical Risk Awareness for Task\n  Planning of Foundation Model-based Embodied AI Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EAIRiskBench: Towards Evaluating Physical Risk Awareness for Task\n  Planning of Foundation Model-based Embodied AI Agents"
                },
                "summary": "Embodied artificial intelligence (EAI) integrates advanced AI models into\nphysical entities for real-world interaction. The emergence of foundation\nmodels as the \"brain\" of EAI agents for high-level task planning has shown\npromising results. However, the deployment of these agents in physical\nenvironments presents significant safety challenges. For instance, a\nhousekeeping robot lacking sufficient risk awareness might place a metal\ncontainer in a microwave, potentially causing a fire. To address these critical\nsafety concerns, comprehensive pre-deployment risk assessments are imperative.\nThis study introduces EAIRiskBench, a novel framework for automated physical\nrisk assessment in EAI scenarios. EAIRiskBench employs a multi-agent\ncooperative system that leverages various foundation models to generate safety\nguidelines, create risk-prone scenarios, make task planning, and evaluate\nsafety systematically. Utilizing this framework, we construct EAIRiskDataset,\ncomprising diverse test cases across various domains, encompassing both textual\nand visual scenarios. Our comprehensive evaluation of state-of-the-art\nfoundation models reveals alarming results: all models exhibit high task risk\nrates (TRR), with an average of 95.75% across all evaluated models. To address\nthese challenges, we further propose two prompting-based risk mitigation\nstrategies. While these strategies demonstrate some efficacy in reducing TRR,\nthe improvements are limited, still indicating substantial safety concerns.\nThis study provides the first large-scale assessment of physical risk awareness\nin EAI agents. Our findings underscore the critical need for enhanced safety\nmeasures in EAI systems and provide valuable insights for future research\ndirections in developing safer embodied artificial intelligence system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied artificial intelligence (EAI) integrates advanced AI models into\nphysical entities for real-world interaction. The emergence of foundation\nmodels as the \"brain\" of EAI agents for high-level task planning has shown\npromising results. However, the deployment of these agents in physical\nenvironments presents significant safety challenges. For instance, a\nhousekeeping robot lacking sufficient risk awareness might place a metal\ncontainer in a microwave, potentially causing a fire. To address these critical\nsafety concerns, comprehensive pre-deployment risk assessments are imperative.\nThis study introduces EAIRiskBench, a novel framework for automated physical\nrisk assessment in EAI scenarios. EAIRiskBench employs a multi-agent\ncooperative system that leverages various foundation models to generate safety\nguidelines, create risk-prone scenarios, make task planning, and evaluate\nsafety systematically. Utilizing this framework, we construct EAIRiskDataset,\ncomprising diverse test cases across various domains, encompassing both textual\nand visual scenarios. Our comprehensive evaluation of state-of-the-art\nfoundation models reveals alarming results: all models exhibit high task risk\nrates (TRR), with an average of 95.75% across all evaluated models. To address\nthese challenges, we further propose two prompting-based risk mitigation\nstrategies. While these strategies demonstrate some efficacy in reducing TRR,\nthe improvements are limited, still indicating substantial safety concerns.\nThis study provides the first large-scale assessment of physical risk awareness\nin EAI agents. Our findings underscore the critical need for enhanced safety\nmeasures in EAI systems and provide valuable insights for future research\ndirections in developing safer embodied artificial intelligence system."
                },
                "authors": [
                    {
                        "name": "Zihao Zhu"
                    },
                    {
                        "name": "Bingzhe Wu"
                    },
                    {
                        "name": "Zhengyou Zhang"
                    },
                    {
                        "name": "Lei Han"
                    },
                    {
                        "name": "Baoyuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Baoyuan Wu"
                },
                "author": "Baoyuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04449v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04449v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10526v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10526v1",
                "updated": "2024-10-14T14:06:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    6,
                    5,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T14:06:05Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    6,
                    5,
                    0,
                    288,
                    0
                ],
                "title": "Generalized Adversarial Code-Suggestions: Exploiting Contexts of\n  LLM-based Code-Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalized Adversarial Code-Suggestions: Exploiting Contexts of\n  LLM-based Code-Completion"
                },
                "summary": "While convenient, relying on LLM-powered code assistants in day-to-day work\ngives rise to severe attacks. For instance, the assistant might introduce\nsubtle flaws and suggest vulnerable code to the user. These adversarial\ncode-suggestions can be introduced via data poisoning and, thus, unknowingly by\nthe model creators. In this paper, we provide a generalized formulation of such\nattacks, spawning and extending related work in this domain. This formulation\nis defined over two components: First, a trigger pattern occurring in the\nprompts of a specific user group, and, second, a learnable map in embedding\nspace from the prompt to an adversarial bait. The latter gives rise to novel\nand more flexible targeted attack-strategies, allowing the adversary to choose\nthe most suitable trigger pattern for a specific user-group arbitrarily,\nwithout restrictions on the pattern's tokens. Our directional-map attacks and\nprompt-indexing attacks increase the stealthiness decisively. We extensively\nevaluate the effectiveness of these attacks and carefully investigate defensive\nmechanisms to explore the limits of generalized adversarial code-suggestions.\nWe find that most defenses unfortunately offer little protection only.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While convenient, relying on LLM-powered code assistants in day-to-day work\ngives rise to severe attacks. For instance, the assistant might introduce\nsubtle flaws and suggest vulnerable code to the user. These adversarial\ncode-suggestions can be introduced via data poisoning and, thus, unknowingly by\nthe model creators. In this paper, we provide a generalized formulation of such\nattacks, spawning and extending related work in this domain. This formulation\nis defined over two components: First, a trigger pattern occurring in the\nprompts of a specific user group, and, second, a learnable map in embedding\nspace from the prompt to an adversarial bait. The latter gives rise to novel\nand more flexible targeted attack-strategies, allowing the adversary to choose\nthe most suitable trigger pattern for a specific user-group arbitrarily,\nwithout restrictions on the pattern's tokens. Our directional-map attacks and\nprompt-indexing attacks increase the stealthiness decisively. We extensively\nevaluate the effectiveness of these attacks and carefully investigate defensive\nmechanisms to explore the limits of generalized adversarial code-suggestions.\nWe find that most defenses unfortunately offer little protection only."
                },
                "authors": [
                    {
                        "name": "Karl Rubel"
                    },
                    {
                        "name": "Maximilian Noppel"
                    },
                    {
                        "name": "Christian Wressnegger"
                    }
                ],
                "author_detail": {
                    "name": "Christian Wressnegger"
                },
                "author": "Christian Wressnegger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10526v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10526v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10519v1",
                "updated": "2024-10-14T13:59:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    59,
                    30,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T13:59:30Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    59,
                    30,
                    0,
                    288,
                    0
                ],
                "title": "AI-based particle track identification in scintillating fibres read out\n  with imaging sensors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-based particle track identification in scintillating fibres read out\n  with imaging sensors"
                },
                "summary": "This paper presents the development and application of an AI-based method for\nparticle track identification using scintillating fibres read out with imaging\nsensors. We propose a variational autoencoder (VAE) to efficiently filter and\nidentify frames containing signal from the substantial data generated by SPAD\narray sensors. Our VAE model, trained on purely background frames, demonstrated\na high capability to distinguish frames containing particle tracks from\nbackground noise. The performance of the VAE-based anomaly detection was\nvalidated with experimental data, demonstrating the method's ability to\nefficiently identify relevant events with rapid processing time, suggesting a\nsolid prospect for deployment as a fast inference tool on hardware for\nreal-time anomaly detection. This work highlights the potential of combining\nadvanced sensor technology with machine learning techniques to enhance particle\ndetection and tracking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the development and application of an AI-based method for\nparticle track identification using scintillating fibres read out with imaging\nsensors. We propose a variational autoencoder (VAE) to efficiently filter and\nidentify frames containing signal from the substantial data generated by SPAD\narray sensors. Our VAE model, trained on purely background frames, demonstrated\na high capability to distinguish frames containing particle tracks from\nbackground noise. The performance of the VAE-based anomaly detection was\nvalidated with experimental data, demonstrating the method's ability to\nefficiently identify relevant events with rapid processing time, suggesting a\nsolid prospect for deployment as a fast inference tool on hardware for\nreal-time anomaly detection. This work highlights the potential of combining\nadvanced sensor technology with machine learning techniques to enhance particle\ndetection and tracking."
                },
                "authors": [
                    {
                        "name": "Noemi Bhrer"
                    },
                    {
                        "name": "Sal Alonso-Monsalve"
                    },
                    {
                        "name": "Matthew Franks"
                    },
                    {
                        "name": "Till Dieminger"
                    },
                    {
                        "name": "Davide Sgalaberna"
                    }
                ],
                "author_detail": {
                    "name": "Davide Sgalaberna"
                },
                "author": "Davide Sgalaberna",
                "arxiv_comment": "19 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02442v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02442v3",
                "updated": "2024-10-14T13:57:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    57,
                    29,
                    0,
                    288,
                    0
                ],
                "published": "2024-08-05T13:08:24Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    13,
                    8,
                    24,
                    0,
                    218,
                    0
                ],
                "title": "Let Me Speak Freely? A Study on the Impact of Format Restrictions on\n  Performance of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let Me Speak Freely? A Study on the Impact of Format Restrictions on\n  Performance of Large Language Models"
                },
                "summary": "Structured generation, the process of producing content in standardized\nformats like JSON and XML, is widely utilized in real-world applications to\nextract key output information from large language models (LLMs). This study\ninvestigates whether such constraints on generation space impact LLMs\nabilities, including reasoning and domain knowledge comprehension.\nSpecifically, we evaluate LLMs performance when restricted to adhere to\nstructured formats versus generating free-form responses across various common\ntasks. Surprisingly, we observe a significant decline in LLMs reasoning\nabilities under format restrictions. Furthermore, we find that stricter format\nconstraints generally lead to greater performance degradation in reasoning\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured generation, the process of producing content in standardized\nformats like JSON and XML, is widely utilized in real-world applications to\nextract key output information from large language models (LLMs). This study\ninvestigates whether such constraints on generation space impact LLMs\nabilities, including reasoning and domain knowledge comprehension.\nSpecifically, we evaluate LLMs performance when restricted to adhere to\nstructured formats versus generating free-form responses across various common\ntasks. Surprisingly, we observe a significant decline in LLMs reasoning\nabilities under format restrictions. Furthermore, we find that stricter format\nconstraints generally lead to greater performance degradation in reasoning\ntasks."
                },
                "authors": [
                    {
                        "name": "Zhi Rui Tam"
                    },
                    {
                        "name": "Cheng-Kuang Wu"
                    },
                    {
                        "name": "Yi-Lin Tsai"
                    },
                    {
                        "name": "Chieh-Yen Lin"
                    },
                    {
                        "name": "Hung-yi Lee"
                    },
                    {
                        "name": "Yun-Nung Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yun-Nung Chen"
                },
                "author": "Yun-Nung Chen",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02442v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02442v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01145v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01145v3",
                "updated": "2024-10-14T13:50:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    50,
                    46,
                    0,
                    288,
                    0
                ],
                "published": "2024-02-02T05:04:51Z",
                "published_parsed": [
                    2024,
                    2,
                    2,
                    5,
                    4,
                    51,
                    4,
                    33,
                    0
                ],
                "title": "ReEvo: Large Language Models as Hyper-Heuristics with Reflective\n  Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReEvo: Large Language Models as Hyper-Heuristics with Reflective\n  Evolution"
                },
                "summary": "The omnipresence of NP-hard combinatorial optimization problems (COPs)\ncompels domain experts to engage in trial-and-error heuristic design. The\nlong-standing endeavor of design automation has gained new momentum with the\nrise of large language models (LLMs). This paper introduces Language\nHyper-Heuristics (LHHs), an emerging variant of Hyper-Heuristics that leverages\nLLMs for heuristic generation, featuring minimal manual intervention and\nopen-ended heuristic spaces. To empower LHHs, we present Reflective Evolution\n(ReEvo), a novel integration of evolutionary search for efficiently exploring\nthe heuristic space, and LLM reflections to provide verbal gradients within the\nspace. Across five heterogeneous algorithmic types, six different COPs, and\nboth white-box and black-box views of COPs, ReEvo yields state-of-the-art and\ncompetitive meta-heuristics, evolutionary algorithms, heuristics, and neural\nsolvers, while being more sample-efficient than prior LHHs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The omnipresence of NP-hard combinatorial optimization problems (COPs)\ncompels domain experts to engage in trial-and-error heuristic design. The\nlong-standing endeavor of design automation has gained new momentum with the\nrise of large language models (LLMs). This paper introduces Language\nHyper-Heuristics (LHHs), an emerging variant of Hyper-Heuristics that leverages\nLLMs for heuristic generation, featuring minimal manual intervention and\nopen-ended heuristic spaces. To empower LHHs, we present Reflective Evolution\n(ReEvo), a novel integration of evolutionary search for efficiently exploring\nthe heuristic space, and LLM reflections to provide verbal gradients within the\nspace. Across five heterogeneous algorithmic types, six different COPs, and\nboth white-box and black-box views of COPs, ReEvo yields state-of-the-art and\ncompetitive meta-heuristics, evolutionary algorithms, heuristics, and neural\nsolvers, while being more sample-efficient than prior LHHs."
                },
                "authors": [
                    {
                        "name": "Haoran Ye"
                    },
                    {
                        "name": "Jiarui Wang"
                    },
                    {
                        "name": "Zhiguang Cao"
                    },
                    {
                        "name": "Federico Berto"
                    },
                    {
                        "name": "Chuanbo Hua"
                    },
                    {
                        "name": "Haeyeon Kim"
                    },
                    {
                        "name": "Jinkyoo Park"
                    },
                    {
                        "name": "Guojie Song"
                    }
                ],
                "author_detail": {
                    "name": "Guojie Song"
                },
                "author": "Guojie Song",
                "arxiv_comment": "Accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.01145v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01145v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10511v1",
                "updated": "2024-10-14T13:49:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    49,
                    6,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T13:49:06Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    49,
                    6,
                    0,
                    288,
                    0
                ],
                "title": "Customize Your Visual Autoregressive Recipe with Set Autoregressive\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Customize Your Visual Autoregressive Recipe with Set Autoregressive\n  Modeling"
                },
                "summary": "We introduce a new paradigm for AutoRegressive (AR) image generation, termed\nSet AutoRegressive Modeling (SAR). SAR generalizes the conventional AR to the\nnext-set setting, i.e., splitting the sequence into arbitrary sets containing\nmultiple tokens, rather than outputting each token in a fixed raster order. To\naccommodate SAR, we develop a straightforward architecture termed Fully Masked\nTransformer. We reveal that existing AR variants correspond to specific design\nchoices of sequence order and output intervals within the SAR framework, with\nAR and Masked AR (MAR) as two extreme instances. Notably, SAR facilitates a\nseamless transition from AR to MAR, where intermediate states allow for\ntraining a causal model that benefits from both few-step inference and KV cache\nacceleration, thus leveraging the advantages of both AR and MAR. On the\nImageNet benchmark, we carefully explore the properties of SAR by analyzing the\nimpact of sequence order and output intervals on performance, as well as the\ngeneralization ability regarding inference order and steps. We further validate\nthe potential of SAR by training a 900M text-to-image model capable of\nsynthesizing photo-realistic images with any resolution. We hope our work may\ninspire more exploration and application of AR-based modeling across diverse\nmodalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a new paradigm for AutoRegressive (AR) image generation, termed\nSet AutoRegressive Modeling (SAR). SAR generalizes the conventional AR to the\nnext-set setting, i.e., splitting the sequence into arbitrary sets containing\nmultiple tokens, rather than outputting each token in a fixed raster order. To\naccommodate SAR, we develop a straightforward architecture termed Fully Masked\nTransformer. We reveal that existing AR variants correspond to specific design\nchoices of sequence order and output intervals within the SAR framework, with\nAR and Masked AR (MAR) as two extreme instances. Notably, SAR facilitates a\nseamless transition from AR to MAR, where intermediate states allow for\ntraining a causal model that benefits from both few-step inference and KV cache\nacceleration, thus leveraging the advantages of both AR and MAR. On the\nImageNet benchmark, we carefully explore the properties of SAR by analyzing the\nimpact of sequence order and output intervals on performance, as well as the\ngeneralization ability regarding inference order and steps. We further validate\nthe potential of SAR by training a 900M text-to-image model capable of\nsynthesizing photo-realistic images with any resolution. We hope our work may\ninspire more exploration and application of AR-based modeling across diverse\nmodalities."
                },
                "authors": [
                    {
                        "name": "Wenze Liu"
                    },
                    {
                        "name": "Le Zhuo"
                    },
                    {
                        "name": "Yi Xin"
                    },
                    {
                        "name": "Sheng Xia"
                    },
                    {
                        "name": "Peng Gao"
                    },
                    {
                        "name": "Xiangyu Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Yue"
                },
                "author": "Xiangyu Yue",
                "arxiv_comment": "19 pages, 17 figures, 8 tables, github repo:\n  https://github.com/poppuppy/SAR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10502v1",
                "updated": "2024-10-14T13:45:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    45,
                    20,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T13:45:20Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    45,
                    20,
                    0,
                    288,
                    0
                ],
                "title": "A Practical Approach to Causal Inference over Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Practical Approach to Causal Inference over Time"
                },
                "summary": "In this paper, we focus on estimating the causal effect of an intervention\nover time on a dynamical system. To that end, we formally define causal\ninterventions and their effects over time on discrete-time stochastic processes\n(DSPs). Then, we show under which conditions the equilibrium states of a DSP,\nboth before and after a causal intervention, can be captured by a structural\ncausal model (SCM). With such an equivalence at hand, we provide an explicit\nmapping from vector autoregressive models (VARs), broadly applied in\neconometrics, to linear, but potentially cyclic and/or affected by unmeasured\nconfounders, SCMs. The resulting causal VAR framework allows us to perform\ncausal inference over time from observational time series data. Our experiments\non synthetic and real-world datasets show that the proposed framework achieves\nstrong performance in terms of observational forecasting while enabling\naccurate estimation of the causal effect of interventions on dynamical systems.\nWe demonstrate, through a case study, the potential practical questions that\ncan be addressed using the proposed causal VAR framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we focus on estimating the causal effect of an intervention\nover time on a dynamical system. To that end, we formally define causal\ninterventions and their effects over time on discrete-time stochastic processes\n(DSPs). Then, we show under which conditions the equilibrium states of a DSP,\nboth before and after a causal intervention, can be captured by a structural\ncausal model (SCM). With such an equivalence at hand, we provide an explicit\nmapping from vector autoregressive models (VARs), broadly applied in\neconometrics, to linear, but potentially cyclic and/or affected by unmeasured\nconfounders, SCMs. The resulting causal VAR framework allows us to perform\ncausal inference over time from observational time series data. Our experiments\non synthetic and real-world datasets show that the proposed framework achieves\nstrong performance in terms of observational forecasting while enabling\naccurate estimation of the causal effect of interventions on dynamical systems.\nWe demonstrate, through a case study, the potential practical questions that\ncan be addressed using the proposed causal VAR framework."
                },
                "authors": [
                    {
                        "name": "Martina Cinquini"
                    },
                    {
                        "name": "Isacco Beretta"
                    },
                    {
                        "name": "Salvatore Ruggieri"
                    },
                    {
                        "name": "Isabel Valera"
                    }
                ],
                "author_detail": {
                    "name": "Isabel Valera"
                },
                "author": "Isabel Valera",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00724v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00724v2",
                "updated": "2024-10-14T13:41:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    41,
                    35,
                    0,
                    288,
                    0
                ],
                "published": "2024-08-01T17:16:04Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    17,
                    16,
                    4,
                    3,
                    214,
                    0
                ],
                "title": "Inference Scaling Laws: An Empirical Analysis of Compute-Optimal\n  Inference for Problem-Solving with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference Scaling Laws: An Empirical Analysis of Compute-Optimal\n  Inference for Problem-Solving with Language Models"
                },
                "summary": "While the scaling laws of large language models (LLMs) training have been\nextensively studied, optimal inference configurations of LLMs remain\nunderexplored. We study inference scaling laws and compute-optimal inference,\nfocusing on the trade-offs between model sizes and generating additional tokens\nwith different inference strategies. As a first step towards understanding and\ndesigning compute-optimal inference methods, we studied cost-performance\ntrade-offs for inference strategies such as greedy search, majority voting,\nbest-of-$n$, weighted voting, and two different tree search algorithms, using\ndifferent model sizes and compute budgets. Our findings indicate smaller models\n(e.g., Llemma-7B) can outperform larger models given the same computation\nbudgets, and that smaller models paired with advanced inference algorithms\nyield Pareto-optimal cost-performance trade-offs. For instance, the Llemma-7B\nmodel, equipped with our novel tree search algorithm, consistently outperforms\nLlemma-34B with standard majority voting on the MATH benchmark across all FLOPs\nbudgets. We hope these findings contribute to a broader understanding of\ninference scaling laws for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While the scaling laws of large language models (LLMs) training have been\nextensively studied, optimal inference configurations of LLMs remain\nunderexplored. We study inference scaling laws and compute-optimal inference,\nfocusing on the trade-offs between model sizes and generating additional tokens\nwith different inference strategies. As a first step towards understanding and\ndesigning compute-optimal inference methods, we studied cost-performance\ntrade-offs for inference strategies such as greedy search, majority voting,\nbest-of-$n$, weighted voting, and two different tree search algorithms, using\ndifferent model sizes and compute budgets. Our findings indicate smaller models\n(e.g., Llemma-7B) can outperform larger models given the same computation\nbudgets, and that smaller models paired with advanced inference algorithms\nyield Pareto-optimal cost-performance trade-offs. For instance, the Llemma-7B\nmodel, equipped with our novel tree search algorithm, consistently outperforms\nLlemma-34B with standard majority voting on the MATH benchmark across all FLOPs\nbudgets. We hope these findings contribute to a broader understanding of\ninference scaling laws for LLMs."
                },
                "authors": [
                    {
                        "name": "Yangzhen Wu"
                    },
                    {
                        "name": "Zhiqing Sun"
                    },
                    {
                        "name": "Shanda Li"
                    },
                    {
                        "name": "Sean Welleck"
                    },
                    {
                        "name": "Yiming Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yiming Yang"
                },
                "author": "Yiming Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00724v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00724v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10489v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10489v1",
                "updated": "2024-10-14T13:33:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    33,
                    0,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T13:33:00Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    33,
                    0,
                    0,
                    288,
                    0
                ],
                "title": "Cultural Fidelity in Large-Language Models: An Evaluation of Online\n  Language Resources as a Driver of Model Performance in Value Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cultural Fidelity in Large-Language Models: An Evaluation of Online\n  Language Resources as a Driver of Model Performance in Value Representation"
                },
                "summary": "The training data for LLMs embeds societal values, increasing their\nfamiliarity with the language's culture. Our analysis found that 44% of the\nvariance in the ability of GPT-4o to reflect the societal values of a country,\nas measured by the World Values Survey, correlates with the availability of\ndigital resources in that language. Notably, the error rate was more than five\ntimes higher for the languages of the lowest resource compared to the languages\nof the highest resource. For GPT-4-turbo, this correlation rose to 72%,\nsuggesting efforts to improve the familiarity with the non-English language\nbeyond the web-scraped data. Our study developed one of the largest and most\nrobust datasets in this topic area with 21 country-language pairs, each of\nwhich contain 94 survey questions verified by native speakers. Our results\nhighlight the link between LLM performance and digital data availability in\ntarget languages. Weaker performance in low-resource languages, especially\nprominent in the Global South, may worsen digital divides. We discuss\nstrategies proposed to address this, including developing multilingual LLMs\nfrom the ground up and enhancing fine-tuning on diverse linguistic datasets, as\nseen in African language initiatives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The training data for LLMs embeds societal values, increasing their\nfamiliarity with the language's culture. Our analysis found that 44% of the\nvariance in the ability of GPT-4o to reflect the societal values of a country,\nas measured by the World Values Survey, correlates with the availability of\ndigital resources in that language. Notably, the error rate was more than five\ntimes higher for the languages of the lowest resource compared to the languages\nof the highest resource. For GPT-4-turbo, this correlation rose to 72%,\nsuggesting efforts to improve the familiarity with the non-English language\nbeyond the web-scraped data. Our study developed one of the largest and most\nrobust datasets in this topic area with 21 country-language pairs, each of\nwhich contain 94 survey questions verified by native speakers. Our results\nhighlight the link between LLM performance and digital data availability in\ntarget languages. Weaker performance in low-resource languages, especially\nprominent in the Global South, may worsen digital divides. We discuss\nstrategies proposed to address this, including developing multilingual LLMs\nfrom the ground up and enhancing fine-tuning on diverse linguistic datasets, as\nseen in African language initiatives."
                },
                "authors": [
                    {
                        "name": "Sharif Kazemi"
                    },
                    {
                        "name": "Gloria Gerhardt"
                    },
                    {
                        "name": "Jonty Katz"
                    },
                    {
                        "name": "Caroline Ida Kuria"
                    },
                    {
                        "name": "Estelle Pan"
                    },
                    {
                        "name": "Umang Prabhakar"
                    }
                ],
                "author_detail": {
                    "name": "Umang Prabhakar"
                },
                "author": "Umang Prabhakar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10489v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10489v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04070v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04070v2",
                "updated": "2024-10-14T13:27:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    27,
                    36,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-05T08:00:55Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    8,
                    0,
                    55,
                    5,
                    279,
                    0
                ],
                "title": "PAD: Personalized Alignment at Decoding-Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAD: Personalized Alignment at Decoding-Time"
                },
                "summary": "Aligning with personalized preferences, which vary significantly across\ncultural, educational, and political differences, poses a significant challenge\ndue to the computational costs and data demands of traditional alignment\nmethods. In response, this paper presents Personalized Alignment at\nDecoding-time (PAD), a novel framework designed to align LLM outputs with\ndiverse personalized preferences during the inference phase, eliminating the\nneed for additional training. By introducing a unique personalized reward\nmodeling strategy, this framework decouples the text generation process from\npersonalized preferences, facilitating the generation of generalizable\ntoken-level personalized rewards. The PAD algorithm leverages these rewards to\nguide the decoding process, dynamically tailoring the base model's predictions\nto personalized preferences. Extensive experimental results demonstrate that\nPAD not only outperforms existing training-based alignment methods in terms of\naligning with diverse preferences but also shows significant generalizability\nto preferences unseen during training and scalability across different base\nmodels. This work advances the capability of LLMs to meet user needs in\nreal-time applications, presenting a substantial step forward in personalized\nLLM alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning with personalized preferences, which vary significantly across\ncultural, educational, and political differences, poses a significant challenge\ndue to the computational costs and data demands of traditional alignment\nmethods. In response, this paper presents Personalized Alignment at\nDecoding-time (PAD), a novel framework designed to align LLM outputs with\ndiverse personalized preferences during the inference phase, eliminating the\nneed for additional training. By introducing a unique personalized reward\nmodeling strategy, this framework decouples the text generation process from\npersonalized preferences, facilitating the generation of generalizable\ntoken-level personalized rewards. The PAD algorithm leverages these rewards to\nguide the decoding process, dynamically tailoring the base model's predictions\nto personalized preferences. Extensive experimental results demonstrate that\nPAD not only outperforms existing training-based alignment methods in terms of\naligning with diverse preferences but also shows significant generalizability\nto preferences unseen during training and scalability across different base\nmodels. This work advances the capability of LLMs to meet user needs in\nreal-time applications, presenting a substantial step forward in personalized\nLLM alignment."
                },
                "authors": [
                    {
                        "name": "Ruizhe Chen"
                    },
                    {
                        "name": "Xiaotian Zhang"
                    },
                    {
                        "name": "Meng Luo"
                    },
                    {
                        "name": "Wenhao Chai"
                    },
                    {
                        "name": "Zuozhu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zuozhu Liu"
                },
                "author": "Zuozhu Liu",
                "arxiv_comment": "This paper presents Personalized Alignment at Decoding-time (PAD), a\n  novel framework designed to align LLM outputs with diverse personalized\n  preferences during the inference phase",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04070v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04070v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03618v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03618v3",
                "updated": "2024-10-14T13:19:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    19,
                    57,
                    0,
                    288,
                    0
                ],
                "published": "2024-06-05T20:32:56Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    20,
                    32,
                    56,
                    2,
                    157,
                    0
                ],
                "title": "TACT: Advancing Complex Aggregative Reasoning with Information\n  Extraction Tools",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TACT: Advancing Complex Aggregative Reasoning with Information\n  Extraction Tools"
                },
                "summary": "Large Language Models (LLMs) often do not perform well on queries that\nrequire the aggregation of information across texts. To better evaluate this\nsetting and facilitate modeling efforts, we introduce TACT - Text And\nCalculations through Tables, a dataset crafted to evaluate LLMs' reasoning and\ncomputational abilities using complex instructions. TACT contains challenging\ninstructions that demand stitching information scattered across one or more\ntexts, and performing complex integration on this information to generate the\nanswer. We construct this dataset by leveraging an existing dataset of texts\nand their associated tables. For each such tables, we formulate new queries,\nand gather their respective answers. We demonstrate that all contemporary LLMs\nperform poorly on this dataset, achieving an accuracy below 38%. To pinpoint\nthe difficulties and thoroughly dissect the problem, we analyze model\nperformance across three components: table-generation, Pandas\ncommand-generation, and execution. Unexpectedly, we discover that each\ncomponent presents substantial challenges for current LLMs. These insights lead\nus to propose a focused modeling framework, which we refer to as IE as a tool.\nSpecifically, we propose to add \"tools\" for each of the above steps, and\nimplement each such tool with few-shot prompting. This approach shows an\nimprovement over existing prompting techniques, offering a promising direction\nfor enhancing model capabilities in these tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often do not perform well on queries that\nrequire the aggregation of information across texts. To better evaluate this\nsetting and facilitate modeling efforts, we introduce TACT - Text And\nCalculations through Tables, a dataset crafted to evaluate LLMs' reasoning and\ncomputational abilities using complex instructions. TACT contains challenging\ninstructions that demand stitching information scattered across one or more\ntexts, and performing complex integration on this information to generate the\nanswer. We construct this dataset by leveraging an existing dataset of texts\nand their associated tables. For each such tables, we formulate new queries,\nand gather their respective answers. We demonstrate that all contemporary LLMs\nperform poorly on this dataset, achieving an accuracy below 38%. To pinpoint\nthe difficulties and thoroughly dissect the problem, we analyze model\nperformance across three components: table-generation, Pandas\ncommand-generation, and execution. Unexpectedly, we discover that each\ncomponent presents substantial challenges for current LLMs. These insights lead\nus to propose a focused modeling framework, which we refer to as IE as a tool.\nSpecifically, we propose to add \"tools\" for each of the above steps, and\nimplement each such tool with few-shot prompting. This approach shows an\nimprovement over existing prompting techniques, offering a promising direction\nfor enhancing model capabilities in these tasks."
                },
                "authors": [
                    {
                        "name": "Avi Caciularu"
                    },
                    {
                        "name": "Alon Jacovi"
                    },
                    {
                        "name": "Eyal Ben-David"
                    },
                    {
                        "name": "Sasha Goldshtein"
                    },
                    {
                        "name": "Tal Schuster"
                    },
                    {
                        "name": "Jonathan Herzig"
                    },
                    {
                        "name": "Gal Elidan"
                    },
                    {
                        "name": "Amir Globerson"
                    }
                ],
                "author_detail": {
                    "name": "Amir Globerson"
                },
                "author": "Amir Globerson",
                "arxiv_comment": "Accepted to NeurIPS 2024. Website (https://tact-benchmark.github.io),\n  Huggingface (https://huggingface.co/datasets/google/TACT)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03618v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03618v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10482v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10482v1",
                "updated": "2024-10-14T13:19:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    19,
                    56,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T13:19:56Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    19,
                    56,
                    0,
                    288,
                    0
                ],
                "title": "Regression Model for Speckled Data with Extremely Variability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regression Model for Speckled Data with Extremely Variability"
                },
                "summary": "Synthetic aperture radar (SAR) is an efficient and widely used remote sensing\ntool. However, data extracted from SAR images are contaminated with speckle,\nwhich precludes the application of techniques based on the assumption of\nadditive and normally distributed noise. One of the most successful approaches\nto describing such data is the multiplicative model, where intensities can\nfollow a variety of distributions with positive support. The $\\mathcal{G}^0_I$\nmodel is among the most successful ones. Although several estimation methods\nfor the $\\mathcal{G}^0_I$ parameters have been proposed, there is no work\nexploring a regression structure for this model. Such a structure could allow\nus to infer unobserved values from available ones. In this work, we propose a\n$\\mathcal{G}^0_I$ regression model and use it to describe the influence of\nintensities from other polarimetric channels. We derive some theoretical\nproperties for the new model: Fisher information matrix, residual measures, and\ninfluential tools. Maximum likelihood point and interval estimation methods are\nproposed and evaluated by Monte Carlo experiments. Results from simulated and\nactual data show that the new model can be helpful for SAR image analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic aperture radar (SAR) is an efficient and widely used remote sensing\ntool. However, data extracted from SAR images are contaminated with speckle,\nwhich precludes the application of techniques based on the assumption of\nadditive and normally distributed noise. One of the most successful approaches\nto describing such data is the multiplicative model, where intensities can\nfollow a variety of distributions with positive support. The $\\mathcal{G}^0_I$\nmodel is among the most successful ones. Although several estimation methods\nfor the $\\mathcal{G}^0_I$ parameters have been proposed, there is no work\nexploring a regression structure for this model. Such a structure could allow\nus to infer unobserved values from available ones. In this work, we propose a\n$\\mathcal{G}^0_I$ regression model and use it to describe the influence of\nintensities from other polarimetric channels. We derive some theoretical\nproperties for the new model: Fisher information matrix, residual measures, and\ninfluential tools. Maximum likelihood point and interval estimation methods are\nproposed and evaluated by Monte Carlo experiments. Results from simulated and\nactual data show that the new model can be helpful for SAR image analysis."
                },
                "authors": [
                    {
                        "name": "A. D. C. Nascimento"
                    },
                    {
                        "name": "J. M. Vasconcelos"
                    },
                    {
                        "name": "R. J. Cintra"
                    },
                    {
                        "name": "A. C. Frery"
                    }
                ],
                "author_detail": {
                    "name": "A. C. Frery"
                },
                "author": "A. C. Frery",
                "arxiv_doi": "10.1016/j.isprsjprs.2024.05.009",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.isprsjprs.2024.05.009",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.10482v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10482v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "29 pages, 6 figures, 3 tables",
                "arxiv_journal_ref": "Elsevier ISPRS Journal of Photogrammetry and Remote Sensing,\n  Volume 213, July 2024, Pages 1-13",
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10481v1",
                "updated": "2024-10-14T13:18:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    18,
                    20,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T13:18:20Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    18,
                    20,
                    0,
                    288,
                    0
                ],
                "title": "Model-Based Differentially Private Knowledge Transfer for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-Based Differentially Private Knowledge Transfer for Large Language\n  Models"
                },
                "summary": "As large language models (LLMs) become increasingly prevalent in web\nservices, effectively leveraging domain-specific knowledge while ensuring\nprivacy has become critical. Existing methods, such as retrieval-augmented\ngeneration (RAG) and differentially private data synthesis, often compromise\neither the utility of domain knowledge or the privacy of sensitive data,\nlimiting their applicability in specialized domains. To address these\nchallenges, we propose \\textit{Llamdex}, a novel framework that integrates\nprivacy-preserving, domain-specific models into LLMs. Our approach\nsignificantly enhances the accuracy of domain-specific tasks, achieving up to a\n26\\% improvement compared to existing methods under the same differential\nprivacy constraints. Experimental results show that Llamdex not only improves\nthe accuracy of LLM responses but also maintains comparable inference\nefficiency to the original LLM, highlighting its potential for real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become increasingly prevalent in web\nservices, effectively leveraging domain-specific knowledge while ensuring\nprivacy has become critical. Existing methods, such as retrieval-augmented\ngeneration (RAG) and differentially private data synthesis, often compromise\neither the utility of domain knowledge or the privacy of sensitive data,\nlimiting their applicability in specialized domains. To address these\nchallenges, we propose \\textit{Llamdex}, a novel framework that integrates\nprivacy-preserving, domain-specific models into LLMs. Our approach\nsignificantly enhances the accuracy of domain-specific tasks, achieving up to a\n26\\% improvement compared to existing methods under the same differential\nprivacy constraints. Experimental results show that Llamdex not only improves\nthe accuracy of LLM responses but also maintains comparable inference\nefficiency to the original LLM, highlighting its potential for real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Zhaomin Wu"
                    },
                    {
                        "name": "Jizhou Guo"
                    },
                    {
                        "name": "Junyi Hou"
                    },
                    {
                        "name": "Bingsheng He"
                    },
                    {
                        "name": "Lixin Fan"
                    },
                    {
                        "name": "Qiang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Yang"
                },
                "author": "Qiang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10479v1",
                "updated": "2024-10-14T13:15:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    15,
                    34,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T13:15:34Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    15,
                    34,
                    0,
                    288,
                    0
                ],
                "title": "TMGBench: A Systematic Game Benchmark for Evaluating Strategic Reasoning\n  Abilities of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TMGBench: A Systematic Game Benchmark for Evaluating Strategic Reasoning\n  Abilities of LLMs"
                },
                "summary": "The rapid advancement of large language models (LLMs) has accelerated their\napplication in reasoning, with strategic reasoning drawing increasing\nattention. To evaluate LLMs' strategic reasoning capabilities, game theory,\nwith its concise structure, has become a preferred approach. However, current\nresearch focuses on a limited selection of games, resulting in low coverage.\nClassic game scenarios risk data leakage, and existing benchmarks often lack\nextensibility, making them inadequate for evaluating state-of-the-art models.\nTo address these challenges, we propose TMGBench, a benchmark with\ncomprehensive game type coverage, novel scenarios, and flexible organization.\nSpecifically, we incorporate all 144 game types summarized by the\nRobinson-Goforth topology of 2x2 games, constructed as classic games. We also\nemploy synthetic data generation to create diverse, higher-quality scenarios\nthrough topic guidance and human inspection, referred to as story-based games.\nLastly, we provide a sustainable framework for increasingly powerful LLMs by\ntreating these games as atomic units and organizing them into more complex\nforms via sequential, parallel, and nested structures. Our comprehensive\nevaluation of mainstream LLMs covers tests on rational reasoning, robustness,\nTheory-of-Mind (ToM), and reasoning in complex forms. Results reveal flaws in\naccuracy, consistency, and varying mastery of ToM. Additionally, o1-mini,\nOpenAI's latest reasoning model, achieved accuracy rates of 66.6%, 60.0%, and\n70.0% on sequential, parallel, and nested games, highlighting TMGBench's\nchallenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has accelerated their\napplication in reasoning, with strategic reasoning drawing increasing\nattention. To evaluate LLMs' strategic reasoning capabilities, game theory,\nwith its concise structure, has become a preferred approach. However, current\nresearch focuses on a limited selection of games, resulting in low coverage.\nClassic game scenarios risk data leakage, and existing benchmarks often lack\nextensibility, making them inadequate for evaluating state-of-the-art models.\nTo address these challenges, we propose TMGBench, a benchmark with\ncomprehensive game type coverage, novel scenarios, and flexible organization.\nSpecifically, we incorporate all 144 game types summarized by the\nRobinson-Goforth topology of 2x2 games, constructed as classic games. We also\nemploy synthetic data generation to create diverse, higher-quality scenarios\nthrough topic guidance and human inspection, referred to as story-based games.\nLastly, we provide a sustainable framework for increasingly powerful LLMs by\ntreating these games as atomic units and organizing them into more complex\nforms via sequential, parallel, and nested structures. Our comprehensive\nevaluation of mainstream LLMs covers tests on rational reasoning, robustness,\nTheory-of-Mind (ToM), and reasoning in complex forms. Results reveal flaws in\naccuracy, consistency, and varying mastery of ToM. Additionally, o1-mini,\nOpenAI's latest reasoning model, achieved accuracy rates of 66.6%, 60.0%, and\n70.0% on sequential, parallel, and nested games, highlighting TMGBench's\nchallenges."
                },
                "authors": [
                    {
                        "name": "Haochuan Wang"
                    },
                    {
                        "name": "Xiachong Feng"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Zhanyue Qin"
                    },
                    {
                        "name": "Dianbo Sui"
                    },
                    {
                        "name": "Lingpeng Kong"
                    }
                ],
                "author_detail": {
                    "name": "Lingpeng Kong"
                },
                "author": "Lingpeng Kong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.05465v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.05465v2",
                "updated": "2024-10-14T13:11:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    11,
                    55,
                    0,
                    288,
                    0
                ],
                "published": "2024-04-08T12:43:32Z",
                "published_parsed": [
                    2024,
                    4,
                    8,
                    12,
                    43,
                    32,
                    0,
                    99,
                    0
                ],
                "title": "HAMMR: HierArchical MultiModal React agents for generic VQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HAMMR: HierArchical MultiModal React agents for generic VQA"
                },
                "summary": "Combining Large Language Models (LLMs) with external specialized tools\n(LLMs+tools) is a recent paradigm to solve multimodal tasks such as Visual\nQuestion Answering (VQA). While this approach was demonstrated to work well\nwhen optimized and evaluated for each individual benchmark, in practice it is\ncrucial for the next generation of real-world AI systems to handle a broad\nrange of multimodal problems. Therefore we pose the VQA problem from a unified\nperspective and evaluate a single system on a varied suite of VQA tasks\nincluding counting, spatial reasoning, OCR-based reasoning, visual pointing,\nexternal knowledge, and more. In this setting, we demonstrate that naively\napplying the LLM+tools approach using the combined set of all tools leads to\npoor results. This motivates us to introduce HAMMR: HierArchical MultiModal\nReact. We start from a multimodal ReAct-based system and make it hierarchical\nby enabling our HAMMR agents to call upon other specialized agents. This\nenhances the compositionality of the LLM+tools approach, which we show to be\ncritical for obtaining high accuracy on generic VQA. Concretely, on our generic\nVQA suite, HAMMR outperforms the naive LLM+tools approach by 19.5%.\nAdditionally, HAMMR achieves state-of-the-art results on this task,\noutperforming the generic standalone PaLI-X VQA model by 5.0%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining Large Language Models (LLMs) with external specialized tools\n(LLMs+tools) is a recent paradigm to solve multimodal tasks such as Visual\nQuestion Answering (VQA). While this approach was demonstrated to work well\nwhen optimized and evaluated for each individual benchmark, in practice it is\ncrucial for the next generation of real-world AI systems to handle a broad\nrange of multimodal problems. Therefore we pose the VQA problem from a unified\nperspective and evaluate a single system on a varied suite of VQA tasks\nincluding counting, spatial reasoning, OCR-based reasoning, visual pointing,\nexternal knowledge, and more. In this setting, we demonstrate that naively\napplying the LLM+tools approach using the combined set of all tools leads to\npoor results. This motivates us to introduce HAMMR: HierArchical MultiModal\nReact. We start from a multimodal ReAct-based system and make it hierarchical\nby enabling our HAMMR agents to call upon other specialized agents. This\nenhances the compositionality of the LLM+tools approach, which we show to be\ncritical for obtaining high accuracy on generic VQA. Concretely, on our generic\nVQA suite, HAMMR outperforms the naive LLM+tools approach by 19.5%.\nAdditionally, HAMMR achieves state-of-the-art results on this task,\noutperforming the generic standalone PaLI-X VQA model by 5.0%."
                },
                "authors": [
                    {
                        "name": "Lluis Castrejon"
                    },
                    {
                        "name": "Thomas Mensink"
                    },
                    {
                        "name": "Howard Zhou"
                    },
                    {
                        "name": "Vittorio Ferrari"
                    },
                    {
                        "name": "Andre Araujo"
                    },
                    {
                        "name": "Jasper Uijlings"
                    }
                ],
                "author_detail": {
                    "name": "Jasper Uijlings"
                },
                "author": "Jasper Uijlings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.05465v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.05465v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10476v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10476v1",
                "updated": "2024-10-14T13:10:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    10,
                    45,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T13:10:45Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    10,
                    45,
                    0,
                    288,
                    0
                ],
                "title": "Will LLMs Replace the Encoder-Only Models in Temporal Relation\n  Classification?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Will LLMs Replace the Encoder-Only Models in Temporal Relation\n  Classification?"
                },
                "summary": "The automatic detection of temporal relations among events has been mainly\ninvestigated with encoder-only models such as RoBERTa. Large Language Models\n(LLM) have recently shown promising performance in temporal reasoning tasks\nsuch as temporal question answering. Nevertheless, recent studies have tested\nthe LLMs' performance in detecting temporal relations of closed-source models\nonly, limiting the interpretability of those results. In this work, we\ninvestigate LLMs' performance and decision process in the Temporal Relation\nClassification task. First, we assess the performance of seven open and\nclosed-sourced LLMs experimenting with in-context learning and lightweight\nfine-tuning approaches. Results show that LLMs with in-context learning\nsignificantly underperform smaller encoder-only models based on RoBERTa. Then,\nwe delve into the possible reasons for this gap by applying explainable\nmethods. The outcome suggests a limitation of LLMs in this task due to their\nautoregressive nature, which causes them to focus only on the last part of the\nsequence. Additionally, we evaluate the word embeddings of these two models to\nbetter understand their pre-training differences. The code and the fine-tuned\nmodels can be found respectively on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The automatic detection of temporal relations among events has been mainly\ninvestigated with encoder-only models such as RoBERTa. Large Language Models\n(LLM) have recently shown promising performance in temporal reasoning tasks\nsuch as temporal question answering. Nevertheless, recent studies have tested\nthe LLMs' performance in detecting temporal relations of closed-source models\nonly, limiting the interpretability of those results. In this work, we\ninvestigate LLMs' performance and decision process in the Temporal Relation\nClassification task. First, we assess the performance of seven open and\nclosed-sourced LLMs experimenting with in-context learning and lightweight\nfine-tuning approaches. Results show that LLMs with in-context learning\nsignificantly underperform smaller encoder-only models based on RoBERTa. Then,\nwe delve into the possible reasons for this gap by applying explainable\nmethods. The outcome suggests a limitation of LLMs in this task due to their\nautoregressive nature, which causes them to focus only on the last part of the\nsequence. Additionally, we evaluate the word embeddings of these two models to\nbetter understand their pre-training differences. The code and the fine-tuned\nmodels can be found respectively on GitHub."
                },
                "authors": [
                    {
                        "name": "Gabriel Roccabruna"
                    },
                    {
                        "name": "Massimo Rizzoli"
                    },
                    {
                        "name": "Giuseppe Riccardi"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Riccardi"
                },
                "author": "Giuseppe Riccardi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10476v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10476v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.10818v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10818v1",
                "updated": "2024-10-14T17:59:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    58,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:59:58Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    58,
                    0,
                    288,
                    0
                ],
                "title": "TemporalBench: Benchmarking Fine-grained Temporal Understanding for\n  Multimodal Video Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TemporalBench: Benchmarking Fine-grained Temporal Understanding for\n  Multimodal Video Models"
                },
                "summary": "Understanding fine-grained temporal dynamics is crucial for multimodal video\ncomprehension and generation. Due to the lack of fine-grained temporal\nannotations, existing video benchmarks mostly resemble static image benchmarks\nand are incompetent at evaluating models for temporal understanding. In this\npaper, we introduce TemporalBench, a new benchmark dedicated to evaluating\nfine-grained temporal understanding in videos. TemporalBench consists of ~10K\nvideo question-answer pairs, derived from ~2K high-quality human annotations\ndetailing the temporal dynamics in video clips. As a result, our benchmark\nprovides a unique testbed for evaluating various temporal understanding and\nreasoning abilities such as action frequency, motion magnitude, event order,\netc. Moreover, it enables evaluations on various tasks like both video question\nanswering and captioning, both short and long video understanding, as well as\ndifferent models such as multimodal video embedding models and text generation\nmodels. Results show that state-of-the-art models like GPT-4o achieve only\n38.5% question answering accuracy on TemporalBench, demonstrating a significant\ngap (~30%) between humans and AI in temporal understanding. Furthermore, we\nnotice a critical pitfall for multi-choice QA where LLMs can detect the subtle\nchanges in negative captions and find a centralized description as a cue for\nits prediction, where we propose Multiple Binary Accuracy (MBA) to correct such\nbias. We hope that TemporalBench can foster research on improving models'\ntemporal reasoning capabilities. Both dataset and evaluation code will be made\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding fine-grained temporal dynamics is crucial for multimodal video\ncomprehension and generation. Due to the lack of fine-grained temporal\nannotations, existing video benchmarks mostly resemble static image benchmarks\nand are incompetent at evaluating models for temporal understanding. In this\npaper, we introduce TemporalBench, a new benchmark dedicated to evaluating\nfine-grained temporal understanding in videos. TemporalBench consists of ~10K\nvideo question-answer pairs, derived from ~2K high-quality human annotations\ndetailing the temporal dynamics in video clips. As a result, our benchmark\nprovides a unique testbed for evaluating various temporal understanding and\nreasoning abilities such as action frequency, motion magnitude, event order,\netc. Moreover, it enables evaluations on various tasks like both video question\nanswering and captioning, both short and long video understanding, as well as\ndifferent models such as multimodal video embedding models and text generation\nmodels. Results show that state-of-the-art models like GPT-4o achieve only\n38.5% question answering accuracy on TemporalBench, demonstrating a significant\ngap (~30%) between humans and AI in temporal understanding. Furthermore, we\nnotice a critical pitfall for multi-choice QA where LLMs can detect the subtle\nchanges in negative captions and find a centralized description as a cue for\nits prediction, where we propose Multiple Binary Accuracy (MBA) to correct such\nbias. We hope that TemporalBench can foster research on improving models'\ntemporal reasoning capabilities. Both dataset and evaluation code will be made\navailable."
                },
                "authors": [
                    {
                        "name": "Mu Cai"
                    },
                    {
                        "name": "Reuben Tan"
                    },
                    {
                        "name": "Jianrui Zhang"
                    },
                    {
                        "name": "Bocheng Zou"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Feng Yao"
                    },
                    {
                        "name": "Fangrui Zhu"
                    },
                    {
                        "name": "Jing Gu"
                    },
                    {
                        "name": "Yiwu Zhong"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yao Dou"
                    },
                    {
                        "name": "Jaden Park"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Yong Jae Lee"
                    },
                    {
                        "name": "Jianwei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jianwei Yang"
                },
                "author": "Jianwei Yang",
                "arxiv_comment": "Project Page: https://temporalbench.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10818v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10818v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10819v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10819v1",
                "updated": "2024-10-14T17:59:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    58,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:59:58Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    58,
                    0,
                    288,
                    0
                ],
                "title": "DuoAttention: Efficient Long-Context LLM Inference with Retrieval and\n  Streaming Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DuoAttention: Efficient Long-Context LLM Inference with Retrieval and\n  Streaming Heads"
                },
                "summary": "Deploying long-context large language models (LLMs) is essential but poses\nsignificant computational and memory challenges. Caching all Key and Value (KV)\nstates across all attention heads consumes substantial memory. Existing KV\ncache pruning methods either damage the long-context capabilities of LLMs or\noffer only limited efficiency improvements. In this paper, we identify that\nonly a fraction of attention heads, a.k.a, Retrieval Heads, are critical for\nprocessing long contexts and require full attention across all tokens. In\ncontrast, all other heads, which primarily focus on recent tokens and attention\nsinks--referred to as Streaming Heads--do not require full attention. Based on\nthis insight, we introduce DuoAttention, a framework that only applies a full\nKV cache to retrieval heads while using a light-weight, constant-length KV\ncache for streaming heads, which reduces both LLM's decoding and pre-filling\nmemory and latency without compromising its long-context abilities.\nDuoAttention uses a lightweight, optimization-based algorithm with synthetic\ndata to identify retrieval heads accurately. Our method significantly reduces\nlong-context inference memory by up to 2.55x for MHA and 1.67x for GQA models\nwhile speeding up decoding by up to 2.18x and 1.50x and accelerating\npre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with\nminimal accuracy loss compared to full attention. Notably, combined with\nquantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context\nlength on a single A100 GPU. Code is provided in\nhttps://github.com/mit-han-lab/duo-attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying long-context large language models (LLMs) is essential but poses\nsignificant computational and memory challenges. Caching all Key and Value (KV)\nstates across all attention heads consumes substantial memory. Existing KV\ncache pruning methods either damage the long-context capabilities of LLMs or\noffer only limited efficiency improvements. In this paper, we identify that\nonly a fraction of attention heads, a.k.a, Retrieval Heads, are critical for\nprocessing long contexts and require full attention across all tokens. In\ncontrast, all other heads, which primarily focus on recent tokens and attention\nsinks--referred to as Streaming Heads--do not require full attention. Based on\nthis insight, we introduce DuoAttention, a framework that only applies a full\nKV cache to retrieval heads while using a light-weight, constant-length KV\ncache for streaming heads, which reduces both LLM's decoding and pre-filling\nmemory and latency without compromising its long-context abilities.\nDuoAttention uses a lightweight, optimization-based algorithm with synthetic\ndata to identify retrieval heads accurately. Our method significantly reduces\nlong-context inference memory by up to 2.55x for MHA and 1.67x for GQA models\nwhile speeding up decoding by up to 2.18x and 1.50x and accelerating\npre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with\nminimal accuracy loss compared to full attention. Notably, combined with\nquantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context\nlength on a single A100 GPU. Code is provided in\nhttps://github.com/mit-han-lab/duo-attention."
                },
                "authors": [
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Jingwei Zuo"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10819v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10813v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10813v1",
                "updated": "2024-10-14T17:59:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    44,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:59:44Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    44,
                    0,
                    288,
                    0
                ],
                "title": "LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive\n  Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive\n  Memory"
                },
                "summary": "Recent large language model (LLM)-driven chat assistant systems have\nintegrated memory components to track user-assistant chat histories, enabling\nmore accurate and personalized responses. However, their long-term memory\ncapabilities in sustained interactions remain underexplored. This paper\nintroduces LongMemEval, a comprehensive benchmark designed to evaluate five\ncore long-term memory abilities of chat assistants: information extraction,\nmulti-session reasoning, temporal reasoning, knowledge updates, and abstention.\nWith 500 meticulously curated questions embedded within freely scalable\nuser-assistant chat histories, LongMemEval presents a significant challenge to\nexisting long-term memory systems, with commercial chat assistants and\nlong-context LLMs showing 30% accuracy drop on memorizing information across\nsustained interactions. We then present a unified framework that breaks down\nthe long-term memory design into four design choices across the indexing,\nretrieval, and reading stages. Built upon key experimental insights, we propose\nseveral memory designs including session decomposition for optimizing value\ngranularity, fact-augmented key expansion for enhancing the index structure,\nand time-aware query expansion for refining the search scope. Experiment\nresults show that these optimizations greatly improve both memory recall and\ndownstream question answering on LongMemEval. Overall, our study provides\nvaluable resources and guidance for advancing the long-term memory capabilities\nof LLM-based chat assistants, paving the way toward more personalized and\nreliable conversational AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language model (LLM)-driven chat assistant systems have\nintegrated memory components to track user-assistant chat histories, enabling\nmore accurate and personalized responses. However, their long-term memory\ncapabilities in sustained interactions remain underexplored. This paper\nintroduces LongMemEval, a comprehensive benchmark designed to evaluate five\ncore long-term memory abilities of chat assistants: information extraction,\nmulti-session reasoning, temporal reasoning, knowledge updates, and abstention.\nWith 500 meticulously curated questions embedded within freely scalable\nuser-assistant chat histories, LongMemEval presents a significant challenge to\nexisting long-term memory systems, with commercial chat assistants and\nlong-context LLMs showing 30% accuracy drop on memorizing information across\nsustained interactions. We then present a unified framework that breaks down\nthe long-term memory design into four design choices across the indexing,\nretrieval, and reading stages. Built upon key experimental insights, we propose\nseveral memory designs including session decomposition for optimizing value\ngranularity, fact-augmented key expansion for enhancing the index structure,\nand time-aware query expansion for refining the search scope. Experiment\nresults show that these optimizations greatly improve both memory recall and\ndownstream question answering on LongMemEval. Overall, our study provides\nvaluable resources and guidance for advancing the long-term memory capabilities\nof LLM-based chat assistants, paving the way toward more personalized and\nreliable conversational AI."
                },
                "authors": [
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Hongwei Wang"
                    },
                    {
                        "name": "Wenhao Yu"
                    },
                    {
                        "name": "Yuwei Zhang"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10813v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10813v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10814v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10814v1",
                "updated": "2024-10-14T17:59:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    44,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:59:44Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    44,
                    0,
                    288,
                    0
                ],
                "title": "Your Mixture-of-Experts LLM Is Secretly an Embedding Model For Free",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Your Mixture-of-Experts LLM Is Secretly an Embedding Model For Free"
                },
                "summary": "While large language models (LLMs) excel on generation tasks, their\ndecoder-only architecture often limits their potential as embedding models if\nno further representation finetuning is applied. Does this contradict their\nclaim of generalists? To answer the question, we take a closer look at\nMixture-of-Experts (MoE) LLMs. Our study shows that the expert routers in MoE\nLLMs can serve as an off-the-shelf embedding model with promising performance\non a diverse class of embedding-focused tasks, without requiring any\nfinetuning. Moreover, our extensive analysis shows that the MoE routing weights\n(RW) is complementary to the hidden state (HS) of LLMs, a widely-used\nembedding. Compared to HS, we find that RW is more robust to the choice of\nprompts and focuses on high-level semantics. Motivated by the analysis, we\npropose MoEE combining RW and HS, which achieves better performance than using\neither separately. Our exploration of their combination and prompting strategy\nshed several novel insights, e.g., a weighted sum of RW and HS similarities\noutperforms the similarity on their concatenation. Our experiments are\nconducted on 6 embedding tasks with 20 datasets from the Massive Text Embedding\nBenchmark (MTEB). The results demonstrate the significant improvement brought\nby MoEE to LLM-based embedding without further finetuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) excel on generation tasks, their\ndecoder-only architecture often limits their potential as embedding models if\nno further representation finetuning is applied. Does this contradict their\nclaim of generalists? To answer the question, we take a closer look at\nMixture-of-Experts (MoE) LLMs. Our study shows that the expert routers in MoE\nLLMs can serve as an off-the-shelf embedding model with promising performance\non a diverse class of embedding-focused tasks, without requiring any\nfinetuning. Moreover, our extensive analysis shows that the MoE routing weights\n(RW) is complementary to the hidden state (HS) of LLMs, a widely-used\nembedding. Compared to HS, we find that RW is more robust to the choice of\nprompts and focuses on high-level semantics. Motivated by the analysis, we\npropose MoEE combining RW and HS, which achieves better performance than using\neither separately. Our exploration of their combination and prompting strategy\nshed several novel insights, e.g., a weighted sum of RW and HS similarities\noutperforms the similarity on their concatenation. Our experiments are\nconducted on 6 embedding tasks with 20 datasets from the Massive Text Embedding\nBenchmark (MTEB). The results demonstrate the significant improvement brought\nby MoEE to LLM-based embedding without further finetuning."
                },
                "authors": [
                    {
                        "name": "Ziyue Li"
                    },
                    {
                        "name": "Tianyi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Zhou"
                },
                "author": "Tianyi Zhou",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10814v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10814v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10803v1",
                "updated": "2024-10-14T17:59:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    0,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:59:00Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    0,
                    0,
                    288,
                    0
                ],
                "title": "Generalizable Humanoid Manipulation with Improved 3D Diffusion Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalizable Humanoid Manipulation with Improved 3D Diffusion Policies"
                },
                "summary": "Humanoid robots capable of autonomous operation in diverse environments have\nlong been a goal for roboticists. However, autonomous manipulation by humanoid\nrobots has largely been restricted to one specific scene, primarily due to the\ndifficulty of acquiring generalizable skills. Recent advances in 3D visuomotor\npolicies, such as the 3D Diffusion Policy (DP3), have shown promise in\nextending these capabilities to wilder environments. However, 3D visuomotor\npolicies often rely on camera calibration and point-cloud segmentation, which\npresent challenges for deployment on mobile robots like humanoids. In this\nwork, we introduce the Improved 3D Diffusion Policy (iDP3), a novel 3D\nvisuomotor policy that eliminates these constraints by leveraging egocentric 3D\nvisual representations. We demonstrate that iDP3 enables a full-sized humanoid\nrobot to autonomously perform skills in diverse real-world scenarios, using\nonly data collected in the lab. Videos are available at:\nhttps://humanoid-manipulation.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humanoid robots capable of autonomous operation in diverse environments have\nlong been a goal for roboticists. However, autonomous manipulation by humanoid\nrobots has largely been restricted to one specific scene, primarily due to the\ndifficulty of acquiring generalizable skills. Recent advances in 3D visuomotor\npolicies, such as the 3D Diffusion Policy (DP3), have shown promise in\nextending these capabilities to wilder environments. However, 3D visuomotor\npolicies often rely on camera calibration and point-cloud segmentation, which\npresent challenges for deployment on mobile robots like humanoids. In this\nwork, we introduce the Improved 3D Diffusion Policy (iDP3), a novel 3D\nvisuomotor policy that eliminates these constraints by leveraging egocentric 3D\nvisual representations. We demonstrate that iDP3 enables a full-sized humanoid\nrobot to autonomously perform skills in diverse real-world scenarios, using\nonly data collected in the lab. Videos are available at:\nhttps://humanoid-manipulation.github.io"
                },
                "authors": [
                    {
                        "name": "Yanjie Ze"
                    },
                    {
                        "name": "Zixuan Chen"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Tianyi Chen"
                    },
                    {
                        "name": "Xialin He"
                    },
                    {
                        "name": "Ying Yuan"
                    },
                    {
                        "name": "Xue Bin Peng"
                    },
                    {
                        "name": "Jiajun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Wu"
                },
                "author": "Jiajun Wu",
                "arxiv_comment": "Project website: https://humanoid-manipulation.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10801v1",
                "updated": "2024-10-14T17:58:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    58,
                    1,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:58:01Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    58,
                    1,
                    0,
                    288,
                    0
                ],
                "title": "Mix Data or Merge Models? Optimizing for Diverse Multi-Task Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mix Data or Merge Models? Optimizing for Diverse Multi-Task Learning"
                },
                "summary": "Large Language Models (LLMs) have been adopted and deployed worldwide for a\nbroad variety of applications. However, ensuring their safe use remains a\nsignificant challenge. Preference training and safety measures often overfit to\nharms prevalent in Western-centric datasets, and safety protocols frequently\nfail to extend to multilingual settings. In this work, we explore model merging\nin a diverse multi-task setting, combining safety and general-purpose tasks\nwithin a multilingual context. Each language introduces unique and varied\nlearning challenges across tasks. We find that objective-based merging is more\neffective than mixing data, with improvements of up to 8% and 10% in general\nperformance and safety respectively. We also find that language-based merging\nis highly effective -- by merging monolingually fine-tuned models, we achieve a\n4% increase in general performance and 7% reduction in harm across all\nlanguages on top of the data mixtures method using the same available data.\nOverall, our comprehensive study of merging approaches provides a useful\nframework for building strong and safe multilingual models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been adopted and deployed worldwide for a\nbroad variety of applications. However, ensuring their safe use remains a\nsignificant challenge. Preference training and safety measures often overfit to\nharms prevalent in Western-centric datasets, and safety protocols frequently\nfail to extend to multilingual settings. In this work, we explore model merging\nin a diverse multi-task setting, combining safety and general-purpose tasks\nwithin a multilingual context. Each language introduces unique and varied\nlearning challenges across tasks. We find that objective-based merging is more\neffective than mixing data, with improvements of up to 8% and 10% in general\nperformance and safety respectively. We also find that language-based merging\nis highly effective -- by merging monolingually fine-tuned models, we achieve a\n4% increase in general performance and 7% reduction in harm across all\nlanguages on top of the data mixtures method using the same available data.\nOverall, our comprehensive study of merging approaches provides a useful\nframework for building strong and safe multilingual models."
                },
                "authors": [
                    {
                        "name": "Aakanksha"
                    },
                    {
                        "name": "Arash Ahmadian"
                    },
                    {
                        "name": "Seraphina Goldfarb-Tarrant"
                    },
                    {
                        "name": "Beyza Ermis"
                    },
                    {
                        "name": "Marzieh Fadaee"
                    },
                    {
                        "name": "Sara Hooker"
                    }
                ],
                "author_detail": {
                    "name": "Sara Hooker"
                },
                "author": "Sara Hooker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10796v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10796v1",
                "updated": "2024-10-14T17:57:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    57,
                    9,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:57:09Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    57,
                    9,
                    0,
                    288,
                    0
                ],
                "title": "Context-Parametric Inversion: Why Instruction Finetuning May Not\n  Actually Improve Context Reliance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-Parametric Inversion: Why Instruction Finetuning May Not\n  Actually Improve Context Reliance"
                },
                "summary": "Large language models are instruction-finetuned to enhance their ability to\nfollow user instructions and process the input context. However, even\nstate-of-the-art models often struggle to follow the instruction, especially\nwhen the input context is not aligned with the model's parametric knowledge.\nThis manifests as various failures, such as hallucinations where the responses\nare outdated, biased or contain unverified facts. In this work, we try to\nunderstand the underlying reason for this poor context reliance, especially\nafter instruction tuning. We observe an intriguing phenomenon: during\ninstruction tuning, the context reliance initially increases as expected, but\nthen gradually decreases as instruction finetuning progresses. We call this\nphenomenon context-parametric inversion and observe it across multiple general\npurpose instruction tuning datasets like TULU, Alpaca and Ultrachat, as well as\nmodel families such as Llama, Mistral and Pythia. In a simple theoretical\nsetup, we isolate why context-parametric inversion occurs along the gradient\ndescent trajectory of instruction finetuning. We tie this phenomena to examples\nin the instruction finetuning data mixture where the input context provides\ninformation that is already present in the model's parametric knowledge. Our\nanalysis suggests natural mitigation strategies that provide some limited\ngains, while also validating our theoretical insights. We hope that our work\nserves as a starting point in addressing this failure mode in a staple part of\nLLM training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are instruction-finetuned to enhance their ability to\nfollow user instructions and process the input context. However, even\nstate-of-the-art models often struggle to follow the instruction, especially\nwhen the input context is not aligned with the model's parametric knowledge.\nThis manifests as various failures, such as hallucinations where the responses\nare outdated, biased or contain unverified facts. In this work, we try to\nunderstand the underlying reason for this poor context reliance, especially\nafter instruction tuning. We observe an intriguing phenomenon: during\ninstruction tuning, the context reliance initially increases as expected, but\nthen gradually decreases as instruction finetuning progresses. We call this\nphenomenon context-parametric inversion and observe it across multiple general\npurpose instruction tuning datasets like TULU, Alpaca and Ultrachat, as well as\nmodel families such as Llama, Mistral and Pythia. In a simple theoretical\nsetup, we isolate why context-parametric inversion occurs along the gradient\ndescent trajectory of instruction finetuning. We tie this phenomena to examples\nin the instruction finetuning data mixture where the input context provides\ninformation that is already present in the model's parametric knowledge. Our\nanalysis suggests natural mitigation strategies that provide some limited\ngains, while also validating our theoretical insights. We hope that our work\nserves as a starting point in addressing this failure mode in a staple part of\nLLM training."
                },
                "authors": [
                    {
                        "name": "Sachin Goyal"
                    },
                    {
                        "name": "Christina Baek"
                    },
                    {
                        "name": "J. Zico Kolter"
                    },
                    {
                        "name": "Aditi Raghunathan"
                    }
                ],
                "author_detail": {
                    "name": "Aditi Raghunathan"
                },
                "author": "Aditi Raghunathan",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10796v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10796v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10779v1",
                "updated": "2024-10-14T17:49:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    49,
                    54,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:49:54Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    49,
                    54,
                    0,
                    288,
                    0
                ],
                "title": "Focused ReAct: Improving ReAct through Reiterate and Early Stop",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Focused ReAct: Improving ReAct through Reiterate and Early Stop"
                },
                "summary": "Large language models (LLMs) have significantly improved their reasoning and\ndecision-making capabilities, as seen in methods like ReAct. However, despite\nits effectiveness in tackling complex tasks, ReAct faces two main challenges:\nlosing focus on the original question and becoming stuck in action loops. To\naddress these issues, we introduce Focused ReAct, an enhanced version of the\nReAct paradigm that incorporates reiteration and early stop mechanisms. These\nimprovements help the model stay focused on the original query and avoid\nrepetitive behaviors. Experimental results show accuracy gains of 18% to 530%\nand a runtime reduction of up to 34% compared to the original ReAct method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have significantly improved their reasoning and\ndecision-making capabilities, as seen in methods like ReAct. However, despite\nits effectiveness in tackling complex tasks, ReAct faces two main challenges:\nlosing focus on the original question and becoming stuck in action loops. To\naddress these issues, we introduce Focused ReAct, an enhanced version of the\nReAct paradigm that incorporates reiteration and early stop mechanisms. These\nimprovements help the model stay focused on the original query and avoid\nrepetitive behaviors. Experimental results show accuracy gains of 18% to 530%\nand a runtime reduction of up to 34% compared to the original ReAct method."
                },
                "authors": [
                    {
                        "name": "Shuoqiu Li"
                    },
                    {
                        "name": "Han Xu"
                    },
                    {
                        "name": "Haipeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haipeng Chen"
                },
                "author": "Haipeng Chen",
                "arxiv_comment": "The Eighth Widening NLP Workshop (WiNLP 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07882v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07882v3",
                "updated": "2024-10-14T17:46:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    46,
                    28,
                    0,
                    288,
                    0
                ],
                "published": "2024-06-12T05:20:16Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    5,
                    20,
                    16,
                    2,
                    164,
                    0
                ],
                "title": "Designing a Dashboard for Transparency and Control of Conversational AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing a Dashboard for Transparency and Control of Conversational AI"
                },
                "summary": "Conversational LLMs function as black box systems, leaving users guessing\nabout why they see the output they do. This lack of transparency is potentially\nproblematic, especially given concerns around bias and truthfulness. To address\nthis issue, we present an end-to-end prototype-connecting interpretability\ntechniques with user experience design-that seeks to make chatbots more\ntransparent. We begin by showing evidence that a prominent open-source LLM has\na \"user model\": examining the internal state of the system, we can extract data\nrelated to a user's age, gender, educational level, and socioeconomic status.\nNext, we describe the design of a dashboard that accompanies the chatbot\ninterface, displaying this user model in real time. The dashboard can also be\nused to control the user model and the system's behavior. Finally, we discuss a\nstudy in which users conversed with the instrumented system. Our results\nsuggest that users appreciate seeing internal states, which helped them expose\nbiased behavior and increased their sense of control. Participants also made\nvaluable suggestions that point to future directions for both design and\nmachine learning research. The project page and video demo of our TalkTuner\nsystem are available at https://bit.ly/talktuner-project-page",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational LLMs function as black box systems, leaving users guessing\nabout why they see the output they do. This lack of transparency is potentially\nproblematic, especially given concerns around bias and truthfulness. To address\nthis issue, we present an end-to-end prototype-connecting interpretability\ntechniques with user experience design-that seeks to make chatbots more\ntransparent. We begin by showing evidence that a prominent open-source LLM has\na \"user model\": examining the internal state of the system, we can extract data\nrelated to a user's age, gender, educational level, and socioeconomic status.\nNext, we describe the design of a dashboard that accompanies the chatbot\ninterface, displaying this user model in real time. The dashboard can also be\nused to control the user model and the system's behavior. Finally, we discuss a\nstudy in which users conversed with the instrumented system. Our results\nsuggest that users appreciate seeing internal states, which helped them expose\nbiased behavior and increased their sense of control. Participants also made\nvaluable suggestions that point to future directions for both design and\nmachine learning research. The project page and video demo of our TalkTuner\nsystem are available at https://bit.ly/talktuner-project-page"
                },
                "authors": [
                    {
                        "name": "Yida Chen"
                    },
                    {
                        "name": "Aoyu Wu"
                    },
                    {
                        "name": "Trevor DePodesta"
                    },
                    {
                        "name": "Catherine Yeh"
                    },
                    {
                        "name": "Kenneth Li"
                    },
                    {
                        "name": "Nicholas Castillo Marin"
                    },
                    {
                        "name": "Oam Patel"
                    },
                    {
                        "name": "Jan Riecke"
                    },
                    {
                        "name": "Shivam Raval"
                    },
                    {
                        "name": "Olivia Seow"
                    },
                    {
                        "name": "Martin Wattenberg"
                    },
                    {
                        "name": "Fernanda Vigas"
                    }
                ],
                "author_detail": {
                    "name": "Fernanda Vigas"
                },
                "author": "Fernanda Vigas",
                "arxiv_comment": "Project page: https://bit.ly/talktuner-project-page, 38 pages, 23\n  figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07882v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07882v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10762v1",
                "updated": "2024-10-14T17:40:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    40,
                    40,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:40:40Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    40,
                    40,
                    0,
                    288,
                    0
                ],
                "title": "AFlow: Automating Agentic Workflow Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AFlow: Automating Agentic Workflow Generation"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable potential in\nsolving complex tasks across diverse domains, typically by employing agentic\nworkflows that follow detailed instructions and operational sequences. However,\nconstructing these workflows requires significant human effort, limiting\nscalability and generalizability. Recent research has sought to automate the\ngeneration and optimization of these workflows, but existing methods still rely\non initial manual setup and fall short of achieving fully automated and\neffective workflow generation. To address this challenge, we reformulate\nworkflow optimization as a search problem over code-represented workflows,\nwhere LLM-invoking nodes are connected by edges. We introduce AFlow, an\nautomated framework that efficiently explores this space using Monte Carlo Tree\nSearch, iteratively refining workflows through code modification,\ntree-structured experience, and execution feedback. Empirical evaluations\nacross six benchmark datasets demonstrate AFlow's efficacy, yielding a 5.7%\naverage improvement over state-of-the-art baselines. Furthermore, AFlow enables\nsmaller models to outperform GPT-4o on specific tasks at 4.55% of its inference\ncost in dollars. The code will be available at\nhttps://github.com/geekan/MetaGPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable potential in\nsolving complex tasks across diverse domains, typically by employing agentic\nworkflows that follow detailed instructions and operational sequences. However,\nconstructing these workflows requires significant human effort, limiting\nscalability and generalizability. Recent research has sought to automate the\ngeneration and optimization of these workflows, but existing methods still rely\non initial manual setup and fall short of achieving fully automated and\neffective workflow generation. To address this challenge, we reformulate\nworkflow optimization as a search problem over code-represented workflows,\nwhere LLM-invoking nodes are connected by edges. We introduce AFlow, an\nautomated framework that efficiently explores this space using Monte Carlo Tree\nSearch, iteratively refining workflows through code modification,\ntree-structured experience, and execution feedback. Empirical evaluations\nacross six benchmark datasets demonstrate AFlow's efficacy, yielding a 5.7%\naverage improvement over state-of-the-art baselines. Furthermore, AFlow enables\nsmaller models to outperform GPT-4o on specific tasks at 4.55% of its inference\ncost in dollars. The code will be available at\nhttps://github.com/geekan/MetaGPT."
                },
                "authors": [
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Jinyu Xiang"
                    },
                    {
                        "name": "Zhaoyang Yu"
                    },
                    {
                        "name": "Fengwei Teng"
                    },
                    {
                        "name": "Xionghui Chen"
                    },
                    {
                        "name": "Jiaqi Chen"
                    },
                    {
                        "name": "Mingchen Zhuge"
                    },
                    {
                        "name": "Xin Cheng"
                    },
                    {
                        "name": "Sirui Hong"
                    },
                    {
                        "name": "Jinlin Wang"
                    },
                    {
                        "name": "Bingnan Zheng"
                    },
                    {
                        "name": "Bang Liu"
                    },
                    {
                        "name": "Yuyu Luo"
                    },
                    {
                        "name": "Chenglin Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chenglin Wu"
                },
                "author": "Chenglin Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10760v1",
                "updated": "2024-10-14T17:39:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    39,
                    31,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:39:31Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    39,
                    31,
                    0,
                    288,
                    0
                ],
                "title": "Denial-of-Service Poisoning Attacks against Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Denial-of-Service Poisoning Attacks against Large Language Models"
                },
                "summary": "Recent studies have shown that LLMs are vulnerable to denial-of-service (DoS)\nattacks, where adversarial inputs like spelling errors or non-semantic prompts\ntrigger endless outputs without generating an [EOS] token. These attacks can\npotentially cause high latency and make LLM services inaccessible to other\nusers or tasks. However, when there are speech-to-text interfaces (e.g., voice\ncommands to a robot), executing such DoS attacks becomes challenging, as it is\ndifficult to introduce spelling errors or non-semantic prompts through speech.\nA simple DoS attack in these scenarios would be to instruct the model to \"Keep\nrepeating Hello\", but we observe that relying solely on natural instructions\nlimits output length, which is bounded by the maximum length of the LLM's\nsupervised finetuning (SFT) data. To overcome this limitation, we propose\npoisoning-based DoS (P-DoS) attacks for LLMs, demonstrating that injecting a\nsingle poisoned sample designed for DoS purposes can break the output length\nlimit. For example, a poisoned sample can successfully attack GPT-4o and GPT-4o\nmini (via OpenAI's finetuning API) using less than $1, causing repeated outputs\nup to the maximum inference length (16K tokens, compared to 0.5K before\npoisoning). Additionally, we perform comprehensive ablation studies on\nopen-source LLMs and extend our method to LLM agents, where attackers can\ncontrol both the finetuning dataset and algorithm. Our findings underscore the\nurgent need for defenses against P-DoS attacks to secure LLMs. Our code is\navailable at https://github.com/sail-sg/P-DoS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have shown that LLMs are vulnerable to denial-of-service (DoS)\nattacks, where adversarial inputs like spelling errors or non-semantic prompts\ntrigger endless outputs without generating an [EOS] token. These attacks can\npotentially cause high latency and make LLM services inaccessible to other\nusers or tasks. However, when there are speech-to-text interfaces (e.g., voice\ncommands to a robot), executing such DoS attacks becomes challenging, as it is\ndifficult to introduce spelling errors or non-semantic prompts through speech.\nA simple DoS attack in these scenarios would be to instruct the model to \"Keep\nrepeating Hello\", but we observe that relying solely on natural instructions\nlimits output length, which is bounded by the maximum length of the LLM's\nsupervised finetuning (SFT) data. To overcome this limitation, we propose\npoisoning-based DoS (P-DoS) attacks for LLMs, demonstrating that injecting a\nsingle poisoned sample designed for DoS purposes can break the output length\nlimit. For example, a poisoned sample can successfully attack GPT-4o and GPT-4o\nmini (via OpenAI's finetuning API) using less than $1, causing repeated outputs\nup to the maximum inference length (16K tokens, compared to 0.5K before\npoisoning). Additionally, we perform comprehensive ablation studies on\nopen-source LLMs and extend our method to LLM agents, where attackers can\ncontrol both the finetuning dataset and algorithm. Our findings underscore the\nurgent need for defenses against P-DoS attacks to secure LLMs. Our code is\navailable at https://github.com/sail-sg/P-DoS."
                },
                "authors": [
                    {
                        "name": "Kuofeng Gao"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Shu-Tao Xia"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10759v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10759v1",
                "updated": "2024-10-14T17:38:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    38,
                    41,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:38:41Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    38,
                    41,
                    0,
                    288,
                    0
                ],
                "title": "SplitLLM: Collaborative Inference of LLMs for Model Placement and\n  Throughput Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SplitLLM: Collaborative Inference of LLMs for Model Placement and\n  Throughput Optimization"
                },
                "summary": "Large language models (LLMs) have been a disruptive innovation in recent\nyears, and they play a crucial role in our daily lives due to their ability to\nunderstand and generate human-like text. Their capabilities include natural\nlanguage understanding, information retrieval and search, translation,\nchatbots, virtual assistance, and many more. However, it is well known that\nLLMs are massive in terms of the number of parameters. Additionally, the\nself-attention mechanism in the underlying architecture of LLMs, Transformers,\nhas quadratic complexity in terms of both computation and memory with respect\nto the input sequence length. For these reasons, LLM inference is\nresource-intensive, and thus, the throughput of LLM inference is limited,\nespecially for the longer sequences. In this report, we design a collaborative\ninference architecture between a server and its clients to alleviate the\nthroughput limit. In this design, we consider the available resources on both\nsides, i.e., the computation and communication costs. We develop a dynamic\nprogramming-based algorithm to optimally allocate computation between the\nserver and the client device to increase the server throughput, while not\nviolating the service level agreement (SLA). We show in the experiments that we\nare able to efficiently distribute the workload allowing for roughly 1/3\nreduction in the server workload, while achieving 19 percent improvement over a\ngreedy method. As a result, we are able to demonstrate that, in an environment\nwith different types of LLM inference requests, the throughput of the server is\nimproved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been a disruptive innovation in recent\nyears, and they play a crucial role in our daily lives due to their ability to\nunderstand and generate human-like text. Their capabilities include natural\nlanguage understanding, information retrieval and search, translation,\nchatbots, virtual assistance, and many more. However, it is well known that\nLLMs are massive in terms of the number of parameters. Additionally, the\nself-attention mechanism in the underlying architecture of LLMs, Transformers,\nhas quadratic complexity in terms of both computation and memory with respect\nto the input sequence length. For these reasons, LLM inference is\nresource-intensive, and thus, the throughput of LLM inference is limited,\nespecially for the longer sequences. In this report, we design a collaborative\ninference architecture between a server and its clients to alleviate the\nthroughput limit. In this design, we consider the available resources on both\nsides, i.e., the computation and communication costs. We develop a dynamic\nprogramming-based algorithm to optimally allocate computation between the\nserver and the client device to increase the server throughput, while not\nviolating the service level agreement (SLA). We show in the experiments that we\nare able to efficiently distribute the workload allowing for roughly 1/3\nreduction in the server workload, while achieving 19 percent improvement over a\ngreedy method. As a result, we are able to demonstrate that, in an environment\nwith different types of LLM inference requests, the throughput of the server is\nimproved."
                },
                "authors": [
                    {
                        "name": "Akrit Mudvari"
                    },
                    {
                        "name": "Yuang Jiang"
                    },
                    {
                        "name": "Leandros Tassiulas"
                    }
                ],
                "author_detail": {
                    "name": "Leandros Tassiulas"
                },
                "author": "Leandros Tassiulas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10759v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10759v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09038v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09038v2",
                "updated": "2024-10-14T17:32:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    32,
                    26,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-11T17:54:14Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    54,
                    14,
                    4,
                    285,
                    0
                ],
                "title": "SimpleStrat: Diversifying Language Model Generation with Stratification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimpleStrat: Diversifying Language Model Generation with Stratification"
                },
                "summary": "Generating diverse responses from large language models (LLMs) is crucial for\napplications such as planning/search and synthetic data generation, where\ndiversity provides distinct answers across generations. Prior approaches rely\non increasing temperature to increase diversity. However, contrary to popular\nbelief, we show not only does this approach produce lower quality individual\ngenerations as temperature increases, but it depends on model's next-token\nprobabilities being similar to the true distribution of answers. We propose\nSimpleStrat, an alternative approach that uses the language model itself to\npartition the space into strata. At inference, a random stratum is selected and\na sample drawn from within the strata. To measure diversity, we introduce\nCoverageQA, a dataset of underspecified questions with multiple equally\nplausible answers, and assess diversity by measuring KL Divergence between the\noutput distribution and uniform distribution over valid ground truth answers.\nAs computing probability per response/solution for proprietary models is\ninfeasible, we measure recall on ground truth solutions. Our evaluation show\nusing SimpleStrat achieves higher recall by 0.05 compared to GPT-4o and 0.36\naverage reduction in KL Divergence compared to Llama 3.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating diverse responses from large language models (LLMs) is crucial for\napplications such as planning/search and synthetic data generation, where\ndiversity provides distinct answers across generations. Prior approaches rely\non increasing temperature to increase diversity. However, contrary to popular\nbelief, we show not only does this approach produce lower quality individual\ngenerations as temperature increases, but it depends on model's next-token\nprobabilities being similar to the true distribution of answers. We propose\nSimpleStrat, an alternative approach that uses the language model itself to\npartition the space into strata. At inference, a random stratum is selected and\na sample drawn from within the strata. To measure diversity, we introduce\nCoverageQA, a dataset of underspecified questions with multiple equally\nplausible answers, and assess diversity by measuring KL Divergence between the\noutput distribution and uniform distribution over valid ground truth answers.\nAs computing probability per response/solution for proprietary models is\ninfeasible, we measure recall on ground truth solutions. Our evaluation show\nusing SimpleStrat achieves higher recall by 0.05 compared to GPT-4o and 0.36\naverage reduction in KL Divergence compared to Llama 3."
                },
                "authors": [
                    {
                        "name": "Justin Wong"
                    },
                    {
                        "name": "Yury Orlovskiy"
                    },
                    {
                        "name": "Michael Luo"
                    },
                    {
                        "name": "Sanjit A. Seshia"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09038v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09038v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10756v1",
                "updated": "2024-10-14T17:30:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    30,
                    8,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:30:08Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    30,
                    8,
                    0,
                    288,
                    0
                ],
                "title": "Use Random Selection for Now: Investigation of Few-Shot Selection\n  Strategies in LLM-based Text Augmentation for Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Use Random Selection for Now: Investigation of Few-Shot Selection\n  Strategies in LLM-based Text Augmentation for Classification"
                },
                "summary": "The generative large language models (LLMs) are increasingly used for data\naugmentation tasks, where text samples are paraphrased (or generated anew) and\nthen used for classifier fine-tuning. Existing works on augmentation leverage\nthe few-shot scenarios, where samples are given to LLMs as part of prompts,\nleading to better augmentations. Yet, the samples are mostly selected randomly\nand a comprehensive overview of the effects of other (more ``informed'') sample\nselection strategies is lacking. In this work, we compare sample selection\nstrategies existing in few-shot learning literature and investigate their\neffects in LLM-based textual augmentation. We evaluate this on in-distribution\nand out-of-distribution classifier performance. Results indicate, that while\nsome ``informed'' selection strategies increase the performance of models,\nespecially for out-of-distribution data, it happens only seldom and with\nmarginal performance increases. Unless further advances are made, a default of\nrandom sample selection remains a good option for augmentation practitioners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generative large language models (LLMs) are increasingly used for data\naugmentation tasks, where text samples are paraphrased (or generated anew) and\nthen used for classifier fine-tuning. Existing works on augmentation leverage\nthe few-shot scenarios, where samples are given to LLMs as part of prompts,\nleading to better augmentations. Yet, the samples are mostly selected randomly\nand a comprehensive overview of the effects of other (more ``informed'') sample\nselection strategies is lacking. In this work, we compare sample selection\nstrategies existing in few-shot learning literature and investigate their\neffects in LLM-based textual augmentation. We evaluate this on in-distribution\nand out-of-distribution classifier performance. Results indicate, that while\nsome ``informed'' selection strategies increase the performance of models,\nespecially for out-of-distribution data, it happens only seldom and with\nmarginal performance increases. Unless further advances are made, a default of\nrandom sample selection remains a good option for augmentation practitioners."
                },
                "authors": [
                    {
                        "name": "Jan Cegin"
                    },
                    {
                        "name": "Branislav Pecher"
                    },
                    {
                        "name": "Jakub Simko"
                    },
                    {
                        "name": "Ivan Srba"
                    },
                    {
                        "name": "Maria Bielikova"
                    },
                    {
                        "name": "Peter Brusilovsky"
                    }
                ],
                "author_detail": {
                    "name": "Peter Brusilovsky"
                },
                "author": "Peter Brusilovsky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09024v2",
                "updated": "2024-10-14T17:28:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    28,
                    8,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-11T17:39:22Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    39,
                    22,
                    4,
                    285,
                    0
                ],
                "title": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents"
                },
                "summary": "The robustness of LLMs to jailbreak attacks, where users design prompts to\ncircumvent safety measures and misuse model capabilities, has been studied\nprimarily for LLMs acting as simple chatbots. Meanwhile, LLM agents -- which\nuse external tools and can execute multi-stage tasks -- may pose a greater risk\nif misused, but their robustness remains underexplored. To facilitate research\non LLM agent misuse, we propose a new benchmark called AgentHarm. The benchmark\nincludes a diverse set of 110 explicitly malicious agent tasks (440 with\naugmentations), covering 11 harm categories including fraud, cybercrime, and\nharassment. In addition to measuring whether models refuse harmful agentic\nrequests, scoring well on AgentHarm requires jailbroken agents to maintain\ntheir capabilities following an attack to complete a multi-step task. We\nevaluate a range of leading LLMs, and find (1) leading LLMs are surprisingly\ncompliant with malicious agent requests without jailbreaking, (2) simple\nuniversal jailbreak templates can be adapted to effectively jailbreak agents,\nand (3) these jailbreaks enable coherent and malicious multi-step agent\nbehavior and retain model capabilities. To enable simple and reliable\nevaluation of attacks and defenses for LLM-based agents, we publicly release\nAgentHarm at https://huggingface.co/datasets/ai-safety-institute/AgentHarm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The robustness of LLMs to jailbreak attacks, where users design prompts to\ncircumvent safety measures and misuse model capabilities, has been studied\nprimarily for LLMs acting as simple chatbots. Meanwhile, LLM agents -- which\nuse external tools and can execute multi-stage tasks -- may pose a greater risk\nif misused, but their robustness remains underexplored. To facilitate research\non LLM agent misuse, we propose a new benchmark called AgentHarm. The benchmark\nincludes a diverse set of 110 explicitly malicious agent tasks (440 with\naugmentations), covering 11 harm categories including fraud, cybercrime, and\nharassment. In addition to measuring whether models refuse harmful agentic\nrequests, scoring well on AgentHarm requires jailbroken agents to maintain\ntheir capabilities following an attack to complete a multi-step task. We\nevaluate a range of leading LLMs, and find (1) leading LLMs are surprisingly\ncompliant with malicious agent requests without jailbreaking, (2) simple\nuniversal jailbreak templates can be adapted to effectively jailbreak agents,\nand (3) these jailbreaks enable coherent and malicious multi-step agent\nbehavior and retain model capabilities. To enable simple and reliable\nevaluation of attacks and defenses for LLM-based agents, we publicly release\nAgentHarm at https://huggingface.co/datasets/ai-safety-institute/AgentHarm."
                },
                "authors": [
                    {
                        "name": "Maksym Andriushchenko"
                    },
                    {
                        "name": "Alexandra Souly"
                    },
                    {
                        "name": "Mateusz Dziemian"
                    },
                    {
                        "name": "Derek Duenas"
                    },
                    {
                        "name": "Maxwell Lin"
                    },
                    {
                        "name": "Justin Wang"
                    },
                    {
                        "name": "Dan Hendrycks"
                    },
                    {
                        "name": "Andy Zou"
                    },
                    {
                        "name": "Zico Kolter"
                    },
                    {
                        "name": "Matt Fredrikson"
                    },
                    {
                        "name": "Eric Winsor"
                    },
                    {
                        "name": "Jerome Wynne"
                    },
                    {
                        "name": "Yarin Gal"
                    },
                    {
                        "name": "Xander Davies"
                    }
                ],
                "author_detail": {
                    "name": "Xander Davies"
                },
                "author": "Xander Davies",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10743v1",
                "updated": "2024-10-14T17:21:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    21,
                    57,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:21:57Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    21,
                    57,
                    0,
                    288,
                    0
                ],
                "title": "NT-LLM: A Novel Node Tokenizer for Integrating Graph Structure into\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NT-LLM: A Novel Node Tokenizer for Integrating Graph Structure into\n  Large Language Models"
                },
                "summary": "Graphs are a fundamental data structure for representing relationships in\nreal-world scenarios. With the success of Large Language Models (LLMs) across\nvarious natural language processing (NLP) tasks, there has been growing\ninterest in integrating LLMs for graph learning. However, applying LLMs to\ngraph-related tasks poses significant challenges, as these models are not\ninherently designed to capture the complex structural information present in\ngraphs. Existing approaches address this challenge through two strategies: the\nchain of tasks approach, which uses Graph Neural Networks (GNNs) to encode the\ngraph structure so that LLMs are relieved from understanding spatial positions;\nand Graph-to-Text Conversion, which translates graph structures into semantic\ntext representations that LLMs can process. Despite their progress, these\nmethods often struggle to fully preserve the topological information of graphs\nor require extensive computational resources, limiting their practical\napplicability.\n  In this work, we introduce Node Tokenizer for Large Language Models (NT-LLM),\na novel framework that efficiently encodes graph structures by selecting key\nnodes as anchors and representing each node based on its relative distance to\nthese anchors. This position-anchored encoding effectively captures the graph\ntopology, enabling enhanced reasoning capabilities in LLMs over graph data.\nAdditionally, we implement a task-specific tuning procedure to further improve\nstructural understanding within LLMs. Through extensive empirical evaluations,\nNT-LLM demonstrates significant performance improvements across a variety of\ngraph-related tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphs are a fundamental data structure for representing relationships in\nreal-world scenarios. With the success of Large Language Models (LLMs) across\nvarious natural language processing (NLP) tasks, there has been growing\ninterest in integrating LLMs for graph learning. However, applying LLMs to\ngraph-related tasks poses significant challenges, as these models are not\ninherently designed to capture the complex structural information present in\ngraphs. Existing approaches address this challenge through two strategies: the\nchain of tasks approach, which uses Graph Neural Networks (GNNs) to encode the\ngraph structure so that LLMs are relieved from understanding spatial positions;\nand Graph-to-Text Conversion, which translates graph structures into semantic\ntext representations that LLMs can process. Despite their progress, these\nmethods often struggle to fully preserve the topological information of graphs\nor require extensive computational resources, limiting their practical\napplicability.\n  In this work, we introduce Node Tokenizer for Large Language Models (NT-LLM),\na novel framework that efficiently encodes graph structures by selecting key\nnodes as anchors and representing each node based on its relative distance to\nthese anchors. This position-anchored encoding effectively captures the graph\ntopology, enabling enhanced reasoning capabilities in LLMs over graph data.\nAdditionally, we implement a task-specific tuning procedure to further improve\nstructural understanding within LLMs. Through extensive empirical evaluations,\nNT-LLM demonstrates significant performance improvements across a variety of\ngraph-related tasks."
                },
                "authors": [
                    {
                        "name": "Yanbiao Ji"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Xin Chen"
                    },
                    {
                        "name": "Yue Ding"
                    },
                    {
                        "name": "Dan Luo"
                    },
                    {
                        "name": "Mei Li"
                    },
                    {
                        "name": "Wenqing Lin"
                    },
                    {
                        "name": "Hongtao Lu"
                    }
                ],
                "author_detail": {
                    "name": "Hongtao Lu"
                },
                "author": "Hongtao Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10741v1",
                "updated": "2024-10-14T17:21:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    21,
                    39,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:21:39Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    21,
                    39,
                    0,
                    288,
                    0
                ],
                "title": "SensorBench: Benchmarking LLMs in Coding-Based Sensor Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SensorBench: Benchmarking LLMs in Coding-Based Sensor Processing"
                },
                "summary": "Effective processing, interpretation, and management of sensor data have\nemerged as a critical component of cyber-physical systems. Traditionally,\nprocessing sensor data requires profound theoretical knowledge and proficiency\nin signal-processing tools. However, recent works show that Large Language\nModels (LLMs) have promising capabilities in processing sensory data,\nsuggesting their potential as copilots for developing sensing systems.\n  To explore this potential, we construct a comprehensive benchmark,\nSensorBench, to establish a quantifiable objective. The benchmark incorporates\ndiverse real-world sensor datasets for various tasks. The results show that\nwhile LLMs exhibit considerable proficiency in simpler tasks, they face\ninherent challenges in processing compositional tasks with parameter selections\ncompared to engineering experts. Additionally, we investigate four prompting\nstrategies for sensor processing and show that self-verification can outperform\nall other baselines in 48% of tasks. Our study provides a comprehensive\nbenchmark and prompting analysis for future developments, paving the way toward\nan LLM-based sensor processing copilot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective processing, interpretation, and management of sensor data have\nemerged as a critical component of cyber-physical systems. Traditionally,\nprocessing sensor data requires profound theoretical knowledge and proficiency\nin signal-processing tools. However, recent works show that Large Language\nModels (LLMs) have promising capabilities in processing sensory data,\nsuggesting their potential as copilots for developing sensing systems.\n  To explore this potential, we construct a comprehensive benchmark,\nSensorBench, to establish a quantifiable objective. The benchmark incorporates\ndiverse real-world sensor datasets for various tasks. The results show that\nwhile LLMs exhibit considerable proficiency in simpler tasks, they face\ninherent challenges in processing compositional tasks with parameter selections\ncompared to engineering experts. Additionally, we investigate four prompting\nstrategies for sensor processing and show that self-verification can outperform\nall other baselines in 48% of tasks. Our study provides a comprehensive\nbenchmark and prompting analysis for future developments, paving the way toward\nan LLM-based sensor processing copilot."
                },
                "authors": [
                    {
                        "name": "Pengrui Quan"
                    },
                    {
                        "name": "Xiaomin Ouyang"
                    },
                    {
                        "name": "Jeya Vikranth Jeyakumar"
                    },
                    {
                        "name": "Ziqi Wang"
                    },
                    {
                        "name": "Yang Xing"
                    },
                    {
                        "name": "Mani Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "Mani Srivastava"
                },
                "author": "Mani Srivastava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10739v1",
                "updated": "2024-10-14T17:20:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    20,
                    30,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:20:30Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    20,
                    30,
                    0,
                    288,
                    0
                ],
                "title": "Balancing Continuous Pre-Training and Instruction Fine-Tuning:\n  Optimizing Instruction-Following in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Balancing Continuous Pre-Training and Instruction Fine-Tuning:\n  Optimizing Instruction-Following in LLMs"
                },
                "summary": "Large Language Models (LLMs) for public use require continuous pre-training\nto remain up-to-date with the latest data. The models also need to be\nfine-tuned with specific instructions to maintain their ability to follow\ninstructions accurately. Typically, LLMs are released in two versions: the Base\nLLM, pre-trained on diverse data, and the instruction-refined LLM, additionally\ntrained with specific instructions for better instruction following. The\nquestion arises as to which model should undergo continuous pre-training to\nmaintain its instruction-following abilities while also staying current with\nthe latest data. In this study, we delve into the intricate relationship\nbetween continuous pre-training and instruction fine-tuning of the LLMs and\ninvestigate the impact of continuous pre-training on the instruction following\nabilities of both the base and its instruction finetuned model. Further, the\ninstruction fine-tuning process is computationally intense and requires a\nsubstantial number of hand-annotated examples for the model to learn\neffectively. This study aims to find the most compute-efficient strategy to\ngain up-to-date knowledge and instruction-following capabilities without\nrequiring any instruction data and fine-tuning. We empirically prove our\nfindings on the LLaMa 3, 3.1 and Qwen 2, 2.5 family of base and instruction\nmodels, providing a comprehensive exploration of our hypotheses across varying\nsizes of pre-training data corpus and different LLMs settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) for public use require continuous pre-training\nto remain up-to-date with the latest data. The models also need to be\nfine-tuned with specific instructions to maintain their ability to follow\ninstructions accurately. Typically, LLMs are released in two versions: the Base\nLLM, pre-trained on diverse data, and the instruction-refined LLM, additionally\ntrained with specific instructions for better instruction following. The\nquestion arises as to which model should undergo continuous pre-training to\nmaintain its instruction-following abilities while also staying current with\nthe latest data. In this study, we delve into the intricate relationship\nbetween continuous pre-training and instruction fine-tuning of the LLMs and\ninvestigate the impact of continuous pre-training on the instruction following\nabilities of both the base and its instruction finetuned model. Further, the\ninstruction fine-tuning process is computationally intense and requires a\nsubstantial number of hand-annotated examples for the model to learn\neffectively. This study aims to find the most compute-efficient strategy to\ngain up-to-date knowledge and instruction-following capabilities without\nrequiring any instruction data and fine-tuning. We empirically prove our\nfindings on the LLaMa 3, 3.1 and Qwen 2, 2.5 family of base and instruction\nmodels, providing a comprehensive exploration of our hypotheses across varying\nsizes of pre-training data corpus and different LLMs settings."
                },
                "authors": [
                    {
                        "name": "Ishan Jindal"
                    },
                    {
                        "name": "Chandana Badrinath"
                    },
                    {
                        "name": "Pranjal Bharti"
                    },
                    {
                        "name": "Lakkidi Vinay"
                    },
                    {
                        "name": "Sachin Dev Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Sachin Dev Sharma"
                },
                "author": "Sachin Dev Sharma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10735v1",
                "updated": "2024-10-14T17:16:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    16,
                    44,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:16:44Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    16,
                    44,
                    0,
                    288,
                    0
                ],
                "title": "Embedding Self-Correction as an Inherent Ability in Large Language\n  Models for Enhanced Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedding Self-Correction as an Inherent Ability in Large Language\n  Models for Enhanced Mathematical Reasoning"
                },
                "summary": "Accurate mathematical reasoning with Large Language Models (LLMs) is crucial\nin revolutionizing domains that heavily rely on such reasoning. However, LLMs\noften encounter difficulties in certain aspects of mathematical reasoning,\nleading to flawed reasoning and erroneous results. To mitigate these issues, we\nintroduce a novel mechanism, the Chain of Self-Correction (CoSC), specifically\ndesigned to embed self-correction as an inherent ability in LLMs, enabling them\nto validate and rectify their own results. The CoSC mechanism operates through\na sequence of self-correction stages. In each stage, the LLMs generate a\nprogram to address a given problem, execute this program using program-based\ntools to obtain an output, subsequently verify this output. Based on the\nverification, the LLMs either proceed to the next correction stage or finalize\nthe answer. This iterative self-correction process allows the LLMs to refine\ntheir reasoning steps and improve the accuracy of their mathematical reasoning.\nTo enable the CoSC mechanism at a low cost, we employ a two-phase finetuning\napproach. In the first phase, the LLMs are trained with a relatively small\nvolume of seeding data generated from GPT-4, establishing an initial CoSC\ncapability. In the second phase, the CoSC capability is further enhanced by\ntraining with a larger volume of self-generated data using the trained model in\nthe first phase, without relying on the paid GPT-4. Our comprehensive\nexperiments demonstrate that CoSC significantly improves performance on\ntraditional mathematical datasets among existing open-source LLMs. Notably, our\nCoSC-Code-34B model achieved a 53.5% score on MATH, the most challenging\nmathematical reasoning dataset in the public domain, surpassing the performance\nof well-established models such as ChatGPT, GPT-4, and even multi-modal LLMs\nlike GPT-4V, Gemini-1.0 Pro, and Gemini-1.0 Ultra.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate mathematical reasoning with Large Language Models (LLMs) is crucial\nin revolutionizing domains that heavily rely on such reasoning. However, LLMs\noften encounter difficulties in certain aspects of mathematical reasoning,\nleading to flawed reasoning and erroneous results. To mitigate these issues, we\nintroduce a novel mechanism, the Chain of Self-Correction (CoSC), specifically\ndesigned to embed self-correction as an inherent ability in LLMs, enabling them\nto validate and rectify their own results. The CoSC mechanism operates through\na sequence of self-correction stages. In each stage, the LLMs generate a\nprogram to address a given problem, execute this program using program-based\ntools to obtain an output, subsequently verify this output. Based on the\nverification, the LLMs either proceed to the next correction stage or finalize\nthe answer. This iterative self-correction process allows the LLMs to refine\ntheir reasoning steps and improve the accuracy of their mathematical reasoning.\nTo enable the CoSC mechanism at a low cost, we employ a two-phase finetuning\napproach. In the first phase, the LLMs are trained with a relatively small\nvolume of seeding data generated from GPT-4, establishing an initial CoSC\ncapability. In the second phase, the CoSC capability is further enhanced by\ntraining with a larger volume of self-generated data using the trained model in\nthe first phase, without relying on the paid GPT-4. Our comprehensive\nexperiments demonstrate that CoSC significantly improves performance on\ntraditional mathematical datasets among existing open-source LLMs. Notably, our\nCoSC-Code-34B model achieved a 53.5% score on MATH, the most challenging\nmathematical reasoning dataset in the public domain, surpassing the performance\nof well-established models such as ChatGPT, GPT-4, and even multi-modal LLMs\nlike GPT-4V, Gemini-1.0 Pro, and Gemini-1.0 Ultra."
                },
                "authors": [
                    {
                        "name": "Kuofeng Gao"
                    },
                    {
                        "name": "Huanqia Cai"
                    },
                    {
                        "name": "Qingyao Shuai"
                    },
                    {
                        "name": "Dihong Gong"
                    },
                    {
                        "name": "Zhifeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhifeng Li"
                },
                "author": "Zhifeng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10728v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10728v1",
                "updated": "2024-10-14T17:09:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    9,
                    14,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:09:14Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    9,
                    14,
                    0,
                    288,
                    0
                ],
                "title": "Towards LLM-guided Efficient and Interpretable Multi-linear Tensor\n  Network Rank Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards LLM-guided Efficient and Interpretable Multi-linear Tensor\n  Network Rank Selection"
                },
                "summary": "We propose a novel framework that leverages large language models (LLMs) to\nguide the rank selection in tensor network models for higher-order data\nanalysis. By utilising the intrinsic reasoning capabilities and domain\nknowledge of LLMs, our approach offers enhanced interpretability of the rank\nchoices and can effectively optimise the objective function. This framework\nenables users without specialised domain expertise to utilise tensor network\ndecompositions and understand the underlying rationale within the rank\nselection process. Experimental results validate our method on financial\nhigher-order datasets, demonstrating interpretable reasoning, strong\ngeneralisation to unseen test data, and its potential for self-enhancement over\nsuccessive iterations. This work is placed at the intersection of large\nlanguage models and higher-order data analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel framework that leverages large language models (LLMs) to\nguide the rank selection in tensor network models for higher-order data\nanalysis. By utilising the intrinsic reasoning capabilities and domain\nknowledge of LLMs, our approach offers enhanced interpretability of the rank\nchoices and can effectively optimise the objective function. This framework\nenables users without specialised domain expertise to utilise tensor network\ndecompositions and understand the underlying rationale within the rank\nselection process. Experimental results validate our method on financial\nhigher-order datasets, demonstrating interpretable reasoning, strong\ngeneralisation to unseen test data, and its potential for self-enhancement over\nsuccessive iterations. This work is placed at the intersection of large\nlanguage models and higher-order data analysis."
                },
                "authors": [
                    {
                        "name": "Giorgos Iacovides"
                    },
                    {
                        "name": "Wuyang Zhou"
                    },
                    {
                        "name": "Danilo Mandic"
                    }
                ],
                "author_detail": {
                    "name": "Danilo Mandic"
                },
                "author": "Danilo Mandic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10728v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10728v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10724v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10724v1",
                "updated": "2024-10-14T17:04:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    4,
                    41,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:04:41Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    4,
                    41,
                    0,
                    288,
                    0
                ],
                "title": "Large Language Models Are Active Critics in NLG Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Are Active Critics in NLG Evaluation"
                },
                "summary": "The conventional paradigm of using large language models (LLMs) for\nevaluating natural language generation (NLG) systems typically relies on two\nkey inputs: (1) a clear definition of the NLG task to be evaluated and (2) a\nlist of pre-defined evaluation criteria. This process treats LLMs as ''passive\ncritics,'' strictly following human-defined criteria for evaluation. However,\nas new NLG tasks emerge, the criteria for assessing text quality can vary\ngreatly. Consequently, these rigid evaluation methods struggle to adapt to\ndiverse NLG tasks without extensive prompt engineering customized for each\nspecific task. To address this limitation, we introduce Active-Critic, a novel\nLLM-based NLG evaluation protocol that enables LLMs to function as ''active\ncritics.'' Specifically, our protocol comprises two key stages. In the first\nstage, the LLM is instructed to infer the target NLG task and establish\nrelevant evaluation criteria from the data. Building on this self-inferred\ninformation, the second stage dynamically optimizes the prompt to guide the LLM\ntoward more human-aligned scoring decisions, while also generating detailed\nexplanations to justify its evaluations. Experiments across four NLG evaluation\ntasks show that our approach achieves stronger alignment with human judgments\nthan state-of-the-art evaluation methods. Our comprehensive analysis further\nhighlights the effectiveness and explainability of Active-Critic with only a\nsmall amount of labeled data. We will share our code and data on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The conventional paradigm of using large language models (LLMs) for\nevaluating natural language generation (NLG) systems typically relies on two\nkey inputs: (1) a clear definition of the NLG task to be evaluated and (2) a\nlist of pre-defined evaluation criteria. This process treats LLMs as ''passive\ncritics,'' strictly following human-defined criteria for evaluation. However,\nas new NLG tasks emerge, the criteria for assessing text quality can vary\ngreatly. Consequently, these rigid evaluation methods struggle to adapt to\ndiverse NLG tasks without extensive prompt engineering customized for each\nspecific task. To address this limitation, we introduce Active-Critic, a novel\nLLM-based NLG evaluation protocol that enables LLMs to function as ''active\ncritics.'' Specifically, our protocol comprises two key stages. In the first\nstage, the LLM is instructed to infer the target NLG task and establish\nrelevant evaluation criteria from the data. Building on this self-inferred\ninformation, the second stage dynamically optimizes the prompt to guide the LLM\ntoward more human-aligned scoring decisions, while also generating detailed\nexplanations to justify its evaluations. Experiments across four NLG evaluation\ntasks show that our approach achieves stronger alignment with human judgments\nthan state-of-the-art evaluation methods. Our comprehensive analysis further\nhighlights the effectiveness and explainability of Active-Critic with only a\nsmall amount of labeled data. We will share our code and data on GitHub."
                },
                "authors": [
                    {
                        "name": "Shuying Xu"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Ming Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Ming Jiang"
                },
                "author": "Ming Jiang",
                "arxiv_comment": "Submitted to ICLR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10724v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10724v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10714v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10714v1",
                "updated": "2024-10-14T16:57:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    57,
                    23,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T16:57:23Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    57,
                    23,
                    0,
                    288,
                    0
                ],
                "title": "SeedLM: Compressing LLM Weights into Seeds of Pseudo-Random Generators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeedLM: Compressing LLM Weights into Seeds of Pseudo-Random Generators"
                },
                "summary": "Large Language Models (LLMs) have transformed natural language processing,\nbut face significant challenges in widespread deployment due to their high\nruntime cost. In this paper, we introduce SeedLM, a novel post-training\ncompression method that uses seeds of pseudo-random generators to encode and\ncompress model weights. Specifically, for each block of weights, we find a seed\nthat is fed into a Linear Feedback Shift Register (LFSR) during inference to\nefficiently generate a random matrix. This matrix is then linearly combined\nwith compressed coefficients to reconstruct the weight block. SeedLM reduces\nmemory access and leverages idle compute cycles during inference, effectively\nspeeding up memory-bound tasks by trading compute for fewer memory accesses.\nUnlike state-of-the-art compression methods that rely on calibration data, our\napproach is data-free and generalizes well across diverse tasks. Our\nexperiments with Llama 3 70B, which is particularly challenging to compress,\nshow that SeedLM achieves significantly better zero-shot accuracy retention at\n4- and 3-bit than state-of-the-art techniques, while maintaining performance\ncomparable to FP16 baselines. Additionally, FPGA-based tests demonstrate that\n4-bit SeedLM, as model size increases to 70B, approaches a 4x speed-up over an\nFP16 Llama 2/3 baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have transformed natural language processing,\nbut face significant challenges in widespread deployment due to their high\nruntime cost. In this paper, we introduce SeedLM, a novel post-training\ncompression method that uses seeds of pseudo-random generators to encode and\ncompress model weights. Specifically, for each block of weights, we find a seed\nthat is fed into a Linear Feedback Shift Register (LFSR) during inference to\nefficiently generate a random matrix. This matrix is then linearly combined\nwith compressed coefficients to reconstruct the weight block. SeedLM reduces\nmemory access and leverages idle compute cycles during inference, effectively\nspeeding up memory-bound tasks by trading compute for fewer memory accesses.\nUnlike state-of-the-art compression methods that rely on calibration data, our\napproach is data-free and generalizes well across diverse tasks. Our\nexperiments with Llama 3 70B, which is particularly challenging to compress,\nshow that SeedLM achieves significantly better zero-shot accuracy retention at\n4- and 3-bit than state-of-the-art techniques, while maintaining performance\ncomparable to FP16 baselines. Additionally, FPGA-based tests demonstrate that\n4-bit SeedLM, as model size increases to 70B, approaches a 4x speed-up over an\nFP16 Llama 2/3 baseline."
                },
                "authors": [
                    {
                        "name": "Rasoul Shafipour"
                    },
                    {
                        "name": "David Harrison"
                    },
                    {
                        "name": "Maxwell Horton"
                    },
                    {
                        "name": "Jeffrey Marker"
                    },
                    {
                        "name": "Houman Bedayat"
                    },
                    {
                        "name": "Sachin Mehta"
                    },
                    {
                        "name": "Mohammad Rastegari"
                    },
                    {
                        "name": "Mahyar Najibi"
                    },
                    {
                        "name": "Saman Naderiparizi"
                    }
                ],
                "author_detail": {
                    "name": "Saman Naderiparizi"
                },
                "author": "Saman Naderiparizi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10714v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10714v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08516v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08516v5",
                "updated": "2024-10-14T16:55:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    55,
                    26,
                    0,
                    288,
                    0
                ],
                "published": "2024-07-11T14:00:53Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    14,
                    0,
                    53,
                    3,
                    193,
                    0
                ],
                "title": "Converging Paradigms: The Synergy of Symbolic and Connectionist AI in\n  LLM-Empowered Autonomous Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Converging Paradigms: The Synergy of Symbolic and Connectionist AI in\n  LLM-Empowered Autonomous Agents"
                },
                "summary": "This article explores the convergence of connectionist and symbolic\nartificial intelligence (AI), from historical debates to contemporary\nadvancements. Traditionally considered distinct paradigms, connectionist AI\nfocuses on neural networks, while symbolic AI emphasizes symbolic\nrepresentation and logic. Recent advancements in large language models (LLMs),\nexemplified by ChatGPT and GPT-4, highlight the potential of connectionist\narchitectures in handling human language as a form of symbols. The study argues\nthat LLM-empowered Autonomous Agents (LAAs) embody this paradigm convergence.\nBy utilizing LLMs for text-based knowledge modeling and representation, LAAs\nintegrate neuro-symbolic AI principles, showcasing enhanced reasoning and\ndecision-making capabilities. Comparing LAAs with Knowledge Graphs within the\nneuro-symbolic AI theme highlights the unique strengths of LAAs in mimicking\nhuman-like reasoning processes, scaling effectively with large datasets, and\nleveraging in-context samples without explicit re-training. The research\nunderscores promising avenues in neuro-vector-symbolic integration,\ninstructional encoding, and implicit reasoning, aimed at further enhancing LAA\ncapabilities. By exploring the progression of neuro-symbolic AI and proposing\nfuture research trajectories, this work advances the understanding and\ndevelopment of AI technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article explores the convergence of connectionist and symbolic\nartificial intelligence (AI), from historical debates to contemporary\nadvancements. Traditionally considered distinct paradigms, connectionist AI\nfocuses on neural networks, while symbolic AI emphasizes symbolic\nrepresentation and logic. Recent advancements in large language models (LLMs),\nexemplified by ChatGPT and GPT-4, highlight the potential of connectionist\narchitectures in handling human language as a form of symbols. The study argues\nthat LLM-empowered Autonomous Agents (LAAs) embody this paradigm convergence.\nBy utilizing LLMs for text-based knowledge modeling and representation, LAAs\nintegrate neuro-symbolic AI principles, showcasing enhanced reasoning and\ndecision-making capabilities. Comparing LAAs with Knowledge Graphs within the\nneuro-symbolic AI theme highlights the unique strengths of LAAs in mimicking\nhuman-like reasoning processes, scaling effectively with large datasets, and\nleveraging in-context samples without explicit re-training. The research\nunderscores promising avenues in neuro-vector-symbolic integration,\ninstructional encoding, and implicit reasoning, aimed at further enhancing LAA\ncapabilities. By exploring the progression of neuro-symbolic AI and proposing\nfuture research trajectories, this work advances the understanding and\ndevelopment of AI technologies."
                },
                "authors": [
                    {
                        "name": "Haoyi Xiong"
                    },
                    {
                        "name": "Zhiyuan Wang"
                    },
                    {
                        "name": "Xuhong Li"
                    },
                    {
                        "name": "Jiang Bian"
                    },
                    {
                        "name": "Zeke Xie"
                    },
                    {
                        "name": "Shahid Mumtaz"
                    },
                    {
                        "name": "Anwer Al-Dulaimi"
                    },
                    {
                        "name": "Laura E. Barnes"
                    }
                ],
                "author_detail": {
                    "name": "Laura E. Barnes"
                },
                "author": "Laura E. Barnes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08516v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08516v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10705v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10705v1",
                "updated": "2024-10-14T16:44:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    44,
                    51,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T16:44:51Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    44,
                    51,
                    0,
                    288,
                    0
                ],
                "title": "High sensitivity pressure and temperature quantum sensing in organic\n  crystals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High sensitivity pressure and temperature quantum sensing in organic\n  crystals"
                },
                "summary": "The inherent sensitivity of quantum sensors to their physical environment can\nmake them good reporters of parameters such as temperature, pressure, strain,\nand electric fields. Here, we present a molecular platform for pressure (P) and\ntemperature (T) sensing using para-terphenyl crystals doped with pentacene. We\nleverage the optically detected magnetic resonance (ODMR) of the photoexcited\ntriplet electron in the pentacene molecule, that serves as a sensitive probe\nfor lattice changes in the host para-terphenyl due to pressure or temperature\nvariations. We observe maximal ODMR frequency variations of df/dP=1.8 MHz/bar\nand df/dT=247 kHz/K, which are over 1,200 times and three times greater,\nrespectively, than those seen in nitrogen-vacancy centers in diamond. This\nresults in a >85-fold improvement in pressure sensitivity over best previously\nreported. The larger variation reflects the weaker nature of the para-terphenyl\nlattice, with first-principles DFT calculations indicating that even\npicometer-level shifts in the molecular orbitals due to P, T changes are\nmeasurable. The platform offers additional advantages including high levels of\nsensor doping, narrow ODMR linewidths and high contrasts, and ease of\ndeployment, leveraging the ability for large single crystals at low cost.\nOverall, this work paves the way for low-cost, optically-interrogated pressure\nand temperature sensors and lays the foundation for even more versatile sensors\nenabled by synthetic tunability in designer molecular systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inherent sensitivity of quantum sensors to their physical environment can\nmake them good reporters of parameters such as temperature, pressure, strain,\nand electric fields. Here, we present a molecular platform for pressure (P) and\ntemperature (T) sensing using para-terphenyl crystals doped with pentacene. We\nleverage the optically detected magnetic resonance (ODMR) of the photoexcited\ntriplet electron in the pentacene molecule, that serves as a sensitive probe\nfor lattice changes in the host para-terphenyl due to pressure or temperature\nvariations. We observe maximal ODMR frequency variations of df/dP=1.8 MHz/bar\nand df/dT=247 kHz/K, which are over 1,200 times and three times greater,\nrespectively, than those seen in nitrogen-vacancy centers in diamond. This\nresults in a >85-fold improvement in pressure sensitivity over best previously\nreported. The larger variation reflects the weaker nature of the para-terphenyl\nlattice, with first-principles DFT calculations indicating that even\npicometer-level shifts in the molecular orbitals due to P, T changes are\nmeasurable. The platform offers additional advantages including high levels of\nsensor doping, narrow ODMR linewidths and high contrasts, and ease of\ndeployment, leveraging the ability for large single crystals at low cost.\nOverall, this work paves the way for low-cost, optically-interrogated pressure\nand temperature sensors and lays the foundation for even more versatile sensors\nenabled by synthetic tunability in designer molecular systems."
                },
                "authors": [
                    {
                        "name": "Harpreet Singh"
                    },
                    {
                        "name": "Noella DSouza"
                    },
                    {
                        "name": "Joseph Garrett"
                    },
                    {
                        "name": "Angad Singh"
                    },
                    {
                        "name": "Brian Blankenship"
                    },
                    {
                        "name": "Emanuel Druga"
                    },
                    {
                        "name": "Riccardo Montis"
                    },
                    {
                        "name": "Liang Tan"
                    },
                    {
                        "name": "Ashok Ajoy"
                    }
                ],
                "author_detail": {
                    "name": "Ashok Ajoy"
                },
                "author": "Ashok Ajoy",
                "arxiv_comment": "7 pages, 4 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10705v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10705v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10700v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10700v1",
                "updated": "2024-10-14T16:41:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    41,
                    49,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T16:41:49Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    41,
                    49,
                    0,
                    288,
                    0
                ],
                "title": "Derail Yourself: Multi-turn LLM Jailbreak Attack through Self-discovered\n  Clues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Derail Yourself: Multi-turn LLM Jailbreak Attack through Self-discovered\n  Clues"
                },
                "summary": "This study exposes the safety vulnerabilities of Large Language Models (LLMs)\nin multi-turn interactions, where malicious users can obscure harmful intents\nacross several queries. We introduce ActorAttack, a novel multi-turn attack\nmethod inspired by actor-network theory, which models a network of semantically\nlinked actors as attack clues to generate diverse and effective attack paths\ntoward harmful targets. ActorAttack addresses two main challenges in multi-turn\nattacks: (1) concealing harmful intents by creating an innocuous conversation\ntopic about the actor, and (2) uncovering diverse attack paths towards the same\nharmful target by leveraging LLMs' knowledge to specify the correlated actors\nas various attack clues. In this way, ActorAttack outperforms existing\nsingle-turn and multi-turn attack methods across advanced aligned LLMs, even\nfor GPT-o1. We will publish a dataset called SafeMTData, which includes\nmulti-turn adversarial prompts and safety alignment data, generated by\nActorAttack. We demonstrate that models safety-tuned using our safety dataset\nare more robust to multi-turn attacks. Code is available at\nhttps://github.com/renqibing/ActorAttack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study exposes the safety vulnerabilities of Large Language Models (LLMs)\nin multi-turn interactions, where malicious users can obscure harmful intents\nacross several queries. We introduce ActorAttack, a novel multi-turn attack\nmethod inspired by actor-network theory, which models a network of semantically\nlinked actors as attack clues to generate diverse and effective attack paths\ntoward harmful targets. ActorAttack addresses two main challenges in multi-turn\nattacks: (1) concealing harmful intents by creating an innocuous conversation\ntopic about the actor, and (2) uncovering diverse attack paths towards the same\nharmful target by leveraging LLMs' knowledge to specify the correlated actors\nas various attack clues. In this way, ActorAttack outperforms existing\nsingle-turn and multi-turn attack methods across advanced aligned LLMs, even\nfor GPT-o1. We will publish a dataset called SafeMTData, which includes\nmulti-turn adversarial prompts and safety alignment data, generated by\nActorAttack. We demonstrate that models safety-tuned using our safety dataset\nare more robust to multi-turn attacks. Code is available at\nhttps://github.com/renqibing/ActorAttack."
                },
                "authors": [
                    {
                        "name": "Qibing Ren"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Zhanxu Xie"
                    },
                    {
                        "name": "Xiaoya Lu"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Lei Sha"
                    },
                    {
                        "name": "Junchi Yan"
                    },
                    {
                        "name": "Lizhuang Ma"
                    },
                    {
                        "name": "Jing Shao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Shao"
                },
                "author": "Jing Shao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10700v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10853v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10853v3",
                "updated": "2024-10-14T16:37:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    37,
                    29,
                    0,
                    288,
                    0
                ],
                "published": "2024-05-17T15:27:52Z",
                "published_parsed": [
                    2024,
                    5,
                    17,
                    15,
                    27,
                    52,
                    4,
                    138,
                    0
                ],
                "title": "The Future of Large Language Model Pre-training is Federated",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Future of Large Language Model Pre-training is Federated"
                },
                "summary": "Generative pre-trained large language models (LLMs) have demonstrated\nimpressive performance over a wide range of tasks, thanks to the unprecedented\namount of data they have been trained on. As established scaling laws indicate,\nLLMs' future performance improvement depends on the amount of computing and\ndata sources they can leverage for pre-training. Federated learning (FL) has\nthe potential to unleash the majority of the planet's data and computational\nresources, which are underutilized by the data-center-focused training\nmethodology of current LLM practice. Our work presents a robust, flexible,\nreproducible FL approach that enables large-scale collaboration across\ninstitutions to train LLMs. We propose a scalable deployment system called\nPhoton to enable the investigation and development of this new training\nparadigm for LLM pre-training. We show that Photon can be used by organizations\ninterested in collaborating with their private data sources and computational\nresources for pre-training LLMs with billions of parameters. This paradigm\nwould mobilize more computational and data resources while matching or\npotentially exceeding centralized performance. We further show the\neffectiveness of the federated training scales with model size and present our\napproach for training billion-scale federated LLMs using limited resources.\nThus far, we have used Photon to train LLM models to the size of 7B parameters\nand anticipate larger models being completed in the near future. Finally, we\nshow that LLM training is highly resilient to the classical challenges of\nfederated statistical and hardware heterogeneity. Furthermore, we show that\nconvergence is robust to partial participation, opening the avenue for\ncompute-efficient collaborative training. Photon will help data-rich actors to\nbecome the protagonists of LLMs pre-training instead of leaving the stage to\ncompute-rich actors alone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative pre-trained large language models (LLMs) have demonstrated\nimpressive performance over a wide range of tasks, thanks to the unprecedented\namount of data they have been trained on. As established scaling laws indicate,\nLLMs' future performance improvement depends on the amount of computing and\ndata sources they can leverage for pre-training. Federated learning (FL) has\nthe potential to unleash the majority of the planet's data and computational\nresources, which are underutilized by the data-center-focused training\nmethodology of current LLM practice. Our work presents a robust, flexible,\nreproducible FL approach that enables large-scale collaboration across\ninstitutions to train LLMs. We propose a scalable deployment system called\nPhoton to enable the investigation and development of this new training\nparadigm for LLM pre-training. We show that Photon can be used by organizations\ninterested in collaborating with their private data sources and computational\nresources for pre-training LLMs with billions of parameters. This paradigm\nwould mobilize more computational and data resources while matching or\npotentially exceeding centralized performance. We further show the\neffectiveness of the federated training scales with model size and present our\napproach for training billion-scale federated LLMs using limited resources.\nThus far, we have used Photon to train LLM models to the size of 7B parameters\nand anticipate larger models being completed in the near future. Finally, we\nshow that LLM training is highly resilient to the classical challenges of\nfederated statistical and hardware heterogeneity. Furthermore, we show that\nconvergence is robust to partial participation, opening the avenue for\ncompute-efficient collaborative training. Photon will help data-rich actors to\nbecome the protagonists of LLMs pre-training instead of leaving the stage to\ncompute-rich actors alone."
                },
                "authors": [
                    {
                        "name": "Lorenzo Sani"
                    },
                    {
                        "name": "Alex Iacob"
                    },
                    {
                        "name": "Zeyu Cao"
                    },
                    {
                        "name": "Bill Marino"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Tomas Paulik"
                    },
                    {
                        "name": "Wanru Zhao"
                    },
                    {
                        "name": "William F. Shen"
                    },
                    {
                        "name": "Preslav Aleksandrov"
                    },
                    {
                        "name": "Xinchi Qiu"
                    },
                    {
                        "name": "Nicholas D. Lane"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas D. Lane"
                },
                "author": "Nicholas D. Lane",
                "arxiv_comment": "24 pages, 15 figures, pre-print",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10853v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10853v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10680v1",
                "updated": "2024-10-14T16:20:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    20,
                    36,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T16:20:36Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    20,
                    36,
                    0,
                    288,
                    0
                ],
                "title": "Evaluating SQL Understanding in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating SQL Understanding in Large Language Models"
                },
                "summary": "The rise of large language models (LLMs) has significantly impacted various\ndomains, including natural language processing (NLP) and image generation, by\nmaking complex computational tasks more accessible. While LLMs demonstrate\nimpressive generative capabilities, there is an ongoing debate about their\nlevel of \"understanding,\" particularly in structured domains like SQL. In this\npaper, we evaluate the extent to which LLMs \"understand\" SQL by testing them on\na series of key SQL tasks. These tasks, such as syntax error detection, missing\ntoken identification, query performance prediction, query equivalence checking,\nand query explanation, assess the models' proficiency in recognition, context\nawareness, semantics, and coherence, which are essential skills for SQL\nunderstanding. We generate labeled datasets from well-known workloads, and\nevaluate the latest LLMs, focusing on how query complexity and syntactic\nfeatures influence performance. Our results indicate that while GPT4 excels at\ntasks requiring recognition and context, all models struggle with deeper\nsemantic understanding and coherence, especially in query equivalence and\nperformance estimation, revealing the limitations of current LLMs in achieving\nfull SQL comprehension.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of large language models (LLMs) has significantly impacted various\ndomains, including natural language processing (NLP) and image generation, by\nmaking complex computational tasks more accessible. While LLMs demonstrate\nimpressive generative capabilities, there is an ongoing debate about their\nlevel of \"understanding,\" particularly in structured domains like SQL. In this\npaper, we evaluate the extent to which LLMs \"understand\" SQL by testing them on\na series of key SQL tasks. These tasks, such as syntax error detection, missing\ntoken identification, query performance prediction, query equivalence checking,\nand query explanation, assess the models' proficiency in recognition, context\nawareness, semantics, and coherence, which are essential skills for SQL\nunderstanding. We generate labeled datasets from well-known workloads, and\nevaluate the latest LLMs, focusing on how query complexity and syntactic\nfeatures influence performance. Our results indicate that while GPT4 excels at\ntasks requiring recognition and context, all models struggle with deeper\nsemantic understanding and coherence, especially in query equivalence and\nperformance estimation, revealing the limitations of current LLMs in achieving\nfull SQL comprehension."
                },
                "authors": [
                    {
                        "name": "Ananya Rahaman"
                    },
                    {
                        "name": "Anny Zheng"
                    },
                    {
                        "name": "Mostafa Milani"
                    },
                    {
                        "name": "Fei Chiang"
                    },
                    {
                        "name": "Rachel Pottinger"
                    }
                ],
                "author_detail": {
                    "name": "Rachel Pottinger"
                },
                "author": "Rachel Pottinger",
                "arxiv_comment": "12 pages conference submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.06659v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.06659v2",
                "updated": "2024-10-14T16:17:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    17,
                    34,
                    0,
                    288,
                    0
                ],
                "published": "2024-02-05T18:55:53Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    18,
                    55,
                    53,
                    0,
                    36,
                    0
                ],
                "title": "Shadowcast: Stealthy Data Poisoning Attacks Against Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shadowcast: Stealthy Data Poisoning Attacks Against Vision-Language\n  Models"
                },
                "summary": "Vision-Language Models (VLMs) excel in generating textual responses from\nvisual inputs, but their versatility raises security concerns. This study takes\nthe first step in exposing VLMs' susceptibility to data poisoning attacks that\ncan manipulate responses to innocuous, everyday prompts. We introduce\nShadowcast, a stealthy data poisoning attack where poison samples are visually\nindistinguishable from benign images with matching texts. Shadowcast\ndemonstrates effectiveness in two attack types. The first is a traditional\nLabel Attack, tricking VLMs into misidentifying class labels, such as confusing\nDonald Trump for Joe Biden. The second is a novel Persuasion Attack, leveraging\nVLMs' text generation capabilities to craft persuasive and seemingly rational\nnarratives for misinformation, such as portraying junk food as healthy. We show\nthat Shadowcast effectively achieves the attacker's intentions using as few as\n50 poison samples. Crucially, the poisoned samples demonstrate transferability\nacross different VLM architectures, posing a significant concern in black-box\nsettings. Moreover, Shadowcast remains potent under realistic conditions\ninvolving various text prompts, training data augmentation, and image\ncompression techniques. This work reveals how poisoned VLMs can disseminate\nconvincing yet deceptive misinformation to everyday, benign users, emphasizing\nthe importance of data integrity for responsible VLM deployments. Our code is\navailable at: https://github.com/umd-huang-lab/VLM-Poisoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) excel in generating textual responses from\nvisual inputs, but their versatility raises security concerns. This study takes\nthe first step in exposing VLMs' susceptibility to data poisoning attacks that\ncan manipulate responses to innocuous, everyday prompts. We introduce\nShadowcast, a stealthy data poisoning attack where poison samples are visually\nindistinguishable from benign images with matching texts. Shadowcast\ndemonstrates effectiveness in two attack types. The first is a traditional\nLabel Attack, tricking VLMs into misidentifying class labels, such as confusing\nDonald Trump for Joe Biden. The second is a novel Persuasion Attack, leveraging\nVLMs' text generation capabilities to craft persuasive and seemingly rational\nnarratives for misinformation, such as portraying junk food as healthy. We show\nthat Shadowcast effectively achieves the attacker's intentions using as few as\n50 poison samples. Crucially, the poisoned samples demonstrate transferability\nacross different VLM architectures, posing a significant concern in black-box\nsettings. Moreover, Shadowcast remains potent under realistic conditions\ninvolving various text prompts, training data augmentation, and image\ncompression techniques. This work reveals how poisoned VLMs can disseminate\nconvincing yet deceptive misinformation to everyday, benign users, emphasizing\nthe importance of data integrity for responsible VLM deployments. Our code is\navailable at: https://github.com/umd-huang-lab/VLM-Poisoning."
                },
                "authors": [
                    {
                        "name": "Yuancheng Xu"
                    },
                    {
                        "name": "Jiarui Yao"
                    },
                    {
                        "name": "Manli Shu"
                    },
                    {
                        "name": "Yanchao Sun"
                    },
                    {
                        "name": "Zichu Wu"
                    },
                    {
                        "name": "Ning Yu"
                    },
                    {
                        "name": "Tom Goldstein"
                    },
                    {
                        "name": "Furong Huang"
                    }
                ],
                "author_detail": {
                    "name": "Furong Huang"
                },
                "author": "Furong Huang",
                "arxiv_comment": "Accepted by Thirty-Eighth Annual Conference on Neural Information\n  Processing Systems (Neurips 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.06659v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.06659v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10672v1",
                "updated": "2024-10-14T16:15:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    15,
                    57,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T16:15:57Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    15,
                    57,
                    0,
                    288,
                    0
                ],
                "title": "Large Language Model Evaluation via Matrix Nuclear-Norm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Evaluation via Matrix Nuclear-Norm"
                },
                "summary": "As large language models (LLMs) continue to evolve, efficient evaluation\nmetrics are vital for assessing their ability to compress information and\nreduce redundancy. While traditional metrics like Matrix Entropy offer valuable\ninsights, they are computationally intensive for large-scale models due to\ntheir \\( O(n^3) \\) time complexity with Singular Value Decomposition (SVD). To\nmitigate this issue, we introduce the Matrix Nuclear-Norm, which not only\nserves as a metric to quantify the data compression proficiency of LLM but also\nprovides a convex approximation of matrix rank to capture both predictive\ndiscriminability and diversity. By employing the \\( L_{1,2}\\text{-norm} \\) to\nfurther approximate the nuclear norm, we can effectively assess the model's\ninformation compression capabilities. This approach reduces the time complexity\nto \\( O(n^2) \\) and eliminates the need for SVD computation. Consequently, the\nMatrix Nuclear-Norm achieves speeds 8 to 24 times faster than Matrix Entropy\nfor the CEREBRAS-GPT model as sizes increase from 111M to 6.7B. This\nperformance gap becomes more pronounced with larger models, as validated in\ntests with other models like Pythia. Additionally, evaluations on benchmarks\nand model responses confirm that our proposed Matrix Nuclear-Norm is a\nreliable, scalable, and efficient tool for assessing LLMs' performance,\nstriking a balance between accuracy and computational efficiency. The code is\navailable at https://github.com/MLGroupJLU/MatrixNuclearNorm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to evolve, efficient evaluation\nmetrics are vital for assessing their ability to compress information and\nreduce redundancy. While traditional metrics like Matrix Entropy offer valuable\ninsights, they are computationally intensive for large-scale models due to\ntheir \\( O(n^3) \\) time complexity with Singular Value Decomposition (SVD). To\nmitigate this issue, we introduce the Matrix Nuclear-Norm, which not only\nserves as a metric to quantify the data compression proficiency of LLM but also\nprovides a convex approximation of matrix rank to capture both predictive\ndiscriminability and diversity. By employing the \\( L_{1,2}\\text{-norm} \\) to\nfurther approximate the nuclear norm, we can effectively assess the model's\ninformation compression capabilities. This approach reduces the time complexity\nto \\( O(n^2) \\) and eliminates the need for SVD computation. Consequently, the\nMatrix Nuclear-Norm achieves speeds 8 to 24 times faster than Matrix Entropy\nfor the CEREBRAS-GPT model as sizes increase from 111M to 6.7B. This\nperformance gap becomes more pronounced with larger models, as validated in\ntests with other models like Pythia. Additionally, evaluations on benchmarks\nand model responses confirm that our proposed Matrix Nuclear-Norm is a\nreliable, scalable, and efficient tool for assessing LLMs' performance,\nstriking a balance between accuracy and computational efficiency. The code is\navailable at https://github.com/MLGroupJLU/MatrixNuclearNorm."
                },
                "authors": [
                    {
                        "name": "Yahan Li"
                    },
                    {
                        "name": "Tingyu Xia"
                    },
                    {
                        "name": "Yi Chang"
                    },
                    {
                        "name": "Yuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Wu"
                },
                "author": "Yuan Wu",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11622v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11622v2",
                "updated": "2024-10-14T16:13:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    13,
                    37,
                    0,
                    288,
                    0
                ],
                "published": "2024-06-17T15:05:43Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    15,
                    5,
                    43,
                    0,
                    169,
                    0
                ],
                "title": "Building Knowledge-Guided Lexica to Model Cultural Variation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building Knowledge-Guided Lexica to Model Cultural Variation"
                },
                "summary": "Cultural variation exists between nations (e.g., the United States vs.\nChina), but also within regions (e.g., California vs. Texas, Los Angeles vs.\nSan Francisco). Measuring this regional cultural variation can illuminate how\nand why people think and behave differently. Historically, it has been\ndifficult to computationally model cultural variation due to a lack of training\ndata and scalability constraints. In this work, we introduce a new research\nproblem for the NLP community: How do we measure variation in cultural\nconstructs across regions using language? We then provide a scalable solution:\nbuilding knowledge-guided lexica to model cultural variation, encouraging\nfuture work at the intersection of NLP and cultural understanding. We also\nhighlight modern LLMs' failure to measure cultural variation or generate\nculturally varied language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cultural variation exists between nations (e.g., the United States vs.\nChina), but also within regions (e.g., California vs. Texas, Los Angeles vs.\nSan Francisco). Measuring this regional cultural variation can illuminate how\nand why people think and behave differently. Historically, it has been\ndifficult to computationally model cultural variation due to a lack of training\ndata and scalability constraints. In this work, we introduce a new research\nproblem for the NLP community: How do we measure variation in cultural\nconstructs across regions using language? We then provide a scalable solution:\nbuilding knowledge-guided lexica to model cultural variation, encouraging\nfuture work at the intersection of NLP and cultural understanding. We also\nhighlight modern LLMs' failure to measure cultural variation or generate\nculturally varied language."
                },
                "authors": [
                    {
                        "name": "Shreya Havaldar"
                    },
                    {
                        "name": "Salvatore Giorgi"
                    },
                    {
                        "name": "Sunny Rai"
                    },
                    {
                        "name": "Young-Min Cho"
                    },
                    {
                        "name": "Thomas Talhelm"
                    },
                    {
                        "name": "Sharath Chandra Guntuku"
                    },
                    {
                        "name": "Lyle Ungar"
                    }
                ],
                "author_detail": {
                    "name": "Lyle Ungar"
                },
                "author": "Lyle Ungar",
                "arxiv_comment": "Accepted at NAACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11622v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10665v1",
                "updated": "2024-10-14T16:11:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    11,
                    4,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T16:11:04Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    11,
                    4,
                    0,
                    288,
                    0
                ],
                "title": "Double Jeopardy and Climate Impact in the Use of Large Language Models:\n  Socio-economic Disparities and Reduced Utility for Non-English Speakers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Double Jeopardy and Climate Impact in the Use of Large Language Models:\n  Socio-economic Disparities and Reduced Utility for Non-English Speakers"
                },
                "summary": "Artificial Intelligence (AI), particularly large language models (LLMs),\nholds the potential to bridge language and information gaps, which can benefit\nthe economies of developing nations. However, our analysis of FLORES-200,\nFLORES+, Ethnologue, and World Development Indicators data reveals that these\nbenefits largely favor English speakers. Speakers of languages in low-income\nand lower-middle-income countries face higher costs when using OpenAI's GPT\nmodels via APIs because of how the system processes the input -- tokenization.\nAround 1.5 billion people, speaking languages primarily from\nlower-middle-income countries, could incur costs that are 4 to 6 times higher\nthan those faced by English speakers. Disparities in LLM performance are\nsignificant, and tokenization in models priced per token amplifies inequalities\nin access, cost, and utility. Moreover, using the quality of translation tasks\nas a proxy measure, we show that LLMs perform poorly in low-resource languages,\npresenting a ``double jeopardy\" of higher costs and poor performance for these\nusers. We also discuss the direct impact of fragmentation in tokenizing\nlow-resource languages on climate. This underscores the need for fairer\nalgorithm development to benefit all linguistic groups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence (AI), particularly large language models (LLMs),\nholds the potential to bridge language and information gaps, which can benefit\nthe economies of developing nations. However, our analysis of FLORES-200,\nFLORES+, Ethnologue, and World Development Indicators data reveals that these\nbenefits largely favor English speakers. Speakers of languages in low-income\nand lower-middle-income countries face higher costs when using OpenAI's GPT\nmodels via APIs because of how the system processes the input -- tokenization.\nAround 1.5 billion people, speaking languages primarily from\nlower-middle-income countries, could incur costs that are 4 to 6 times higher\nthan those faced by English speakers. Disparities in LLM performance are\nsignificant, and tokenization in models priced per token amplifies inequalities\nin access, cost, and utility. Moreover, using the quality of translation tasks\nas a proxy measure, we show that LLMs perform poorly in low-resource languages,\npresenting a ``double jeopardy\" of higher costs and poor performance for these\nusers. We also discuss the direct impact of fragmentation in tokenizing\nlow-resource languages on climate. This underscores the need for fairer\nalgorithm development to benefit all linguistic groups."
                },
                "authors": [
                    {
                        "name": "Aivin V. Solatorio"
                    },
                    {
                        "name": "Gabriel Stefanini Vicente"
                    },
                    {
                        "name": "Holly Krambeck"
                    },
                    {
                        "name": "Olivier Dupriez"
                    }
                ],
                "author_detail": {
                    "name": "Olivier Dupriez"
                },
                "author": "Olivier Dupriez",
                "arxiv_comment": "Project GitHub repository at\n  https://github.com/worldbank/double-jeopardy-in-llms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10661v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10661v1",
                "updated": "2024-10-14T16:09:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    9,
                    3,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T16:09:03Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    9,
                    3,
                    0,
                    288,
                    0
                ],
                "title": "Energetic Analysis of Emerging Quantum Communication Protocols",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energetic Analysis of Emerging Quantum Communication Protocols"
                },
                "summary": "With the rapid development and early industrialization of quantum\ntechnologies, it is of great interest to analyze their overall energy\nconsumption before planning for their wide-scale deployments. The evaluation of\nthe total energy requirements of quantum networks is a challenging task:\ndifferent networks require very disparate techniques to create, distribute,\nmanipulate, detect, and process quantum signals. This paper aims to lay the\nfoundations of a framework to model the energy requirements of different\nquantum technologies and protocols applied to near-term quantum networks.\nDifferent figures of merit are discussed and a benchmark on the energy\nconsumption of bipartite and multipartite network protocols is presented. An\nopen-source software to estimate the energy consumption of photonic setups is\nalso provided.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development and early industrialization of quantum\ntechnologies, it is of great interest to analyze their overall energy\nconsumption before planning for their wide-scale deployments. The evaluation of\nthe total energy requirements of quantum networks is a challenging task:\ndifferent networks require very disparate techniques to create, distribute,\nmanipulate, detect, and process quantum signals. This paper aims to lay the\nfoundations of a framework to model the energy requirements of different\nquantum technologies and protocols applied to near-term quantum networks.\nDifferent figures of merit are discussed and a benchmark on the energy\nconsumption of bipartite and multipartite network protocols is presented. An\nopen-source software to estimate the energy consumption of photonic setups is\nalso provided."
                },
                "authors": [
                    {
                        "name": "Raja Yehia"
                    },
                    {
                        "name": "Yoann Pitri"
                    },
                    {
                        "name": "Carlos Pascual-Garca"
                    },
                    {
                        "name": "Pascal Lefebvre"
                    },
                    {
                        "name": "Federico Centrone"
                    }
                ],
                "author_detail": {
                    "name": "Federico Centrone"
                },
                "author": "Federico Centrone",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10661v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10661v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10657v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10657v1",
                "updated": "2024-10-14T16:06:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    6,
                    35,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T16:06:35Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    6,
                    35,
                    0,
                    288,
                    0
                ],
                "title": "AutoTurb: Using Large Language Models for Automatic Algebraic Model\n  Discovery of Turbulence Closure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoTurb: Using Large Language Models for Automatic Algebraic Model\n  Discovery of Turbulence Closure"
                },
                "summary": "Symbolic regression (SR) methods have been extensively investigated to\nexplore explicit algebraic Reynolds stress models (EARSM) for turbulence\nclosure of Reynolds-averaged Navier-Stokes (RANS) equations. The deduced EARSM\ncan be readily implemented in existing computational fluid dynamic (CFD) codes\nand promotes the identification of physically interpretable turbulence models.\nThe existing SR methods, such as genetic programming, sparse regression, or\nartificial neural networks, require user-defined functional operators, a\nlibrary of candidates, or complex optimization algorithms. In this work, a\nnovel framework using LLMs to automatically discover algebraic expressions for\ncorrecting the RSM is proposed. The direct observation of Reynolds stress and\nthe indirect output of the CFD simulation are both involved in the training\nprocess to guarantee data consistency and avoid numerical stiffness.\nConstraints of functional complexity and convergence are supplementally imposed\nin the objective function on account of the tremendous flexibility of LLMs. The\nevolutionary search is employed for global optimization. The proposed method is\nperformed for separated flow over periodic hills at Re = 10,595. The\ngeneralizability of the discovered model is verified on a set of 2D turbulent\nseparated flow configurations with different Reynolds numbers and geometries.\nIt is demonstrated that the corrective RANS can improve the prediction for both\nthe Reynolds stress and mean velocity fields. Compared with algebraic models\ndiscovered by other works, the discovered model performs better in accuracy and\ngeneralization capability. The proposed approach provides a promising paradigm\nfor using LLMs to improve turbulence modeling for a given class of flows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symbolic regression (SR) methods have been extensively investigated to\nexplore explicit algebraic Reynolds stress models (EARSM) for turbulence\nclosure of Reynolds-averaged Navier-Stokes (RANS) equations. The deduced EARSM\ncan be readily implemented in existing computational fluid dynamic (CFD) codes\nand promotes the identification of physically interpretable turbulence models.\nThe existing SR methods, such as genetic programming, sparse regression, or\nartificial neural networks, require user-defined functional operators, a\nlibrary of candidates, or complex optimization algorithms. In this work, a\nnovel framework using LLMs to automatically discover algebraic expressions for\ncorrecting the RSM is proposed. The direct observation of Reynolds stress and\nthe indirect output of the CFD simulation are both involved in the training\nprocess to guarantee data consistency and avoid numerical stiffness.\nConstraints of functional complexity and convergence are supplementally imposed\nin the objective function on account of the tremendous flexibility of LLMs. The\nevolutionary search is employed for global optimization. The proposed method is\nperformed for separated flow over periodic hills at Re = 10,595. The\ngeneralizability of the discovered model is verified on a set of 2D turbulent\nseparated flow configurations with different Reynolds numbers and geometries.\nIt is demonstrated that the corrective RANS can improve the prediction for both\nthe Reynolds stress and mean velocity fields. Compared with algebraic models\ndiscovered by other works, the discovered model performs better in accuracy and\ngeneralization capability. The proposed approach provides a promising paradigm\nfor using LLMs to improve turbulence modeling for a given class of flows."
                },
                "authors": [
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Kefeng Zheng"
                    },
                    {
                        "name": "Fei Liu"
                    },
                    {
                        "name": "Qingfu Zhang"
                    },
                    {
                        "name": "Zhenkun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhenkun Wang"
                },
                "author": "Zhenkun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10657v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10657v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10650v1",
                "updated": "2024-10-14T16:01:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    1,
                    1,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T16:01:01Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    1,
                    1,
                    0,
                    288,
                    0
                ],
                "title": "Generative AI and Its Impact on Personalized Intelligent Tutoring\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI and Its Impact on Personalized Intelligent Tutoring\n  Systems"
                },
                "summary": "Generative Artificial Intelligence (AI) is revolutionizing educational\ntechnology by enabling highly personalized and adaptive learning environments\nwithin Intelligent Tutoring Systems (ITS). This report delves into the\nintegration of Generative AI, particularly large language models (LLMs) like\nGPT-4, into ITS to enhance personalized education through dynamic content\ngeneration, real-time feedback, and adaptive learning pathways. We explore key\napplications such as automated question generation, customized feedback\nmechanisms, and interactive dialogue systems that respond to individual learner\nneeds. The report also addresses significant challenges, including ensuring\npedagogical accuracy, mitigating inherent biases in AI models, and maintaining\nlearner engagement. Future directions highlight the potential advancements in\nmultimodal AI integration, emotional intelligence in tutoring systems, and the\nethical implications of AI-driven education. By synthesizing current research\nand practical implementations, this report underscores the transformative\npotential of Generative AI in creating more effective, equitable, and engaging\neducational experiences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Artificial Intelligence (AI) is revolutionizing educational\ntechnology by enabling highly personalized and adaptive learning environments\nwithin Intelligent Tutoring Systems (ITS). This report delves into the\nintegration of Generative AI, particularly large language models (LLMs) like\nGPT-4, into ITS to enhance personalized education through dynamic content\ngeneration, real-time feedback, and adaptive learning pathways. We explore key\napplications such as automated question generation, customized feedback\nmechanisms, and interactive dialogue systems that respond to individual learner\nneeds. The report also addresses significant challenges, including ensuring\npedagogical accuracy, mitigating inherent biases in AI models, and maintaining\nlearner engagement. Future directions highlight the potential advancements in\nmultimodal AI integration, emotional intelligence in tutoring systems, and the\nethical implications of AI-driven education. By synthesizing current research\nand practical implementations, this report underscores the transformative\npotential of Generative AI in creating more effective, equitable, and engaging\neducational experiences."
                },
                "authors": [
                    {
                        "name": "Subhankar Maity"
                    },
                    {
                        "name": "Aniket Deroy"
                    }
                ],
                "author_detail": {
                    "name": "Aniket Deroy"
                },
                "author": "Aniket Deroy",
                "arxiv_comment": "Scientific Report (Under Review)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10648v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10648v1",
                "updated": "2024-10-14T15:59:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    59,
                    16,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T15:59:16Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    59,
                    16,
                    0,
                    288,
                    0
                ],
                "title": "A Simple Baseline for Predicting Events with Auto-Regressive Tabular\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple Baseline for Predicting Events with Auto-Regressive Tabular\n  Transformers"
                },
                "summary": "Many real-world applications of tabular data involve using historic events to\npredict properties of new ones, for example whether a credit card transaction\nis fraudulent or what rating a customer will assign a product on a retail\nplatform. Existing approaches to event prediction include costly, brittle, and\napplication-dependent techniques such as time-aware positional embeddings,\nlearned row and field encodings, and oversampling methods for addressing class\nimbalance. Moreover, these approaches often assume specific use-cases, for\nexample that we know the labels of all historic events or that we only predict\na pre-specified label and not the data's features themselves. In this work, we\npropose a simple but flexible baseline using standard autoregressive LLM-style\ntransformers with elementary positional embeddings and a causal language\nmodeling objective. Our baseline outperforms existing approaches across popular\ndatasets and can be employed for various use-cases. We demonstrate that the\nsame model can predict labels, impute missing values, or model event sequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many real-world applications of tabular data involve using historic events to\npredict properties of new ones, for example whether a credit card transaction\nis fraudulent or what rating a customer will assign a product on a retail\nplatform. Existing approaches to event prediction include costly, brittle, and\napplication-dependent techniques such as time-aware positional embeddings,\nlearned row and field encodings, and oversampling methods for addressing class\nimbalance. Moreover, these approaches often assume specific use-cases, for\nexample that we know the labels of all historic events or that we only predict\na pre-specified label and not the data's features themselves. In this work, we\npropose a simple but flexible baseline using standard autoregressive LLM-style\ntransformers with elementary positional embeddings and a causal language\nmodeling objective. Our baseline outperforms existing approaches across popular\ndatasets and can be employed for various use-cases. We demonstrate that the\nsame model can predict labels, impute missing values, or model event sequences."
                },
                "authors": [
                    {
                        "name": "Alex Stein"
                    },
                    {
                        "name": "Samuel Sharpe"
                    },
                    {
                        "name": "Doron Bergman"
                    },
                    {
                        "name": "Senthil Kumar"
                    },
                    {
                        "name": "Bayan Bruss"
                    },
                    {
                        "name": "John Dickerson"
                    },
                    {
                        "name": "Tom Goldstein"
                    },
                    {
                        "name": "Micah Goldblum"
                    }
                ],
                "author_detail": {
                    "name": "Micah Goldblum"
                },
                "author": "Micah Goldblum",
                "arxiv_comment": "10 pages, 6 pages of references+appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10648v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10648v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10644v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10644v1",
                "updated": "2024-10-14T15:53:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    53,
                    30,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T15:53:30Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    53,
                    30,
                    0,
                    288,
                    0
                ],
                "title": "Functional Flexibility in Generative AI Interfaces: Text Editing with\n  LLMs through Conversations, Toolbars, and Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Functional Flexibility in Generative AI Interfaces: Text Editing with\n  LLMs through Conversations, Toolbars, and Prompts"
                },
                "summary": "Prompting-based user interfaces (UIs) shift the task of defining and\naccessing relevant functions from developers to users. However, how UIs shape\nthis flexibility has not yet been investigated explicitly. We explored\ninteraction with Large Language Models (LLMs) over four years, before and after\nthe rise of general-purpose LLMs: (1) Our survey (N=121) elicited how users\nenvision to delegate writing tasks to AI. This informed a conversational UI\ndesign. (2) A user study (N=10) revealed that people regressed to using short\ncommand-like prompts. (3) When providing these directly as shortcuts in a\ntoolbar UI, in addition to prompting, users in our second study (N=12)\ndynamically switched between specified and flexible AI functions. We discuss\nfunctional flexibility as a new theoretical construct and thinking tool. Our\nwork highlights the value of moving beyond conversational UIs, by considering\nhow different UIs shape users' access to the functional space of generative AI\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting-based user interfaces (UIs) shift the task of defining and\naccessing relevant functions from developers to users. However, how UIs shape\nthis flexibility has not yet been investigated explicitly. We explored\ninteraction with Large Language Models (LLMs) over four years, before and after\nthe rise of general-purpose LLMs: (1) Our survey (N=121) elicited how users\nenvision to delegate writing tasks to AI. This informed a conversational UI\ndesign. (2) A user study (N=10) revealed that people regressed to using short\ncommand-like prompts. (3) When providing these directly as shortcuts in a\ntoolbar UI, in addition to prompting, users in our second study (N=12)\ndynamically switched between specified and flexible AI functions. We discuss\nfunctional flexibility as a new theoretical construct and thinking tool. Our\nwork highlights the value of moving beyond conversational UIs, by considering\nhow different UIs shape users' access to the functional space of generative AI\nmodels."
                },
                "authors": [
                    {
                        "name": "Florian Lehmann"
                    },
                    {
                        "name": "Daniel Buschek"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Buschek"
                },
                "author": "Daniel Buschek",
                "arxiv_comment": "Just submitted. This is the author's version of the work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10644v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10644v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.04701v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.04701v2",
                "updated": "2024-10-14T15:52:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    52,
                    31,
                    0,
                    288,
                    0
                ],
                "published": "2023-05-08T13:32:41Z",
                "published_parsed": [
                    2023,
                    5,
                    8,
                    13,
                    32,
                    41,
                    0,
                    128,
                    0
                ],
                "title": "Differentially Private Attention Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentially Private Attention Computation"
                },
                "summary": "Large language models (LLMs), especially those based on the Transformer\narchitecture, have had a profound impact on various aspects of daily life, such\nas natural language processing, content generation, research methodologies, and\nmore. Nevertheless, a crucial concern regarding the inference results of large\nlanguage models is the issue of security and privacy. Given that large language\nmodels can generate results that may leak sensitive confidential or copyright\ninformation in many scenarios, it is crucial to compute the attention matrix\nwith provable privacy guarantees, as attention is all you need.\n  In this work, we propose a novel and efficient algorithm for approximating\nthe attention matrix while providing differential privacy (DP) guarantees. To\nachieve this, we build on recent advancements in fast attention computation and\ndifferentially private matrix publishing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), especially those based on the Transformer\narchitecture, have had a profound impact on various aspects of daily life, such\nas natural language processing, content generation, research methodologies, and\nmore. Nevertheless, a crucial concern regarding the inference results of large\nlanguage models is the issue of security and privacy. Given that large language\nmodels can generate results that may leak sensitive confidential or copyright\ninformation in many scenarios, it is crucial to compute the attention matrix\nwith provable privacy guarantees, as attention is all you need.\n  In this work, we propose a novel and efficient algorithm for approximating\nthe attention matrix while providing differential privacy (DP) guarantees. To\nachieve this, we build on recent advancements in fast attention computation and\ndifferentially private matrix publishing."
                },
                "authors": [
                    {
                        "name": "Yeqi Gao"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Xin Yang"
                    },
                    {
                        "name": "Yufa Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yufa Zhou"
                },
                "author": "Yufa Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.04701v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.04701v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10639v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10639v1",
                "updated": "2024-10-14T15:50:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    50,
                    35,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T15:50:35Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    50,
                    35,
                    0,
                    288,
                    0
                ],
                "title": "Generating Model Parameters for Controlling: Parameter Diffusion for\n  Controllable Multi-Task Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Model Parameters for Controlling: Parameter Diffusion for\n  Controllable Multi-Task Recommendation"
                },
                "summary": "Commercial recommender systems face the challenge that task requirements from\nplatforms or users often change dynamically (e.g., varying preferences for\naccuracy or diversity). Ideally, the model should be re-trained after resetting\na new objective function, adapting to these changes in task requirements.\nHowever, in practice, the high computational costs associated with retraining\nmake this process impractical for models already deployed to online\nenvironments. This raises a new challenging problem: how to efficiently adapt\nthe learning model to different task requirements by controlling model\nparameters after deployment, without the need for retraining. To address this\nissue, we propose a novel controllable learning approach via Parameter\nDiffusion for controllable multi-task Recommendation (PaDiRec), which allows\nthe customization and adaptation of recommendation model parameters to new task\nrequirements without retraining. Specifically, we first obtain the optimized\nmodel parameters through adapter tunning based on the feasible task\nrequirements. Then, we utilize the diffusion model as a parameter generator,\nemploying classifier-free guidance in conditional training to learn the\ndistribution of optimized model parameters under various task requirements.\nFinally, the diffusion model is applied to effectively generate model\nparameters in a test-time adaptation manner given task requirements. As a\nmodel-agnostic approach, PaDiRec can leverage existing recommendation models as\nbackbones to enhance their controllability. Extensive experiments on public\ndatasets and a dataset from a commercial app, indicate that PaDiRec can\neffectively enhance controllability through efficient model parameter\ngeneration. The code is released at\nhttps://anonymous.4open.science/r/PaDiRec-DD13.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commercial recommender systems face the challenge that task requirements from\nplatforms or users often change dynamically (e.g., varying preferences for\naccuracy or diversity). Ideally, the model should be re-trained after resetting\na new objective function, adapting to these changes in task requirements.\nHowever, in practice, the high computational costs associated with retraining\nmake this process impractical for models already deployed to online\nenvironments. This raises a new challenging problem: how to efficiently adapt\nthe learning model to different task requirements by controlling model\nparameters after deployment, without the need for retraining. To address this\nissue, we propose a novel controllable learning approach via Parameter\nDiffusion for controllable multi-task Recommendation (PaDiRec), which allows\nthe customization and adaptation of recommendation model parameters to new task\nrequirements without retraining. Specifically, we first obtain the optimized\nmodel parameters through adapter tunning based on the feasible task\nrequirements. Then, we utilize the diffusion model as a parameter generator,\nemploying classifier-free guidance in conditional training to learn the\ndistribution of optimized model parameters under various task requirements.\nFinally, the diffusion model is applied to effectively generate model\nparameters in a test-time adaptation manner given task requirements. As a\nmodel-agnostic approach, PaDiRec can leverage existing recommendation models as\nbackbones to enhance their controllability. Extensive experiments on public\ndatasets and a dataset from a commercial app, indicate that PaDiRec can\neffectively enhance controllability through efficient model parameter\ngeneration. The code is released at\nhttps://anonymous.4open.science/r/PaDiRec-DD13."
                },
                "authors": [
                    {
                        "name": "Chenglei Shen"
                    },
                    {
                        "name": "Jiahao Zhao"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Weijie Yu"
                    },
                    {
                        "name": "Ming He"
                    },
                    {
                        "name": "Jianping Fan"
                    }
                ],
                "author_detail": {
                    "name": "Jianping Fan"
                },
                "author": "Jianping Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10639v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10639v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10636v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10636v1",
                "updated": "2024-10-14T15:48:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    48,
                    9,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T15:48:09Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    48,
                    9,
                    0,
                    288,
                    0
                ],
                "title": "Adapt-$\\infty$: Scalable Lifelong Multimodal Instruction Tuning via\n  Dynamic Data Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapt-$\\infty$: Scalable Lifelong Multimodal Instruction Tuning via\n  Dynamic Data Selection"
                },
                "summary": "Visual instruction datasets from various distributors are released at\ndifferent times and often contain a significant number of semantically\nredundant text-image pairs, depending on their task compositions (i.e., skills)\nor reference sources. This redundancy greatly limits the efficient deployment\nof lifelong adaptable multimodal large language models, hindering their ability\nto refine existing skills and acquire new competencies over time. To address\nthis, we reframe the problem of Lifelong Instruction Tuning (LiIT) via data\nselection, where the model automatically selects beneficial samples to learn\nfrom earlier and new datasets based on the current state of acquired knowledge\nin the model. Based on empirical analyses that show that selecting the best\ndata subset using a static importance measure is often ineffective for\nmulti-task datasets with evolving distributions, we propose Adapt-$\\infty$, a\nnew multi-way and adaptive data selection approach that dynamically balances\nsample efficiency and effectiveness during LiIT. We construct pseudo-skill\nclusters by grouping gradient-based sample vectors. Next, we select the\nbest-performing data selector for each skill cluster from a pool of selector\nexperts, including our newly proposed scoring function, Image Grounding score.\nThis data selector samples a subset of the most important samples from each\nskill cluster for training. To prevent the continuous increase in the size of\nthe dataset pool during LiIT, which would result in excessive computation, we\nfurther introduce a cluster-wise permanent data pruning strategy to remove the\nmost semantically redundant samples from each cluster, keeping computational\nrequirements manageable. Training with samples selected by Adapt-$\\infty$\nalleviates catastrophic forgetting, especially for rare tasks, and promotes\nforward transfer across the continuum using only a fraction of the original\ndatasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual instruction datasets from various distributors are released at\ndifferent times and often contain a significant number of semantically\nredundant text-image pairs, depending on their task compositions (i.e., skills)\nor reference sources. This redundancy greatly limits the efficient deployment\nof lifelong adaptable multimodal large language models, hindering their ability\nto refine existing skills and acquire new competencies over time. To address\nthis, we reframe the problem of Lifelong Instruction Tuning (LiIT) via data\nselection, where the model automatically selects beneficial samples to learn\nfrom earlier and new datasets based on the current state of acquired knowledge\nin the model. Based on empirical analyses that show that selecting the best\ndata subset using a static importance measure is often ineffective for\nmulti-task datasets with evolving distributions, we propose Adapt-$\\infty$, a\nnew multi-way and adaptive data selection approach that dynamically balances\nsample efficiency and effectiveness during LiIT. We construct pseudo-skill\nclusters by grouping gradient-based sample vectors. Next, we select the\nbest-performing data selector for each skill cluster from a pool of selector\nexperts, including our newly proposed scoring function, Image Grounding score.\nThis data selector samples a subset of the most important samples from each\nskill cluster for training. To prevent the continuous increase in the size of\nthe dataset pool during LiIT, which would result in excessive computation, we\nfurther introduce a cluster-wise permanent data pruning strategy to remove the\nmost semantically redundant samples from each cluster, keeping computational\nrequirements manageable. Training with samples selected by Adapt-$\\infty$\nalleviates catastrophic forgetting, especially for rare tasks, and promotes\nforward transfer across the continuum using only a fraction of the original\ndatasets."
                },
                "authors": [
                    {
                        "name": "Adyasha Maharana"
                    },
                    {
                        "name": "Jaehong Yoon"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "First two authors contributed equally. Code:\n  https://github.com/adymaharana/adapt-inf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10636v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10636v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10634v1",
                "updated": "2024-10-14T15:45:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    45,
                    54,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T15:45:54Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    45,
                    54,
                    0,
                    288,
                    0
                ],
                "title": "A GPU-accelerated Molecular Docking Workflow with Kubernetes and Apache\n  Airflow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A GPU-accelerated Molecular Docking Workflow with Kubernetes and Apache\n  Airflow"
                },
                "summary": "Complex workflows play a critical role in accelerating scientific discovery.\nIn many scientific domains, efficient workflow management can lead to faster\nscientific output and broader user groups. Workflows that can leverage\nresources across the boundary between cloud and HPC are a strong driver for the\nconvergence of HPC and cloud. This study investigates the transition and\ndeployment of a GPU-accelerated molecular docking workflow that was designed\nfor HPC systems onto a cloud-native environment with Kubernetes and Apache\nAirflow. The case study focuses on state-of-of-the-art molecular docking\nsoftware for drug discovery. We provide a DAG-based implementation in Apache\nAirflow and technical details for GPU-accelerated deployment. We evaluated the\nworkflow using the SWEETLEAD bioinformatics dataset and executed it in a Cloud\nenvironment with heterogeneous computing resources. Our workflow can\neffectively overlap different stages when mapped onto different computing\nresources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complex workflows play a critical role in accelerating scientific discovery.\nIn many scientific domains, efficient workflow management can lead to faster\nscientific output and broader user groups. Workflows that can leverage\nresources across the boundary between cloud and HPC are a strong driver for the\nconvergence of HPC and cloud. This study investigates the transition and\ndeployment of a GPU-accelerated molecular docking workflow that was designed\nfor HPC systems onto a cloud-native environment with Kubernetes and Apache\nAirflow. The case study focuses on state-of-of-the-art molecular docking\nsoftware for drug discovery. We provide a DAG-based implementation in Apache\nAirflow and technical details for GPU-accelerated deployment. We evaluated the\nworkflow using the SWEETLEAD bioinformatics dataset and executed it in a Cloud\nenvironment with heterogeneous computing resources. Our workflow can\neffectively overlap different stages when mapped onto different computing\nresources."
                },
                "authors": [
                    {
                        "name": "Daniel Medeiros"
                    },
                    {
                        "name": "Gabin Schieffer"
                    },
                    {
                        "name": "Jacob Wahlgren"
                    },
                    {
                        "name": "Ivy Peng"
                    }
                ],
                "author_detail": {
                    "name": "Ivy Peng"
                },
                "author": "Ivy Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14504v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14504v2",
                "updated": "2024-10-14T15:39:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    39,
                    36,
                    0,
                    288,
                    0
                ],
                "published": "2024-06-20T17:06:58Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    17,
                    6,
                    58,
                    3,
                    172,
                    0
                ],
                "title": "Translating Across Cultures: LLMs for Intralingual Cultural Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Translating Across Cultures: LLMs for Intralingual Cultural Adaptation"
                },
                "summary": "LLMs are increasingly being deployed for multilingual applications and have\ndemonstrated impressive translation capabilities between several low and\nhigh-resource languages. An aspect of translation that often gets overlooked is\nthat of cultural adaptation, or modifying source culture references to suit the\ntarget culture. While specialized translation models still outperform LLMs on\nthe machine translation task when viewed from the lens of correctness, they are\nnot sensitive to cultural differences often requiring manual correction. LLMs\non the other hand have a rich reservoir of cultural knowledge embedded within\nits parameters that can be potentially exploited for such applications. In this\npaper, we define the task of cultural adaptation and create an evaluation\nframework to evaluate the performance of modern LLMs for cultural adaptation\nand analyze their cross-cultural knowledge while connecting related concepts\nacross different cultures. We also analyze possible issues with automatic\nadaptation. We hope that this task will offer more insight into the cultural\nunderstanding of LLMs and their creativity in cross-cultural scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are increasingly being deployed for multilingual applications and have\ndemonstrated impressive translation capabilities between several low and\nhigh-resource languages. An aspect of translation that often gets overlooked is\nthat of cultural adaptation, or modifying source culture references to suit the\ntarget culture. While specialized translation models still outperform LLMs on\nthe machine translation task when viewed from the lens of correctness, they are\nnot sensitive to cultural differences often requiring manual correction. LLMs\non the other hand have a rich reservoir of cultural knowledge embedded within\nits parameters that can be potentially exploited for such applications. In this\npaper, we define the task of cultural adaptation and create an evaluation\nframework to evaluate the performance of modern LLMs for cultural adaptation\nand analyze their cross-cultural knowledge while connecting related concepts\nacross different cultures. We also analyze possible issues with automatic\nadaptation. We hope that this task will offer more insight into the cultural\nunderstanding of LLMs and their creativity in cross-cultural scenarios."
                },
                "authors": [
                    {
                        "name": "Pushpdeep Singh"
                    },
                    {
                        "name": "Mayur Patidar"
                    },
                    {
                        "name": "Lovekesh Vig"
                    }
                ],
                "author_detail": {
                    "name": "Lovekesh Vig"
                },
                "author": "Lovekesh Vig",
                "arxiv_comment": "Accepted to CoNLL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14504v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14504v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10630v1",
                "updated": "2024-10-14T15:38:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    38,
                    56,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T15:38:56Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    38,
                    56,
                    0,
                    288,
                    0
                ],
                "title": "Thinking LLMs: General Instruction Following with Thought Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thinking LLMs: General Instruction Following with Thought Generation"
                },
                "summary": "LLMs are typically trained to answer user questions or follow instructions\nsimilarly to how human experts respond. However, in the standard alignment\nframework they lack the basic ability of explicit thinking before answering.\nThinking is important for complex questions that require reasoning and planning\n-- but can be applied to any task. We propose a training method for equipping\nexisting LLMs with such thinking abilities for general instruction following\nwithout use of additional human data. We achieve this by an iterative search\nand optimization procedure that explores the space of possible thought\ngenerations, allowing the model to learn how to think without direct\nsupervision. For each instruction, the thought candidates are scored using a\njudge model to evaluate their responses only, and then optimized via preference\noptimization. We show that this procedure leads to superior performance on\nAlpacaEval and Arena-Hard, and shows gains from thinking on non-reasoning\ncategories such as marketing, health and general knowledge, in addition to more\ntraditional reasoning & problem-solving tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are typically trained to answer user questions or follow instructions\nsimilarly to how human experts respond. However, in the standard alignment\nframework they lack the basic ability of explicit thinking before answering.\nThinking is important for complex questions that require reasoning and planning\n-- but can be applied to any task. We propose a training method for equipping\nexisting LLMs with such thinking abilities for general instruction following\nwithout use of additional human data. We achieve this by an iterative search\nand optimization procedure that explores the space of possible thought\ngenerations, allowing the model to learn how to think without direct\nsupervision. For each instruction, the thought candidates are scored using a\njudge model to evaluate their responses only, and then optimized via preference\noptimization. We show that this procedure leads to superior performance on\nAlpacaEval and Arena-Hard, and shows gains from thinking on non-reasoning\ncategories such as marketing, health and general knowledge, in addition to more\ntraditional reasoning & problem-solving tasks."
                },
                "authors": [
                    {
                        "name": "Tianhao Wu"
                    },
                    {
                        "name": "Janice Lan"
                    },
                    {
                        "name": "Weizhe Yuan"
                    },
                    {
                        "name": "Jiantao Jiao"
                    },
                    {
                        "name": "Jason Weston"
                    },
                    {
                        "name": "Sainbayar Sukhbaatar"
                    }
                ],
                "author_detail": {
                    "name": "Sainbayar Sukhbaatar"
                },
                "author": "Sainbayar Sukhbaatar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10629v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10629v2",
                "updated": "2024-10-15T06:19:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    6,
                    19,
                    7,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-14T15:36:42Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    36,
                    42,
                    0,
                    288,
                    0
                ],
                "title": "SANA: Efficient High-Resolution Image Synthesis with Linear Diffusion\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SANA: Efficient High-Resolution Image Synthesis with Linear Diffusion\n  Transformers"
                },
                "summary": "We introduce Sana, a text-to-image framework that can efficiently generate\nimages up to 4096$\\times$4096 resolution. Sana can synthesize high-resolution,\nhigh-quality images with strong text-image alignment at a remarkably fast\nspeed, deployable on laptop GPU. Core designs include: (1) Deep compression\nautoencoder: unlike traditional AEs, which compress images only 8$\\times$, we\ntrained an AE that can compress images 32$\\times$, effectively reducing the\nnumber of latent tokens. (2) Linear DiT: we replace all vanilla attention in\nDiT with linear attention, which is more efficient at high resolutions without\nsacrificing quality. (3) Decoder-only text encoder: we replaced T5 with modern\ndecoder-only small LLM as the text encoder and designed complex human\ninstruction with in-context learning to enhance the image-text alignment. (4)\nEfficient training and sampling: we propose Flow-DPM-Solver to reduce sampling\nsteps, with efficient caption labeling and selection to accelerate convergence.\nAs a result, Sana-0.6B is very competitive with modern giant diffusion model\n(e.g. Flux-12B), being 20 times smaller and 100+ times faster in measured\nthroughput. Moreover, Sana-0.6B can be deployed on a 16GB laptop GPU, taking\nless than 1 second to generate a 1024$\\times$1024 resolution image. Sana\nenables content creation at low cost. Code and model will be publicly released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Sana, a text-to-image framework that can efficiently generate\nimages up to 4096$\\times$4096 resolution. Sana can synthesize high-resolution,\nhigh-quality images with strong text-image alignment at a remarkably fast\nspeed, deployable on laptop GPU. Core designs include: (1) Deep compression\nautoencoder: unlike traditional AEs, which compress images only 8$\\times$, we\ntrained an AE that can compress images 32$\\times$, effectively reducing the\nnumber of latent tokens. (2) Linear DiT: we replace all vanilla attention in\nDiT with linear attention, which is more efficient at high resolutions without\nsacrificing quality. (3) Decoder-only text encoder: we replaced T5 with modern\ndecoder-only small LLM as the text encoder and designed complex human\ninstruction with in-context learning to enhance the image-text alignment. (4)\nEfficient training and sampling: we propose Flow-DPM-Solver to reduce sampling\nsteps, with efficient caption labeling and selection to accelerate convergence.\nAs a result, Sana-0.6B is very competitive with modern giant diffusion model\n(e.g. Flux-12B), being 20 times smaller and 100+ times faster in measured\nthroughput. Moreover, Sana-0.6B can be deployed on a 16GB laptop GPU, taking\nless than 1 second to generate a 1024$\\times$1024 resolution image. Sana\nenables content creation at low cost. Code and model will be publicly released."
                },
                "authors": [
                    {
                        "name": "Enze Xie"
                    },
                    {
                        "name": "Junsong Chen"
                    },
                    {
                        "name": "Junyu Chen"
                    },
                    {
                        "name": "Han Cai"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Zhekai Zhang"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10629v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10629v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10628v1",
                "updated": "2024-10-14T15:35:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    35,
                    44,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T15:35:44Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    35,
                    44,
                    0,
                    288,
                    0
                ],
                "title": "Test smells in LLM-Generated Unit Tests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test smells in LLM-Generated Unit Tests"
                },
                "summary": "The use of Large Language Models (LLMs) in automated test generation is\ngaining popularity, with much of the research focusing on metrics like\ncompilability rate, code coverage and bug detection. However, an equally\nimportant quality metric is the presence of test smells design flaws or anti\npatterns in test code that hinder maintainability and readability. In this\nstudy, we explore the diffusion of test smells in LLM generated unit test\nsuites and compare them to those found in human written ones. We analyze a\nbenchmark of 20,500 LLM-generated test suites produced by four models (GPT-3.5,\nGPT-4, Mistral 7B, and Mixtral 8x7B) across five prompt engineering techniques,\nalongside a dataset of 780,144 human written test suites from 34,637 projects.\nLeveraging TsDetect, a state of the art tool capable of detecting 21 different\ntypes of test smells, we identify and analyze the prevalence and co-occurrence\nof various test smells in both human written and LLM-generated test suites. Our\nfindings reveal new insights into the strengths and limitations of LLMs in test\ngeneration. First, regarding prevalence, we observe that LLMs frequently\ngenerate tests with common test smells, such as Magic Number Test and Assertion\nRoulette. Second, in terms of co occurrence, certain smells, like Long Test and\nUseless Test, tend to co occur in LLM-generated suites, influenced by specific\nprompt techniques. Third, we find that project complexity and LLM specific\nfactors, including model size and context length, significantly affect the\nprevalence of test smells. Finally, the patterns of test smells in\nLLM-generated tests often mirror those in human-written tests, suggesting\npotential data leakage from training datasets. These insights underscore the\nneed to refine LLM-based test generation for cleaner code and suggest\nimprovements in both LLM capabilities and software testing practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of Large Language Models (LLMs) in automated test generation is\ngaining popularity, with much of the research focusing on metrics like\ncompilability rate, code coverage and bug detection. However, an equally\nimportant quality metric is the presence of test smells design flaws or anti\npatterns in test code that hinder maintainability and readability. In this\nstudy, we explore the diffusion of test smells in LLM generated unit test\nsuites and compare them to those found in human written ones. We analyze a\nbenchmark of 20,500 LLM-generated test suites produced by four models (GPT-3.5,\nGPT-4, Mistral 7B, and Mixtral 8x7B) across five prompt engineering techniques,\nalongside a dataset of 780,144 human written test suites from 34,637 projects.\nLeveraging TsDetect, a state of the art tool capable of detecting 21 different\ntypes of test smells, we identify and analyze the prevalence and co-occurrence\nof various test smells in both human written and LLM-generated test suites. Our\nfindings reveal new insights into the strengths and limitations of LLMs in test\ngeneration. First, regarding prevalence, we observe that LLMs frequently\ngenerate tests with common test smells, such as Magic Number Test and Assertion\nRoulette. Second, in terms of co occurrence, certain smells, like Long Test and\nUseless Test, tend to co occur in LLM-generated suites, influenced by specific\nprompt techniques. Third, we find that project complexity and LLM specific\nfactors, including model size and context length, significantly affect the\nprevalence of test smells. Finally, the patterns of test smells in\nLLM-generated tests often mirror those in human-written tests, suggesting\npotential data leakage from training datasets. These insights underscore the\nneed to refine LLM-based test generation for cleaner code and suggest\nimprovements in both LLM capabilities and software testing practices."
                },
                "authors": [
                    {
                        "name": "Wendkuni C. Oudraogo"
                    },
                    {
                        "name": "Yinghua Li"
                    },
                    {
                        "name": "Kader Kabor"
                    },
                    {
                        "name": "Xunzhu Tang"
                    },
                    {
                        "name": "Anil Koyuncu"
                    },
                    {
                        "name": "Jacques Klein"
                    },
                    {
                        "name": "David Lo"
                    },
                    {
                        "name": "Tegawend F. Bissyand"
                    }
                ],
                "author_detail": {
                    "name": "Tegawend F. Bissyand"
                },
                "author": "Tegawend F. Bissyand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10626v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10626v1",
                "updated": "2024-10-14T15:31:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    31,
                    54,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T15:31:54Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    31,
                    54,
                    0,
                    288,
                    0
                ],
                "title": "Efficiently Democratizing Medical LLMs for 50 Languages via a Mixture of\n  Language Family Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently Democratizing Medical LLMs for 50 Languages via a Mixture of\n  Language Family Experts"
                },
                "summary": "Adapting medical Large Language Models to local languages can reduce barriers\nto accessing healthcare services, but data scarcity remains a significant\nchallenge, particularly for low-resource languages. To address this, we first\nconstruct a high-quality medical dataset and conduct analysis to ensure its\nquality. In order to leverage the generalization capability of multilingual\nLLMs to efficiently scale to more resource-constrained languages, we explore\nthe internal information flow of LLMs from a multilingual perspective using\nMixture of Experts (MoE) modularity. Technically, we propose a novel MoE\nrouting method that employs language-specific experts and cross-lingual\nrouting. Inspired by circuit theory, our routing analysis revealed a Spread Out\nin the End information flow mechanism: while earlier layers concentrate\ncross-lingual information flow, the later layers exhibit language-specific\ndivergence. This insight directly led to the development of the Post-MoE\narchitecture, which applies sparse routing only in the later layers while\nmaintaining dense others. Experimental results demonstrate that this approach\nenhances the generalization of multilingual models to other languages while\npreserving interpretability. Finally, to efficiently scale the model to 50\nlanguages, we introduce the concept of language family experts, drawing on\nlinguistic priors, which enables scaling the number of languages without adding\nadditional parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting medical Large Language Models to local languages can reduce barriers\nto accessing healthcare services, but data scarcity remains a significant\nchallenge, particularly for low-resource languages. To address this, we first\nconstruct a high-quality medical dataset and conduct analysis to ensure its\nquality. In order to leverage the generalization capability of multilingual\nLLMs to efficiently scale to more resource-constrained languages, we explore\nthe internal information flow of LLMs from a multilingual perspective using\nMixture of Experts (MoE) modularity. Technically, we propose a novel MoE\nrouting method that employs language-specific experts and cross-lingual\nrouting. Inspired by circuit theory, our routing analysis revealed a Spread Out\nin the End information flow mechanism: while earlier layers concentrate\ncross-lingual information flow, the later layers exhibit language-specific\ndivergence. This insight directly led to the development of the Post-MoE\narchitecture, which applies sparse routing only in the later layers while\nmaintaining dense others. Experimental results demonstrate that this approach\nenhances the generalization of multilingual models to other languages while\npreserving interpretability. Finally, to efficiently scale the model to 50\nlanguages, we introduce the concept of language family experts, drawing on\nlinguistic priors, which enables scaling the number of languages without adding\nadditional parameters."
                },
                "authors": [
                    {
                        "name": "Guorui Zheng"
                    },
                    {
                        "name": "Xidong Wang"
                    },
                    {
                        "name": "Juhao Liang"
                    },
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Yuping Zheng"
                    },
                    {
                        "name": "Benyou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Benyou Wang"
                },
                "author": "Benyou Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10626v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10624v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10624v1",
                "updated": "2024-10-14T15:30:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    30,
                    41,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T15:30:41Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    30,
                    41,
                    0,
                    288,
                    0
                ],
                "title": "SensorLLM: Aligning Large Language Models with Motion Sensors for Human\n  Activity Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SensorLLM: Aligning Large Language Models with Motion Sensors for Human\n  Activity Recognition"
                },
                "summary": "In this work, we bridge the gap between wearable sensor technology and\npersonalized AI assistants by enabling Large Language Models (LLMs) to\nunderstand time-series tasks like human activity recognition (HAR). Despite the\nstrong reasoning and generalization capabilities of LLMs, leveraging them for\nsensor data tasks remains largely unexplored. This gap stems from challenges\nlike the lack of semantic context in time-series data, computational\nlimitations, and LLMs' difficulty processing numerical inputs. To address these\nissues, we introduce SensorLLM, a two-stage framework to unlock LLMs' potential\nfor sensor data tasks. In the Sensor-Language Alignment Stage, we introduce\nspecial tokens for each sensor channel and automatically generate\ntrend-descriptive text to align sensor data with textual inputs, enabling\nSensorLLM to capture numerical changes, channel-specific information, and\nsensor data of varying lengths-capabilities that existing LLMs typically\nstruggle with, all without the need for human annotations. Next, in Task-Aware\nTuning Stage, we refine the model for HAR classification using the frozen LLM\nand alignment module, achieving performance on par with or surpassing\nstate-of-the-art models. We further demonstrate that SensorLLM evolves into an\neffective sensor learner, reasoner, and classifier through Sensor-Language\nAlignment, enabling it to generalize across diverse datasets for HAR tasks. We\nstrongly believe our work lays the stepstone for future time-series and text\nalignment research, offering a path toward foundation models for sensor data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we bridge the gap between wearable sensor technology and\npersonalized AI assistants by enabling Large Language Models (LLMs) to\nunderstand time-series tasks like human activity recognition (HAR). Despite the\nstrong reasoning and generalization capabilities of LLMs, leveraging them for\nsensor data tasks remains largely unexplored. This gap stems from challenges\nlike the lack of semantic context in time-series data, computational\nlimitations, and LLMs' difficulty processing numerical inputs. To address these\nissues, we introduce SensorLLM, a two-stage framework to unlock LLMs' potential\nfor sensor data tasks. In the Sensor-Language Alignment Stage, we introduce\nspecial tokens for each sensor channel and automatically generate\ntrend-descriptive text to align sensor data with textual inputs, enabling\nSensorLLM to capture numerical changes, channel-specific information, and\nsensor data of varying lengths-capabilities that existing LLMs typically\nstruggle with, all without the need for human annotations. Next, in Task-Aware\nTuning Stage, we refine the model for HAR classification using the frozen LLM\nand alignment module, achieving performance on par with or surpassing\nstate-of-the-art models. We further demonstrate that SensorLLM evolves into an\neffective sensor learner, reasoner, and classifier through Sensor-Language\nAlignment, enabling it to generalize across diverse datasets for HAR tasks. We\nstrongly believe our work lays the stepstone for future time-series and text\nalignment research, offering a path toward foundation models for sensor data."
                },
                "authors": [
                    {
                        "name": "Zechen Li"
                    },
                    {
                        "name": "Shohreh Deldari"
                    },
                    {
                        "name": "Linyao Chen"
                    },
                    {
                        "name": "Hao Xue"
                    },
                    {
                        "name": "Flora D. Salim"
                    }
                ],
                "author_detail": {
                    "name": "Flora D. Salim"
                },
                "author": "Flora D. Salim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10624v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10624v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08776v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08776v2",
                "updated": "2024-10-14T15:04:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    4,
                    51,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-11T12:49:05Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    49,
                    5,
                    4,
                    285,
                    0
                ],
                "title": "F2A: An Innovative Approach for Prompt Injection by Utilizing Feign\n  Security Detection Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "F2A: An Innovative Approach for Prompt Injection by Utilizing Feign\n  Security Detection Agents"
                },
                "summary": "With the rapid development of Large Language Models (LLMs), numerous mature\napplications of LLMs have emerged in the field of content safety detection.\nHowever, we have found that LLMs exhibit blind trust in safety detection\nagents. The general LLMs can be compromised by hackers with this vulnerability.\nHence, this paper proposed an attack named Feign Agent Attack (F2A).Through\nsuch malicious forgery methods, adding fake safety detection results into the\nprompt, the defense mechanism of LLMs can be bypassed, thereby obtaining\nharmful content and hijacking the normal conversation. Continually, a series of\nexperiments were conducted. In these experiments, the hijacking capability of\nF2A on LLMs was analyzed and demonstrated, exploring the fundamental reasons\nwhy LLMs blindly trust safety detection results. The experiments involved\nvarious scenarios where fake safety detection results were injected into\nprompts, and the responses were closely monitored to understand the extent of\nthe vulnerability. Also, this paper provided a reasonable solution to this\nattack, emphasizing that it is important for LLMs to critically evaluate the\nresults of augmented agents to prevent the generating harmful content. By doing\nso, the reliability and security can be significantly improved, protecting the\nLLMs from F2A.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of Large Language Models (LLMs), numerous mature\napplications of LLMs have emerged in the field of content safety detection.\nHowever, we have found that LLMs exhibit blind trust in safety detection\nagents. The general LLMs can be compromised by hackers with this vulnerability.\nHence, this paper proposed an attack named Feign Agent Attack (F2A).Through\nsuch malicious forgery methods, adding fake safety detection results into the\nprompt, the defense mechanism of LLMs can be bypassed, thereby obtaining\nharmful content and hijacking the normal conversation. Continually, a series of\nexperiments were conducted. In these experiments, the hijacking capability of\nF2A on LLMs was analyzed and demonstrated, exploring the fundamental reasons\nwhy LLMs blindly trust safety detection results. The experiments involved\nvarious scenarios where fake safety detection results were injected into\nprompts, and the responses were closely monitored to understand the extent of\nthe vulnerability. Also, this paper provided a reasonable solution to this\nattack, emphasizing that it is important for LLMs to critically evaluate the\nresults of augmented agents to prevent the generating harmful content. By doing\nso, the reliability and security can be significantly improved, protecting the\nLLMs from F2A."
                },
                "authors": [
                    {
                        "name": "Yupeng Ren"
                    }
                ],
                "author_detail": {
                    "name": "Yupeng Ren"
                },
                "author": "Yupeng Ren",
                "arxiv_comment": "1. Fixed typo in abstract 2. Provisionally completed the article\n  update to facilitate future version revisions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08776v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08776v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10594v1",
                "updated": "2024-10-14T15:04:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    4,
                    18,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T15:04:18Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    4,
                    18,
                    0,
                    288,
                    0
                ],
                "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality\n  Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality\n  Documents"
                },
                "summary": "Retrieval-augmented generation (RAG) is an effective technique that enables\nlarge language models (LLMs) to utilize external knowledge sources for\ngeneration. However, current RAG systems are solely based on text, rendering it\nimpossible to utilize vision information like layout and images that play\ncrucial roles in real-world multi-modality documents. In this paper, we\nintroduce VisRAG, which tackles this issue by establishing a vision-language\nmodel (VLM)-based RAG pipeline. In this pipeline, instead of first parsing the\ndocument to obtain text, the document is directly embedded using a VLM as an\nimage and then retrieved to enhance the generation of a VLM. Compared to\ntraditional text-based RAG, VisRAG maximizes the retention and utilization of\nthe data information in the original documents, eliminating the information\nloss introduced during the parsing process. We collect both open-source and\nsynthetic data to train the retriever in VisRAG and explore a variety of\ngeneration methods. Experiments demonstrate that VisRAG outperforms traditional\nRAG in both the retrieval and generation stages, achieving a 25--39\\%\nend-to-end performance gain over traditional text-based RAG pipeline. Further\nanalysis reveals that VisRAG is effective in utilizing training data and\ndemonstrates strong generalization capability, positioning it as a promising\nsolution for RAG on multi-modality documents. Our code and data are available\nat https://github.com/openbmb/visrag .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) is an effective technique that enables\nlarge language models (LLMs) to utilize external knowledge sources for\ngeneration. However, current RAG systems are solely based on text, rendering it\nimpossible to utilize vision information like layout and images that play\ncrucial roles in real-world multi-modality documents. In this paper, we\nintroduce VisRAG, which tackles this issue by establishing a vision-language\nmodel (VLM)-based RAG pipeline. In this pipeline, instead of first parsing the\ndocument to obtain text, the document is directly embedded using a VLM as an\nimage and then retrieved to enhance the generation of a VLM. Compared to\ntraditional text-based RAG, VisRAG maximizes the retention and utilization of\nthe data information in the original documents, eliminating the information\nloss introduced during the parsing process. We collect both open-source and\nsynthetic data to train the retriever in VisRAG and explore a variety of\ngeneration methods. Experiments demonstrate that VisRAG outperforms traditional\nRAG in both the retrieval and generation stages, achieving a 25--39\\%\nend-to-end performance gain over traditional text-based RAG pipeline. Further\nanalysis reveals that VisRAG is effective in utilizing training data and\ndemonstrates strong generalization capability, positioning it as a promising\nsolution for RAG on multi-modality documents. Our code and data are available\nat https://github.com/openbmb/visrag ."
                },
                "authors": [
                    {
                        "name": "Shi Yu"
                    },
                    {
                        "name": "Chaoyue Tang"
                    },
                    {
                        "name": "Bokai Xu"
                    },
                    {
                        "name": "Junbo Cui"
                    },
                    {
                        "name": "Junhao Ran"
                    },
                    {
                        "name": "Yukun Yan"
                    },
                    {
                        "name": "Zhenghao Liu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10584v1",
                "updated": "2024-10-14T14:56:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    56,
                    1,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T14:56:01Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    56,
                    1,
                    0,
                    288,
                    0
                ],
                "title": "STACKFEED: Structured Textual Actor-Critic Knowledge Base Editing with\n  FeedBack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STACKFEED: Structured Textual Actor-Critic Knowledge Base Editing with\n  FeedBack"
                },
                "summary": "Large Language Models (LLMs) often generate incorrect or outdated\ninformation, especially in low-resource settings or when dealing with private\ndata. To address this, Retrieval-Augmented Generation (RAG) uses external\nknowledge bases (KBs), but these can also suffer from inaccuracies. We\nintroduce STACKFEED, a novel Structured Textual Actor-Critic Knowledge base\nediting with FEEDback approach that iteratively refines the KB based on expert\nfeedback using a multi-actor, centralized critic reinforcement learning\nframework. Each document is assigned to an actor, modeled as a ReACT agent,\nwhich performs structured edits based on document-specific targeted\ninstructions from a centralized critic. Experimental results show that\nSTACKFEED significantly improves KB quality and RAG system performance,\nenhancing accuracy by up to 8% over baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often generate incorrect or outdated\ninformation, especially in low-resource settings or when dealing with private\ndata. To address this, Retrieval-Augmented Generation (RAG) uses external\nknowledge bases (KBs), but these can also suffer from inaccuracies. We\nintroduce STACKFEED, a novel Structured Textual Actor-Critic Knowledge base\nediting with FEEDback approach that iteratively refines the KB based on expert\nfeedback using a multi-actor, centralized critic reinforcement learning\nframework. Each document is assigned to an actor, modeled as a ReACT agent,\nwhich performs structured edits based on document-specific targeted\ninstructions from a centralized critic. Experimental results show that\nSTACKFEED significantly improves KB quality and RAG system performance,\nenhancing accuracy by up to 8% over baselines."
                },
                "authors": [
                    {
                        "name": "Naman Gupta"
                    },
                    {
                        "name": "Shashank Kirtania"
                    },
                    {
                        "name": "Priyanshu Gupta"
                    },
                    {
                        "name": "Krishna Kariya"
                    },
                    {
                        "name": "Sumit Gulwani"
                    },
                    {
                        "name": "Arun Iyer"
                    },
                    {
                        "name": "Suresh Parthasarathy"
                    },
                    {
                        "name": "Arjun Radhakrishna"
                    },
                    {
                        "name": "Sriram K. Rajamani"
                    },
                    {
                        "name": "Gustavo Soares"
                    }
                ],
                "author_detail": {
                    "name": "Gustavo Soares"
                },
                "author": "Gustavo Soares",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10577v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10577v1",
                "updated": "2024-10-14T14:51:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    51,
                    27,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T14:51:27Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    51,
                    27,
                    0,
                    288,
                    0
                ],
                "title": "Words to Wheels: Vision-Based Autonomous Driving Understanding Human\n  Language Instructions Using Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Words to Wheels: Vision-Based Autonomous Driving Understanding Human\n  Language Instructions Using Foundation Models"
                },
                "summary": "This paper introduces an innovative application of foundation models,\nenabling Unmanned Ground Vehicles (UGVs) equipped with an RGB-D camera to\nnavigate to designated destinations based on human language instructions.\nUnlike learning-based methods, this approach does not require prior training\nbut instead leverages existing foundation models, thus facilitating\ngeneralization to novel environments. Upon receiving human language\ninstructions, these are transformed into a 'cognitive route description' using\na large language model (LLM)-a detailed navigation route expressed in human\nlanguage. The vehicle then decomposes this description into landmarks and\nnavigation maneuvers. The vehicle also determines elevation costs and\nidentifies navigability levels of different regions through a terrain\nsegmentation model, GANav, trained on open datasets. Semantic elevation costs,\nwhich take both elevation and navigability levels into account, are estimated\nand provided to the Model Predictive Path Integral (MPPI) planner, responsible\nfor local path planning. Concurrently, the vehicle searches for target\nlandmarks using foundation models, including YOLO-World and EfficientViT-SAM.\nUltimately, the vehicle executes the navigation commands to reach the\ndesignated destination, the final landmark. Our experiments demonstrate that\nthis application successfully guides UGVs to their destinations following human\nlanguage instructions in novel environments, such as unfamiliar terrain or\nurban settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an innovative application of foundation models,\nenabling Unmanned Ground Vehicles (UGVs) equipped with an RGB-D camera to\nnavigate to designated destinations based on human language instructions.\nUnlike learning-based methods, this approach does not require prior training\nbut instead leverages existing foundation models, thus facilitating\ngeneralization to novel environments. Upon receiving human language\ninstructions, these are transformed into a 'cognitive route description' using\na large language model (LLM)-a detailed navigation route expressed in human\nlanguage. The vehicle then decomposes this description into landmarks and\nnavigation maneuvers. The vehicle also determines elevation costs and\nidentifies navigability levels of different regions through a terrain\nsegmentation model, GANav, trained on open datasets. Semantic elevation costs,\nwhich take both elevation and navigability levels into account, are estimated\nand provided to the Model Predictive Path Integral (MPPI) planner, responsible\nfor local path planning. Concurrently, the vehicle searches for target\nlandmarks using foundation models, including YOLO-World and EfficientViT-SAM.\nUltimately, the vehicle executes the navigation commands to reach the\ndesignated destination, the final landmark. Our experiments demonstrate that\nthis application successfully guides UGVs to their destinations following human\nlanguage instructions in novel environments, such as unfamiliar terrain or\nurban settings."
                },
                "authors": [
                    {
                        "name": "Chanhoe Ryu"
                    },
                    {
                        "name": "Hyunki Seong"
                    },
                    {
                        "name": "Daegyu Lee"
                    },
                    {
                        "name": "Seongwoo Moon"
                    },
                    {
                        "name": "Sungjae Min"
                    },
                    {
                        "name": "D. Hyunchul Shim"
                    }
                ],
                "author_detail": {
                    "name": "D. Hyunchul Shim"
                },
                "author": "D. Hyunchul Shim",
                "arxiv_comment": "7 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10577v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10577v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10570v1",
                "updated": "2024-10-14T14:47:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    47,
                    32,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T14:47:32Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    47,
                    32,
                    0,
                    288,
                    0
                ],
                "title": "Mindalogue: LLM -- Powered Nonlinear Interaction for Effective Learning\n  and Task Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mindalogue: LLM -- Powered Nonlinear Interaction for Effective Learning\n  and Task Exploration"
                },
                "summary": "Current generative AI models like ChatGPT, Claude, and Gemini are widely used\nfor knowledge dissemination, task decomposition, and creative thinking.\nHowever, their linear interaction methods often force users to repeatedly\ncompare and copy contextual information when handling complex tasks, increasing\ncognitive load and operational costs. Moreover, the ambiguity in model\nresponses requires users to refine and simplify the information further. To\naddress these issues, we developed \"Mindalogue\", a system using a non-linear\ninteraction model based on \"nodes + canvas\" to enhance user efficiency and\nfreedom while generating structured responses. A formative study with 11 users\ninformed the design of Mindalogue, which was then evaluated through a study\nwith 16 participants. The results showed that Mindalogue significantly reduced\ntask steps and improved users' comprehension of complex information. This study\nhighlights the potential of non-linear interaction in improving AI tool\nefficiency and user experience in the HCI field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current generative AI models like ChatGPT, Claude, and Gemini are widely used\nfor knowledge dissemination, task decomposition, and creative thinking.\nHowever, their linear interaction methods often force users to repeatedly\ncompare and copy contextual information when handling complex tasks, increasing\ncognitive load and operational costs. Moreover, the ambiguity in model\nresponses requires users to refine and simplify the information further. To\naddress these issues, we developed \"Mindalogue\", a system using a non-linear\ninteraction model based on \"nodes + canvas\" to enhance user efficiency and\nfreedom while generating structured responses. A formative study with 11 users\ninformed the design of Mindalogue, which was then evaluated through a study\nwith 16 participants. The results showed that Mindalogue significantly reduced\ntask steps and improved users' comprehension of complex information. This study\nhighlights the potential of non-linear interaction in improving AI tool\nefficiency and user experience in the HCI field."
                },
                "authors": [
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Ziyao Zhang"
                    },
                    {
                        "name": "Fengliang Zhu"
                    },
                    {
                        "name": "Jiajie Zhou"
                    },
                    {
                        "name": "Anyi Rao"
                    }
                ],
                "author_detail": {
                    "name": "Anyi Rao"
                },
                "author": "Anyi Rao",
                "arxiv_comment": "17 pages, 9 figures. Submitted to CHI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68U35(Primary), 68T20(Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10553v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10553v1",
                "updated": "2024-10-14T14:32:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    32,
                    55,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T14:32:55Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    32,
                    55,
                    0,
                    288,
                    0
                ],
                "title": "SLaNC: Static LayerNorm Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLaNC: Static LayerNorm Calibration"
                },
                "summary": "The ever increasing sizes of Large Language Models (LLMs) beyond hundreds of\nbillions of parameters have generated enormous pressure on the manufacturers of\ndedicated hardware accelerators and made the innovative design of the latter\none of the most rapidly expanding fields of the AI industry. Various approaches\nhave been explored to enable efficient and accurate processing of LLMs on the\navailable accelerators given their computational and storage limitations. Among\nthese, various quantization techniques have become the main focus of the\ncommunity as a means of reducing the compute, communication and storage\nrequirements. Quantization to lower precision formats naturally poses a number\nof challenges caused by the limited range of the available value\nrepresentations. When it comes to processing the popular Transformer models on\nhardware, one of the main issues becomes calculation of the LayerNorm simply\nbecause accumulation of the variance requires a much wider dynamic range than\nthe hardware enables. In this article, we address this matter and propose a\ncomputationally-efficient scaling technique that can be easily applied to\nTransformer models during inference. Our method suggests a straightforward way\nof scaling the LayerNorm inputs based on the static weights of the immediately\npreceding linear layers. The scaling factors are computed offline, based solely\non the linear layer weights, hence no latency or computational overhead is\nadded during inference. Most importantly, our technique ensures that no\nnumerical issues such as overflow or underflow could happen during the compute.\nThis approach offers smooth, accurate and resource-effective inference across a\nwide range of hardware architectures. The article provides theoretical\njustification as well as supporting numerical simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ever increasing sizes of Large Language Models (LLMs) beyond hundreds of\nbillions of parameters have generated enormous pressure on the manufacturers of\ndedicated hardware accelerators and made the innovative design of the latter\none of the most rapidly expanding fields of the AI industry. Various approaches\nhave been explored to enable efficient and accurate processing of LLMs on the\navailable accelerators given their computational and storage limitations. Among\nthese, various quantization techniques have become the main focus of the\ncommunity as a means of reducing the compute, communication and storage\nrequirements. Quantization to lower precision formats naturally poses a number\nof challenges caused by the limited range of the available value\nrepresentations. When it comes to processing the popular Transformer models on\nhardware, one of the main issues becomes calculation of the LayerNorm simply\nbecause accumulation of the variance requires a much wider dynamic range than\nthe hardware enables. In this article, we address this matter and propose a\ncomputationally-efficient scaling technique that can be easily applied to\nTransformer models during inference. Our method suggests a straightforward way\nof scaling the LayerNorm inputs based on the static weights of the immediately\npreceding linear layers. The scaling factors are computed offline, based solely\non the linear layer weights, hence no latency or computational overhead is\nadded during inference. Most importantly, our technique ensures that no\nnumerical issues such as overflow or underflow could happen during the compute.\nThis approach offers smooth, accurate and resource-effective inference across a\nwide range of hardware architectures. The article provides theoretical\njustification as well as supporting numerical simulations."
                },
                "authors": [
                    {
                        "name": "Mahsa Salmani"
                    },
                    {
                        "name": "Nikita Trukhanov"
                    },
                    {
                        "name": "Ilya Soloveychik"
                    }
                ],
                "author_detail": {
                    "name": "Ilya Soloveychik"
                },
                "author": "Ilya Soloveychik",
                "arxiv_comment": "9 pages, 3 figures, NeurIPS 2024 MLNCP Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10553v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10553v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05180v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05180v2",
                "updated": "2024-10-14T14:27:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    27,
                    34,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-07T16:40:21Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    40,
                    21,
                    0,
                    281,
                    0
                ],
                "title": "Mitigating the Risk of Health Inequity Exacerbated by Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating the Risk of Health Inequity Exacerbated by Large Language\n  Models"
                },
                "summary": "Recent advancements in large language models have demonstrated their\npotential in numerous medical applications, particularly in automating clinical\ntrial matching for translational research and enhancing medical question\nanswering for clinical decision support. However, our study shows that\nincorporating non decisive sociodemographic factors such as race, sex, income\nlevel, LGBT+ status, homelessness, illiteracy, disability, and unemployment\ninto the input of LLMs can lead to incorrect and harmful outputs for these\npopulations. These discrepancies risk exacerbating existing health disparities\nif LLMs are widely adopted in healthcare. To address this issue, we introduce\nEquityGuard, a novel framework designed to detect and mitigate the risk of\nhealth inequities in LLM based medical applications. Our evaluation\ndemonstrates its efficacy in promoting equitable outcomes across diverse\npopulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models have demonstrated their\npotential in numerous medical applications, particularly in automating clinical\ntrial matching for translational research and enhancing medical question\nanswering for clinical decision support. However, our study shows that\nincorporating non decisive sociodemographic factors such as race, sex, income\nlevel, LGBT+ status, homelessness, illiteracy, disability, and unemployment\ninto the input of LLMs can lead to incorrect and harmful outputs for these\npopulations. These discrepancies risk exacerbating existing health disparities\nif LLMs are widely adopted in healthcare. To address this issue, we introduce\nEquityGuard, a novel framework designed to detect and mitigate the risk of\nhealth inequities in LLM based medical applications. Our evaluation\ndemonstrates its efficacy in promoting equitable outcomes across diverse\npopulations."
                },
                "authors": [
                    {
                        "name": "Yuelyu Ji"
                    },
                    {
                        "name": "Wenhe Ma"
                    },
                    {
                        "name": "Sonish Sivarajkumar"
                    },
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Eugene Mathew Sadhu"
                    },
                    {
                        "name": "Zhuochun Li"
                    },
                    {
                        "name": "Xizhi Wu"
                    },
                    {
                        "name": "Shyam Visweswaran"
                    },
                    {
                        "name": "Yanshan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yanshan Wang"
                },
                "author": "Yanshan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05180v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05180v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04556v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04556v2",
                "updated": "2024-10-14T14:27:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    27,
                    4,
                    0,
                    288,
                    0
                ],
                "published": "2024-08-08T16:13:26Z",
                "published_parsed": [
                    2024,
                    8,
                    8,
                    16,
                    13,
                    26,
                    3,
                    221,
                    0
                ],
                "title": "BA-LoRA: Bias-Alleviating Low-Rank Adaptation to Mitigate Catastrophic\n  Inheritance in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BA-LoRA: Bias-Alleviating Low-Rank Adaptation to Mitigate Catastrophic\n  Inheritance in Large Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable proficiency across\nvarious natural language processing (NLP) tasks. However, adapting LLMs to\ndownstream applications requires computationally intensive and memory-demanding\nfine-tuning procedures. To alleviate these burdens, parameter-efficient\nfine-tuning (PEFT) techniques have emerged as a promising approach to tailor\nLLMs with minimal computational overhead. While PEFT methods offer substantial\nadvantages, they do not fully address the pervasive issue of bias propagation\nfrom pre-training data. This work introduces Bias-Alleviating Low-Rank\nAdaptation (BA-LoRA), a novel PEFT method designed to counteract bias\ninheritance. BA-LoRA incorporates three distinct regularization terms: (1) a\nconsistency regularizer, (2) a diversity regularizer, and (3) a singular value\ndecomposition regularizer. These regularizers aim to enhance the models'\nconsistency, diversity, and generalization capabilities during fine-tuning. We\nconduct extensive experiments on natural language understanding (NLU) and\nnatural language generation (NLG) tasks using prominent LLMs such as LLaMA,\nMistral, and Gemma. The results demonstrate that BA-LoRA outperforms LoRA and\nits state-of-the-art variants. Moreover, our method effectively mitigates the\nadverse effects of pre-training bias, leading to more reliable and robust model\noutputs. The code is available at https://github.com/cyp-jlu-ai/BA-LoRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable proficiency across\nvarious natural language processing (NLP) tasks. However, adapting LLMs to\ndownstream applications requires computationally intensive and memory-demanding\nfine-tuning procedures. To alleviate these burdens, parameter-efficient\nfine-tuning (PEFT) techniques have emerged as a promising approach to tailor\nLLMs with minimal computational overhead. While PEFT methods offer substantial\nadvantages, they do not fully address the pervasive issue of bias propagation\nfrom pre-training data. This work introduces Bias-Alleviating Low-Rank\nAdaptation (BA-LoRA), a novel PEFT method designed to counteract bias\ninheritance. BA-LoRA incorporates three distinct regularization terms: (1) a\nconsistency regularizer, (2) a diversity regularizer, and (3) a singular value\ndecomposition regularizer. These regularizers aim to enhance the models'\nconsistency, diversity, and generalization capabilities during fine-tuning. We\nconduct extensive experiments on natural language understanding (NLU) and\nnatural language generation (NLG) tasks using prominent LLMs such as LLaMA,\nMistral, and Gemma. The results demonstrate that BA-LoRA outperforms LoRA and\nits state-of-the-art variants. Moreover, our method effectively mitigates the\nadverse effects of pre-training bias, leading to more reliable and robust model\noutputs. The code is available at https://github.com/cyp-jlu-ai/BA-LoRA."
                },
                "authors": [
                    {
                        "name": "Yupeng Chang"
                    },
                    {
                        "name": "Yi Chang"
                    },
                    {
                        "name": "Yuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Wu"
                },
                "author": "Yuan Wu",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04556v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04556v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05152v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05152v2",
                "updated": "2024-10-14T14:24:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    24,
                    8,
                    0,
                    288,
                    0
                ],
                "published": "2024-03-08T08:41:14Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    8,
                    41,
                    14,
                    4,
                    68,
                    0
                ],
                "title": "Towards a Psychology of Machines: Large Language Models Predict Human\n  Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a Psychology of Machines: Large Language Models Predict Human\n  Memory"
                },
                "summary": "Large language models (LLMs) are excelling across various tasks despite not\nbeing based on human cognition, prompting an investigation into their potential\nto offer insights into human cognitive mechanisms. This study examines\nChatGPT's ability to predict human performance in a language-based memory task.\nFollowing theories of text comprehension, we hypothesized that recognizing\nambiguous sentences is easier with relevant preceding context. Participants,\nincluding humans and ChatGPT, were given pairs of sentences: the second always\na garden-path sentence, and the first providing either fitting or unfitting\ncontext. We measured their ratings of sentence relatedness and memorability.\nResults showed a strong alignment between ChatGPT's assessments and human\nmemory performance. Sentences in the fitting context were rated as being more\nrelated and memorable by ChatGPT and were better remembered by humans,\nhighlighting LLMs' potential to predict human performance and contribute to\npsychological theories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are excelling across various tasks despite not\nbeing based on human cognition, prompting an investigation into their potential\nto offer insights into human cognitive mechanisms. This study examines\nChatGPT's ability to predict human performance in a language-based memory task.\nFollowing theories of text comprehension, we hypothesized that recognizing\nambiguous sentences is easier with relevant preceding context. Participants,\nincluding humans and ChatGPT, were given pairs of sentences: the second always\na garden-path sentence, and the first providing either fitting or unfitting\ncontext. We measured their ratings of sentence relatedness and memorability.\nResults showed a strong alignment between ChatGPT's assessments and human\nmemory performance. Sentences in the fitting context were rated as being more\nrelated and memorable by ChatGPT and were better remembered by humans,\nhighlighting LLMs' potential to predict human performance and contribute to\npsychological theories."
                },
                "authors": [
                    {
                        "name": "Markus Huff"
                    },
                    {
                        "name": "Elanur Ulak"
                    }
                ],
                "author_detail": {
                    "name": "Elanur Ulak"
                },
                "author": "Elanur Ulak",
                "arxiv_comment": "33 pages, 3 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05152v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05152v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08820v2",
                "updated": "2024-10-14T14:22:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    22,
                    40,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-11T14:02:42Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    2,
                    42,
                    4,
                    285,
                    0
                ],
                "title": "Which Demographics do LLMs Default to During Annotation?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Which Demographics do LLMs Default to During Annotation?"
                },
                "summary": "Demographics and cultural background of annotators influence the labels they\nassign in text annotation -- for instance, an elderly woman might find it\noffensive to read a message addressed to a \"bro\", but a male teenager might\nfind it appropriate. It is therefore important to acknowledge label variations\nto not under-represent members of a society. Two research directions developed\nout of this observation in the context of using large language models (LLM) for\ndata annotations, namely (1) studying biases and inherent knowledge of LLMs and\n(2) injecting diversity in the output by manipulating the prompt with\ndemographic information. We combine these two strands of research and ask the\nquestion to which demographics an LLM resorts to when no demographics is given.\nTo answer this question, we evaluate which attributes of human annotators LLMs\ninherently mimic. Furthermore, we compare non-demographic conditioned prompts\nand placebo-conditioned prompts (e.g., \"you are an annotator who lives in house\nnumber 5\") to demographics-conditioned prompts (\"You are a 45 year old man and\nan expert on politeness annotation. How do you rate {instance}\"). We study\nthese questions for politeness and offensiveness annotations on the POPQUORN\ndata set, a corpus created in a controlled manner to investigate human label\nvariations based on demographics which has not been used for LLM-based analyses\nso far. We observe notable influences related to gender, race, and age in\ndemographic prompting, which contrasts with previous studies that found no such\neffects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demographics and cultural background of annotators influence the labels they\nassign in text annotation -- for instance, an elderly woman might find it\noffensive to read a message addressed to a \"bro\", but a male teenager might\nfind it appropriate. It is therefore important to acknowledge label variations\nto not under-represent members of a society. Two research directions developed\nout of this observation in the context of using large language models (LLM) for\ndata annotations, namely (1) studying biases and inherent knowledge of LLMs and\n(2) injecting diversity in the output by manipulating the prompt with\ndemographic information. We combine these two strands of research and ask the\nquestion to which demographics an LLM resorts to when no demographics is given.\nTo answer this question, we evaluate which attributes of human annotators LLMs\ninherently mimic. Furthermore, we compare non-demographic conditioned prompts\nand placebo-conditioned prompts (e.g., \"you are an annotator who lives in house\nnumber 5\") to demographics-conditioned prompts (\"You are a 45 year old man and\nan expert on politeness annotation. How do you rate {instance}\"). We study\nthese questions for politeness and offensiveness annotations on the POPQUORN\ndata set, a corpus created in a controlled manner to investigate human label\nvariations based on demographics which has not been used for LLM-based analyses\nso far. We observe notable influences related to gender, race, and age in\ndemographic prompting, which contrasts with previous studies that found no such\neffects."
                },
                "authors": [
                    {
                        "name": "Johannes Schfer"
                    },
                    {
                        "name": "Aidan Combs"
                    },
                    {
                        "name": "Christopher Bagdon"
                    },
                    {
                        "name": "Jiahui Li"
                    },
                    {
                        "name": "Nadine Probol"
                    },
                    {
                        "name": "Lynn Greschner"
                    },
                    {
                        "name": "Sean Papay"
                    },
                    {
                        "name": "Yarik Menchaca Resendiz"
                    },
                    {
                        "name": "Aswathy Velutharambath"
                    },
                    {
                        "name": "Amelie Whrl"
                    },
                    {
                        "name": "Sabine Weber"
                    },
                    {
                        "name": "Roman Klinger"
                    }
                ],
                "author_detail": {
                    "name": "Roman Klinger"
                },
                "author": "Roman Klinger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08102v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08102v2",
                "updated": "2024-10-14T14:22:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    22,
                    30,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-10T16:45:28Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    45,
                    28,
                    3,
                    284,
                    0
                ],
                "title": "Multi-Agent Collaborative Data Selection for Efficient LLM Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Collaborative Data Selection for Efficient LLM Pretraining"
                },
                "summary": "Efficient data selection is crucial to accelerate the pretraining of large\nlanguage models (LLMs). While various methods have been proposed to enhance\ndata efficiency, limited research has addressed the inherent conflicts between\nthese approaches to achieve optimal data selection for LLM pretraining. To\ntackle this problem, we propose a novel multi-agent collaborative data\nselection mechanism. In this framework, each data selection method serves as an\nindependent agent, and an agent console is designed to dynamically integrate\nthe information from all agents throughout the LLM training process. We conduct\nextensive empirical studies to evaluate our multi-agent framework. The\nexperimental results demonstrate that our approach significantly improves data\nefficiency, accelerates convergence in LLM training, and achieves an average\nperformance gain up to 10.5% across multiple language model benchmarks compared\nto the state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient data selection is crucial to accelerate the pretraining of large\nlanguage models (LLMs). While various methods have been proposed to enhance\ndata efficiency, limited research has addressed the inherent conflicts between\nthese approaches to achieve optimal data selection for LLM pretraining. To\ntackle this problem, we propose a novel multi-agent collaborative data\nselection mechanism. In this framework, each data selection method serves as an\nindependent agent, and an agent console is designed to dynamically integrate\nthe information from all agents throughout the LLM training process. We conduct\nextensive empirical studies to evaluate our multi-agent framework. The\nexperimental results demonstrate that our approach significantly improves data\nefficiency, accelerates convergence in LLM training, and achieves an average\nperformance gain up to 10.5% across multiple language model benchmarks compared\nto the state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Tianyi Bai"
                    },
                    {
                        "name": "Ling Yang"
                    },
                    {
                        "name": "Zhen Hao Wong"
                    },
                    {
                        "name": "Jiahui Peng"
                    },
                    {
                        "name": "Xinlin Zhuang"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Lijun Wu"
                    },
                    {
                        "name": "Jiantao Qiu"
                    },
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Conghui He"
                    }
                ],
                "author_detail": {
                    "name": "Conghui He"
                },
                "author": "Conghui He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08102v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08102v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10542v1",
                "updated": "2024-10-14T14:22:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    22,
                    12,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T14:22:12Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    22,
                    12,
                    0,
                    288,
                    0
                ],
                "title": "Rethinking Legal Judgement Prediction in a Realistic Scenario in the Era\n  of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Legal Judgement Prediction in a Realistic Scenario in the Era\n  of Large Language Models"
                },
                "summary": "This study investigates judgment prediction in a realistic scenario within\nthe context of Indian judgments, utilizing a range of transformer-based models,\nincluding InLegalBERT, BERT, and XLNet, alongside LLMs such as Llama-2 and\nGPT-3.5 Turbo. In this realistic scenario, we simulate how judgments are\npredicted at the point when a case is presented for a decision in court, using\nonly the information available at that time, such as the facts of the case,\nstatutes, precedents, and arguments. This approach mimics real-world\nconditions, where decisions must be made without the benefit of hindsight,\nunlike retrospective analyses often found in previous studies. For transformer\nmodels, we experiment with hierarchical transformers and the summarization of\njudgment facts to optimize input for these models. Our experiments with LLMs\nreveal that GPT-3.5 Turbo excels in realistic scenarios, demonstrating robust\nperformance in judgment prediction. Furthermore, incorporating additional legal\ninformation, such as statutes and precedents, significantly improves the\noutcome of the prediction task. The LLMs also provide explanations for their\npredictions. To evaluate the quality of these predictions and explanations, we\nintroduce two human evaluation metrics: Clarity and Linking. Our findings from\nboth automatic and human evaluations indicate that, despite advancements in\nLLMs, they are yet to achieve expert-level performance in judgment prediction\nand explanation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates judgment prediction in a realistic scenario within\nthe context of Indian judgments, utilizing a range of transformer-based models,\nincluding InLegalBERT, BERT, and XLNet, alongside LLMs such as Llama-2 and\nGPT-3.5 Turbo. In this realistic scenario, we simulate how judgments are\npredicted at the point when a case is presented for a decision in court, using\nonly the information available at that time, such as the facts of the case,\nstatutes, precedents, and arguments. This approach mimics real-world\nconditions, where decisions must be made without the benefit of hindsight,\nunlike retrospective analyses often found in previous studies. For transformer\nmodels, we experiment with hierarchical transformers and the summarization of\njudgment facts to optimize input for these models. Our experiments with LLMs\nreveal that GPT-3.5 Turbo excels in realistic scenarios, demonstrating robust\nperformance in judgment prediction. Furthermore, incorporating additional legal\ninformation, such as statutes and precedents, significantly improves the\noutcome of the prediction task. The LLMs also provide explanations for their\npredictions. To evaluate the quality of these predictions and explanations, we\nintroduce two human evaluation metrics: Clarity and Linking. Our findings from\nboth automatic and human evaluations indicate that, despite advancements in\nLLMs, they are yet to achieve expert-level performance in judgment prediction\nand explanation tasks."
                },
                "authors": [
                    {
                        "name": "Shubham Kumar Nigam"
                    },
                    {
                        "name": "Aniket Deroy"
                    },
                    {
                        "name": "Subhankar Maity"
                    },
                    {
                        "name": "Arnab Bhattacharya"
                    }
                ],
                "author_detail": {
                    "name": "Arnab Bhattacharya"
                },
                "author": "Arnab Bhattacharya",
                "arxiv_comment": "Accepted on NLLP at EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10537v1",
                "updated": "2024-10-14T14:17:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    17,
                    52,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T14:17:52Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    17,
                    52,
                    0,
                    288,
                    0
                ],
                "title": "Reproducible Machine Learning-based Voice Pathology Detection:\n  Introducing the Pitch Difference Feature",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reproducible Machine Learning-based Voice Pathology Detection:\n  Introducing the Pitch Difference Feature"
                },
                "summary": "In this study, we propose a robust set of features derived from a thorough\nresearch of contemporary practices in voice pathology detection. The feature\nset is based on the combination of acoustic handcrafted features. Additionally,\nwe introduce pitch difference as a novel feature. We combine this feature set,\ncontaining data from the publicly available Saarbr\\\"ucken Voice Database (SVD),\nwith preprocessing using the K-Means Synthetic Minority Over-Sampling Technique\nalgorithm to address class imbalance.\n  Moreover, we applied multiple ML models as binary classifiers. We utilized\nsupport vector machine, k-nearest neighbors, naive Bayes, decision tree, random\nforest and AdaBoost classifiers. To determine the best classification approach,\nwe performed grid search on feasible hyperparameters of respective classifiers\nand subsections of features.\n  Our approach has achieved the state-of-the-art performance, measured by\nunweighted average recall in voice pathology detection on SVD database. We\nintentionally omit accuracy as it is highly biased metric in case of unbalanced\ndata compared to aforementioned metrics. The results are further enhanced by\neliminating the potential overestimation of the results with repeated\nstratified cross-validation. This advancement demonstrates significant\npotential for the clinical deployment of ML methods, offering a valuable tool\nfor an objective examination of voice pathologies. To support our claims, we\nprovide a publicly available GitHub repository with DOI\n10.5281/zenodo.13771573. Finally, we provide REFORMS checklist.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we propose a robust set of features derived from a thorough\nresearch of contemporary practices in voice pathology detection. The feature\nset is based on the combination of acoustic handcrafted features. Additionally,\nwe introduce pitch difference as a novel feature. We combine this feature set,\ncontaining data from the publicly available Saarbr\\\"ucken Voice Database (SVD),\nwith preprocessing using the K-Means Synthetic Minority Over-Sampling Technique\nalgorithm to address class imbalance.\n  Moreover, we applied multiple ML models as binary classifiers. We utilized\nsupport vector machine, k-nearest neighbors, naive Bayes, decision tree, random\nforest and AdaBoost classifiers. To determine the best classification approach,\nwe performed grid search on feasible hyperparameters of respective classifiers\nand subsections of features.\n  Our approach has achieved the state-of-the-art performance, measured by\nunweighted average recall in voice pathology detection on SVD database. We\nintentionally omit accuracy as it is highly biased metric in case of unbalanced\ndata compared to aforementioned metrics. The results are further enhanced by\neliminating the potential overestimation of the results with repeated\nstratified cross-validation. This advancement demonstrates significant\npotential for the clinical deployment of ML methods, offering a valuable tool\nfor an objective examination of voice pathologies. To support our claims, we\nprovide a publicly available GitHub repository with DOI\n10.5281/zenodo.13771573. Finally, we provide REFORMS checklist."
                },
                "authors": [
                    {
                        "name": "Jan Vrba"
                    },
                    {
                        "name": "Jakub Steinbach"
                    },
                    {
                        "name": "Tom Jirsa"
                    },
                    {
                        "name": "Laura Verde"
                    },
                    {
                        "name": "Roberta De Fazio"
                    },
                    {
                        "name": "Noriyasu Homma"
                    },
                    {
                        "name": "Yuwen Zeng"
                    },
                    {
                        "name": "Key Ichiji"
                    },
                    {
                        "name": "Luk Hjek"
                    },
                    {
                        "name": "Zuzana Sedlkov"
                    },
                    {
                        "name": "Jan Mare"
                    }
                ],
                "author_detail": {
                    "name": "Jan Mare"
                },
                "author": "Jan Mare",
                "arxiv_comment": "33 pages, 8 figures, code repository:\n  https://github.com/aailab-uct/Automated-Robust-and-Reproducible-Voice-Pathology-Detection",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04449v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04449v3",
                "updated": "2024-10-15T07:45:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    7,
                    45,
                    31,
                    1,
                    289,
                    0
                ],
                "published": "2024-08-08T13:19:37Z",
                "published_parsed": [
                    2024,
                    8,
                    8,
                    13,
                    19,
                    37,
                    3,
                    221,
                    0
                ],
                "title": "EAIRiskBench: Towards Evaluating Physical Risk Awareness for Task\n  Planning of Foundation Model-based Embodied AI Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EAIRiskBench: Towards Evaluating Physical Risk Awareness for Task\n  Planning of Foundation Model-based Embodied AI Agents"
                },
                "summary": "Embodied artificial intelligence (EAI) integrates advanced AI models into\nphysical entities for real-world interaction. The emergence of foundation\nmodels as the \"brain\" of EAI agents for high-level task planning has shown\npromising results. However, the deployment of these agents in physical\nenvironments presents significant safety challenges. For instance, a\nhousekeeping robot lacking sufficient risk awareness might place a metal\ncontainer in a microwave, potentially causing a fire. To address these critical\nsafety concerns, comprehensive pre-deployment risk assessments are imperative.\nThis study introduces EAIRiskBench, a novel framework for automated physical\nrisk assessment in EAI scenarios. EAIRiskBench employs a multi-agent\ncooperative system that leverages various foundation models to generate safety\nguidelines, create risk-prone scenarios, make task planning, and evaluate\nsafety systematically. Utilizing this framework, we construct EAIRiskDataset,\ncomprising diverse test cases across various domains, encompassing both textual\nand visual scenarios. Our comprehensive evaluation of state-of-the-art\nfoundation models reveals alarming results: all models exhibit high task risk\nrates (TRR), with an average of 95.75% across all evaluated models. To address\nthese challenges, we further propose two prompting-based risk mitigation\nstrategies. While these strategies demonstrate some efficacy in reducing TRR,\nthe improvements are limited, still indicating substantial safety concerns.\nThis study provides the first large-scale assessment of physical risk awareness\nin EAI agents. Our findings underscore the critical need for enhanced safety\nmeasures in EAI systems and provide valuable insights for future research\ndirections in developing safer embodied artificial intelligence system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied artificial intelligence (EAI) integrates advanced AI models into\nphysical entities for real-world interaction. The emergence of foundation\nmodels as the \"brain\" of EAI agents for high-level task planning has shown\npromising results. However, the deployment of these agents in physical\nenvironments presents significant safety challenges. For instance, a\nhousekeeping robot lacking sufficient risk awareness might place a metal\ncontainer in a microwave, potentially causing a fire. To address these critical\nsafety concerns, comprehensive pre-deployment risk assessments are imperative.\nThis study introduces EAIRiskBench, a novel framework for automated physical\nrisk assessment in EAI scenarios. EAIRiskBench employs a multi-agent\ncooperative system that leverages various foundation models to generate safety\nguidelines, create risk-prone scenarios, make task planning, and evaluate\nsafety systematically. Utilizing this framework, we construct EAIRiskDataset,\ncomprising diverse test cases across various domains, encompassing both textual\nand visual scenarios. Our comprehensive evaluation of state-of-the-art\nfoundation models reveals alarming results: all models exhibit high task risk\nrates (TRR), with an average of 95.75% across all evaluated models. To address\nthese challenges, we further propose two prompting-based risk mitigation\nstrategies. While these strategies demonstrate some efficacy in reducing TRR,\nthe improvements are limited, still indicating substantial safety concerns.\nThis study provides the first large-scale assessment of physical risk awareness\nin EAI agents. Our findings underscore the critical need for enhanced safety\nmeasures in EAI systems and provide valuable insights for future research\ndirections in developing safer embodied artificial intelligence system."
                },
                "authors": [
                    {
                        "name": "Zihao Zhu"
                    },
                    {
                        "name": "Bingzhe Wu"
                    },
                    {
                        "name": "Zhengyou Zhang"
                    },
                    {
                        "name": "Lei Han"
                    },
                    {
                        "name": "Baoyuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Baoyuan Wu"
                },
                "author": "Baoyuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04449v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04449v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10526v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10526v1",
                "updated": "2024-10-14T14:06:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    6,
                    5,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T14:06:05Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    6,
                    5,
                    0,
                    288,
                    0
                ],
                "title": "Generalized Adversarial Code-Suggestions: Exploiting Contexts of\n  LLM-based Code-Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalized Adversarial Code-Suggestions: Exploiting Contexts of\n  LLM-based Code-Completion"
                },
                "summary": "While convenient, relying on LLM-powered code assistants in day-to-day work\ngives rise to severe attacks. For instance, the assistant might introduce\nsubtle flaws and suggest vulnerable code to the user. These adversarial\ncode-suggestions can be introduced via data poisoning and, thus, unknowingly by\nthe model creators. In this paper, we provide a generalized formulation of such\nattacks, spawning and extending related work in this domain. This formulation\nis defined over two components: First, a trigger pattern occurring in the\nprompts of a specific user group, and, second, a learnable map in embedding\nspace from the prompt to an adversarial bait. The latter gives rise to novel\nand more flexible targeted attack-strategies, allowing the adversary to choose\nthe most suitable trigger pattern for a specific user-group arbitrarily,\nwithout restrictions on the pattern's tokens. Our directional-map attacks and\nprompt-indexing attacks increase the stealthiness decisively. We extensively\nevaluate the effectiveness of these attacks and carefully investigate defensive\nmechanisms to explore the limits of generalized adversarial code-suggestions.\nWe find that most defenses unfortunately offer little protection only.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While convenient, relying on LLM-powered code assistants in day-to-day work\ngives rise to severe attacks. For instance, the assistant might introduce\nsubtle flaws and suggest vulnerable code to the user. These adversarial\ncode-suggestions can be introduced via data poisoning and, thus, unknowingly by\nthe model creators. In this paper, we provide a generalized formulation of such\nattacks, spawning and extending related work in this domain. This formulation\nis defined over two components: First, a trigger pattern occurring in the\nprompts of a specific user group, and, second, a learnable map in embedding\nspace from the prompt to an adversarial bait. The latter gives rise to novel\nand more flexible targeted attack-strategies, allowing the adversary to choose\nthe most suitable trigger pattern for a specific user-group arbitrarily,\nwithout restrictions on the pattern's tokens. Our directional-map attacks and\nprompt-indexing attacks increase the stealthiness decisively. We extensively\nevaluate the effectiveness of these attacks and carefully investigate defensive\nmechanisms to explore the limits of generalized adversarial code-suggestions.\nWe find that most defenses unfortunately offer little protection only."
                },
                "authors": [
                    {
                        "name": "Karl Rubel"
                    },
                    {
                        "name": "Maximilian Noppel"
                    },
                    {
                        "name": "Christian Wressnegger"
                    }
                ],
                "author_detail": {
                    "name": "Christian Wressnegger"
                },
                "author": "Christian Wressnegger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10526v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10526v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10519v1",
                "updated": "2024-10-14T13:59:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    59,
                    30,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T13:59:30Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    59,
                    30,
                    0,
                    288,
                    0
                ],
                "title": "AI-based particle track identification in scintillating fibres read out\n  with imaging sensors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-based particle track identification in scintillating fibres read out\n  with imaging sensors"
                },
                "summary": "This paper presents the development and application of an AI-based method for\nparticle track identification using scintillating fibres read out with imaging\nsensors. We propose a variational autoencoder (VAE) to efficiently filter and\nidentify frames containing signal from the substantial data generated by SPAD\narray sensors. Our VAE model, trained on purely background frames, demonstrated\na high capability to distinguish frames containing particle tracks from\nbackground noise. The performance of the VAE-based anomaly detection was\nvalidated with experimental data, demonstrating the method's ability to\nefficiently identify relevant events with rapid processing time, suggesting a\nsolid prospect for deployment as a fast inference tool on hardware for\nreal-time anomaly detection. This work highlights the potential of combining\nadvanced sensor technology with machine learning techniques to enhance particle\ndetection and tracking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the development and application of an AI-based method for\nparticle track identification using scintillating fibres read out with imaging\nsensors. We propose a variational autoencoder (VAE) to efficiently filter and\nidentify frames containing signal from the substantial data generated by SPAD\narray sensors. Our VAE model, trained on purely background frames, demonstrated\na high capability to distinguish frames containing particle tracks from\nbackground noise. The performance of the VAE-based anomaly detection was\nvalidated with experimental data, demonstrating the method's ability to\nefficiently identify relevant events with rapid processing time, suggesting a\nsolid prospect for deployment as a fast inference tool on hardware for\nreal-time anomaly detection. This work highlights the potential of combining\nadvanced sensor technology with machine learning techniques to enhance particle\ndetection and tracking."
                },
                "authors": [
                    {
                        "name": "Noemi Bhrer"
                    },
                    {
                        "name": "Sal Alonso-Monsalve"
                    },
                    {
                        "name": "Matthew Franks"
                    },
                    {
                        "name": "Till Dieminger"
                    },
                    {
                        "name": "Davide Sgalaberna"
                    }
                ],
                "author_detail": {
                    "name": "Davide Sgalaberna"
                },
                "author": "Davide Sgalaberna",
                "arxiv_comment": "19 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02442v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02442v3",
                "updated": "2024-10-14T13:57:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    57,
                    29,
                    0,
                    288,
                    0
                ],
                "published": "2024-08-05T13:08:24Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    13,
                    8,
                    24,
                    0,
                    218,
                    0
                ],
                "title": "Let Me Speak Freely? A Study on the Impact of Format Restrictions on\n  Performance of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let Me Speak Freely? A Study on the Impact of Format Restrictions on\n  Performance of Large Language Models"
                },
                "summary": "Structured generation, the process of producing content in standardized\nformats like JSON and XML, is widely utilized in real-world applications to\nextract key output information from large language models (LLMs). This study\ninvestigates whether such constraints on generation space impact LLMs\nabilities, including reasoning and domain knowledge comprehension.\nSpecifically, we evaluate LLMs performance when restricted to adhere to\nstructured formats versus generating free-form responses across various common\ntasks. Surprisingly, we observe a significant decline in LLMs reasoning\nabilities under format restrictions. Furthermore, we find that stricter format\nconstraints generally lead to greater performance degradation in reasoning\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured generation, the process of producing content in standardized\nformats like JSON and XML, is widely utilized in real-world applications to\nextract key output information from large language models (LLMs). This study\ninvestigates whether such constraints on generation space impact LLMs\nabilities, including reasoning and domain knowledge comprehension.\nSpecifically, we evaluate LLMs performance when restricted to adhere to\nstructured formats versus generating free-form responses across various common\ntasks. Surprisingly, we observe a significant decline in LLMs reasoning\nabilities under format restrictions. Furthermore, we find that stricter format\nconstraints generally lead to greater performance degradation in reasoning\ntasks."
                },
                "authors": [
                    {
                        "name": "Zhi Rui Tam"
                    },
                    {
                        "name": "Cheng-Kuang Wu"
                    },
                    {
                        "name": "Yi-Lin Tsai"
                    },
                    {
                        "name": "Chieh-Yen Lin"
                    },
                    {
                        "name": "Hung-yi Lee"
                    },
                    {
                        "name": "Yun-Nung Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yun-Nung Chen"
                },
                "author": "Yun-Nung Chen",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02442v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02442v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01145v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01145v3",
                "updated": "2024-10-14T13:50:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    50,
                    46,
                    0,
                    288,
                    0
                ],
                "published": "2024-02-02T05:04:51Z",
                "published_parsed": [
                    2024,
                    2,
                    2,
                    5,
                    4,
                    51,
                    4,
                    33,
                    0
                ],
                "title": "ReEvo: Large Language Models as Hyper-Heuristics with Reflective\n  Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReEvo: Large Language Models as Hyper-Heuristics with Reflective\n  Evolution"
                },
                "summary": "The omnipresence of NP-hard combinatorial optimization problems (COPs)\ncompels domain experts to engage in trial-and-error heuristic design. The\nlong-standing endeavor of design automation has gained new momentum with the\nrise of large language models (LLMs). This paper introduces Language\nHyper-Heuristics (LHHs), an emerging variant of Hyper-Heuristics that leverages\nLLMs for heuristic generation, featuring minimal manual intervention and\nopen-ended heuristic spaces. To empower LHHs, we present Reflective Evolution\n(ReEvo), a novel integration of evolutionary search for efficiently exploring\nthe heuristic space, and LLM reflections to provide verbal gradients within the\nspace. Across five heterogeneous algorithmic types, six different COPs, and\nboth white-box and black-box views of COPs, ReEvo yields state-of-the-art and\ncompetitive meta-heuristics, evolutionary algorithms, heuristics, and neural\nsolvers, while being more sample-efficient than prior LHHs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The omnipresence of NP-hard combinatorial optimization problems (COPs)\ncompels domain experts to engage in trial-and-error heuristic design. The\nlong-standing endeavor of design automation has gained new momentum with the\nrise of large language models (LLMs). This paper introduces Language\nHyper-Heuristics (LHHs), an emerging variant of Hyper-Heuristics that leverages\nLLMs for heuristic generation, featuring minimal manual intervention and\nopen-ended heuristic spaces. To empower LHHs, we present Reflective Evolution\n(ReEvo), a novel integration of evolutionary search for efficiently exploring\nthe heuristic space, and LLM reflections to provide verbal gradients within the\nspace. Across five heterogeneous algorithmic types, six different COPs, and\nboth white-box and black-box views of COPs, ReEvo yields state-of-the-art and\ncompetitive meta-heuristics, evolutionary algorithms, heuristics, and neural\nsolvers, while being more sample-efficient than prior LHHs."
                },
                "authors": [
                    {
                        "name": "Haoran Ye"
                    },
                    {
                        "name": "Jiarui Wang"
                    },
                    {
                        "name": "Zhiguang Cao"
                    },
                    {
                        "name": "Federico Berto"
                    },
                    {
                        "name": "Chuanbo Hua"
                    },
                    {
                        "name": "Haeyeon Kim"
                    },
                    {
                        "name": "Jinkyoo Park"
                    },
                    {
                        "name": "Guojie Song"
                    }
                ],
                "author_detail": {
                    "name": "Guojie Song"
                },
                "author": "Guojie Song",
                "arxiv_comment": "Accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.01145v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01145v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00724v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00724v2",
                "updated": "2024-10-14T13:41:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    41,
                    35,
                    0,
                    288,
                    0
                ],
                "published": "2024-08-01T17:16:04Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    17,
                    16,
                    4,
                    3,
                    214,
                    0
                ],
                "title": "Inference Scaling Laws: An Empirical Analysis of Compute-Optimal\n  Inference for Problem-Solving with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference Scaling Laws: An Empirical Analysis of Compute-Optimal\n  Inference for Problem-Solving with Language Models"
                },
                "summary": "While the scaling laws of large language models (LLMs) training have been\nextensively studied, optimal inference configurations of LLMs remain\nunderexplored. We study inference scaling laws and compute-optimal inference,\nfocusing on the trade-offs between model sizes and generating additional tokens\nwith different inference strategies. As a first step towards understanding and\ndesigning compute-optimal inference methods, we studied cost-performance\ntrade-offs for inference strategies such as greedy search, majority voting,\nbest-of-$n$, weighted voting, and two different tree search algorithms, using\ndifferent model sizes and compute budgets. Our findings indicate smaller models\n(e.g., Llemma-7B) can outperform larger models given the same computation\nbudgets, and that smaller models paired with advanced inference algorithms\nyield Pareto-optimal cost-performance trade-offs. For instance, the Llemma-7B\nmodel, equipped with our novel tree search algorithm, consistently outperforms\nLlemma-34B with standard majority voting on the MATH benchmark across all FLOPs\nbudgets. We hope these findings contribute to a broader understanding of\ninference scaling laws for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While the scaling laws of large language models (LLMs) training have been\nextensively studied, optimal inference configurations of LLMs remain\nunderexplored. We study inference scaling laws and compute-optimal inference,\nfocusing on the trade-offs between model sizes and generating additional tokens\nwith different inference strategies. As a first step towards understanding and\ndesigning compute-optimal inference methods, we studied cost-performance\ntrade-offs for inference strategies such as greedy search, majority voting,\nbest-of-$n$, weighted voting, and two different tree search algorithms, using\ndifferent model sizes and compute budgets. Our findings indicate smaller models\n(e.g., Llemma-7B) can outperform larger models given the same computation\nbudgets, and that smaller models paired with advanced inference algorithms\nyield Pareto-optimal cost-performance trade-offs. For instance, the Llemma-7B\nmodel, equipped with our novel tree search algorithm, consistently outperforms\nLlemma-34B with standard majority voting on the MATH benchmark across all FLOPs\nbudgets. We hope these findings contribute to a broader understanding of\ninference scaling laws for LLMs."
                },
                "authors": [
                    {
                        "name": "Yangzhen Wu"
                    },
                    {
                        "name": "Zhiqing Sun"
                    },
                    {
                        "name": "Shanda Li"
                    },
                    {
                        "name": "Sean Welleck"
                    },
                    {
                        "name": "Yiming Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yiming Yang"
                },
                "author": "Yiming Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00724v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00724v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10489v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10489v1",
                "updated": "2024-10-14T13:33:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    33,
                    0,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T13:33:00Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    33,
                    0,
                    0,
                    288,
                    0
                ],
                "title": "Cultural Fidelity in Large-Language Models: An Evaluation of Online\n  Language Resources as a Driver of Model Performance in Value Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cultural Fidelity in Large-Language Models: An Evaluation of Online\n  Language Resources as a Driver of Model Performance in Value Representation"
                },
                "summary": "The training data for LLMs embeds societal values, increasing their\nfamiliarity with the language's culture. Our analysis found that 44% of the\nvariance in the ability of GPT-4o to reflect the societal values of a country,\nas measured by the World Values Survey, correlates with the availability of\ndigital resources in that language. Notably, the error rate was more than five\ntimes higher for the languages of the lowest resource compared to the languages\nof the highest resource. For GPT-4-turbo, this correlation rose to 72%,\nsuggesting efforts to improve the familiarity with the non-English language\nbeyond the web-scraped data. Our study developed one of the largest and most\nrobust datasets in this topic area with 21 country-language pairs, each of\nwhich contain 94 survey questions verified by native speakers. Our results\nhighlight the link between LLM performance and digital data availability in\ntarget languages. Weaker performance in low-resource languages, especially\nprominent in the Global South, may worsen digital divides. We discuss\nstrategies proposed to address this, including developing multilingual LLMs\nfrom the ground up and enhancing fine-tuning on diverse linguistic datasets, as\nseen in African language initiatives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The training data for LLMs embeds societal values, increasing their\nfamiliarity with the language's culture. Our analysis found that 44% of the\nvariance in the ability of GPT-4o to reflect the societal values of a country,\nas measured by the World Values Survey, correlates with the availability of\ndigital resources in that language. Notably, the error rate was more than five\ntimes higher for the languages of the lowest resource compared to the languages\nof the highest resource. For GPT-4-turbo, this correlation rose to 72%,\nsuggesting efforts to improve the familiarity with the non-English language\nbeyond the web-scraped data. Our study developed one of the largest and most\nrobust datasets in this topic area with 21 country-language pairs, each of\nwhich contain 94 survey questions verified by native speakers. Our results\nhighlight the link between LLM performance and digital data availability in\ntarget languages. Weaker performance in low-resource languages, especially\nprominent in the Global South, may worsen digital divides. We discuss\nstrategies proposed to address this, including developing multilingual LLMs\nfrom the ground up and enhancing fine-tuning on diverse linguistic datasets, as\nseen in African language initiatives."
                },
                "authors": [
                    {
                        "name": "Sharif Kazemi"
                    },
                    {
                        "name": "Gloria Gerhardt"
                    },
                    {
                        "name": "Jonty Katz"
                    },
                    {
                        "name": "Caroline Ida Kuria"
                    },
                    {
                        "name": "Estelle Pan"
                    },
                    {
                        "name": "Umang Prabhakar"
                    }
                ],
                "author_detail": {
                    "name": "Umang Prabhakar"
                },
                "author": "Umang Prabhakar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10489v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10489v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04070v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04070v2",
                "updated": "2024-10-14T13:27:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    27,
                    36,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-05T08:00:55Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    8,
                    0,
                    55,
                    5,
                    279,
                    0
                ],
                "title": "PAD: Personalized Alignment at Decoding-Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAD: Personalized Alignment at Decoding-Time"
                },
                "summary": "Aligning with personalized preferences, which vary significantly across\ncultural, educational, and political differences, poses a significant challenge\ndue to the computational costs and data demands of traditional alignment\nmethods. In response, this paper presents Personalized Alignment at\nDecoding-time (PAD), a novel framework designed to align LLM outputs with\ndiverse personalized preferences during the inference phase, eliminating the\nneed for additional training. By introducing a unique personalized reward\nmodeling strategy, this framework decouples the text generation process from\npersonalized preferences, facilitating the generation of generalizable\ntoken-level personalized rewards. The PAD algorithm leverages these rewards to\nguide the decoding process, dynamically tailoring the base model's predictions\nto personalized preferences. Extensive experimental results demonstrate that\nPAD not only outperforms existing training-based alignment methods in terms of\naligning with diverse preferences but also shows significant generalizability\nto preferences unseen during training and scalability across different base\nmodels. This work advances the capability of LLMs to meet user needs in\nreal-time applications, presenting a substantial step forward in personalized\nLLM alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning with personalized preferences, which vary significantly across\ncultural, educational, and political differences, poses a significant challenge\ndue to the computational costs and data demands of traditional alignment\nmethods. In response, this paper presents Personalized Alignment at\nDecoding-time (PAD), a novel framework designed to align LLM outputs with\ndiverse personalized preferences during the inference phase, eliminating the\nneed for additional training. By introducing a unique personalized reward\nmodeling strategy, this framework decouples the text generation process from\npersonalized preferences, facilitating the generation of generalizable\ntoken-level personalized rewards. The PAD algorithm leverages these rewards to\nguide the decoding process, dynamically tailoring the base model's predictions\nto personalized preferences. Extensive experimental results demonstrate that\nPAD not only outperforms existing training-based alignment methods in terms of\naligning with diverse preferences but also shows significant generalizability\nto preferences unseen during training and scalability across different base\nmodels. This work advances the capability of LLMs to meet user needs in\nreal-time applications, presenting a substantial step forward in personalized\nLLM alignment."
                },
                "authors": [
                    {
                        "name": "Ruizhe Chen"
                    },
                    {
                        "name": "Xiaotian Zhang"
                    },
                    {
                        "name": "Meng Luo"
                    },
                    {
                        "name": "Wenhao Chai"
                    },
                    {
                        "name": "Zuozhu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zuozhu Liu"
                },
                "author": "Zuozhu Liu",
                "arxiv_comment": "This paper presents Personalized Alignment at Decoding-time (PAD), a\n  novel framework designed to align LLM outputs with diverse personalized\n  preferences during the inference phase",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04070v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04070v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03618v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03618v3",
                "updated": "2024-10-14T13:19:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    19,
                    57,
                    0,
                    288,
                    0
                ],
                "published": "2024-06-05T20:32:56Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    20,
                    32,
                    56,
                    2,
                    157,
                    0
                ],
                "title": "TACT: Advancing Complex Aggregative Reasoning with Information\n  Extraction Tools",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TACT: Advancing Complex Aggregative Reasoning with Information\n  Extraction Tools"
                },
                "summary": "Large Language Models (LLMs) often do not perform well on queries that\nrequire the aggregation of information across texts. To better evaluate this\nsetting and facilitate modeling efforts, we introduce TACT - Text And\nCalculations through Tables, a dataset crafted to evaluate LLMs' reasoning and\ncomputational abilities using complex instructions. TACT contains challenging\ninstructions that demand stitching information scattered across one or more\ntexts, and performing complex integration on this information to generate the\nanswer. We construct this dataset by leveraging an existing dataset of texts\nand their associated tables. For each such tables, we formulate new queries,\nand gather their respective answers. We demonstrate that all contemporary LLMs\nperform poorly on this dataset, achieving an accuracy below 38%. To pinpoint\nthe difficulties and thoroughly dissect the problem, we analyze model\nperformance across three components: table-generation, Pandas\ncommand-generation, and execution. Unexpectedly, we discover that each\ncomponent presents substantial challenges for current LLMs. These insights lead\nus to propose a focused modeling framework, which we refer to as IE as a tool.\nSpecifically, we propose to add \"tools\" for each of the above steps, and\nimplement each such tool with few-shot prompting. This approach shows an\nimprovement over existing prompting techniques, offering a promising direction\nfor enhancing model capabilities in these tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often do not perform well on queries that\nrequire the aggregation of information across texts. To better evaluate this\nsetting and facilitate modeling efforts, we introduce TACT - Text And\nCalculations through Tables, a dataset crafted to evaluate LLMs' reasoning and\ncomputational abilities using complex instructions. TACT contains challenging\ninstructions that demand stitching information scattered across one or more\ntexts, and performing complex integration on this information to generate the\nanswer. We construct this dataset by leveraging an existing dataset of texts\nand their associated tables. For each such tables, we formulate new queries,\nand gather their respective answers. We demonstrate that all contemporary LLMs\nperform poorly on this dataset, achieving an accuracy below 38%. To pinpoint\nthe difficulties and thoroughly dissect the problem, we analyze model\nperformance across three components: table-generation, Pandas\ncommand-generation, and execution. Unexpectedly, we discover that each\ncomponent presents substantial challenges for current LLMs. These insights lead\nus to propose a focused modeling framework, which we refer to as IE as a tool.\nSpecifically, we propose to add \"tools\" for each of the above steps, and\nimplement each such tool with few-shot prompting. This approach shows an\nimprovement over existing prompting techniques, offering a promising direction\nfor enhancing model capabilities in these tasks."
                },
                "authors": [
                    {
                        "name": "Avi Caciularu"
                    },
                    {
                        "name": "Alon Jacovi"
                    },
                    {
                        "name": "Eyal Ben-David"
                    },
                    {
                        "name": "Sasha Goldshtein"
                    },
                    {
                        "name": "Tal Schuster"
                    },
                    {
                        "name": "Jonathan Herzig"
                    },
                    {
                        "name": "Gal Elidan"
                    },
                    {
                        "name": "Amir Globerson"
                    }
                ],
                "author_detail": {
                    "name": "Amir Globerson"
                },
                "author": "Amir Globerson",
                "arxiv_comment": "Accepted to NeurIPS 2024. Website (https://tact-benchmark.github.io),\n  Huggingface (https://huggingface.co/datasets/google/TACT)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03618v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03618v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10481v1",
                "updated": "2024-10-14T13:18:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    18,
                    20,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T13:18:20Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    18,
                    20,
                    0,
                    288,
                    0
                ],
                "title": "Model-Based Differentially Private Knowledge Transfer for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-Based Differentially Private Knowledge Transfer for Large Language\n  Models"
                },
                "summary": "As large language models (LLMs) become increasingly prevalent in web\nservices, effectively leveraging domain-specific knowledge while ensuring\nprivacy has become critical. Existing methods, such as retrieval-augmented\ngeneration (RAG) and differentially private data synthesis, often compromise\neither the utility of domain knowledge or the privacy of sensitive data,\nlimiting their applicability in specialized domains. To address these\nchallenges, we propose \\textit{Llamdex}, a novel framework that integrates\nprivacy-preserving, domain-specific models into LLMs. Our approach\nsignificantly enhances the accuracy of domain-specific tasks, achieving up to a\n26\\% improvement compared to existing methods under the same differential\nprivacy constraints. Experimental results show that Llamdex not only improves\nthe accuracy of LLM responses but also maintains comparable inference\nefficiency to the original LLM, highlighting its potential for real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become increasingly prevalent in web\nservices, effectively leveraging domain-specific knowledge while ensuring\nprivacy has become critical. Existing methods, such as retrieval-augmented\ngeneration (RAG) and differentially private data synthesis, often compromise\neither the utility of domain knowledge or the privacy of sensitive data,\nlimiting their applicability in specialized domains. To address these\nchallenges, we propose \\textit{Llamdex}, a novel framework that integrates\nprivacy-preserving, domain-specific models into LLMs. Our approach\nsignificantly enhances the accuracy of domain-specific tasks, achieving up to a\n26\\% improvement compared to existing methods under the same differential\nprivacy constraints. Experimental results show that Llamdex not only improves\nthe accuracy of LLM responses but also maintains comparable inference\nefficiency to the original LLM, highlighting its potential for real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Zhaomin Wu"
                    },
                    {
                        "name": "Jizhou Guo"
                    },
                    {
                        "name": "Junyi Hou"
                    },
                    {
                        "name": "Bingsheng He"
                    },
                    {
                        "name": "Lixin Fan"
                    },
                    {
                        "name": "Qiang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Yang"
                },
                "author": "Qiang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10479v1",
                "updated": "2024-10-14T13:15:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    15,
                    34,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T13:15:34Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    15,
                    34,
                    0,
                    288,
                    0
                ],
                "title": "TMGBench: A Systematic Game Benchmark for Evaluating Strategic Reasoning\n  Abilities of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TMGBench: A Systematic Game Benchmark for Evaluating Strategic Reasoning\n  Abilities of LLMs"
                },
                "summary": "The rapid advancement of large language models (LLMs) has accelerated their\napplication in reasoning, with strategic reasoning drawing increasing\nattention. To evaluate LLMs' strategic reasoning capabilities, game theory,\nwith its concise structure, has become a preferred approach. However, current\nresearch focuses on a limited selection of games, resulting in low coverage.\nClassic game scenarios risk data leakage, and existing benchmarks often lack\nextensibility, making them inadequate for evaluating state-of-the-art models.\nTo address these challenges, we propose TMGBench, a benchmark with\ncomprehensive game type coverage, novel scenarios, and flexible organization.\nSpecifically, we incorporate all 144 game types summarized by the\nRobinson-Goforth topology of 2x2 games, constructed as classic games. We also\nemploy synthetic data generation to create diverse, higher-quality scenarios\nthrough topic guidance and human inspection, referred to as story-based games.\nLastly, we provide a sustainable framework for increasingly powerful LLMs by\ntreating these games as atomic units and organizing them into more complex\nforms via sequential, parallel, and nested structures. Our comprehensive\nevaluation of mainstream LLMs covers tests on rational reasoning, robustness,\nTheory-of-Mind (ToM), and reasoning in complex forms. Results reveal flaws in\naccuracy, consistency, and varying mastery of ToM. Additionally, o1-mini,\nOpenAI's latest reasoning model, achieved accuracy rates of 66.6%, 60.0%, and\n70.0% on sequential, parallel, and nested games, highlighting TMGBench's\nchallenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has accelerated their\napplication in reasoning, with strategic reasoning drawing increasing\nattention. To evaluate LLMs' strategic reasoning capabilities, game theory,\nwith its concise structure, has become a preferred approach. However, current\nresearch focuses on a limited selection of games, resulting in low coverage.\nClassic game scenarios risk data leakage, and existing benchmarks often lack\nextensibility, making them inadequate for evaluating state-of-the-art models.\nTo address these challenges, we propose TMGBench, a benchmark with\ncomprehensive game type coverage, novel scenarios, and flexible organization.\nSpecifically, we incorporate all 144 game types summarized by the\nRobinson-Goforth topology of 2x2 games, constructed as classic games. We also\nemploy synthetic data generation to create diverse, higher-quality scenarios\nthrough topic guidance and human inspection, referred to as story-based games.\nLastly, we provide a sustainable framework for increasingly powerful LLMs by\ntreating these games as atomic units and organizing them into more complex\nforms via sequential, parallel, and nested structures. Our comprehensive\nevaluation of mainstream LLMs covers tests on rational reasoning, robustness,\nTheory-of-Mind (ToM), and reasoning in complex forms. Results reveal flaws in\naccuracy, consistency, and varying mastery of ToM. Additionally, o1-mini,\nOpenAI's latest reasoning model, achieved accuracy rates of 66.6%, 60.0%, and\n70.0% on sequential, parallel, and nested games, highlighting TMGBench's\nchallenges."
                },
                "authors": [
                    {
                        "name": "Haochuan Wang"
                    },
                    {
                        "name": "Xiachong Feng"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Zhanyue Qin"
                    },
                    {
                        "name": "Dianbo Sui"
                    },
                    {
                        "name": "Lingpeng Kong"
                    }
                ],
                "author_detail": {
                    "name": "Lingpeng Kong"
                },
                "author": "Lingpeng Kong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.05465v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.05465v2",
                "updated": "2024-10-14T13:11:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    11,
                    55,
                    0,
                    288,
                    0
                ],
                "published": "2024-04-08T12:43:32Z",
                "published_parsed": [
                    2024,
                    4,
                    8,
                    12,
                    43,
                    32,
                    0,
                    99,
                    0
                ],
                "title": "HAMMR: HierArchical MultiModal React agents for generic VQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HAMMR: HierArchical MultiModal React agents for generic VQA"
                },
                "summary": "Combining Large Language Models (LLMs) with external specialized tools\n(LLMs+tools) is a recent paradigm to solve multimodal tasks such as Visual\nQuestion Answering (VQA). While this approach was demonstrated to work well\nwhen optimized and evaluated for each individual benchmark, in practice it is\ncrucial for the next generation of real-world AI systems to handle a broad\nrange of multimodal problems. Therefore we pose the VQA problem from a unified\nperspective and evaluate a single system on a varied suite of VQA tasks\nincluding counting, spatial reasoning, OCR-based reasoning, visual pointing,\nexternal knowledge, and more. In this setting, we demonstrate that naively\napplying the LLM+tools approach using the combined set of all tools leads to\npoor results. This motivates us to introduce HAMMR: HierArchical MultiModal\nReact. We start from a multimodal ReAct-based system and make it hierarchical\nby enabling our HAMMR agents to call upon other specialized agents. This\nenhances the compositionality of the LLM+tools approach, which we show to be\ncritical for obtaining high accuracy on generic VQA. Concretely, on our generic\nVQA suite, HAMMR outperforms the naive LLM+tools approach by 19.5%.\nAdditionally, HAMMR achieves state-of-the-art results on this task,\noutperforming the generic standalone PaLI-X VQA model by 5.0%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining Large Language Models (LLMs) with external specialized tools\n(LLMs+tools) is a recent paradigm to solve multimodal tasks such as Visual\nQuestion Answering (VQA). While this approach was demonstrated to work well\nwhen optimized and evaluated for each individual benchmark, in practice it is\ncrucial for the next generation of real-world AI systems to handle a broad\nrange of multimodal problems. Therefore we pose the VQA problem from a unified\nperspective and evaluate a single system on a varied suite of VQA tasks\nincluding counting, spatial reasoning, OCR-based reasoning, visual pointing,\nexternal knowledge, and more. In this setting, we demonstrate that naively\napplying the LLM+tools approach using the combined set of all tools leads to\npoor results. This motivates us to introduce HAMMR: HierArchical MultiModal\nReact. We start from a multimodal ReAct-based system and make it hierarchical\nby enabling our HAMMR agents to call upon other specialized agents. This\nenhances the compositionality of the LLM+tools approach, which we show to be\ncritical for obtaining high accuracy on generic VQA. Concretely, on our generic\nVQA suite, HAMMR outperforms the naive LLM+tools approach by 19.5%.\nAdditionally, HAMMR achieves state-of-the-art results on this task,\noutperforming the generic standalone PaLI-X VQA model by 5.0%."
                },
                "authors": [
                    {
                        "name": "Lluis Castrejon"
                    },
                    {
                        "name": "Thomas Mensink"
                    },
                    {
                        "name": "Howard Zhou"
                    },
                    {
                        "name": "Vittorio Ferrari"
                    },
                    {
                        "name": "Andre Araujo"
                    },
                    {
                        "name": "Jasper Uijlings"
                    }
                ],
                "author_detail": {
                    "name": "Jasper Uijlings"
                },
                "author": "Jasper Uijlings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.05465v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.05465v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10476v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10476v1",
                "updated": "2024-10-14T13:10:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    10,
                    45,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T13:10:45Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    10,
                    45,
                    0,
                    288,
                    0
                ],
                "title": "Will LLMs Replace the Encoder-Only Models in Temporal Relation\n  Classification?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Will LLMs Replace the Encoder-Only Models in Temporal Relation\n  Classification?"
                },
                "summary": "The automatic detection of temporal relations among events has been mainly\ninvestigated with encoder-only models such as RoBERTa. Large Language Models\n(LLM) have recently shown promising performance in temporal reasoning tasks\nsuch as temporal question answering. Nevertheless, recent studies have tested\nthe LLMs' performance in detecting temporal relations of closed-source models\nonly, limiting the interpretability of those results. In this work, we\ninvestigate LLMs' performance and decision process in the Temporal Relation\nClassification task. First, we assess the performance of seven open and\nclosed-sourced LLMs experimenting with in-context learning and lightweight\nfine-tuning approaches. Results show that LLMs with in-context learning\nsignificantly underperform smaller encoder-only models based on RoBERTa. Then,\nwe delve into the possible reasons for this gap by applying explainable\nmethods. The outcome suggests a limitation of LLMs in this task due to their\nautoregressive nature, which causes them to focus only on the last part of the\nsequence. Additionally, we evaluate the word embeddings of these two models to\nbetter understand their pre-training differences. The code and the fine-tuned\nmodels can be found respectively on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The automatic detection of temporal relations among events has been mainly\ninvestigated with encoder-only models such as RoBERTa. Large Language Models\n(LLM) have recently shown promising performance in temporal reasoning tasks\nsuch as temporal question answering. Nevertheless, recent studies have tested\nthe LLMs' performance in detecting temporal relations of closed-source models\nonly, limiting the interpretability of those results. In this work, we\ninvestigate LLMs' performance and decision process in the Temporal Relation\nClassification task. First, we assess the performance of seven open and\nclosed-sourced LLMs experimenting with in-context learning and lightweight\nfine-tuning approaches. Results show that LLMs with in-context learning\nsignificantly underperform smaller encoder-only models based on RoBERTa. Then,\nwe delve into the possible reasons for this gap by applying explainable\nmethods. The outcome suggests a limitation of LLMs in this task due to their\nautoregressive nature, which causes them to focus only on the last part of the\nsequence. Additionally, we evaluate the word embeddings of these two models to\nbetter understand their pre-training differences. The code and the fine-tuned\nmodels can be found respectively on GitHub."
                },
                "authors": [
                    {
                        "name": "Gabriel Roccabruna"
                    },
                    {
                        "name": "Massimo Rizzoli"
                    },
                    {
                        "name": "Giuseppe Riccardi"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Riccardi"
                },
                "author": "Giuseppe Riccardi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10476v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10476v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10456v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10456v2",
                "updated": "2024-10-15T02:56:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    2,
                    56,
                    14,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-14T12:50:04Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    12,
                    50,
                    4,
                    0,
                    288,
                    0
                ],
                "title": "Ada-K Routing: Boosting the Efficiency of MoE-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-K Routing: Boosting the Efficiency of MoE-based LLMs"
                },
                "summary": "In the era of Large Language Models (LLMs), Mixture-of-Experts (MoE)\narchitectures offer a promising approach to managing computational costs while\nscaling up model parameters. Conventional MoE-based LLMs typically employ\nstatic Top-K routing, which activates a fixed and equal number of experts for\neach token regardless of their significance within the context. In this paper,\nwe propose a novel Ada-K routing strategy that dynamically adjusts the number\nof activated experts for each token, thereby improving the balance between\ncomputational efficiency and model performance. Specifically, our strategy\nincorporates learnable and lightweight allocator modules that decide customized\nexpert resource allocation tailored to the contextual needs for each token.\nThese allocators are designed to be fully pluggable, making it broadly\napplicable across all mainstream MoE-based LLMs. We leverage the Proximal\nPolicy Optimization (PPO) algorithm to facilitate an end-to-end learning\nprocess for this non-differentiable decision-making framework. Extensive\nevaluations on four popular baseline models demonstrate that our Ada-K routing\nmethod significantly outperforms conventional Top-K routing. Compared to Top-K,\nour method achieves over 25% reduction in FLOPs and more than 20% inference\nspeedup while still improving performance across various benchmarks. Moreover,\nthe training of Ada-K is highly efficient. Even for Mixtral-8x22B, a MoE-based\nLLM with more than 140B parameters, the training time is limited to 8 hours.\nDetailed analysis shows that harder tasks, middle layers, and content words\ntend to activate more experts, providing valuable insights for future adaptive\nMoE system designs. Both the training code and model checkpoints will be\npublicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of Large Language Models (LLMs), Mixture-of-Experts (MoE)\narchitectures offer a promising approach to managing computational costs while\nscaling up model parameters. Conventional MoE-based LLMs typically employ\nstatic Top-K routing, which activates a fixed and equal number of experts for\neach token regardless of their significance within the context. In this paper,\nwe propose a novel Ada-K routing strategy that dynamically adjusts the number\nof activated experts for each token, thereby improving the balance between\ncomputational efficiency and model performance. Specifically, our strategy\nincorporates learnable and lightweight allocator modules that decide customized\nexpert resource allocation tailored to the contextual needs for each token.\nThese allocators are designed to be fully pluggable, making it broadly\napplicable across all mainstream MoE-based LLMs. We leverage the Proximal\nPolicy Optimization (PPO) algorithm to facilitate an end-to-end learning\nprocess for this non-differentiable decision-making framework. Extensive\nevaluations on four popular baseline models demonstrate that our Ada-K routing\nmethod significantly outperforms conventional Top-K routing. Compared to Top-K,\nour method achieves over 25% reduction in FLOPs and more than 20% inference\nspeedup while still improving performance across various benchmarks. Moreover,\nthe training of Ada-K is highly efficient. Even for Mixtral-8x22B, a MoE-based\nLLM with more than 140B parameters, the training time is limited to 8 hours.\nDetailed analysis shows that harder tasks, middle layers, and content words\ntend to activate more experts, providing valuable insights for future adaptive\nMoE system designs. Both the training code and model checkpoints will be\npublicly available."
                },
                "authors": [
                    {
                        "name": "Tongtian Yue"
                    },
                    {
                        "name": "Longteng Guo"
                    },
                    {
                        "name": "Jie Cheng"
                    },
                    {
                        "name": "Xuange Gao"
                    },
                    {
                        "name": "Jing Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jing Liu"
                },
                "author": "Jing Liu",
                "arxiv_comment": "Coauthors do not reach a consensus on submitting the current version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10456v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10456v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10455v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10455v1",
                "updated": "2024-10-14T12:49:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    12,
                    49,
                    13,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T12:49:13Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    12,
                    49,
                    13,
                    0,
                    288,
                    0
                ],
                "title": "Advancing Academic Knowledge Retrieval via LLM-enhanced Representation\n  Similarity Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Academic Knowledge Retrieval via LLM-enhanced Representation\n  Similarity Fusion"
                },
                "summary": "In an era marked by robust technological growth and swift information\nrenewal, furnishing researchers and the populace with top-tier, avant-garde\nacademic insights spanning various domains has become an urgent necessity. The\nKDD Cup 2024 AQA Challenge is geared towards advancing retrieval models to\nidentify pertinent academic terminologies from suitable papers for scientific\ninquiries. This paper introduces the LLM-KnowSimFuser proposed by Robo Space,\nwhich wins the 2nd place in the competition. With inspirations drawed from the\nsuperior performance of LLMs on multiple tasks, after careful analysis of the\nprovided datasets, we firstly perform fine-tuning and inference using\nLLM-enhanced pre-trained retrieval models to introduce the tremendous language\nunderstanding and open-domain knowledge of LLMs into this task, followed by a\nweighted fusion based on the similarity matrix derived from the inference\nresults. Finally, experiments conducted on the competition datasets show the\nsuperiority of our proposal, which achieved a score of 0.20726 on the final\nleaderboard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In an era marked by robust technological growth and swift information\nrenewal, furnishing researchers and the populace with top-tier, avant-garde\nacademic insights spanning various domains has become an urgent necessity. The\nKDD Cup 2024 AQA Challenge is geared towards advancing retrieval models to\nidentify pertinent academic terminologies from suitable papers for scientific\ninquiries. This paper introduces the LLM-KnowSimFuser proposed by Robo Space,\nwhich wins the 2nd place in the competition. With inspirations drawed from the\nsuperior performance of LLMs on multiple tasks, after careful analysis of the\nprovided datasets, we firstly perform fine-tuning and inference using\nLLM-enhanced pre-trained retrieval models to introduce the tremendous language\nunderstanding and open-domain knowledge of LLMs into this task, followed by a\nweighted fusion based on the similarity matrix derived from the inference\nresults. Finally, experiments conducted on the competition datasets show the\nsuperiority of our proposal, which achieved a score of 0.20726 on the final\nleaderboard."
                },
                "authors": [
                    {
                        "name": "Wei Dai"
                    },
                    {
                        "name": "Peng Fu"
                    },
                    {
                        "name": "Chunjing Gan"
                    }
                ],
                "author_detail": {
                    "name": "Chunjing Gan"
                },
                "author": "Chunjing Gan",
                "arxiv_comment": "The 2nd Place of KDD Cup 2024 OAG-Challenge AQA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10455v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10455v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10450v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10450v1",
                "updated": "2024-10-14T12:45:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    12,
                    45,
                    10,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T12:45:10Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    12,
                    45,
                    10,
                    0,
                    288,
                    0
                ],
                "title": "KBLaM: Knowledge Base augmented Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KBLaM: Knowledge Base augmented Language Model"
                },
                "summary": "In this paper, we propose Knowledge Base augmented Language Model (KBLaM), a\nnew method for augmenting Large Language Models (LLMs) with external knowledge.\nKBLaM works with a knowledge base (KB) constructed from a corpus of documents,\ntransforming each piece of knowledge in the KB into continuous key-value vector\npairs via pre-trained sentence encoders with linear adapters and integrating\nthem into pre-trained LLMs via a specialized rectangular attention mechanism.\nUnlike Retrieval-Augmented Generation, KBLaM eliminates external retrieval\nmodules, and unlike in-context learning, its computational overhead scales\nlinearly with KB size rather than quadratically. Our approach enables\nintegrating a large KB of more than 10K triples into an 8B pre-trained LLM of\nonly 8K context window on one single A100 80GB GPU and allows for dynamic\nupdates without model fine-tuning or retraining. Experiments demonstrate\nKBLaM's effectiveness in various tasks, including question-answering and\nopen-ended reasoning, while providing interpretable insights into its use of\nthe augmented knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose Knowledge Base augmented Language Model (KBLaM), a\nnew method for augmenting Large Language Models (LLMs) with external knowledge.\nKBLaM works with a knowledge base (KB) constructed from a corpus of documents,\ntransforming each piece of knowledge in the KB into continuous key-value vector\npairs via pre-trained sentence encoders with linear adapters and integrating\nthem into pre-trained LLMs via a specialized rectangular attention mechanism.\nUnlike Retrieval-Augmented Generation, KBLaM eliminates external retrieval\nmodules, and unlike in-context learning, its computational overhead scales\nlinearly with KB size rather than quadratically. Our approach enables\nintegrating a large KB of more than 10K triples into an 8B pre-trained LLM of\nonly 8K context window on one single A100 80GB GPU and allows for dynamic\nupdates without model fine-tuning or retraining. Experiments demonstrate\nKBLaM's effectiveness in various tasks, including question-answering and\nopen-ended reasoning, while providing interpretable insights into its use of\nthe augmented knowledge."
                },
                "authors": [
                    {
                        "name": "Xi Wang"
                    },
                    {
                        "name": "Liana Mikaelyan"
                    },
                    {
                        "name": "Taketomo Isazawa"
                    },
                    {
                        "name": "James Hensman"
                    }
                ],
                "author_detail": {
                    "name": "James Hensman"
                },
                "author": "James Hensman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10450v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10450v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02273v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02273v2",
                "updated": "2024-10-14T12:43:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    12,
                    43,
                    35,
                    0,
                    288,
                    0
                ],
                "published": "2024-07-02T14:02:53Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    14,
                    2,
                    53,
                    1,
                    184,
                    0
                ],
                "title": "Language Model Alignment in Multilingual Trolley Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Model Alignment in Multilingual Trolley Problems"
                },
                "summary": "We evaluate the moral alignment of large language models (LLMs) with human\npreferences in multilingual trolley problems. Building on the Moral Machine\nexperiment, which captures over 40 million human judgments across 200+\ncountries, we develop a cross-lingual corpus of moral dilemma vignettes in over\n100 languages called $\\mathrm{MultiTP}$. This dataset enables the assessment of\nLLMs' decision-making processes in diverse linguistic contexts. Our analysis\nexplores the alignment of 19 different LLMs with human judgments, capturing\npreferences across six moral dimensions: species, gender, fitness, status, age,\nand the number of lives involved. By correlating these preferences with the\ndemographic distribution of language speakers and examining the consistency of\nLLM responses to various prompt paraphrasings, our findings provide insights\ninto cross-lingual and ethical biases of LLMs and their intersection. We\ndiscover significant variance in alignment across languages, challenging the\nassumption of uniform moral reasoning in AI systems and highlighting the\nimportance of incorporating diverse perspectives in AI ethics. The results\nunderscore the need for further research on the integration of multilingual\ndimensions in responsible AI research to ensure fair and equitable AI\ninteractions worldwide.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We evaluate the moral alignment of large language models (LLMs) with human\npreferences in multilingual trolley problems. Building on the Moral Machine\nexperiment, which captures over 40 million human judgments across 200+\ncountries, we develop a cross-lingual corpus of moral dilemma vignettes in over\n100 languages called $\\mathrm{MultiTP}$. This dataset enables the assessment of\nLLMs' decision-making processes in diverse linguistic contexts. Our analysis\nexplores the alignment of 19 different LLMs with human judgments, capturing\npreferences across six moral dimensions: species, gender, fitness, status, age,\nand the number of lives involved. By correlating these preferences with the\ndemographic distribution of language speakers and examining the consistency of\nLLM responses to various prompt paraphrasings, our findings provide insights\ninto cross-lingual and ethical biases of LLMs and their intersection. We\ndiscover significant variance in alignment across languages, challenging the\nassumption of uniform moral reasoning in AI systems and highlighting the\nimportance of incorporating diverse perspectives in AI ethics. The results\nunderscore the need for further research on the integration of multilingual\ndimensions in responsible AI research to ensure fair and equitable AI\ninteractions worldwide."
                },
                "authors": [
                    {
                        "name": "Zhijing Jin"
                    },
                    {
                        "name": "Sydney Levine"
                    },
                    {
                        "name": "Max Kleiman-Weiner"
                    },
                    {
                        "name": "Giorgio Piatti"
                    },
                    {
                        "name": "Jiarui Liu"
                    },
                    {
                        "name": "Fernando Gonzalez Adauto"
                    },
                    {
                        "name": "Francesco Ortu"
                    },
                    {
                        "name": "Andrs Strausz"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    },
                    {
                        "name": "Rada Mihalcea"
                    },
                    {
                        "name": "Yejin Choi"
                    },
                    {
                        "name": "Bernhard Schlkopf"
                    }
                ],
                "author_detail": {
                    "name": "Bernhard Schlkopf"
                },
                "author": "Bernhard Schlkopf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02273v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02273v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10441v1",
                "updated": "2024-10-14T12:35:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    12,
                    35,
                    12,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T12:35:12Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    12,
                    35,
                    12,
                    0,
                    288,
                    0
                ],
                "title": "Free Video-LLM: Prompt-guided Visual Perception for Efficient\n  Training-free Video LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Free Video-LLM: Prompt-guided Visual Perception for Efficient\n  Training-free Video LLMs"
                },
                "summary": "Vision-language large models have achieved remarkable success in various\nmulti-modal tasks, yet applying them to video understanding remains challenging\ndue to the inherent complexity and computational demands of video data. While\ntraining-based video-LLMs deliver high performance, they often require\nsubstantial resources for training and inference. Conversely, training-free\napproaches offer a more efficient alternative by adapting pre-trained\nimage-LLMs models for video tasks without additional training, but they face\ninference efficiency bottlenecks due to the large number of visual tokens\ngenerated from video frames. In this work, we present a novel prompt-guided\nvisual perception framework (abbreviated as \\emph{Free Video-LLM}) for\nefficient inference of training-free video LLMs. The proposed framework\ndecouples spatial-temporal dimension and performs temporal frame sampling and\nspatial RoI cropping respectively based on task-specific prompts. Our method\neffectively reduces the number of visual tokens while maintaining high\nperformance across multiple video question-answering benchmarks. Extensive\nexperiments demonstrate that our approach achieves competitive results with\nsignificantly fewer tokens, offering an optimal trade-off between accuracy and\ncomputational efficiency compared to state-of-the-art video LLMs. The code will\nbe available at \\url{https://github.com/contrastive/FreeVideoLLM}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language large models have achieved remarkable success in various\nmulti-modal tasks, yet applying them to video understanding remains challenging\ndue to the inherent complexity and computational demands of video data. While\ntraining-based video-LLMs deliver high performance, they often require\nsubstantial resources for training and inference. Conversely, training-free\napproaches offer a more efficient alternative by adapting pre-trained\nimage-LLMs models for video tasks without additional training, but they face\ninference efficiency bottlenecks due to the large number of visual tokens\ngenerated from video frames. In this work, we present a novel prompt-guided\nvisual perception framework (abbreviated as \\emph{Free Video-LLM}) for\nefficient inference of training-free video LLMs. The proposed framework\ndecouples spatial-temporal dimension and performs temporal frame sampling and\nspatial RoI cropping respectively based on task-specific prompts. Our method\neffectively reduces the number of visual tokens while maintaining high\nperformance across multiple video question-answering benchmarks. Extensive\nexperiments demonstrate that our approach achieves competitive results with\nsignificantly fewer tokens, offering an optimal trade-off between accuracy and\ncomputational efficiency compared to state-of-the-art video LLMs. The code will\nbe available at \\url{https://github.com/contrastive/FreeVideoLLM}."
                },
                "authors": [
                    {
                        "name": "Kai Han"
                    },
                    {
                        "name": "Jianyuan Guo"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Wei He"
                    },
                    {
                        "name": "Enhua Wu"
                    },
                    {
                        "name": "Yunhe Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhe Wang"
                },
                "author": "Yunhe Wang",
                "arxiv_comment": "Tech report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14531v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14531v2",
                "updated": "2024-10-14T12:34:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    12,
                    34,
                    55,
                    0,
                    288,
                    0
                ],
                "published": "2024-02-22T13:24:10Z",
                "published_parsed": [
                    2024,
                    2,
                    22,
                    13,
                    24,
                    10,
                    3,
                    53,
                    0
                ],
                "title": "Should We Respect LLMs? A Cross-Lingual Study on the Influence of Prompt\n  Politeness on LLM Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Should We Respect LLMs? A Cross-Lingual Study on the Influence of Prompt\n  Politeness on LLM Performance"
                },
                "summary": "We investigate the impact of politeness levels in prompts on the performance\nof large language models (LLMs). Polite language in human communications often\ngarners more compliance and effectiveness, while rudeness can cause aversion,\nimpacting response quality. We consider that LLMs mirror human communication\ntraits, suggesting they align with human cultural norms. We assess the impact\nof politeness in prompts on LLMs across English, Chinese, and Japanese tasks.\nWe observed that impolite prompts often result in poor performance, but overly\npolite language does not guarantee better outcomes. The best politeness level\nis different according to the language. This phenomenon suggests that LLMs not\nonly reflect human behavior but are also influenced by language, particularly\nin different cultural contexts. Our findings highlight the need to factor in\npoliteness for cross-cultural natural language processing and LLM usage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the impact of politeness levels in prompts on the performance\nof large language models (LLMs). Polite language in human communications often\ngarners more compliance and effectiveness, while rudeness can cause aversion,\nimpacting response quality. We consider that LLMs mirror human communication\ntraits, suggesting they align with human cultural norms. We assess the impact\nof politeness in prompts on LLMs across English, Chinese, and Japanese tasks.\nWe observed that impolite prompts often result in poor performance, but overly\npolite language does not guarantee better outcomes. The best politeness level\nis different according to the language. This phenomenon suggests that LLMs not\nonly reflect human behavior but are also influenced by language, particularly\nin different cultural contexts. Our findings highlight the need to factor in\npoliteness for cross-cultural natural language processing and LLM usage."
                },
                "authors": [
                    {
                        "name": "Ziqi Yin"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Kaito Horio"
                    },
                    {
                        "name": "Daisuke Kawahara"
                    },
                    {
                        "name": "Satoshi Sekine"
                    }
                ],
                "author_detail": {
                    "name": "Satoshi Sekine"
                },
                "author": "Satoshi Sekine",
                "arxiv_comment": "SICon 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14531v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14531v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10414v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10414v1",
                "updated": "2024-10-14T12:04:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    12,
                    4,
                    6,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T12:04:06Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    12,
                    4,
                    6,
                    0,
                    288,
                    0
                ],
                "title": "On Calibration of LLM-based Guard Models for Reliable Content Moderation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Calibration of LLM-based Guard Models for Reliable Content Moderation"
                },
                "summary": "Large language models (LLMs) pose significant risks due to the potential for\ngenerating harmful content or users attempting to evade guardrails. Existing\nstudies have developed LLM-based guard models designed to moderate the input\nand output of threat LLMs, ensuring adherence to safety policies by blocking\ncontent that violates these protocols upon deployment. However, limited\nattention has been given to the reliability and calibration of such guard\nmodels. In this work, we empirically conduct comprehensive investigations of\nconfidence calibration for 9 existing LLM-based guard models on 12 benchmarks\nin both user input and model output classification. Our findings reveal that\ncurrent LLM-based guard models tend to 1) produce overconfident predictions, 2)\nexhibit significant miscalibration when subjected to jailbreak attacks, and 3)\ndemonstrate limited robustness to the outputs generated by different types of\nresponse models. Additionally, we assess the effectiveness of post-hoc\ncalibration methods to mitigate miscalibration. We demonstrate the efficacy of\ntemperature scaling and, for the first time, highlight the benefits of\ncontextual calibration for confidence calibration of guard models, particularly\nin the absence of validation sets. Our analysis and experiments underscore the\nlimitations of current LLM-based guard models and provide valuable insights for\nthe future development of well-calibrated guard models toward more reliable\ncontent moderation. We also advocate for incorporating reliability evaluation\nof confidence calibration when releasing future LLM-based guard models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) pose significant risks due to the potential for\ngenerating harmful content or users attempting to evade guardrails. Existing\nstudies have developed LLM-based guard models designed to moderate the input\nand output of threat LLMs, ensuring adherence to safety policies by blocking\ncontent that violates these protocols upon deployment. However, limited\nattention has been given to the reliability and calibration of such guard\nmodels. In this work, we empirically conduct comprehensive investigations of\nconfidence calibration for 9 existing LLM-based guard models on 12 benchmarks\nin both user input and model output classification. Our findings reveal that\ncurrent LLM-based guard models tend to 1) produce overconfident predictions, 2)\nexhibit significant miscalibration when subjected to jailbreak attacks, and 3)\ndemonstrate limited robustness to the outputs generated by different types of\nresponse models. Additionally, we assess the effectiveness of post-hoc\ncalibration methods to mitigate miscalibration. We demonstrate the efficacy of\ntemperature scaling and, for the first time, highlight the benefits of\ncontextual calibration for confidence calibration of guard models, particularly\nin the absence of validation sets. Our analysis and experiments underscore the\nlimitations of current LLM-based guard models and provide valuable insights for\nthe future development of well-calibrated guard models toward more reliable\ncontent moderation. We also advocate for incorporating reliability evaluation\nof confidence calibration when releasing future LLM-based guard models."
                },
                "authors": [
                    {
                        "name": "Hongfu Liu"
                    },
                    {
                        "name": "Hengguan Huang"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Xiangming Gu"
                    },
                    {
                        "name": "Ye Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ye Wang"
                },
                "author": "Ye Wang",
                "arxiv_comment": "19 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10414v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10414v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10408v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10408v1",
                "updated": "2024-10-14T12:00:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    12,
                    0,
                    58,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T12:00:58Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    12,
                    0,
                    58,
                    0,
                    288,
                    0
                ],
                "title": "Medico: Towards Hallucination Detection and Correction with Multi-source\n  Evidence Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medico: Towards Hallucination Detection and Correction with Multi-source\n  Evidence Fusion"
                },
                "summary": "As we all know, hallucinations prevail in Large Language Models (LLMs), where\nthe generated content is coherent but factually incorrect, which inflicts a\nheavy blow on the widespread application of LLMs. Previous studies have shown\nthat LLMs could confidently state non-existent facts rather than answering ``I\ndon't know''. Therefore, it is necessary to resort to external knowledge to\ndetect and correct the hallucinated content. Since manual detection and\ncorrection of factual errors is labor-intensive, developing an automatic\nend-to-end hallucination-checking approach is indeed a needful thing. To this\nend, we present Medico, a Multi-source evidence fusion enhanced hallucination\ndetection and correction framework. It fuses diverse evidence from multiple\nsources, detects whether the generated content contains factual errors,\nprovides the rationale behind the judgment, and iteratively revises the\nhallucinated content. Experimental results on evidence retrieval (0.964 HR@5,\n0.908 MRR@5), hallucination detection (0.927-0.951 F1), and hallucination\ncorrection (0.973-0.979 approval rate) manifest the great potential of Medico.\nA video demo of Medico can be found at https://youtu.be/RtsO6CSesBI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As we all know, hallucinations prevail in Large Language Models (LLMs), where\nthe generated content is coherent but factually incorrect, which inflicts a\nheavy blow on the widespread application of LLMs. Previous studies have shown\nthat LLMs could confidently state non-existent facts rather than answering ``I\ndon't know''. Therefore, it is necessary to resort to external knowledge to\ndetect and correct the hallucinated content. Since manual detection and\ncorrection of factual errors is labor-intensive, developing an automatic\nend-to-end hallucination-checking approach is indeed a needful thing. To this\nend, we present Medico, a Multi-source evidence fusion enhanced hallucination\ndetection and correction framework. It fuses diverse evidence from multiple\nsources, detects whether the generated content contains factual errors,\nprovides the rationale behind the judgment, and iteratively revises the\nhallucinated content. Experimental results on evidence retrieval (0.964 HR@5,\n0.908 MRR@5), hallucination detection (0.927-0.951 F1), and hallucination\ncorrection (0.973-0.979 approval rate) manifest the great potential of Medico.\nA video demo of Medico can be found at https://youtu.be/RtsO6CSesBI."
                },
                "authors": [
                    {
                        "name": "Xinping Zhao"
                    },
                    {
                        "name": "Jindi Yu"
                    },
                    {
                        "name": "Zhenyu Liu"
                    },
                    {
                        "name": "Jifang Wang"
                    },
                    {
                        "name": "Dongfang Li"
                    },
                    {
                        "name": "Yibin Chen"
                    },
                    {
                        "name": "Baotian Hu"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "12 pages, 3 figures, 6 tables. Accepted by EMNLP 2024's demo track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10408v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.11819v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.11819v3",
                "updated": "2024-10-14T11:57:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    11,
                    57,
                    0,
                    0,
                    288,
                    0
                ],
                "published": "2023-12-19T03:24:55Z",
                "published_parsed": [
                    2023,
                    12,
                    19,
                    3,
                    24,
                    55,
                    1,
                    353,
                    0
                ],
                "title": "An Adaptive Placement and Parallelism Framework for Accelerating RLHF\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Adaptive Placement and Parallelism Framework for Accelerating RLHF\n  Training"
                },
                "summary": "Recently, ChatGPT or InstructGPT like large language models (LLM) has made a\nsignificant impact in the AI world. Many works have attempted to reproduce the\ncomplex InstructGPT's training pipeline, namely Reinforcement Learning with\nHuman Feedback (RLHF). However, the mainstream distributed RLHF training\nmethods typically adopt a fixed model placement strategy, referred to as the\nCo-located strategy. This strategy treats all four interdependent models\ninvolved in RLHF as a single entity, distributing them across all devices and\napplying parallelism techniques designed for a single model, regardless of the\nworkload heterogeneity inherent to each model. As a result, this strategy\nexacerbates the generation bottlenecks in the RLHF training and degrades the\noverall training efficiency. To address these issues, we propose a flexible\nmodel placement framework that offers two general and agile model placement\nstrategies. The Interleaving strategy helps reduce memory redundancy and\ncommunication costs of RLHF training by placing models without dependencies on\nexclusive devices with careful orchestration. On the other hand, the\nDisaggregated strategy improves the throughput of model training by separating\nthe training and inference runtime of the RLHF pipeline with additional shadow\nmodels. Furthermore, our framework provides a simple user interface and\nguidelines to easily and flexibly configure these strategies in various\ntraining scenarios. Our experiments have shown that our strategy can achieve\nnotable improvements up to 11x, compared to the current state-of-the-art (SOTA)\napproaches. The results highlight the effectiveness and adaptability of our\nmethods in accelerating the training of distributed RLHF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, ChatGPT or InstructGPT like large language models (LLM) has made a\nsignificant impact in the AI world. Many works have attempted to reproduce the\ncomplex InstructGPT's training pipeline, namely Reinforcement Learning with\nHuman Feedback (RLHF). However, the mainstream distributed RLHF training\nmethods typically adopt a fixed model placement strategy, referred to as the\nCo-located strategy. This strategy treats all four interdependent models\ninvolved in RLHF as a single entity, distributing them across all devices and\napplying parallelism techniques designed for a single model, regardless of the\nworkload heterogeneity inherent to each model. As a result, this strategy\nexacerbates the generation bottlenecks in the RLHF training and degrades the\noverall training efficiency. To address these issues, we propose a flexible\nmodel placement framework that offers two general and agile model placement\nstrategies. The Interleaving strategy helps reduce memory redundancy and\ncommunication costs of RLHF training by placing models without dependencies on\nexclusive devices with careful orchestration. On the other hand, the\nDisaggregated strategy improves the throughput of model training by separating\nthe training and inference runtime of the RLHF pipeline with additional shadow\nmodels. Furthermore, our framework provides a simple user interface and\nguidelines to easily and flexibly configure these strategies in various\ntraining scenarios. Our experiments have shown that our strategy can achieve\nnotable improvements up to 11x, compared to the current state-of-the-art (SOTA)\napproaches. The results highlight the effectiveness and adaptability of our\nmethods in accelerating the training of distributed RLHF."
                },
                "authors": [
                    {
                        "name": "Youshao Xiao"
                    },
                    {
                        "name": "Zhenglei Zhou"
                    },
                    {
                        "name": "Fagui Mao"
                    },
                    {
                        "name": "Weichang Wu"
                    },
                    {
                        "name": "Shangchun Zhao"
                    },
                    {
                        "name": "Lin Ju"
                    },
                    {
                        "name": "Lei Liang"
                    },
                    {
                        "name": "Xiaolu Zhang"
                    },
                    {
                        "name": "Jun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhou"
                },
                "author": "Jun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.11819v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.11819v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20288v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20288v2",
                "updated": "2024-10-14T11:51:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    11,
                    51,
                    47,
                    0,
                    288,
                    0
                ],
                "published": "2024-09-30T13:44:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    13,
                    44,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "LexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have made significant progress in natural\nlanguage processing tasks and demonstrate considerable potential in the legal\ndomain. However, legal applications demand high standards of accuracy,\nreliability, and fairness. Applying existing LLMs to legal systems without\ncareful evaluation of their potential and limitations could pose significant\nrisks in legal practice. To this end, we introduce a standardized comprehensive\nChinese legal benchmark LexEval. This benchmark is notable in the following\nthree aspects: (1) Ability Modeling: We propose a new taxonomy of legal\ncognitive abilities to organize different tasks. (2) Scale: To our knowledge,\nLexEval is currently the largest Chinese legal evaluation dataset, comprising\n23 tasks and 14,150 questions. (3) Data: we utilize formatted existing\ndatasets, exam datasets and newly annotated datasets by legal experts to\ncomprehensively evaluate the various capabilities of LLMs. LexEval not only\nfocuses on the ability of LLMs to apply fundamental legal knowledge but also\ndedicates efforts to examining the ethical issues involved in their\napplication. We evaluated 38 open-source and commercial LLMs and obtained some\ninteresting findings. The experiments and findings offer valuable insights into\nthe challenges and potential solutions for developing Chinese legal systems and\nLLM evaluation pipelines. The LexEval dataset and leaderboard are publicly\navailable at \\url{https://github.com/CSHaitao/LexEval} and will be continuously\nupdated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made significant progress in natural\nlanguage processing tasks and demonstrate considerable potential in the legal\ndomain. However, legal applications demand high standards of accuracy,\nreliability, and fairness. Applying existing LLMs to legal systems without\ncareful evaluation of their potential and limitations could pose significant\nrisks in legal practice. To this end, we introduce a standardized comprehensive\nChinese legal benchmark LexEval. This benchmark is notable in the following\nthree aspects: (1) Ability Modeling: We propose a new taxonomy of legal\ncognitive abilities to organize different tasks. (2) Scale: To our knowledge,\nLexEval is currently the largest Chinese legal evaluation dataset, comprising\n23 tasks and 14,150 questions. (3) Data: we utilize formatted existing\ndatasets, exam datasets and newly annotated datasets by legal experts to\ncomprehensively evaluate the various capabilities of LLMs. LexEval not only\nfocuses on the ability of LLMs to apply fundamental legal knowledge but also\ndedicates efforts to examining the ethical issues involved in their\napplication. We evaluated 38 open-source and commercial LLMs and obtained some\ninteresting findings. The experiments and findings offer valuable insights into\nthe challenges and potential solutions for developing Chinese legal systems and\nLLM evaluation pipelines. The LexEval dataset and leaderboard are publicly\navailable at \\url{https://github.com/CSHaitao/LexEval} and will be continuously\nupdated."
                },
                "authors": [
                    {
                        "name": "Haitao Li"
                    },
                    {
                        "name": "You Chen"
                    },
                    {
                        "name": "Qingyao Ai"
                    },
                    {
                        "name": "Yueyue Wu"
                    },
                    {
                        "name": "Ruizhe Zhang"
                    },
                    {
                        "name": "Yiqun Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yiqun Liu"
                },
                "author": "Yiqun Liu",
                "arxiv_comment": "NeurIPs 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20288v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20288v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10398v1",
                "updated": "2024-10-14T11:39:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    11,
                    39,
                    5,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T11:39:05Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    11,
                    39,
                    5,
                    0,
                    288,
                    0
                ],
                "title": "FairMindSim: Alignment of Behavior, Emotion, and Belief in Humans and\n  LLM Agents Amid Ethical Dilemmas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FairMindSim: Alignment of Behavior, Emotion, and Belief in Humans and\n  LLM Agents Amid Ethical Dilemmas"
                },
                "summary": "AI alignment is a pivotal issue concerning AI control and safety. It should\nconsider not only value-neutral human preferences but also moral and ethical\nconsiderations. In this study, we introduced FairMindSim, which simulates the\nmoral dilemma through a series of unfair scenarios. We used LLM agents to\nsimulate human behavior, ensuring alignment across various stages. To explore\nthe various socioeconomic motivations, which we refer to as beliefs, that drive\nboth humans and LLM agents as bystanders to intervene in unjust situations\ninvolving others, and how these beliefs interact to influence individual\nbehavior, we incorporated knowledge from relevant sociological fields and\nproposed the Belief-Reward Alignment Behavior Evolution Model (BREM) based on\nthe recursive reward model (RRM). Our findings indicate that, behaviorally,\nGPT-4o exhibits a stronger sense of social justice, while humans display a\nricher range of emotions. Additionally, we discussed the potential impact of\nemotions on behavior. This study provides a theoretical foundation for\napplications in aligning LLMs with altruistic values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI alignment is a pivotal issue concerning AI control and safety. It should\nconsider not only value-neutral human preferences but also moral and ethical\nconsiderations. In this study, we introduced FairMindSim, which simulates the\nmoral dilemma through a series of unfair scenarios. We used LLM agents to\nsimulate human behavior, ensuring alignment across various stages. To explore\nthe various socioeconomic motivations, which we refer to as beliefs, that drive\nboth humans and LLM agents as bystanders to intervene in unjust situations\ninvolving others, and how these beliefs interact to influence individual\nbehavior, we incorporated knowledge from relevant sociological fields and\nproposed the Belief-Reward Alignment Behavior Evolution Model (BREM) based on\nthe recursive reward model (RRM). Our findings indicate that, behaviorally,\nGPT-4o exhibits a stronger sense of social justice, while humans display a\nricher range of emotions. Additionally, we discussed the potential impact of\nemotions on behavior. This study provides a theoretical foundation for\napplications in aligning LLMs with altruistic values."
                },
                "authors": [
                    {
                        "name": "Yu Lei"
                    },
                    {
                        "name": "Hao Liu"
                    },
                    {
                        "name": "Chengxing Xie"
                    },
                    {
                        "name": "Songjia Liu"
                    },
                    {
                        "name": "Zhiyu Yin"
                    },
                    {
                        "name": "Canyu chen"
                    },
                    {
                        "name": "Guohao Li"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Zhen Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Wu"
                },
                "author": "Zhen Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07011v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07011v2",
                "updated": "2024-10-14T11:33:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    11,
                    33,
                    2,
                    0,
                    288,
                    0
                ],
                "published": "2024-07-09T16:29:21Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    16,
                    29,
                    21,
                    1,
                    191,
                    0
                ],
                "title": "Induction Heads as an Essential Mechanism for Pattern Matching in\n  In-context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Induction Heads as an Essential Mechanism for Pattern Matching in\n  In-context Learning"
                },
                "summary": "Large language models (LLMs) have shown a remarkable ability to learn and\nperform complex tasks through in-context learning (ICL). However, a\ncomprehensive understanding of its internal mechanisms is still lacking. This\npaper explores the role of induction heads in a few-shot ICL setting. We\nanalyse two state-of-the-art models, Llama-3-8B and InternLM2-20B on abstract\npattern recognition and NLP tasks. Our results show that even a minimal\nablation of induction heads leads to ICL performance decreases of up to ~32%\nfor abstract pattern recognition tasks, bringing the performance close to\nrandom. For NLP tasks, this ablation substantially decreases the model's\nability to benefit from examples, bringing few-shot ICL performance close to\nthat of zero-shot prompts. We further use attention knockout to disable\nspecific induction patterns, and present fine-grained evidence for the role\nthat the induction mechanism plays in ICL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown a remarkable ability to learn and\nperform complex tasks through in-context learning (ICL). However, a\ncomprehensive understanding of its internal mechanisms is still lacking. This\npaper explores the role of induction heads in a few-shot ICL setting. We\nanalyse two state-of-the-art models, Llama-3-8B and InternLM2-20B on abstract\npattern recognition and NLP tasks. Our results show that even a minimal\nablation of induction heads leads to ICL performance decreases of up to ~32%\nfor abstract pattern recognition tasks, bringing the performance close to\nrandom. For NLP tasks, this ablation substantially decreases the model's\nability to benefit from examples, bringing few-shot ICL performance close to\nthat of zero-shot prompts. We further use attention knockout to disable\nspecific induction patterns, and present fine-grained evidence for the role\nthat the induction mechanism plays in ICL."
                },
                "authors": [
                    {
                        "name": "Joy Crosbie"
                    },
                    {
                        "name": "Ekaterina Shutova"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Shutova"
                },
                "author": "Ekaterina Shutova",
                "arxiv_comment": "9 pages, 7 figures; mean-ablation experiments instead of\n  zero-ablations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07011v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07011v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10392v1",
                "updated": "2024-10-14T11:28:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    11,
                    28,
                    30,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T11:28:30Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    11,
                    28,
                    30,
                    0,
                    288,
                    0
                ],
                "title": "Optimizing Instruction Synthesis: Effective Exploration of Evolutionary\n  Space with Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Instruction Synthesis: Effective Exploration of Evolutionary\n  Space with Tree Search"
                },
                "summary": "Instruction tuning is a crucial technique for aligning language models with\nhumans' actual goals in the real world. Extensive research has highlighted the\nquality of instruction data is essential for the success of this alignment.\nHowever, creating high-quality data manually is labor-intensive and\ntime-consuming, which leads researchers to explore using LLMs to synthesize\ndata. Recent studies have focused on using a stronger LLM to iteratively\nenhance existing instruction data, showing promising results. Nevertheless,\nprevious work often lacks control over the evolution direction, resulting in\nhigh uncertainty in the data synthesis process and low-quality instructions. In\nthis paper, we introduce a general and scalable framework, IDEA-MCTS\n(Instruction Data Enhancement using Monte Carlo Tree Search), a scalable\nframework for efficiently synthesizing instructions. With tree search and\nevaluation models, it can efficiently guide each instruction to evolve into a\nhigh-quality form, aiding in instruction fine-tuning. Experimental results show\nthat IDEA-MCTS significantly enhances the seed instruction data, raising the\naverage evaluation scores of quality, diversity, and complexity from 2.19 to\n3.81. Furthermore, in open-domain benchmarks, experimental results show that\nIDEA-MCTS improves the accuracy of real-world instruction-following skills in\nLLMs by an average of 5\\% in low-resource settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning is a crucial technique for aligning language models with\nhumans' actual goals in the real world. Extensive research has highlighted the\nquality of instruction data is essential for the success of this alignment.\nHowever, creating high-quality data manually is labor-intensive and\ntime-consuming, which leads researchers to explore using LLMs to synthesize\ndata. Recent studies have focused on using a stronger LLM to iteratively\nenhance existing instruction data, showing promising results. Nevertheless,\nprevious work often lacks control over the evolution direction, resulting in\nhigh uncertainty in the data synthesis process and low-quality instructions. In\nthis paper, we introduce a general and scalable framework, IDEA-MCTS\n(Instruction Data Enhancement using Monte Carlo Tree Search), a scalable\nframework for efficiently synthesizing instructions. With tree search and\nevaluation models, it can efficiently guide each instruction to evolve into a\nhigh-quality form, aiding in instruction fine-tuning. Experimental results show\nthat IDEA-MCTS significantly enhances the seed instruction data, raising the\naverage evaluation scores of quality, diversity, and complexity from 2.19 to\n3.81. Furthermore, in open-domain benchmarks, experimental results show that\nIDEA-MCTS improves the accuracy of real-world instruction-following skills in\nLLMs by an average of 5\\% in low-resource settings."
                },
                "authors": [
                    {
                        "name": "Chenglin Li"
                    },
                    {
                        "name": "Qianglong Chen"
                    },
                    {
                        "name": "Zhi Li"
                    },
                    {
                        "name": "Feng Tao"
                    },
                    {
                        "name": "Yicheng Li"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Fei Yu"
                    },
                    {
                        "name": "Yin Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yin Zhang"
                },
                "author": "Yin Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05628v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05628v2",
                "updated": "2024-10-14T11:22:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    11,
                    22,
                    39,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-08T02:23:53Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    2,
                    23,
                    53,
                    1,
                    282,
                    0
                ],
                "title": "Versatile Motion Langauge Models for Multi-Turn Interactive Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Versatile Motion Langauge Models for Multi-Turn Interactive Agents"
                },
                "summary": "Recent advancements in large language models (LLMs) have greatly enhanced\ntheir ability to generate natural and contextually relevant text, making AI\ninteractions more human-like. However, generating and understanding interactive\nhuman-like motion, where two individuals engage in coordinated movements,\nremains a challenge due to the complexity of modeling these coordinated\ninteractions. Furthermore, a versatile model is required to handle diverse\ninteractive scenarios, such as chat systems that follow user instructions or\nadapt to their assigned role while adjusting interaction dynamics. To tackle\nthis problem, we introduce VIM, short for the Versatile Interactive Motion\nlanguage model, which integrates both language and motion modalities to\neffectively understand, generate, and control interactive motions in multi-turn\nconversational contexts. To address the scarcity of multi-turn interactive\nmotion data, we introduce a synthetic dataset, INERT-MT2, where we utilize\npre-trained models to create diverse instructional datasets with interactive\nmotion. Our approach first trains a motion tokenizer that encodes interactive\nmotions into residual discrete tokens. In the pretraining stage, the model\nlearns to align motion and text representations with these discrete tokens.\nDuring the instruction fine-tuning stage, VIM adapts to multi-turn\nconversations using the INTER-MT2 dataset. We evaluate the versatility of our\nmethod across motion-related tasks, motion to text, text to motion, reaction\ngeneration, motion editing, and reasoning about motion sequences. The results\nhighlight the versatility and effectiveness of proposed method in handling\ncomplex interactive motion synthesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have greatly enhanced\ntheir ability to generate natural and contextually relevant text, making AI\ninteractions more human-like. However, generating and understanding interactive\nhuman-like motion, where two individuals engage in coordinated movements,\nremains a challenge due to the complexity of modeling these coordinated\ninteractions. Furthermore, a versatile model is required to handle diverse\ninteractive scenarios, such as chat systems that follow user instructions or\nadapt to their assigned role while adjusting interaction dynamics. To tackle\nthis problem, we introduce VIM, short for the Versatile Interactive Motion\nlanguage model, which integrates both language and motion modalities to\neffectively understand, generate, and control interactive motions in multi-turn\nconversational contexts. To address the scarcity of multi-turn interactive\nmotion data, we introduce a synthetic dataset, INERT-MT2, where we utilize\npre-trained models to create diverse instructional datasets with interactive\nmotion. Our approach first trains a motion tokenizer that encodes interactive\nmotions into residual discrete tokens. In the pretraining stage, the model\nlearns to align motion and text representations with these discrete tokens.\nDuring the instruction fine-tuning stage, VIM adapts to multi-turn\nconversations using the INTER-MT2 dataset. We evaluate the versatility of our\nmethod across motion-related tasks, motion to text, text to motion, reaction\ngeneration, motion editing, and reasoning about motion sequences. The results\nhighlight the versatility and effectiveness of proposed method in handling\ncomplex interactive motion synthesis."
                },
                "authors": [
                    {
                        "name": "Jeongeun Park"
                    },
                    {
                        "name": "Sungjoon Choi"
                    },
                    {
                        "name": "Sangdoo Yun"
                    }
                ],
                "author_detail": {
                    "name": "Sangdoo Yun"
                },
                "author": "Sangdoo Yun",
                "arxiv_comment": "https://vim-motion-language.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05628v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05628v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10908v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10908v4",
                "updated": "2024-10-14T11:01:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    11,
                    1,
                    10,
                    0,
                    288,
                    0
                ],
                "published": "2024-06-16T12:11:46Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    12,
                    11,
                    46,
                    6,
                    168,
                    0
                ],
                "title": "Demonstration Organization via Logit Separability: Advancing In-Context\n  Learning through Multiple Class-Related Words",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demonstration Organization via Logit Separability: Advancing In-Context\n  Learning through Multiple Class-Related Words"
                },
                "summary": "Effective organization of in-context learning (ICL) demonstrations is key to\nimproving the quality of large language model (LLM) responses. To create better\ndemonstrations that guide LLM understanding, we introduce logit separability, a\ncriterion to assess the clarity of both samples and class-related words at the\nlogit level. This facilitates the optimization of sample and label selection,\nenhancing the precision of information provided in ICL demonstrations.\nAdditionally, we find that incorporating multiple class-related words for each\nsample, rather than relying on a single class name, improves performance by\noffering a broader range of label information. Building on these insights, we\npropose LICL, a logit separability-based method that jointly organizes samples\nand integrates multiple class-related words into each sample-label pair.\nEvaluations across seven classification datasets show that this approach\nsignificantly improves ICL performance by providing clearer instructions and\nricher label information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective organization of in-context learning (ICL) demonstrations is key to\nimproving the quality of large language model (LLM) responses. To create better\ndemonstrations that guide LLM understanding, we introduce logit separability, a\ncriterion to assess the clarity of both samples and class-related words at the\nlogit level. This facilitates the optimization of sample and label selection,\nenhancing the precision of information provided in ICL demonstrations.\nAdditionally, we find that incorporating multiple class-related words for each\nsample, rather than relying on a single class name, improves performance by\noffering a broader range of label information. Building on these insights, we\npropose LICL, a logit separability-based method that jointly organizes samples\nand integrates multiple class-related words into each sample-label pair.\nEvaluations across seven classification datasets show that this approach\nsignificantly improves ICL performance by providing clearer instructions and\nricher label information."
                },
                "authors": [
                    {
                        "name": "Zhu Zixiao"
                    },
                    {
                        "name": "Feng Zijian"
                    },
                    {
                        "name": "Zhou Hanzhang"
                    },
                    {
                        "name": "Qian Junlang"
                    },
                    {
                        "name": "Mao Kezhi"
                    }
                ],
                "author_detail": {
                    "name": "Mao Kezhi"
                },
                "author": "Mao Kezhi",
                "arxiv_comment": "20 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10908v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10908v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04466v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04466v2",
                "updated": "2024-10-14T10:53:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    10,
                    53,
                    29,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-06T12:42:04Z",
                "published_parsed": [
                    2024,
                    10,
                    6,
                    12,
                    42,
                    4,
                    6,
                    280,
                    0
                ],
                "title": "Large Language Model Inference Acceleration: A Comprehensive Hardware\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Inference Acceleration: A Comprehensive Hardware\n  Perspective"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious fields, from natural language understanding to text generation.\nCompared to non-generative LLMs like BERT and DeBERTa, generative LLMs like GPT\nseries and Llama series are currently the main focus due to their superior\nalgorithmic performance. The advancements in generative LLMs are closely\nintertwined with the development of hardware capabilities. Various hardware\nplatforms exhibit distinct hardware characteristics, which can help improve LLM\ninference performance. Therefore, this paper comprehensively surveys efficient\ngenerative LLM inference on different hardware platforms. First, we provide an\noverview of the algorithm architecture of mainstream generative LLMs and delve\ninto the inference process. Then, we summarize different optimization methods\nfor different platforms such as CPU, GPU, FPGA, ASIC, and PIM/NDP, and provide\ninference results for generative LLMs. Furthermore, we perform a qualitative\nand quantitative comparison of inference performance with batch sizes 1 and 8\non different hardware platforms by considering hardware power consumption,\nabsolute inference speed (tokens/s), and energy efficiency (tokens/J). We\ncompare the performance of the same optimization methods across different\nhardware platforms, the performance across different hardware platforms, and\nthe performance of different methods on the same hardware platform. This\nprovides a systematic and comprehensive summary of existing inference\nacceleration work by integrating software optimization methods and hardware\nplatforms, which can point to the future trends and potential developments of\ngenerative LLMs and hardware technology for edge-side scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious fields, from natural language understanding to text generation.\nCompared to non-generative LLMs like BERT and DeBERTa, generative LLMs like GPT\nseries and Llama series are currently the main focus due to their superior\nalgorithmic performance. The advancements in generative LLMs are closely\nintertwined with the development of hardware capabilities. Various hardware\nplatforms exhibit distinct hardware characteristics, which can help improve LLM\ninference performance. Therefore, this paper comprehensively surveys efficient\ngenerative LLM inference on different hardware platforms. First, we provide an\noverview of the algorithm architecture of mainstream generative LLMs and delve\ninto the inference process. Then, we summarize different optimization methods\nfor different platforms such as CPU, GPU, FPGA, ASIC, and PIM/NDP, and provide\ninference results for generative LLMs. Furthermore, we perform a qualitative\nand quantitative comparison of inference performance with batch sizes 1 and 8\non different hardware platforms by considering hardware power consumption,\nabsolute inference speed (tokens/s), and energy efficiency (tokens/J). We\ncompare the performance of the same optimization methods across different\nhardware platforms, the performance across different hardware platforms, and\nthe performance of different methods on the same hardware platform. This\nprovides a systematic and comprehensive summary of existing inference\nacceleration work by integrating software optimization methods and hardware\nplatforms, which can point to the future trends and potential developments of\ngenerative LLMs and hardware technology for edge-side scenarios."
                },
                "authors": [
                    {
                        "name": "Jinhao Li"
                    },
                    {
                        "name": "Jiaming Xu"
                    },
                    {
                        "name": "Shan Huang"
                    },
                    {
                        "name": "Yonghua Chen"
                    },
                    {
                        "name": "Wen Li"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Yaoxiu Lian"
                    },
                    {
                        "name": "Jiayi Pan"
                    },
                    {
                        "name": "Li Ding"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Guohao Dai"
                    }
                ],
                "author_detail": {
                    "name": "Guohao Dai"
                },
                "author": "Guohao Dai",
                "arxiv_comment": "43 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04466v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04466v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10370v1",
                "updated": "2024-10-14T10:50:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    10,
                    50,
                    16,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T10:50:16Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    10,
                    50,
                    16,
                    0,
                    288,
                    0
                ],
                "title": "Innovative Thinking, Infinite Humor: Humor Research of Large Language\n  Models through Structured Thought Leaps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Innovative Thinking, Infinite Humor: Humor Research of Large Language\n  Models through Structured Thought Leaps"
                },
                "summary": "Humor is a culturally nuanced aspect of human language that presents\nchallenges for understanding and generation, requiring participants to possess\ngood creativity and strong associative thinking. Similar to reasoning tasks\nlike solving math problems, humor generation requires continuous reflection and\nrevision to foster creative thinking, rather than relying on a sudden flash of\ninspiration like Creative Leap-of-Thought (CLoT) paradigm. Although CLoT can\nrealize the ability of remote association generation, this paradigm fails to\ngenerate humor content. Therefore, in this paper, we propose a systematic way\nof thinking about generating humor and based on it, we built Creative Leap of\nStructured Thought (CLoST) frame. First, a reward model is necessary achieve\nthe purpose of being able to correct errors, since there is currently no expert\nmodel of humor and a usable rule to determine whether a piece of content is\nhumorous. Judgement-oriented instructions are designed to improve the\ncapability of a model, and we also propose an open-domain instruction\nevolutionary method to fully unleash the potential. Then, through reinforcement\nlearning, the model learns to hone its rationales of the thought chain and\nrefine the strategies it uses. Thus, it learns to recognize and correct its\nmistakes, and finally generate the most humorous and creative answer. These\nfindings deepen our understanding of the creative capabilities of LLMs and\nprovide ways to enhance LLMs' creative abilities for cross-domain innovative\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humor is a culturally nuanced aspect of human language that presents\nchallenges for understanding and generation, requiring participants to possess\ngood creativity and strong associative thinking. Similar to reasoning tasks\nlike solving math problems, humor generation requires continuous reflection and\nrevision to foster creative thinking, rather than relying on a sudden flash of\ninspiration like Creative Leap-of-Thought (CLoT) paradigm. Although CLoT can\nrealize the ability of remote association generation, this paradigm fails to\ngenerate humor content. Therefore, in this paper, we propose a systematic way\nof thinking about generating humor and based on it, we built Creative Leap of\nStructured Thought (CLoST) frame. First, a reward model is necessary achieve\nthe purpose of being able to correct errors, since there is currently no expert\nmodel of humor and a usable rule to determine whether a piece of content is\nhumorous. Judgement-oriented instructions are designed to improve the\ncapability of a model, and we also propose an open-domain instruction\nevolutionary method to fully unleash the potential. Then, through reinforcement\nlearning, the model learns to hone its rationales of the thought chain and\nrefine the strategies it uses. Thus, it learns to recognize and correct its\nmistakes, and finally generate the most humorous and creative answer. These\nfindings deepen our understanding of the creative capabilities of LLMs and\nprovide ways to enhance LLMs' creative abilities for cross-domain innovative\napplications."
                },
                "authors": [
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Yilin Zhao"
                    },
                    {
                        "name": "Dian Li"
                    },
                    {
                        "name": "Xiaohan Wang"
                    },
                    {
                        "name": "Gang Liu"
                    },
                    {
                        "name": "Xuguang Lan"
                    },
                    {
                        "name": "Hui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hui Wang"
                },
                "author": "Hui Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10360v1",
                "updated": "2024-10-14T10:26:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    10,
                    26,
                    57,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T10:26:57Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    10,
                    26,
                    57,
                    0,
                    288,
                    0
                ],
                "title": "Parenting: Optimizing Knowledge Selection of Retrieval-Augmented\n  Language Models with Parameter Decoupling and Tailored Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parenting: Optimizing Knowledge Selection of Retrieval-Augmented\n  Language Models with Parameter Decoupling and Tailored Tuning"
                },
                "summary": "Retrieval-Augmented Generation (RAG) offers an effective solution to the\nissues faced by Large Language Models (LLMs) in hallucination generation and\nknowledge obsolescence by incorporating externally retrieved knowledge.\nHowever, due to potential conflicts between internal and external knowledge, as\nwell as retrieval noise, LLMs often struggle to effectively integrate external\nevidence, leading to a decline in performance. Although existing methods\nattempt to tackle these challenges, they often struggle to strike a balance\nbetween model adherence and robustness, resulting in significant learning\nvariance. Inspired by human cognitive processes, we propose Parenting, a novel\nframework that decouples adherence and robustness within the parameter space of\nLLMs. Specifically, Parenting utilizes a key parameter mining method based on\nforward activation gain to identify and isolate the crucial parameter units\nthat are strongly linked to adherence and robustness. Then, Parenting employs a\ntype-guided tailored tuning strategy, applying specific and appropriate\nfine-tuning methods to parameter units representing different capabilities,\naiming to achieve a balanced enhancement of adherence and robustness. Extensive\nexperiments on various datasets and models validate the effectiveness and\ngeneralizability of our methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) offers an effective solution to the\nissues faced by Large Language Models (LLMs) in hallucination generation and\nknowledge obsolescence by incorporating externally retrieved knowledge.\nHowever, due to potential conflicts between internal and external knowledge, as\nwell as retrieval noise, LLMs often struggle to effectively integrate external\nevidence, leading to a decline in performance. Although existing methods\nattempt to tackle these challenges, they often struggle to strike a balance\nbetween model adherence and robustness, resulting in significant learning\nvariance. Inspired by human cognitive processes, we propose Parenting, a novel\nframework that decouples adherence and robustness within the parameter space of\nLLMs. Specifically, Parenting utilizes a key parameter mining method based on\nforward activation gain to identify and isolate the crucial parameter units\nthat are strongly linked to adherence and robustness. Then, Parenting employs a\ntype-guided tailored tuning strategy, applying specific and appropriate\nfine-tuning methods to parameter units representing different capabilities,\naiming to achieve a balanced enhancement of adherence and robustness. Extensive\nexperiments on various datasets and models validate the effectiveness and\ngeneralizability of our methods."
                },
                "authors": [
                    {
                        "name": "Yongxin Xu"
                    },
                    {
                        "name": "Ruizhe Zhang"
                    },
                    {
                        "name": "Xinke Jiang"
                    },
                    {
                        "name": "Yujie Feng"
                    },
                    {
                        "name": "Yuzhen Xiao"
                    },
                    {
                        "name": "Xinyu Ma"
                    },
                    {
                        "name": "Runchuan Zhu"
                    },
                    {
                        "name": "Xu Chu"
                    },
                    {
                        "name": "Junfeng Zhao"
                    },
                    {
                        "name": "Yasha Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yasha Wang"
                },
                "author": "Yasha Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07609v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07609v2",
                "updated": "2024-10-14T10:19:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    10,
                    19,
                    37,
                    0,
                    288,
                    0
                ],
                "published": "2024-05-13T10:20:31Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    10,
                    20,
                    31,
                    0,
                    134,
                    0
                ],
                "title": "NoiseBench: Benchmarking the Impact of Real Label Noise on Named Entity\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NoiseBench: Benchmarking the Impact of Real Label Noise on Named Entity\n  Recognition"
                },
                "summary": "Available training data for named entity recognition (NER) often contains a\nsignificant percentage of incorrect labels for entity types and entity\nboundaries. Such label noise poses challenges for supervised learning and may\nsignificantly deteriorate model quality. To address this, prior work proposed\nvarious noise-robust learning approaches capable of learning from data with\npartially incorrect labels. These approaches are typically evaluated using\nsimulated noise where the labels in a clean dataset are automatically\ncorrupted. However, as we show in this paper, this leads to unrealistic noise\nthat is far easier to handle than real noise caused by human error or\nsemi-automatic annotation. To enable the study of the impact of various types\nof real noise, we introduce NoiseBench, an NER benchmark consisting of clean\ntraining data corrupted with 6 types of real noise, including expert errors,\ncrowdsourcing errors, automatic annotation errors and LLM errors. We present an\nanalysis that shows that real noise is significantly more challenging than\nsimulated noise, and show that current state-of-the-art models for noise-robust\nlearning fall far short of their theoretically achievable upper bound. We\nrelease NoiseBench to the research community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Available training data for named entity recognition (NER) often contains a\nsignificant percentage of incorrect labels for entity types and entity\nboundaries. Such label noise poses challenges for supervised learning and may\nsignificantly deteriorate model quality. To address this, prior work proposed\nvarious noise-robust learning approaches capable of learning from data with\npartially incorrect labels. These approaches are typically evaluated using\nsimulated noise where the labels in a clean dataset are automatically\ncorrupted. However, as we show in this paper, this leads to unrealistic noise\nthat is far easier to handle than real noise caused by human error or\nsemi-automatic annotation. To enable the study of the impact of various types\nof real noise, we introduce NoiseBench, an NER benchmark consisting of clean\ntraining data corrupted with 6 types of real noise, including expert errors,\ncrowdsourcing errors, automatic annotation errors and LLM errors. We present an\nanalysis that shows that real noise is significantly more challenging than\nsimulated noise, and show that current state-of-the-art models for noise-robust\nlearning fall far short of their theoretically achievable upper bound. We\nrelease NoiseBench to the research community."
                },
                "authors": [
                    {
                        "name": "Elena Merdjanovska"
                    },
                    {
                        "name": "Ansar Aynetdinov"
                    },
                    {
                        "name": "Alan Akbik"
                    }
                ],
                "author_detail": {
                    "name": "Alan Akbik"
                },
                "author": "Alan Akbik",
                "arxiv_comment": "data available at https://github.com/elenamer/NoiseBench; to appear\n  at EMNLP2024 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.07609v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07609v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10349v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10349v1",
                "updated": "2024-10-14T10:07:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    10,
                    7,
                    29,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T10:07:29Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    10,
                    7,
                    29,
                    0,
                    288,
                    0
                ],
                "title": "LLM-based Code-Switched Text Generation for Grammatical Error Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Code-Switched Text Generation for Grammatical Error Correction"
                },
                "summary": "With the rise of globalisation, code-switching (CSW) has become a ubiquitous\npart of multilingual conversation, posing new challenges for natural language\nprocessing (NLP), especially in Grammatical Error Correction (GEC). This work\nexplores the complexities of applying GEC systems to CSW texts. Our objectives\ninclude evaluating the performance of state-of-the-art GEC systems on an\nauthentic CSW dataset from English as a Second Language (ESL) learners,\nexploring synthetic data generation as a solution to data scarcity, and\ndeveloping a model capable of correcting grammatical errors in monolingual and\nCSW texts. We generated synthetic CSW GEC data, resulting in one of the first\nsubstantial datasets for this task, and showed that a model trained on this\ndata is capable of significant improvements over existing systems. This work\ntargets ESL learners, aiming to provide educational technologies that aid in\nthe development of their English grammatical correctness without constraining\ntheir natural multilingualism.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of globalisation, code-switching (CSW) has become a ubiquitous\npart of multilingual conversation, posing new challenges for natural language\nprocessing (NLP), especially in Grammatical Error Correction (GEC). This work\nexplores the complexities of applying GEC systems to CSW texts. Our objectives\ninclude evaluating the performance of state-of-the-art GEC systems on an\nauthentic CSW dataset from English as a Second Language (ESL) learners,\nexploring synthetic data generation as a solution to data scarcity, and\ndeveloping a model capable of correcting grammatical errors in monolingual and\nCSW texts. We generated synthetic CSW GEC data, resulting in one of the first\nsubstantial datasets for this task, and showed that a model trained on this\ndata is capable of significant improvements over existing systems. This work\ntargets ESL learners, aiming to provide educational technologies that aid in\nthe development of their English grammatical correctness without constraining\ntheir natural multilingualism."
                },
                "authors": [
                    {
                        "name": "Tom Potter"
                    },
                    {
                        "name": "Zheng Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Yuan"
                },
                "author": "Zheng Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10349v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10349v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10348v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10348v1",
                "updated": "2024-10-14T10:06:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    10,
                    6,
                    58,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T10:06:58Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    10,
                    6,
                    58,
                    0,
                    288,
                    0
                ],
                "title": "Augmenting In-Context-Learning in LLMs via Automatic Data Labeling and\n  Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmenting In-Context-Learning in LLMs via Automatic Data Labeling and\n  Refinement"
                },
                "summary": "It has been shown that Large Language Models' (LLMs) performance can be\nimproved for many tasks using Chain of Thought (CoT) or In-Context Learning\n(ICL), which involve demonstrating the steps needed to solve a task using a few\nexamples. However, while datasets with input-output pairs are relatively easy\nto produce, providing demonstrations which include intermediate steps requires\ncumbersome manual work. These steps may be executable programs, as in agentic\nflows, or step-by-step reasoning as in CoT. In this work, we propose Automatic\nData Labeling and Refinement (ADLR), a method to automatically generate and\nfilter demonstrations which include the above intermediate steps, starting from\na small seed of manually crafted examples. We demonstrate the advantage of ADLR\nin code-based table QA and mathematical reasoning, achieving up to a 5.5% gain.\nThe code implementing our method is provided in the Supplementary material and\nwill be made available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It has been shown that Large Language Models' (LLMs) performance can be\nimproved for many tasks using Chain of Thought (CoT) or In-Context Learning\n(ICL), which involve demonstrating the steps needed to solve a task using a few\nexamples. However, while datasets with input-output pairs are relatively easy\nto produce, providing demonstrations which include intermediate steps requires\ncumbersome manual work. These steps may be executable programs, as in agentic\nflows, or step-by-step reasoning as in CoT. In this work, we propose Automatic\nData Labeling and Refinement (ADLR), a method to automatically generate and\nfilter demonstrations which include the above intermediate steps, starting from\na small seed of manually crafted examples. We demonstrate the advantage of ADLR\nin code-based table QA and mathematical reasoning, achieving up to a 5.5% gain.\nThe code implementing our method is provided in the Supplementary material and\nwill be made available."
                },
                "authors": [
                    {
                        "name": "Joseph Shtok"
                    },
                    {
                        "name": "Amit Alfassy"
                    },
                    {
                        "name": "Foad Abo Dahood"
                    },
                    {
                        "name": "Eliyahu Schwartz"
                    },
                    {
                        "name": "Sivan Doveh"
                    },
                    {
                        "name": "Assaf Arbelle"
                    }
                ],
                "author_detail": {
                    "name": "Assaf Arbelle"
                },
                "author": "Assaf Arbelle",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10348v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10348v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00696v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00696v2",
                "updated": "2024-10-14T10:01:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    10,
                    1,
                    33,
                    0,
                    288,
                    0
                ],
                "published": "2024-09-01T11:24:54Z",
                "published_parsed": [
                    2024,
                    9,
                    1,
                    11,
                    24,
                    54,
                    6,
                    245,
                    0
                ],
                "title": "Polyrating: A Cost-Effective and Bias-Aware Rating System for LLM\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Polyrating: A Cost-Effective and Bias-Aware Rating System for LLM\n  Evaluation"
                },
                "summary": "Rating-based human evaluation has become an essential tool to accurately\nevaluate the impressive performance of large language models (LLMs). However,\ncurrent rating systems suffer from several important limitations: first, they\nfail to account for biases that significantly influence evaluation results,\nsecond, they require large and expensive preference datasets to obtain accurate\nratings, and third, they do not facilitate meaningful comparisons of model\nratings across different tasks. To address these issues, we introduce\nPolyrating, an expressive and flexible rating system based on maximum a\nposteriori estimation that enables a more nuanced and thorough analysis of\nmodel performance at lower costs. Polyrating can detect and quantify biases\naffecting human preferences, ensuring fairer model comparisons. Further,\nPolyrating can reduce the cost of human evaluations by up to $41\\%$ for new\nmodels and up to $77\\%$ for new tasks by leveraging existing benchmark scores.\nLastly, Polyrating enables direct comparisons of ratings across different\ntasks, providing a comprehensive understanding of an LLMs' strengths,\nweaknesses, and relative performance across different applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rating-based human evaluation has become an essential tool to accurately\nevaluate the impressive performance of large language models (LLMs). However,\ncurrent rating systems suffer from several important limitations: first, they\nfail to account for biases that significantly influence evaluation results,\nsecond, they require large and expensive preference datasets to obtain accurate\nratings, and third, they do not facilitate meaningful comparisons of model\nratings across different tasks. To address these issues, we introduce\nPolyrating, an expressive and flexible rating system based on maximum a\nposteriori estimation that enables a more nuanced and thorough analysis of\nmodel performance at lower costs. Polyrating can detect and quantify biases\naffecting human preferences, ensuring fairer model comparisons. Further,\nPolyrating can reduce the cost of human evaluations by up to $41\\%$ for new\nmodels and up to $77\\%$ for new tasks by leveraging existing benchmark scores.\nLastly, Polyrating enables direct comparisons of ratings across different\ntasks, providing a comprehensive understanding of an LLMs' strengths,\nweaknesses, and relative performance across different applications."
                },
                "authors": [
                    {
                        "name": "Jasper Dekoninck"
                    },
                    {
                        "name": "Maximilian Baader"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00696v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00696v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10347v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10347v1",
                "updated": "2024-10-14T10:00:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    10,
                    0,
                    49,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T10:00:49Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    10,
                    0,
                    49,
                    0,
                    288,
                    0
                ],
                "title": "A Unified Approach to Routing and Cascading for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Approach to Routing and Cascading for LLMs"
                },
                "summary": "The widespread applicability of large language models (LLMs) has increased\nthe availability of many fine-tuned models of various sizes targeting specific\ntasks. Given a set of such specialized models, to maximize overall performance,\nit is important to figure out the optimal strategy for selecting the right\nmodel for a given user query. An effective strategy could drastically increase\noverall performance and even offer improvements over a single large monolithic\nmodel. Existing approaches typically fall into two categories: routing, where a\nsingle model is selected for each query, and cascading, which runs a sequence\nof increasingly larger models until a satisfactory answer is obtained. However,\nboth have notable limitations: routing commits to an initial model without\nflexibility, while cascading requires executing every model in sequence, which\ncan be inefficient. Additionally, the conditions under which these strategies\nare provably optimal remain unclear. In this work, we derive optimal strategies\nfor both routing and cascading. Building on this analysis, we propose a novel\napproach called cascade routing, which combines the adaptability of routing\nwith the cost-efficiency of cascading. Our experiments demonstrate that cascade\nrouting consistently outperforms both routing and cascading across a variety of\nsettings, improving both output quality and lowering computational cost, thus\noffering a unified and efficient solution to the model selection problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread applicability of large language models (LLMs) has increased\nthe availability of many fine-tuned models of various sizes targeting specific\ntasks. Given a set of such specialized models, to maximize overall performance,\nit is important to figure out the optimal strategy for selecting the right\nmodel for a given user query. An effective strategy could drastically increase\noverall performance and even offer improvements over a single large monolithic\nmodel. Existing approaches typically fall into two categories: routing, where a\nsingle model is selected for each query, and cascading, which runs a sequence\nof increasingly larger models until a satisfactory answer is obtained. However,\nboth have notable limitations: routing commits to an initial model without\nflexibility, while cascading requires executing every model in sequence, which\ncan be inefficient. Additionally, the conditions under which these strategies\nare provably optimal remain unclear. In this work, we derive optimal strategies\nfor both routing and cascading. Building on this analysis, we propose a novel\napproach called cascade routing, which combines the adaptability of routing\nwith the cost-efficiency of cascading. Our experiments demonstrate that cascade\nrouting consistently outperforms both routing and cascading across a variety of\nsettings, improving both output quality and lowering computational cost, thus\noffering a unified and efficient solution to the model selection problem."
                },
                "authors": [
                    {
                        "name": "Jasper Dekoninck"
                    },
                    {
                        "name": "Maximilian Baader"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10347v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10347v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08442v2",
                "updated": "2024-10-14T09:58:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    9,
                    58,
                    31,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-11T01:20:49Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    1,
                    20,
                    49,
                    4,
                    285,
                    0
                ],
                "title": "JurEE not Judges: safeguarding llm interactions with small, specialised\n  Encoder Ensembles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JurEE not Judges: safeguarding llm interactions with small, specialised\n  Encoder Ensembles"
                },
                "summary": "We introduce JurEE, an ensemble of efficient, encoder-only transformer models\ndesigned to strengthen safeguards in AI-User interactions within LLM-based\nsystems. Unlike existing LLM-as-Judge methods, which often struggle with\ngeneralization across risk taxonomies and only provide textual outputs, JurEE\noffers probabilistic risk estimates across a wide range of prevalent risks. Our\napproach leverages diverse data sources and employs progressive synthetic data\ngeneration techniques, including LLM-assisted augmentation, to enhance model\nrobustness and performance. We create an in-house benchmark comprising of other\nreputable benchmarks such as the OpenAI Moderation Dataset and ToxicChat, where\nwe find JurEE significantly outperforms baseline models, demonstrating superior\naccuracy, speed, and cost-efficiency. This makes it particularly suitable for\napplications requiring stringent content moderation, such as customer-facing\nchatbots. The encoder-ensemble's modular design allows users to set tailored\nrisk thresholds, enhancing its versatility across various safety-related\napplications. JurEE's collective decision-making process, where each\nspecialized encoder model contributes to the final output, not only improves\npredictive accuracy but also enhances interpretability. This approach provides\na more efficient, performant, and economical alternative to traditional LLMs\nfor large-scale implementations requiring robust content moderation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce JurEE, an ensemble of efficient, encoder-only transformer models\ndesigned to strengthen safeguards in AI-User interactions within LLM-based\nsystems. Unlike existing LLM-as-Judge methods, which often struggle with\ngeneralization across risk taxonomies and only provide textual outputs, JurEE\noffers probabilistic risk estimates across a wide range of prevalent risks. Our\napproach leverages diverse data sources and employs progressive synthetic data\ngeneration techniques, including LLM-assisted augmentation, to enhance model\nrobustness and performance. We create an in-house benchmark comprising of other\nreputable benchmarks such as the OpenAI Moderation Dataset and ToxicChat, where\nwe find JurEE significantly outperforms baseline models, demonstrating superior\naccuracy, speed, and cost-efficiency. This makes it particularly suitable for\napplications requiring stringent content moderation, such as customer-facing\nchatbots. The encoder-ensemble's modular design allows users to set tailored\nrisk thresholds, enhancing its versatility across various safety-related\napplications. JurEE's collective decision-making process, where each\nspecialized encoder model contributes to the final output, not only improves\npredictive accuracy but also enhances interpretability. This approach provides\na more efficient, performant, and economical alternative to traditional LLMs\nfor large-scale implementations requiring robust content moderation."
                },
                "authors": [
                    {
                        "name": "Dom Nasrabadi"
                    }
                ],
                "author_detail": {
                    "name": "Dom Nasrabadi"
                },
                "author": "Dom Nasrabadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10343v1",
                "updated": "2024-10-14T09:58:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    9,
                    58,
                    29,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T09:58:29Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    9,
                    58,
                    29,
                    0,
                    288,
                    0
                ],
                "title": "Locking Down the Finetuned LLMs Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locking Down the Finetuned LLMs Safety"
                },
                "summary": "Fine-tuning large language models (LLMs) on additional datasets is often\nnecessary to optimize them for specific downstream tasks. However, existing\nsafety alignment measures, which restrict harmful behavior during inference,\nare insufficient to mitigate safety risks during fine-tuning. Alarmingly,\nfine-tuning with just 10 toxic sentences can make models comply with harmful\ninstructions. We introduce SafetyLock, a novel alignment intervention method\nthat maintains robust safety post-fine-tuning through efficient and\ntransferable mechanisms. SafetyLock leverages our discovery that fine-tuned\nmodels retain similar safety-related activation representations to their base\nmodels. This insight enables us to extract what we term the Meta-SafetyLock, a\nset of safety bias directions representing key activation patterns associated\nwith safe responses in the original model. We can then apply these directions\nuniversally to fine-tuned models to enhance their safety. By searching for\nactivation directions across multiple token dimensions, SafetyLock achieves\nenhanced robustness and transferability. SafetyLock re-aligns fine-tuned models\nin under 0.01 seconds without additional computational cost. Our experiments\ndemonstrate that SafetyLock can reduce the harmful instruction response rate\nfrom 60% to below 1% in toxic fine-tuned models. It surpasses traditional\nmethods in both performance and efficiency, offering a scalable, non-invasive\nsolution for ensuring the safety of customized LLMs. Our analysis across\nvarious fine-tuning scenarios confirms SafetyLock's robustness, advocating its\nintegration into safety protocols for aligned LLMs. The code is released at\nhttps://github.com/zhu-minjun/SafetyLock.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) on additional datasets is often\nnecessary to optimize them for specific downstream tasks. However, existing\nsafety alignment measures, which restrict harmful behavior during inference,\nare insufficient to mitigate safety risks during fine-tuning. Alarmingly,\nfine-tuning with just 10 toxic sentences can make models comply with harmful\ninstructions. We introduce SafetyLock, a novel alignment intervention method\nthat maintains robust safety post-fine-tuning through efficient and\ntransferable mechanisms. SafetyLock leverages our discovery that fine-tuned\nmodels retain similar safety-related activation representations to their base\nmodels. This insight enables us to extract what we term the Meta-SafetyLock, a\nset of safety bias directions representing key activation patterns associated\nwith safe responses in the original model. We can then apply these directions\nuniversally to fine-tuned models to enhance their safety. By searching for\nactivation directions across multiple token dimensions, SafetyLock achieves\nenhanced robustness and transferability. SafetyLock re-aligns fine-tuned models\nin under 0.01 seconds without additional computational cost. Our experiments\ndemonstrate that SafetyLock can reduce the harmful instruction response rate\nfrom 60% to below 1% in toxic fine-tuned models. It surpasses traditional\nmethods in both performance and efficiency, offering a scalable, non-invasive\nsolution for ensuring the safety of customized LLMs. Our analysis across\nvarious fine-tuning scenarios confirms SafetyLock's robustness, advocating its\nintegration into safety protocols for aligned LLMs. The code is released at\nhttps://github.com/zhu-minjun/SafetyLock."
                },
                "authors": [
                    {
                        "name": "Minjun Zhu"
                    },
                    {
                        "name": "Linyi Yang"
                    },
                    {
                        "name": "Yifan Wei"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Yue Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhang"
                },
                "author": "Yue Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12775v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12775v2",
                "updated": "2024-10-14T09:55:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    9,
                    55,
                    12,
                    0,
                    288,
                    0
                ],
                "published": "2024-06-18T16:44:13Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    16,
                    44,
                    13,
                    1,
                    170,
                    0
                ],
                "title": "Hopping Too Late: Exploring the Limitations of Large Language Models on\n  Multi-Hop Queries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hopping Too Late: Exploring the Limitations of Large Language Models on\n  Multi-Hop Queries"
                },
                "summary": "Large language models (LLMs) can solve complex multi-step problems, but\nlittle is known about how these computations are implemented internally.\nMotivated by this, we study how LLMs answer multi-hop queries such as \"The\nspouse of the performer of Imagine is\". These queries require two information\nextraction steps: a latent one for resolving the first hop (\"the performer of\nImagine\") into the bridge entity (John Lennon), and another for resolving the\nsecond hop (\"the spouse of John Lennon\") into the target entity (Yoko Ono).\nUnderstanding how the latent step is computed internally is key to\nunderstanding the overall computation. By carefully analyzing the internal\ncomputations of transformer-based LLMs, we discover that the bridge entity is\nresolved in the early layers of the model. Then, only after this resolution,\nthe two-hop query is solved in the later layers. Because the second hop\ncommences in later layers, there could be cases where these layers no longer\nencode the necessary knowledge for correctly predicting the answer. Motivated\nby this, we propose a novel \"back-patching\" analysis method whereby a hidden\nrepresentation from a later layer is patched back to an earlier layer. We find\nthat in up to 66% of previously incorrect cases there exists a back-patch that\nresults in the correct generation of the answer, showing that the later layers\nindeed sometimes lack the needed functionality. Overall, our methods and\nfindings open further opportunities for understanding and improving latent\nreasoning in transformer-based LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can solve complex multi-step problems, but\nlittle is known about how these computations are implemented internally.\nMotivated by this, we study how LLMs answer multi-hop queries such as \"The\nspouse of the performer of Imagine is\". These queries require two information\nextraction steps: a latent one for resolving the first hop (\"the performer of\nImagine\") into the bridge entity (John Lennon), and another for resolving the\nsecond hop (\"the spouse of John Lennon\") into the target entity (Yoko Ono).\nUnderstanding how the latent step is computed internally is key to\nunderstanding the overall computation. By carefully analyzing the internal\ncomputations of transformer-based LLMs, we discover that the bridge entity is\nresolved in the early layers of the model. Then, only after this resolution,\nthe two-hop query is solved in the later layers. Because the second hop\ncommences in later layers, there could be cases where these layers no longer\nencode the necessary knowledge for correctly predicting the answer. Motivated\nby this, we propose a novel \"back-patching\" analysis method whereby a hidden\nrepresentation from a later layer is patched back to an earlier layer. We find\nthat in up to 66% of previously incorrect cases there exists a back-patch that\nresults in the correct generation of the answer, showing that the later layers\nindeed sometimes lack the needed functionality. Overall, our methods and\nfindings open further opportunities for understanding and improving latent\nreasoning in transformer-based LLMs."
                },
                "authors": [
                    {
                        "name": "Eden Biran"
                    },
                    {
                        "name": "Daniela Gottesman"
                    },
                    {
                        "name": "Sohee Yang"
                    },
                    {
                        "name": "Mor Geva"
                    },
                    {
                        "name": "Amir Globerson"
                    }
                ],
                "author_detail": {
                    "name": "Amir Globerson"
                },
                "author": "Amir Globerson",
                "arxiv_comment": "Accepted at EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12775v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12775v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10340v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10340v1",
                "updated": "2024-10-14T09:52:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    9,
                    52,
                    32,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T09:52:32Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    9,
                    52,
                    32,
                    0,
                    288,
                    0
                ],
                "title": "Work-in-Progress: Real-Time Neural Network Inference on a Custom RISC-V\n  Multicore Vector Processor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Work-in-Progress: Real-Time Neural Network Inference on a Custom RISC-V\n  Multicore Vector Processor"
                },
                "summary": "Neural networks are increasingly used in real-time systems, such as automated\ndriving applications. This requires high-performance hardware with predictable\ntiming behavior. State-of-the-art real-time hardware is limited in memory and\ncompute resources. On the other hand, modern accelerator systems lack the\nnecessary predictability properties, mainly due to interference in the memory\nsubsystem.\n  We present a new hardware architecture with an accompanying compiler-based\ndeployment toolchain to close this gap between performance and predictability.\nThe hardware architecture consists of a multicore vector processor with\npredictable cores, each with local scratchpad memories. A central management\ncore facilitates access to shared external memory through a static schedule\ncalculated at compile-time. The presented compiler exploits the fixed data flow\nof neural networks and WCET estimates of subtasks running on individual cores\nto compute this schedule.\n  Through this approach, the WCET estimate of the overall system can be\nobtained from the subtask WCET estimates, data transfer times, and access times\nof the shared memory in conjunction with the schedule calculated by the\ncompiler.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural networks are increasingly used in real-time systems, such as automated\ndriving applications. This requires high-performance hardware with predictable\ntiming behavior. State-of-the-art real-time hardware is limited in memory and\ncompute resources. On the other hand, modern accelerator systems lack the\nnecessary predictability properties, mainly due to interference in the memory\nsubsystem.\n  We present a new hardware architecture with an accompanying compiler-based\ndeployment toolchain to close this gap between performance and predictability.\nThe hardware architecture consists of a multicore vector processor with\npredictable cores, each with local scratchpad memories. A central management\ncore facilitates access to shared external memory through a static schedule\ncalculated at compile-time. The presented compiler exploits the fixed data flow\nof neural networks and WCET estimates of subtasks running on individual cores\nto compute this schedule.\n  Through this approach, the WCET estimate of the overall system can be\nobtained from the subtask WCET estimates, data transfer times, and access times\nof the shared memory in conjunction with the schedule calculated by the\ncompiler."
                },
                "authors": [
                    {
                        "name": "Maximilian Kirschner"
                    },
                    {
                        "name": "Konstantin Dudzik"
                    },
                    {
                        "name": "Jrgen Becker"
                    }
                ],
                "author_detail": {
                    "name": "Jrgen Becker"
                },
                "author": "Jrgen Becker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10340v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10340v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10336v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10336v1",
                "updated": "2024-10-14T09:48:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    9,
                    48,
                    41,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T09:48:41Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    9,
                    48,
                    41,
                    0,
                    288,
                    0
                ],
                "title": "CoMAT: Chain of Mathematically Annotated Thought Improves Mathematical\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoMAT: Chain of Mathematically Annotated Thought Improves Mathematical\n  Reasoning"
                },
                "summary": "Mathematical reasoning remains a significant challenge for large language\nmodels (LLMs), despite progress in prompting techniques such as\nChain-of-Thought (CoT). We present Chain of Mathematically Annotated Thought\n(CoMAT), which enhances reasoning through two stages: Symbolic Conversion\n(converting natural language queries into symbolic form) and Reasoning\nExecution (deriving answers from symbolic representations). CoMAT operates\nentirely with a single LLM and without external solvers. Across four LLMs,\nCoMAT outperforms traditional CoT on six out of seven benchmarks, achieving\ngains of 4.48% on MMLU-Redux (MATH) and 4.58% on GaoKao MCQ. In addition to\nimproved performance, CoMAT ensures faithfulness and verifiability, offering a\ntransparent reasoning process for complex mathematical tasks",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical reasoning remains a significant challenge for large language\nmodels (LLMs), despite progress in prompting techniques such as\nChain-of-Thought (CoT). We present Chain of Mathematically Annotated Thought\n(CoMAT), which enhances reasoning through two stages: Symbolic Conversion\n(converting natural language queries into symbolic form) and Reasoning\nExecution (deriving answers from symbolic representations). CoMAT operates\nentirely with a single LLM and without external solvers. Across four LLMs,\nCoMAT outperforms traditional CoT on six out of seven benchmarks, achieving\ngains of 4.48% on MMLU-Redux (MATH) and 4.58% on GaoKao MCQ. In addition to\nimproved performance, CoMAT ensures faithfulness and verifiability, offering a\ntransparent reasoning process for complex mathematical tasks"
                },
                "authors": [
                    {
                        "name": "Joshua Ong Jun Leang"
                    },
                    {
                        "name": "Aryo Pradipta Gema"
                    },
                    {
                        "name": "Shay B. Cohen"
                    }
                ],
                "author_detail": {
                    "name": "Shay B. Cohen"
                },
                "author": "Shay B. Cohen",
                "arxiv_comment": "8 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10336v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10336v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]