[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.04449v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04449v2",
                "updated": "2025-08-06T16:57:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    57,
                    39,
                    2,
                    218,
                    0
                ],
                "published": "2024-12-05T18:58:03Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    58,
                    3,
                    3,
                    340,
                    0
                ],
                "title": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay"
                },
                "summary": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. In this paper, we propose p-MoD, an efficient MLLM\narchitecture that significantly reduces training and inference costs while\nmaintaining model performance. The majority of computation in MLLMs stems from\nthe overwhelming volume of vision tokens processed by the transformer-based\nLLM. Accordingly, we leverage the Mixture-of-Depths (MoD) mechanism, where each\nLLM layer selects essential vision tokens to process while skipping redundant\nones. However, integrating MoD into MLLMs is non-trivial. To address the\nchallenges of training and inference stability as well as limited training\ndata, we adapt the MoD module with two novel designs: tanh-gated weight\nnormalization (TanhNorm) and symmetric token reweighting (STRing). Moreover, we\nobserve that vision tokens exhibit higher redundancy in deeper layers and thus\ndesign a progressive ratio decay (PRD) strategy, which gradually reduces the\ntoken retention ratio layer by layer, employing a shifted cosine schedule. This\ncrucial design fully unleashes the potential of MoD, significantly boosting the\nefficiency and performance of our models. Extensive experiments on two baseline\nmodels across 15 benchmarks show that our model matches or even surpasses the\nperformance of corresponding baselines, while requiring only 55.6% TFLOPs and\n53.7% KV cache storage during inference, and 77.7% GPU hours during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. In this paper, we propose p-MoD, an efficient MLLM\narchitecture that significantly reduces training and inference costs while\nmaintaining model performance. The majority of computation in MLLMs stems from\nthe overwhelming volume of vision tokens processed by the transformer-based\nLLM. Accordingly, we leverage the Mixture-of-Depths (MoD) mechanism, where each\nLLM layer selects essential vision tokens to process while skipping redundant\nones. However, integrating MoD into MLLMs is non-trivial. To address the\nchallenges of training and inference stability as well as limited training\ndata, we adapt the MoD module with two novel designs: tanh-gated weight\nnormalization (TanhNorm) and symmetric token reweighting (STRing). Moreover, we\nobserve that vision tokens exhibit higher redundancy in deeper layers and thus\ndesign a progressive ratio decay (PRD) strategy, which gradually reduces the\ntoken retention ratio layer by layer, employing a shifted cosine schedule. This\ncrucial design fully unleashes the potential of MoD, significantly boosting the\nefficiency and performance of our models. Extensive experiments on two baseline\nmodels across 15 benchmarks show that our model matches or even surpasses the\nperformance of corresponding baselines, while requiring only 55.6% TFLOPs and\n53.7% KV cache storage during inference, and 77.7% GPU hours during training."
                },
                "authors": [
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Desen Meng"
                    },
                    {
                        "name": "Zhengming Zhang"
                    },
                    {
                        "name": "Zhenpeng Huang"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Limin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Limin Wang"
                },
                "author": "Limin Wang",
                "arxiv_comment": "Accepted by ICCV 2025; Code released at\n  https://github.com/MCG-NJU/p-MoD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04449v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04449v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04581v1",
                "updated": "2025-08-06T16:06:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    6,
                    43,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T16:06:43Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    6,
                    43,
                    2,
                    218,
                    0
                ],
                "title": "Share Your Attention: Transformer Weight Sharing via Matrix-based\n  Dictionary Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Share Your Attention: Transformer Weight Sharing via Matrix-based\n  Dictionary Learning"
                },
                "summary": "Large language models (LLMs) have revolutionized AI applications, yet their\nhigh computational and memory demands hinder their widespread deployment.\nExisting compression techniques focus on intra-block optimizations (e.g.\nlow-rank approximation, attention head pruning), while the repetitive layered\nstructure of transformers implies significant inter-block redundancy - a\ndimension largely unexplored beyond key-value (KV) caching. Inspired by\ndictionary learning in CNNs, we propose a framework for structured weight\nsharing across transformer layers. Our approach decomposes attention projection\nmatrices into shared dictionary atoms, reducing the attention module's\nparameters by 66.7% while achieving on-par performance. Unlike complex methods\nrequiring distillation or architectural changes, MASA (Matrix Atom Sharing in\nAttention) operates as a drop-in replacement - trained with standard optimizers\n- and represents each layer's weights as linear combinations of shared matrix\natoms. Experiments across scales (100M-700M parameters) show that MASA achieves\nbetter benchmark accuracy and perplexity than grouped-query attention (GQA),\nlow-rank baselines and recently proposed Repeat-all-over/Sequential sharing at\ncomparable parameter budgets. Ablation studies confirm robustness to the\ndictionary size and the efficacy of shared representations in capturing\ncross-layer statistical regularities. Extending to Vision Transformers (ViT),\nMASA matches performance metrics on image classification and detection tasks\nwith 66.7% fewer attention parameters. By combining dictionary learning\nstrategies with transformer efficiency, MASA offers a scalable blueprint for\nparameter-efficient models without sacrificing performance. Finally, we\ninvestigate the possibility of employing MASA on pretrained LLMs to reduce\ntheir number of parameters without experiencing any significant drop in their\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized AI applications, yet their\nhigh computational and memory demands hinder their widespread deployment.\nExisting compression techniques focus on intra-block optimizations (e.g.\nlow-rank approximation, attention head pruning), while the repetitive layered\nstructure of transformers implies significant inter-block redundancy - a\ndimension largely unexplored beyond key-value (KV) caching. Inspired by\ndictionary learning in CNNs, we propose a framework for structured weight\nsharing across transformer layers. Our approach decomposes attention projection\nmatrices into shared dictionary atoms, reducing the attention module's\nparameters by 66.7% while achieving on-par performance. Unlike complex methods\nrequiring distillation or architectural changes, MASA (Matrix Atom Sharing in\nAttention) operates as a drop-in replacement - trained with standard optimizers\n- and represents each layer's weights as linear combinations of shared matrix\natoms. Experiments across scales (100M-700M parameters) show that MASA achieves\nbetter benchmark accuracy and perplexity than grouped-query attention (GQA),\nlow-rank baselines and recently proposed Repeat-all-over/Sequential sharing at\ncomparable parameter budgets. Ablation studies confirm robustness to the\ndictionary size and the efficacy of shared representations in capturing\ncross-layer statistical regularities. Extending to Vision Transformers (ViT),\nMASA matches performance metrics on image classification and detection tasks\nwith 66.7% fewer attention parameters. By combining dictionary learning\nstrategies with transformer efficiency, MASA offers a scalable blueprint for\nparameter-efficient models without sacrificing performance. Finally, we\ninvestigate the possibility of employing MASA on pretrained LLMs to reduce\ntheir number of parameters without experiencing any significant drop in their\nperformance."
                },
                "authors": [
                    {
                        "name": "Magauiya Zhussip"
                    },
                    {
                        "name": "Dmitriy Shopkhoev"
                    },
                    {
                        "name": "Ammar Ali"
                    },
                    {
                        "name": "Stamatios Lefkimmiatis"
                    }
                ],
                "author_detail": {
                    "name": "Stamatios Lefkimmiatis"
                },
                "author": "Stamatios Lefkimmiatis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.01516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.01516v3",
                "updated": "2025-08-06T15:38:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    15,
                    38,
                    6,
                    2,
                    218,
                    0
                ],
                "published": "2023-05-02T15:27:16Z",
                "published_parsed": [
                    2023,
                    5,
                    2,
                    15,
                    27,
                    16,
                    1,
                    122,
                    0
                ],
                "title": "From FASTER to F2: Evolving Concurrent Key-Value Store Designs for Large\n  Skewed Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From FASTER to F2: Evolving Concurrent Key-Value Store Designs for Large\n  Skewed Workloads"
                },
                "summary": "Modern large-scale services such as search engines, messaging platforms, and\nserverless functions, rely on key-value (KV) stores to maintain high\nperformance at scale. When such services are deployed in constrained memory\nenvironments, they present challenging requirements: point operations requiring\nhigh throughput, working sets much larger than main memory, and natural skew in\nkey access patterns. Traditional KV stores, based on LSM- and B-Trees, have\nbeen widely used to handle such use cases, but they often suffer from\nsuboptimal use of modern hardware resources. The FASTER project, developed as a\nhigh-performance open-source KV storage library, has demonstrated remarkable\nsuccess in both in-memory and hybrid storage environments. However, when tasked\nwith serving large skewed workloads, it faced challenges, including high\nindexing and compactions overheads, and inefficient management of\nnon-overlapping read-hot and write-hot working sets.\n  In this paper, we introduce F2 (for FASTER v2), an evolution of FASTER\ndesigned to meet the requirements of large skewed workloads common in industry\napplications. F2 adopts a two-tier record-oriented design to handle\nlarger-than-memory skewed workloads, along with new concurrent latch-free\nmechanisms and components to maximize performance on modern hardware. To\nrealize this design, F2 tackles key challenges and introduces several\ninnovations, including new latch-free algorithms for multi-threaded log\ncompaction, a two-level hash index to reduce indexing overhead for cold\nrecords, and a read-cache for serving read-hot records. Our evaluation shows\nthat F2 achieves 2-11.9x better throughput compared to existing KV stores,\neffectively serving the target workload. F2 is open-source and available as\npart of the FASTER project.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large-scale services such as search engines, messaging platforms, and\nserverless functions, rely on key-value (KV) stores to maintain high\nperformance at scale. When such services are deployed in constrained memory\nenvironments, they present challenging requirements: point operations requiring\nhigh throughput, working sets much larger than main memory, and natural skew in\nkey access patterns. Traditional KV stores, based on LSM- and B-Trees, have\nbeen widely used to handle such use cases, but they often suffer from\nsuboptimal use of modern hardware resources. The FASTER project, developed as a\nhigh-performance open-source KV storage library, has demonstrated remarkable\nsuccess in both in-memory and hybrid storage environments. However, when tasked\nwith serving large skewed workloads, it faced challenges, including high\nindexing and compactions overheads, and inefficient management of\nnon-overlapping read-hot and write-hot working sets.\n  In this paper, we introduce F2 (for FASTER v2), an evolution of FASTER\ndesigned to meet the requirements of large skewed workloads common in industry\napplications. F2 adopts a two-tier record-oriented design to handle\nlarger-than-memory skewed workloads, along with new concurrent latch-free\nmechanisms and components to maximize performance on modern hardware. To\nrealize this design, F2 tackles key challenges and introduces several\ninnovations, including new latch-free algorithms for multi-threaded log\ncompaction, a two-level hash index to reduce indexing overhead for cold\nrecords, and a read-cache for serving read-hot records. Our evaluation shows\nthat F2 achieves 2-11.9x better throughput compared to existing KV stores,\neffectively serving the target workload. F2 is open-source and available as\npart of the FASTER project."
                },
                "authors": [
                    {
                        "name": "Konstantinos Kanellis"
                    },
                    {
                        "name": "Badrish Chandramouli"
                    },
                    {
                        "name": "Ted Hart"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "arxiv_comment": "Proceedings of the VLDB Endowment 18 (VLDB'25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.01516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.01516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04462v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04462v1",
                "updated": "2025-08-06T14:02:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    2,
                    10,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T14:02:10Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    2,
                    10,
                    2,
                    218,
                    0
                ],
                "title": "CARD: Cache-Assisted Parallel Speculative Decoding for Efficient Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CARD: Cache-Assisted Parallel Speculative Decoding for Efficient Large\n  Language Model Inference"
                },
                "summary": "Speculative decoding (SD), where an extra draft model first provides multiple\ndraft tokens and the original target model then verifies these tokens in\nparallel, has shown great power for LLM inference acceleration. However,\nexisting SD methods must adhere to the 'draft-then-verify' paradigm, which\nforces drafting and verification processes to execute sequentially during SD,\nresulting in inefficient inference performance and limiting the size of the\ndraft model. Furthermore, once a single token in the candidate sequence is\nrejected during the drafting process, all subsequent candidate tokens must be\ndiscarded, leading to inefficient drafting. To address these challenges, we\npropose a cache-based parallel speculative decoding framework employing a\n'query-and-correct' paradigm. Specifically, CARD decouples drafting and\nverification: the draft model generates candidate tokens to populate a shared\ncache, while the target model concurrently rectifies the draft model's\ngeneration direction. This effectively enables the target model to perform\ninference at speed approaching that of the draft model. Our approach achieves\nup to 4.83 speedup over vanilla decoding without requiring fine-tuning of\neither the draft or target models. Our code is available at\nhttps://github.com/hunzhizi/CARD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding (SD), where an extra draft model first provides multiple\ndraft tokens and the original target model then verifies these tokens in\nparallel, has shown great power for LLM inference acceleration. However,\nexisting SD methods must adhere to the 'draft-then-verify' paradigm, which\nforces drafting and verification processes to execute sequentially during SD,\nresulting in inefficient inference performance and limiting the size of the\ndraft model. Furthermore, once a single token in the candidate sequence is\nrejected during the drafting process, all subsequent candidate tokens must be\ndiscarded, leading to inefficient drafting. To address these challenges, we\npropose a cache-based parallel speculative decoding framework employing a\n'query-and-correct' paradigm. Specifically, CARD decouples drafting and\nverification: the draft model generates candidate tokens to populate a shared\ncache, while the target model concurrently rectifies the draft model's\ngeneration direction. This effectively enables the target model to perform\ninference at speed approaching that of the draft model. Our approach achieves\nup to 4.83 speedup over vanilla decoding without requiring fine-tuning of\neither the draft or target models. Our code is available at\nhttps://github.com/hunzhizi/CARD."
                },
                "authors": [
                    {
                        "name": "Enyu Zhou"
                    },
                    {
                        "name": "Kai Sheng"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Xin He"
                    }
                ],
                "author_detail": {
                    "name": "Xin He"
                },
                "author": "Xin He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04462v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04462v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24007v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24007v3",
                "updated": "2025-08-06T11:46:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    46,
                    11,
                    2,
                    218,
                    0
                ],
                "published": "2025-03-31T12:32:23Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    32,
                    23,
                    0,
                    90,
                    0
                ],
                "title": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting"
                },
                "summary": "In practical time series forecasting, covariates provide rich contextual\ninformation that can potentially enhance the forecast of target variables.\nAlthough some covariates extend into the future forecasting horizon (e.g.,\ncalendar events, discount schedules), most multivariate models fail to leverage\nthis pivotal insight due to the length discrepancy with target variables.\nAdditionally, capturing the dependency between target variables and covariates\nis non-trivial, as models must precisely reflect the local impact of covariates\nwhile also capturing global cross-variate dependencies. To overcome these\nchallenges, we propose CITRAS, a decoder-only Transformer that flexibly\nleverages multiple targets, past covariates, and future covariates. While\npreserving strong autoregressive capabilities, CITRAS introduces two novel\nmechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift and\nAttention Score Smoothing. KV Shift seamlessly incorporates future covariates\ninto the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing refines locally accurate\npatch-wise cross-variate dependencies into global variate-level dependencies by\nsmoothing the past series of attention scores. Experimentally, CITRAS\noutperforms state-of-the-art models on thirteen real-world benchmarks from both\ncovariate-informed and multivariate settings, demonstrating its versatile\nability to leverage cross-variate and cross-time dependencies for improved\nforecasting accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In practical time series forecasting, covariates provide rich contextual\ninformation that can potentially enhance the forecast of target variables.\nAlthough some covariates extend into the future forecasting horizon (e.g.,\ncalendar events, discount schedules), most multivariate models fail to leverage\nthis pivotal insight due to the length discrepancy with target variables.\nAdditionally, capturing the dependency between target variables and covariates\nis non-trivial, as models must precisely reflect the local impact of covariates\nwhile also capturing global cross-variate dependencies. To overcome these\nchallenges, we propose CITRAS, a decoder-only Transformer that flexibly\nleverages multiple targets, past covariates, and future covariates. While\npreserving strong autoregressive capabilities, CITRAS introduces two novel\nmechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift and\nAttention Score Smoothing. KV Shift seamlessly incorporates future covariates\ninto the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing refines locally accurate\npatch-wise cross-variate dependencies into global variate-level dependencies by\nsmoothing the past series of attention scores. Experimentally, CITRAS\noutperforms state-of-the-art models on thirteen real-world benchmarks from both\ncovariate-informed and multivariate settings, demonstrating its versatile\nability to leverage cross-variate and cross-time dependencies for improved\nforecasting accuracy."
                },
                "authors": [
                    {
                        "name": "Yosuke Yamaguchi"
                    },
                    {
                        "name": "Issei Suemitsu"
                    },
                    {
                        "name": "Wenpeng Wei"
                    }
                ],
                "author_detail": {
                    "name": "Wenpeng Wei"
                },
                "author": "Wenpeng Wei",
                "arxiv_comment": "Submission under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24007v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24007v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04257v1",
                "updated": "2025-08-06T09:40:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    9,
                    40,
                    9,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T09:40:09Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    9,
                    40,
                    9,
                    2,
                    218,
                    0
                ],
                "title": "KVSink: Understanding and Enhancing the Preservation of Attention Sinks\n  in KV Cache Quantization for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVSink: Understanding and Enhancing the Preservation of Attention Sinks\n  in KV Cache Quantization for LLMs"
                },
                "summary": "Key-Value (KV) cache quantization has become a widely adopted optimization\ntechnique for efficient large language models (LLMs) inference by reducing KV\ncache memory usage and mitigating memory-bound constraints. Recent studies have\nemphasized the importance of preserving the original precision of KVs for the\nfirst few tokens to ensure the protection of attention sinks. While this\napproach has proven effective in mitigating performance degradation, its\nunderlying principles remain insufficiently understood. Moreover, it fails to\naddress the recent discovery that attention sinks can emerge beyond the initial\ntoken positions. In this work, we elucidate the underlying mechanisms of\nattention sinks during inference by examining their role in the cross-layer\nevolution of extreme activation outliers. Additionally, we provide a\ncomprehensive analysis of the interplay between attention sinks and KV cache\nquantization. Based on our enhanced understanding, we introduce\n\\textit{\\textbf{KVSink}}, a plug-and-play method that effectively predicts sink\ntokens with negligible overhead, enabling more thorough preservation. Extensive\nexperiments demonstrate that KVSink outperforms the existing Preserve-First-N\n(PFN) strategy, offering more effective preservation of attention sinks during\nKV cache quantization. Moreover, when applied to the well-established KVQuant\nmethod, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit\nnumerical outliers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache quantization has become a widely adopted optimization\ntechnique for efficient large language models (LLMs) inference by reducing KV\ncache memory usage and mitigating memory-bound constraints. Recent studies have\nemphasized the importance of preserving the original precision of KVs for the\nfirst few tokens to ensure the protection of attention sinks. While this\napproach has proven effective in mitigating performance degradation, its\nunderlying principles remain insufficiently understood. Moreover, it fails to\naddress the recent discovery that attention sinks can emerge beyond the initial\ntoken positions. In this work, we elucidate the underlying mechanisms of\nattention sinks during inference by examining their role in the cross-layer\nevolution of extreme activation outliers. Additionally, we provide a\ncomprehensive analysis of the interplay between attention sinks and KV cache\nquantization. Based on our enhanced understanding, we introduce\n\\textit{\\textbf{KVSink}}, a plug-and-play method that effectively predicts sink\ntokens with negligible overhead, enabling more thorough preservation. Extensive\nexperiments demonstrate that KVSink outperforms the existing Preserve-First-N\n(PFN) strategy, offering more effective preservation of attention sinks during\nKV cache quantization. Moreover, when applied to the well-established KVQuant\nmethod, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit\nnumerical outliers."
                },
                "authors": [
                    {
                        "name": "Zunhai Su"
                    },
                    {
                        "name": "Kehong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Kehong Yuan"
                },
                "author": "Kehong Yuan",
                "arxiv_comment": "Published as a conference paper at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00370v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00370v2",
                "updated": "2025-08-06T08:32:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    8,
                    32,
                    53,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-01T07:03:16Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    7,
                    3,
                    16,
                    4,
                    213,
                    0
                ],
                "title": "EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level\n  Efficiency for Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level\n  Efficiency for Edge Devices"
                },
                "summary": "Deploying Transformer-based large language models (LLMs) on\nresource-constrained edge devices for long-sequence tasks remains challenging\ndue to the quadratic time complexity of self-attention and growing Key-Value\n(KV) cache demands. While existing KV cache optimizations improve memory\nefficiency, they often fail to reduce time to first token (TTFT) and may\ndegrade performance through token pruning. Alternative sequence modeling\narchitectures address some of these limitations, but typically require full\nretraining and lack infrastructure support. EdgeInfinite offers an efficient\nsolution by fine-tuning only a small subset of parameters, maintaining quality\nwhile reducing both computational and memory costs, including improved TTFT.\nHowever, its instruction-following ability is limited, and it lacks\nmobile-specific optimizations. To address these issues, we propose\nEdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning\n(S-SFT) strategy tailored to long-sequence tasks such as summarization and\nquestion answering. We further optimized EdgeInfinite-Instruct for efficient\ndeployment on edge NPUs by employing fine-grained post-training quantization\n(PTQ) to reduce computational demands while maintaining accuracy, and by\nimplementing a fixed-shape computation graph that balances memory usage and\non-device efficiency through scenario-specific customization of input token and\ncache sizes. Experiments on long-context benchmarks and real-world mobile tasks\nshow that our approach improves domain-specific performance while maintaining\nefficiency on NPU-accelerated edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying Transformer-based large language models (LLMs) on\nresource-constrained edge devices for long-sequence tasks remains challenging\ndue to the quadratic time complexity of self-attention and growing Key-Value\n(KV) cache demands. While existing KV cache optimizations improve memory\nefficiency, they often fail to reduce time to first token (TTFT) and may\ndegrade performance through token pruning. Alternative sequence modeling\narchitectures address some of these limitations, but typically require full\nretraining and lack infrastructure support. EdgeInfinite offers an efficient\nsolution by fine-tuning only a small subset of parameters, maintaining quality\nwhile reducing both computational and memory costs, including improved TTFT.\nHowever, its instruction-following ability is limited, and it lacks\nmobile-specific optimizations. To address these issues, we propose\nEdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning\n(S-SFT) strategy tailored to long-sequence tasks such as summarization and\nquestion answering. We further optimized EdgeInfinite-Instruct for efficient\ndeployment on edge NPUs by employing fine-grained post-training quantization\n(PTQ) to reduce computational demands while maintaining accuracy, and by\nimplementing a fixed-shape computation graph that balances memory usage and\non-device efficiency through scenario-specific customization of input token and\ncache sizes. Experiments on long-context benchmarks and real-world mobile tasks\nshow that our approach improves domain-specific performance while maintaining\nefficiency on NPU-accelerated edge devices."
                },
                "authors": [
                    {
                        "name": "Jiyu Chen"
                    },
                    {
                        "name": "Poh Seng Lim"
                    },
                    {
                        "name": "Shuang Peng"
                    },
                    {
                        "name": "Daxiong Luo"
                    },
                    {
                        "name": "JungHau Foo"
                    },
                    {
                        "name": "Yap Deep"
                    },
                    {
                        "name": "Timothy Lee Jun Jie"
                    },
                    {
                        "name": "Kelvin Teh Kae Wen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Danyu Feng"
                    },
                    {
                        "name": "Hao-Yun Chen"
                    },
                    {
                        "name": "Peng-Wen Chen"
                    },
                    {
                        "name": "Fangyuan Li"
                    },
                    {
                        "name": "Xiaoxin Chen"
                    },
                    {
                        "name": "Wong Wai Mun"
                    }
                ],
                "author_detail": {
                    "name": "Wong Wai Mun"
                },
                "author": "Wong Wai Mun",
                "arxiv_comment": "The data and method in the paper need to be re-audited",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00370v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00370v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03974v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03974v1",
                "updated": "2025-08-05T23:47:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    23,
                    47,
                    34,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-05T23:47:34Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    23,
                    47,
                    34,
                    1,
                    217,
                    0
                ],
                "title": "Managing Data for Scalable and Interactive Event Sequence Visualization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Managing Data for Scalable and Interactive Event Sequence Visualization"
                },
                "summary": "Parallel event sequences, such as those collected in program execution traces\nand automated manufacturing pipelines, are typically visualized as interactive\nparallel timelines. As the dataset size grows, these charts frequently\nexperience lag during common interactions such as zooming, panning, and\nfiltering. Summarization approaches can improve interaction performance, but at\nthe cost of accuracy in representation. To address this challenge, we introduce\nESeMan (Event Sequence Manager), an event sequence management system designed\nto support interactive rendering of timeline visualizations with tunable\naccuracy. ESeMan employs hierarchical data structures and intelligent caching\nto provide visualizations with only the data necessary to generate accurate\nsummarizations with significantly reduced data fetch time. We evaluate ESeMan's\nquery times against summed area tables, M4 aggregation, and statistical\nsub-sampling on a variety of program execution traces. Our results demonstrate\nESeMan provides better performance, achieving sub-100ms fetch times while\nmaintaining visualization accuracy at the pixel level. We further present our\nbenchmarking harness, enabling future performance evaluations for event\nsequence visualization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel event sequences, such as those collected in program execution traces\nand automated manufacturing pipelines, are typically visualized as interactive\nparallel timelines. As the dataset size grows, these charts frequently\nexperience lag during common interactions such as zooming, panning, and\nfiltering. Summarization approaches can improve interaction performance, but at\nthe cost of accuracy in representation. To address this challenge, we introduce\nESeMan (Event Sequence Manager), an event sequence management system designed\nto support interactive rendering of timeline visualizations with tunable\naccuracy. ESeMan employs hierarchical data structures and intelligent caching\nto provide visualizations with only the data necessary to generate accurate\nsummarizations with significantly reduced data fetch time. We evaluate ESeMan's\nquery times against summed area tables, M4 aggregation, and statistical\nsub-sampling on a variety of program execution traces. Our results demonstrate\nESeMan provides better performance, achieving sub-100ms fetch times while\nmaintaining visualization accuracy at the pixel level. We further present our\nbenchmarking harness, enabling future performance evaluations for event\nsequence visualization."
                },
                "authors": [
                    {
                        "name": "Sayef Azad Sakin"
                    },
                    {
                        "name": "Katherine E. Isaacs"
                    }
                ],
                "author_detail": {
                    "name": "Katherine E. Isaacs"
                },
                "author": "Katherine E. Isaacs",
                "arxiv_comment": "The 15th IEEE Workshop on Large Data Analysis and Visualization",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03974v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03974v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03837v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03837v1",
                "updated": "2025-08-05T18:34:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    18,
                    34,
                    48,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-05T18:34:48Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    18,
                    34,
                    48,
                    1,
                    217,
                    0
                ],
                "title": "Rhea: a Framework for Fast Design and Validation of RTL Cache-Coherent\n  Memory Subsystems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rhea: a Framework for Fast Design and Validation of RTL Cache-Coherent\n  Memory Subsystems"
                },
                "summary": "Designing and validating efficient cache-coherent memory subsystems is a\ncritical yet complex task in the development of modern multi-core\nsystem-on-chip architectures. Rhea is a unified framework that streamlines the\ndesign and system-level validation of RTL cache-coherent memory subsystems. On\nthe design side, Rhea generates synthesizable, highly configurable RTL\nsupporting various architectural parameters. On the validation side, Rhea\nintegrates Verilator's cycle-accurate RTL simulation with gem5's full-system\nsimulation, allowing realistic workloads and operating systems to run alongside\nthe actual RTL under test. We apply Rhea to design MSI-based RTL memory\nsubsystems with one and two levels of private caches and scaling up to sixteen\ncores. Their evaluation with 22 applications from state-of-the-art benchmark\nsuites shows intermediate performance relative to gem5 Ruby's MI and MOESI\nmodels. The hybrid gem5-Verilator co-simulation flow incurs a moderate\nsimulation overhead, up to 2.7 times compared to gem5 MI, but achieves higher\nfidelity by simulating real RTL hardware. This overhead decreases with scale,\ndown to 1.6 times in sixteen-core scenarios. These results demonstrate Rhea's\neffectiveness and scalability in enabling fast development of RTL\ncache-coherent memory subsystem designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing and validating efficient cache-coherent memory subsystems is a\ncritical yet complex task in the development of modern multi-core\nsystem-on-chip architectures. Rhea is a unified framework that streamlines the\ndesign and system-level validation of RTL cache-coherent memory subsystems. On\nthe design side, Rhea generates synthesizable, highly configurable RTL\nsupporting various architectural parameters. On the validation side, Rhea\nintegrates Verilator's cycle-accurate RTL simulation with gem5's full-system\nsimulation, allowing realistic workloads and operating systems to run alongside\nthe actual RTL under test. We apply Rhea to design MSI-based RTL memory\nsubsystems with one and two levels of private caches and scaling up to sixteen\ncores. Their evaluation with 22 applications from state-of-the-art benchmark\nsuites shows intermediate performance relative to gem5 Ruby's MI and MOESI\nmodels. The hybrid gem5-Verilator co-simulation flow incurs a moderate\nsimulation overhead, up to 2.7 times compared to gem5 MI, but achieves higher\nfidelity by simulating real RTL hardware. This overhead decreases with scale,\ndown to 1.6 times in sixteen-core scenarios. These results demonstrate Rhea's\neffectiveness and scalability in enabling fast development of RTL\ncache-coherent memory subsystem designs."
                },
                "authors": [
                    {
                        "name": "Davide Zoni"
                    },
                    {
                        "name": "Andrea Galimberti"
                    },
                    {
                        "name": "Adriano Guarisco"
                    }
                ],
                "author_detail": {
                    "name": "Adriano Guarisco"
                },
                "author": "Adriano Guarisco",
                "arxiv_comment": "9 pages, 13 figures, 1 table, accepted for presentation at 2025\n  International Conference on Computer-Aided Design (ICCAD), Munich, Germany,\n  October 26-30, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03837v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03837v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07120v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07120v2",
                "updated": "2025-08-05T16:17:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    16,
                    17,
                    1,
                    1,
                    217,
                    0
                ],
                "published": "2025-03-10T09:49:18Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    49,
                    18,
                    0,
                    69,
                    0
                ],
                "title": "FEB-Cache: Frequency-Guided Exposure Bias Reduction for Enhancing\n  Diffusion Transformer Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FEB-Cache: Frequency-Guided Exposure Bias Reduction for Enhancing\n  Diffusion Transformer Caching"
                },
                "summary": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis issue, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing why caching damage the generation processes. In this paper, we first\nconfirm that the cache greatly amplifies the exposure bias, resulting in a\ndecline in the generation quality. However, directly applying noise scaling is\nchallenging for this issue due to the non-smoothness of exposure bias. We found\nthat this phenomenon stems from the mismatch between its frequency response\ncharacteristics and the simple cache of Attention and MLP. Since these two\ncomponents exhibit unique preferences for frequency signals, which provides us\nwith a caching strategy to separate Attention and MLP to achieve an enhanced\nfit of exposure bias and reduce it. Based on this, we introduced FEB-Cache, a\njoint caching strategy that aligns with the non-exposed bias diffusion process\n(which gives us a higher performance cap) of caching Attention and MLP based on\nthe frequency-guided cache table. Our approach combines a comprehensive\nunderstanding of the caching mechanism and offers a new perspective on\nleveraging caching to accelerate the diffusion process. Empirical results\nindicate that FEB-Cache optimizes model performance while concurrently\nfacilitating acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis issue, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing why caching damage the generation processes. In this paper, we first\nconfirm that the cache greatly amplifies the exposure bias, resulting in a\ndecline in the generation quality. However, directly applying noise scaling is\nchallenging for this issue due to the non-smoothness of exposure bias. We found\nthat this phenomenon stems from the mismatch between its frequency response\ncharacteristics and the simple cache of Attention and MLP. Since these two\ncomponents exhibit unique preferences for frequency signals, which provides us\nwith a caching strategy to separate Attention and MLP to achieve an enhanced\nfit of exposure bias and reduce it. Based on this, we introduced FEB-Cache, a\njoint caching strategy that aligns with the non-exposed bias diffusion process\n(which gives us a higher performance cap) of caching Attention and MLP based on\nthe frequency-guided cache table. Our approach combines a comprehensive\nunderstanding of the caching mechanism and offers a new perspective on\nleveraging caching to accelerate the diffusion process. Empirical results\nindicate that FEB-Cache optimizes model performance while concurrently\nfacilitating acceleration."
                },
                "authors": [
                    {
                        "name": "Zhen Zou"
                    },
                    {
                        "name": "Feng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhao"
                },
                "author": "Feng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07120v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07120v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03321v1",
                "updated": "2025-08-05T11:00:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    11,
                    0,
                    41,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-05T11:00:41Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    11,
                    0,
                    41,
                    1,
                    217,
                    0
                ],
                "title": "Bidirectional TLS Handshake Caching for Constrained Industrial IoT\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bidirectional TLS Handshake Caching for Constrained Industrial IoT\n  Scenarios"
                },
                "summary": "While TLS has become the de-facto standard for end-to-end security, its use\nto secure critical communication in evolving industrial IoT scenarios is\nseverely limited by prevalent resource constraints of devices and networks.\nMost notably, the TLS handshake to establish secure connections incurs\nsignificant bandwidth and processing overhead that often cannot be handled in\nconstrained environments. To alleviate this situation, we present BiTHaC which\nrealizes bidirectional TLS handshake caching by exploiting that significant\nparts of repeated TLS handshakes, especially certificates, are static. Thus,\nredundant information neither needs to be transmitted nor corresponding\ncomputations performed, saving valuable bandwidth and processing resources. By\nimplementing BiTHaC for wolfSSL, we show that we can reduce the bandwidth\nconsumption of TLS handshakes by up to 61.1% and the computational overhead by\nup to 8.5%, while incurring only well-manageable memory overhead and preserving\nthe strict security guarantees of TLS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While TLS has become the de-facto standard for end-to-end security, its use\nto secure critical communication in evolving industrial IoT scenarios is\nseverely limited by prevalent resource constraints of devices and networks.\nMost notably, the TLS handshake to establish secure connections incurs\nsignificant bandwidth and processing overhead that often cannot be handled in\nconstrained environments. To alleviate this situation, we present BiTHaC which\nrealizes bidirectional TLS handshake caching by exploiting that significant\nparts of repeated TLS handshakes, especially certificates, are static. Thus,\nredundant information neither needs to be transmitted nor corresponding\ncomputations performed, saving valuable bandwidth and processing resources. By\nimplementing BiTHaC for wolfSSL, we show that we can reduce the bandwidth\nconsumption of TLS handshakes by up to 61.1% and the computational overhead by\nup to 8.5%, while incurring only well-manageable memory overhead and preserving\nthe strict security guarantees of TLS."
                },
                "authors": [
                    {
                        "name": "Jrn Bodenhausen"
                    },
                    {
                        "name": "Simon Mangel"
                    },
                    {
                        "name": "Thomas Vogt"
                    },
                    {
                        "name": "Martin Henze"
                    }
                ],
                "author_detail": {
                    "name": "Martin Henze"
                },
                "author": "Martin Henze",
                "arxiv_comment": "Accepted for publication in Proceedings of the 2025 IEEE 50th\n  Conference on Local Computer Networks (LCN)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03258v1",
                "updated": "2025-08-05T09:35:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    9,
                    35,
                    52,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-05T09:35:52Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    9,
                    35,
                    52,
                    1,
                    217,
                    0
                ],
                "title": "SmartLLMs Scheduler: A Framework for Cost-Effective LLMs Utilization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmartLLMs Scheduler: A Framework for Cost-Effective LLMs Utilization"
                },
                "summary": "Large Language Models (LLMs) such as GPT-4 and Llama have shown remarkable\ncapabilities in a variety of software engineering tasks. Despite the\nadvancements, their practical deployment faces challenges, including high\nfinancial costs, long response time, and varying performance, especially when\nhandling a large number of queries (jobs). Existing optimization strategies for\ndeploying LLMs for diverse tasks focus on static scheduling, which requires\nextensive training data for performance prediction, increasing the\ncomputational costs and limiting the applicability and flexibility. In this\npaper, we propose the SmartLLMs Scheduler (SLS), a dynamic and cost-effective\nscheduling solution. The key idea is to learn LLMs' performance on diverse\ntasks and incorporate their real-time feedback to update strategies\nperiodically. Specifically, SLS incorporates three key components, including an\nAdaptive Cache Manager, a Performance-Cost Optimized Scheduler, and a Dynamic\nUpdate Manager. The Cache Manager stores the outputs of previously processed\nqueries and employs an adaptive strategy to reduce redundant computations and\nminimize response times. For queries not found in the cache, the Scheduler\ndynamically allocates them to the most suitable LLM based on the predicted\nperformance and cost from models that take both query-specific and LLM-specific\nfeatures as input. The Update Manager continuously refines the cache and\nscheduling strategies based on real-time feedback from the assigned queries to\nenhance decision-making and adapt to evolving task characteristics. To evaluate\nthe effectiveness of SLS, we conduct extensive experiments on two LLM-based\nsoftware engineering tasks, including log parsing and code generation. The\nresults show that SLS significantly outperforms the baseline methods, achieving\nan average performance improvement of 198.82% and an average processing time\nreduction of 63.28%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) such as GPT-4 and Llama have shown remarkable\ncapabilities in a variety of software engineering tasks. Despite the\nadvancements, their practical deployment faces challenges, including high\nfinancial costs, long response time, and varying performance, especially when\nhandling a large number of queries (jobs). Existing optimization strategies for\ndeploying LLMs for diverse tasks focus on static scheduling, which requires\nextensive training data for performance prediction, increasing the\ncomputational costs and limiting the applicability and flexibility. In this\npaper, we propose the SmartLLMs Scheduler (SLS), a dynamic and cost-effective\nscheduling solution. The key idea is to learn LLMs' performance on diverse\ntasks and incorporate their real-time feedback to update strategies\nperiodically. Specifically, SLS incorporates three key components, including an\nAdaptive Cache Manager, a Performance-Cost Optimized Scheduler, and a Dynamic\nUpdate Manager. The Cache Manager stores the outputs of previously processed\nqueries and employs an adaptive strategy to reduce redundant computations and\nminimize response times. For queries not found in the cache, the Scheduler\ndynamically allocates them to the most suitable LLM based on the predicted\nperformance and cost from models that take both query-specific and LLM-specific\nfeatures as input. The Update Manager continuously refines the cache and\nscheduling strategies based on real-time feedback from the assigned queries to\nenhance decision-making and adapt to evolving task characteristics. To evaluate\nthe effectiveness of SLS, we conduct extensive experiments on two LLM-based\nsoftware engineering tasks, including log parsing and code generation. The\nresults show that SLS significantly outperforms the baseline methods, achieving\nan average performance improvement of 198.82% and an average processing time\nreduction of 63.28%."
                },
                "authors": [
                    {
                        "name": "Yueyue Liu"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Yuantian Miao"
                    }
                ],
                "author_detail": {
                    "name": "Yuantian Miao"
                },
                "author": "Yuantian Miao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02240v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02240v2",
                "updated": "2025-08-05T02:13:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    2,
                    13,
                    39,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-04T09:39:31Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    39,
                    31,
                    0,
                    216,
                    0
                ],
                "title": "Forecasting When to Forecast: Accelerating Diffusion Models with\n  Confidence-Gated Taylor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting When to Forecast: Accelerating Diffusion Models with\n  Confidence-Gated Taylor"
                },
                "summary": "Diffusion Transformers (DiTs) have demonstrated remarkable performance in\nvisual generation tasks. However, their low inference speed limits their\ndeployment in low-resource applications. Recent training-free approaches\nexploit the redundancy of features across timesteps by caching and reusing past\nrepresentations to accelerate inference. Building on this idea, TaylorSeer\ninstead uses cached features to predict future ones via Taylor expansion.\nHowever, its module-level prediction across all transformer blocks (e.g.,\nattention or feedforward modules) requires storing fine-grained intermediate\nfeatures, leading to notable memory and computation overhead. Moreover, it\nadopts a fixed caching schedule without considering the varying accuracy of\npredictions across timesteps, which can lead to degraded outputs when\nprediction fails. To address these limitations, we propose a novel approach to\nbetter leverage Taylor-based acceleration. First, we shift the Taylor\nprediction target from the module level to the last block level, significantly\nreducing the number of cached features. Furthermore, observing strong\nsequential dependencies among Transformer blocks, we propose to use the error\nbetween the Taylor-estimated and actual outputs of the first block as an\nindicator of prediction reliability. If the error is small, we trust the Taylor\nprediction for the last block; otherwise, we fall back to full computation,\nthereby enabling a dynamic caching mechanism. Empirical results show that our\nmethod achieves a better balance between speed and quality, achieving a 3.17x\nacceleration on FLUX, 2.36x on DiT, and 4.14x on Wan Video with negligible\nquality drop. The Project Page is\n\\href{https://cg-taylor-acce.github.io/CG-Taylor/}{here.}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have demonstrated remarkable performance in\nvisual generation tasks. However, their low inference speed limits their\ndeployment in low-resource applications. Recent training-free approaches\nexploit the redundancy of features across timesteps by caching and reusing past\nrepresentations to accelerate inference. Building on this idea, TaylorSeer\ninstead uses cached features to predict future ones via Taylor expansion.\nHowever, its module-level prediction across all transformer blocks (e.g.,\nattention or feedforward modules) requires storing fine-grained intermediate\nfeatures, leading to notable memory and computation overhead. Moreover, it\nadopts a fixed caching schedule without considering the varying accuracy of\npredictions across timesteps, which can lead to degraded outputs when\nprediction fails. To address these limitations, we propose a novel approach to\nbetter leverage Taylor-based acceleration. First, we shift the Taylor\nprediction target from the module level to the last block level, significantly\nreducing the number of cached features. Furthermore, observing strong\nsequential dependencies among Transformer blocks, we propose to use the error\nbetween the Taylor-estimated and actual outputs of the first block as an\nindicator of prediction reliability. If the error is small, we trust the Taylor\nprediction for the last block; otherwise, we fall back to full computation,\nthereby enabling a dynamic caching mechanism. Empirical results show that our\nmethod achieves a better balance between speed and quality, achieving a 3.17x\nacceleration on FLUX, 2.36x on DiT, and 4.14x on Wan Video with negligible\nquality drop. The Project Page is\n\\href{https://cg-taylor-acce.github.io/CG-Taylor/}{here.}"
                },
                "authors": [
                    {
                        "name": "Xiaoliu Guan"
                    },
                    {
                        "name": "Lielin Jiang"
                    },
                    {
                        "name": "Hanqi Chen"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Jiaxing Yan"
                    },
                    {
                        "name": "Guanzhong Wang"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Zetao Zhang"
                    },
                    {
                        "name": "Yu Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wu"
                },
                "author": "Yu Wu",
                "arxiv_comment": "15 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02240v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02240v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14866v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14866v5",
                "updated": "2025-08-05T00:25:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    0,
                    25,
                    53,
                    1,
                    217,
                    0
                ],
                "published": "2025-04-21T05:27:33Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    27,
                    33,
                    0,
                    111,
                    0
                ],
                "title": "GainSight: A Unified Framework for Data Lifetime Profiling and\n  Heterogeneous Memory Composition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GainSight: A Unified Framework for Data Lifetime Profiling and\n  Heterogeneous Memory Composition"
                },
                "summary": "As AI workloads drive increasing memory requirements, domain-specific\naccelerators need higher-density on-chip memory beyond what current SRAM\nscaling trends can provide. Simultaneously, the vast amounts of short-lived\ndata in these workloads make SRAM overprovisioned in retention capability. To\naddress this mismatch, we propose a wholesale shift from uniform SRAM arrays to\nheterogeneous on-chip memory, incorporating denser short-term RAM (StRAM)\ndevices whose limited retention times align with transient data lifetimes. To\nfacilitate this shift, we introduce GainSight, the first comprehensive,\nopen-source framework that aligns dynamic, fine-grained workload lifetime\nprofiles with memory device characteristics to enable generation of optimal\nStRAM memory compositions. GainSight combines retargetable profiling backends\nwith an architecture-agnostic analytical frontend. The various backends capture\ncycle-accurate data lifetimes, while the frontend correlates workload patterns\nwith StRAM retention properties to generate optimal memory compositions and\nproject performance. GainSight elevates data lifetime to a first-class design\nconsideration for next-generation AI accelerators, enabling systematic\nexploitation of data transience for improved on-chip memory density and\nefficiency. Applying GainSight to MLPerf Inference and PolyBench workloads\nreveals that 64.3% of first-level GPU cache accesses and 79.01% of systolic\narray scratchpad accesses exhibit sub-microsecond lifetimes suitable for\nhigh-density StRAM, with optimal heterogeneous on-chip memory compositions\nachieving up to 3x active energy and 4x area reductions compared to uniform\nSRAM hierarchies. To facilitate adoption and further research, GainSight is\nopen-sourced at https://gainsight.stanford.edu/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI workloads drive increasing memory requirements, domain-specific\naccelerators need higher-density on-chip memory beyond what current SRAM\nscaling trends can provide. Simultaneously, the vast amounts of short-lived\ndata in these workloads make SRAM overprovisioned in retention capability. To\naddress this mismatch, we propose a wholesale shift from uniform SRAM arrays to\nheterogeneous on-chip memory, incorporating denser short-term RAM (StRAM)\ndevices whose limited retention times align with transient data lifetimes. To\nfacilitate this shift, we introduce GainSight, the first comprehensive,\nopen-source framework that aligns dynamic, fine-grained workload lifetime\nprofiles with memory device characteristics to enable generation of optimal\nStRAM memory compositions. GainSight combines retargetable profiling backends\nwith an architecture-agnostic analytical frontend. The various backends capture\ncycle-accurate data lifetimes, while the frontend correlates workload patterns\nwith StRAM retention properties to generate optimal memory compositions and\nproject performance. GainSight elevates data lifetime to a first-class design\nconsideration for next-generation AI accelerators, enabling systematic\nexploitation of data transience for improved on-chip memory density and\nefficiency. Applying GainSight to MLPerf Inference and PolyBench workloads\nreveals that 64.3% of first-level GPU cache accesses and 79.01% of systolic\narray scratchpad accesses exhibit sub-microsecond lifetimes suitable for\nhigh-density StRAM, with optimal heterogeneous on-chip memory compositions\nachieving up to 3x active energy and 4x area reductions compared to uniform\nSRAM hierarchies. To facilitate adoption and further research, GainSight is\nopen-sourced at https://gainsight.stanford.edu/."
                },
                "authors": [
                    {
                        "name": "Peijing Li"
                    },
                    {
                        "name": "Matthew Hung"
                    },
                    {
                        "name": "Yiming Tan"
                    },
                    {
                        "name": "Konstantin Hofeld"
                    },
                    {
                        "name": "Jake Cheng Jiajun"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Lixian Yan"
                    },
                    {
                        "name": "Xinxin Wang"
                    },
                    {
                        "name": "Philip Levis"
                    },
                    {
                        "name": "H. -S. Philip Wong"
                    },
                    {
                        "name": "Thierry Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Tambe"
                },
                "author": "Thierry Tambe",
                "arxiv_comment": "14 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14866v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14866v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.7.1; B.3.1; C.3; I.6; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02558v1",
                "updated": "2025-08-04T16:14:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    14,
                    3,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T16:14:03Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    14,
                    3,
                    0,
                    216,
                    0
                ],
                "title": "Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction"
                },
                "summary": "Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and\nparallel decoding but suffer from prohibitive quadratic computational\ncomplexity and memory overhead during inference. Current caching techniques\naccelerate decoding by storing full-layer states, yet impose substantial memory\nusage that limit long-context applications. Our analysis of attention patterns\nin dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining\nsalient across decoding steps and low-relevance tokens staying unimportant,\nmotivating selective cache eviction. We propose Sparse-dLLM, the first\ntraining-free framework integrating dynamic cache eviction with sparse\nattention via delayed bidirectional sparse caching. By leveraging the stability\nof token saliency over steps, it retains critical tokens and dynamically evicts\nunimportant prefix/suffix entries using an attention-guided strategy. Extensive\nexperiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to\n10$\\times$ higher throughput than vanilla dLLMs, with comparable performance\nand similar peak memory costs, outperforming previous methods in efficiency and\neffectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and\nparallel decoding but suffer from prohibitive quadratic computational\ncomplexity and memory overhead during inference. Current caching techniques\naccelerate decoding by storing full-layer states, yet impose substantial memory\nusage that limit long-context applications. Our analysis of attention patterns\nin dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining\nsalient across decoding steps and low-relevance tokens staying unimportant,\nmotivating selective cache eviction. We propose Sparse-dLLM, the first\ntraining-free framework integrating dynamic cache eviction with sparse\nattention via delayed bidirectional sparse caching. By leveraging the stability\nof token saliency over steps, it retains critical tokens and dynamically evicts\nunimportant prefix/suffix entries using an attention-guided strategy. Extensive\nexperiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to\n10$\\times$ higher throughput than vanilla dLLMs, with comparable performance\nand similar peak memory costs, outperforming previous methods in efficiency and\neffectiveness."
                },
                "authors": [
                    {
                        "name": "Yuerong Song"
                    },
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Ruixiao Li"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Zengfeng Huang"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02401v1",
                "updated": "2025-08-04T13:26:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    26,
                    16,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T13:26:16Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    26,
                    16,
                    0,
                    216,
                    0
                ],
                "title": "CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important\n  Before Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important\n  Before Generation"
                },
                "summary": "Recent advances in large language models (LLMs) have significantly boosted\nlong-context processing. However, the increasing key-value (KV) cache size\nposes critical challenges to memory and execution efficiency. Most KV cache\ncompression methods rely on heuristic token eviction using all attention heads\nin Grouped Query Attention (GQA)-based LLMs. This method ignores the different\nfunctionalities of attention heads, leading to the eviction of critical tokens\nand thus degrades the performance of LLMs.\n  To address the issue above, instead of using all the attention heads in\nGQA-based LLMs to determine important tokens as in the previous work, we first\nidentify the attention heads in each layer that are not only capable of\nretrieving the initial and final tokens of a prompt, but also capable of\nretrieving important tokens within the text and attending to their surrounding\nsemantic context. Afterwards, we exploit such heads to determine the important\ntokens and retain their corresponding KV cache pairs. Furthermore, we analyze\nthe cache eviction error of each layer individually and introduce a\nlayer-adaptive KV cache allocation strategy. Experimental results demonstrate\nthe proposed CompressKV consistently outperforms state-of-the-art approaches\nunder various memory budgets on LongBench and Needle-in-a-Haystack benchmarks.\nOur code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have significantly boosted\nlong-context processing. However, the increasing key-value (KV) cache size\nposes critical challenges to memory and execution efficiency. Most KV cache\ncompression methods rely on heuristic token eviction using all attention heads\nin Grouped Query Attention (GQA)-based LLMs. This method ignores the different\nfunctionalities of attention heads, leading to the eviction of critical tokens\nand thus degrades the performance of LLMs.\n  To address the issue above, instead of using all the attention heads in\nGQA-based LLMs to determine important tokens as in the previous work, we first\nidentify the attention heads in each layer that are not only capable of\nretrieving the initial and final tokens of a prompt, but also capable of\nretrieving important tokens within the text and attending to their surrounding\nsemantic context. Afterwards, we exploit such heads to determine the important\ntokens and retain their corresponding KV cache pairs. Furthermore, we analyze\nthe cache eviction error of each layer individually and introduce a\nlayer-adaptive KV cache allocation strategy. Experimental results demonstrate\nthe proposed CompressKV consistently outperforms state-of-the-art approaches\nunder various memory budgets on LongBench and Needle-in-a-Haystack benchmarks.\nOur code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git."
                },
                "authors": [
                    {
                        "name": "Xiaolin Lin"
                    },
                    {
                        "name": "Jingcun Wang"
                    },
                    {
                        "name": "Olga Kondrateva"
                    },
                    {
                        "name": "Yiyu Shi"
                    },
                    {
                        "name": "Bing Li"
                    },
                    {
                        "name": "Grace Li Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Grace Li Zhang"
                },
                "author": "Grace Li Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02280v1",
                "updated": "2025-08-04T10:51:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    10,
                    51,
                    20,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T10:51:20Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    10,
                    51,
                    20,
                    0,
                    216,
                    0
                ],
                "title": "OnPair: Short Strings Compression for Fast Random Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OnPair: Short Strings Compression for Fast Random Access"
                },
                "summary": "We present OnPair, a dictionary-based compression algorithm designed to meet\nthe needs of in-memory database systems that require both high compression and\nfast random access. Existing methods either achieve strong compression ratios\nat significant computational and memory cost (e.g., BPE) or prioritize speed at\nthe expense of compression quality (e.g., FSST). OnPair bridges this gap by\nemploying a cache-friendly dictionary construction technique that incrementally\nmerges frequent adjacent substrings in a single sequential pass over a data\nsample. This enables fast, memory-efficient training without tracking global\npair positions, as required by traditional BPE. We also introduce OnPair16, a\nvariant that limits dictionary entries to 16 bytes, enabling faster parsing via\noptimized longest prefix matching. Both variants compress strings\nindependently, supporting fine-grained random access without block-level\noverhead. Experiments on real-world datasets show that OnPair and OnPair16\nachieve compression ratios comparable to BPE while significantly improving\ncompression speed and memory usage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present OnPair, a dictionary-based compression algorithm designed to meet\nthe needs of in-memory database systems that require both high compression and\nfast random access. Existing methods either achieve strong compression ratios\nat significant computational and memory cost (e.g., BPE) or prioritize speed at\nthe expense of compression quality (e.g., FSST). OnPair bridges this gap by\nemploying a cache-friendly dictionary construction technique that incrementally\nmerges frequent adjacent substrings in a single sequential pass over a data\nsample. This enables fast, memory-efficient training without tracking global\npair positions, as required by traditional BPE. We also introduce OnPair16, a\nvariant that limits dictionary entries to 16 bytes, enabling faster parsing via\noptimized longest prefix matching. Both variants compress strings\nindependently, supporting fine-grained random access without block-level\noverhead. Experiments on real-world datasets show that OnPair and OnPair16\nachieve compression ratios comparable to BPE while significantly improving\ncompression speed and memory usage."
                },
                "authors": [
                    {
                        "name": "Francesco Gargiulo"
                    },
                    {
                        "name": "Rossano Venturini"
                    }
                ],
                "author_detail": {
                    "name": "Rossano Venturini"
                },
                "author": "Rossano Venturini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.4; E.4; H.3.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02215v1",
                "updated": "2025-08-04T09:08:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    8,
                    43,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T09:08:43Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    8,
                    43,
                    0,
                    216,
                    0
                ],
                "title": "LeanK: Learnable K Cache Channel Pruning for Efficient Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LeanK: Learnable K Cache Channel Pruning for Efficient Decoding"
                },
                "summary": "Large language models (LLMs) enable long-context tasks but face efficiency\nchallenges due to the growing key-value (KV) cache. We propose LeanK, a\nlearning-based method that prunes unimportant key (K) cache channels by\nleveraging static channel sparsity. With a novel two-stage training process,\nLeanK learns channel-wise static mask that could satisfy specific sparsity\nratio and hardware alignment requirement. LeanK reduces GPU memory and\naccelerates decoding without sacrificing accuracy. Experiments demonstrate up\nto 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel\nenables 1.3x speedup for attention computation. We also provide insights into\nmodel channels and attention heads during long-context inference by analyzing\nthe learned importance distribution. Our code is available at\nhttps://aka.ms/LeanK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) enable long-context tasks but face efficiency\nchallenges due to the growing key-value (KV) cache. We propose LeanK, a\nlearning-based method that prunes unimportant key (K) cache channels by\nleveraging static channel sparsity. With a novel two-stage training process,\nLeanK learns channel-wise static mask that could satisfy specific sparsity\nratio and hardware alignment requirement. LeanK reduces GPU memory and\naccelerates decoding without sacrificing accuracy. Experiments demonstrate up\nto 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel\nenables 1.3x speedup for attention computation. We also provide insights into\nmodel channels and attention heads during long-context inference by analyzing\nthe learned importance distribution. Our code is available at\nhttps://aka.ms/LeanK."
                },
                "authors": [
                    {
                        "name": "Yike Zhang"
                    },
                    {
                        "name": "Zhiyuan He"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Jianyong Wang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19906v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19906v2",
                "updated": "2025-08-04T08:19:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    8,
                    19,
                    26,
                    0,
                    216,
                    0
                ],
                "published": "2025-07-26T10:34:53Z",
                "published_parsed": [
                    2025,
                    7,
                    26,
                    10,
                    34,
                    53,
                    5,
                    207,
                    0
                ],
                "title": "CaliDrop: KV Cache Compression with Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaliDrop: KV Cache Compression with Calibration"
                },
                "summary": "Large Language Models (LLMs) require substantial computational resources\nduring generation. While the Key-Value (KV) cache significantly accelerates\nthis process by storing attention intermediates, its memory footprint grows\nlinearly with sequence length, batch size, and model size, creating a\nbottleneck in long-context scenarios. Various KV cache compression techniques,\nincluding token eviction, quantization, and low-rank projection, have been\nproposed to mitigate this bottleneck, often complementing each other. This\npaper focuses on enhancing token eviction strategies. Token eviction leverages\nthe observation that the attention patterns are often sparse, allowing for the\nremoval of less critical KV entries to save memory. However, this reduction\nusually comes at the cost of notable accuracy degradation, particularly under\nhigh compression ratios. To address this issue, we propose \\textbf{CaliDrop}, a\nnovel strategy that enhances token eviction through calibration. Our\npreliminary experiments show that queries at nearby positions exhibit high\nsimilarity. Building on this observation, CaliDrop performs speculative\ncalibration on the discarded tokens to mitigate the accuracy loss caused by\ntoken eviction. Extensive experiments demonstrate that CaliDrop significantly\nimproves the accuracy of existing token eviction methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) require substantial computational resources\nduring generation. While the Key-Value (KV) cache significantly accelerates\nthis process by storing attention intermediates, its memory footprint grows\nlinearly with sequence length, batch size, and model size, creating a\nbottleneck in long-context scenarios. Various KV cache compression techniques,\nincluding token eviction, quantization, and low-rank projection, have been\nproposed to mitigate this bottleneck, often complementing each other. This\npaper focuses on enhancing token eviction strategies. Token eviction leverages\nthe observation that the attention patterns are often sparse, allowing for the\nremoval of less critical KV entries to save memory. However, this reduction\nusually comes at the cost of notable accuracy degradation, particularly under\nhigh compression ratios. To address this issue, we propose \\textbf{CaliDrop}, a\nnovel strategy that enhances token eviction through calibration. Our\npreliminary experiments show that queries at nearby positions exhibit high\nsimilarity. Building on this observation, CaliDrop performs speculative\ncalibration on the discarded tokens to mitigate the accuracy loss caused by\ntoken eviction. Extensive experiments demonstrate that CaliDrop significantly\nimproves the accuracy of existing token eviction methods."
                },
                "authors": [
                    {
                        "name": "Yi Su"
                    },
                    {
                        "name": "Quantong Qiu"
                    },
                    {
                        "name": "Yuechi Zhou"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Qingrong Xia"
                    },
                    {
                        "name": "Ping Li"
                    },
                    {
                        "name": "Xinyu Duan"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19906v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19906v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21492v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21492v2",
                "updated": "2025-08-04T04:48:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    4,
                    48,
                    41,
                    0,
                    216,
                    0
                ],
                "published": "2025-07-29T04:21:11Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    4,
                    21,
                    11,
                    1,
                    210,
                    0
                ],
                "title": "Bridging Cache-Friendliness and Concurrency: A Locality-Optimized\n  In-Memory B-Skiplist",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Cache-Friendliness and Concurrency: A Locality-Optimized\n  In-Memory B-Skiplist"
                },
                "summary": "Skiplists are widely used for in-memory indexing in many key-value stores,\nsuch as RocksDB and LevelDB, due to their ease of implementation and simple\nconcurrency control mechanisms. However, traditional skiplists suffer from poor\ncache locality, as they store only a single element per node, leaving\nperformance on the table. Minimizing last-level cache misses is key to\nmaximizing in-memory index performance, making high cache locality essential.\nIn this paper, we present a practical concurrent B-skiplist that enhances cache\nlocality and performance while preserving the simplicity of traditional\nskiplist structures and concurrency control schemes. Our key contributions\ninclude a top-down, single-pass insertion algorithm for B-skiplists and a\ncorresponding simple and efficient top-down concurrency control scheme. On 128\nthreads, the proposed concurrent B-skiplist achieves between 2x-9x higher\nthroughput compared to state-of-the-art concurrent skiplist implementations,\nincluding Facebook's concurrent skiplist from Folly and the Java\nConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves\ncompetitive (0.9x-1.7x) throughput on point workloads compared to\nstate-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a\nmore complete picture of the performance, we also measure the latency of\nskiplist and tree-based indices and find that the B-skiplist achieves between\n3.5x-103x lower 99% latency compared to other concurrent skiplists and between\n0.85x-64x lower 99% latency compared to tree-based indices on point workloads\nwith inserts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skiplists are widely used for in-memory indexing in many key-value stores,\nsuch as RocksDB and LevelDB, due to their ease of implementation and simple\nconcurrency control mechanisms. However, traditional skiplists suffer from poor\ncache locality, as they store only a single element per node, leaving\nperformance on the table. Minimizing last-level cache misses is key to\nmaximizing in-memory index performance, making high cache locality essential.\nIn this paper, we present a practical concurrent B-skiplist that enhances cache\nlocality and performance while preserving the simplicity of traditional\nskiplist structures and concurrency control schemes. Our key contributions\ninclude a top-down, single-pass insertion algorithm for B-skiplists and a\ncorresponding simple and efficient top-down concurrency control scheme. On 128\nthreads, the proposed concurrent B-skiplist achieves between 2x-9x higher\nthroughput compared to state-of-the-art concurrent skiplist implementations,\nincluding Facebook's concurrent skiplist from Folly and the Java\nConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves\ncompetitive (0.9x-1.7x) throughput on point workloads compared to\nstate-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a\nmore complete picture of the performance, we also measure the latency of\nskiplist and tree-based indices and find that the B-skiplist achieves between\n3.5x-103x lower 99% latency compared to other concurrent skiplists and between\n0.85x-64x lower 99% latency compared to tree-based indices on point workloads\nwith inserts."
                },
                "authors": [
                    {
                        "name": "Yicong Luo"
                    },
                    {
                        "name": "Senhe Hao"
                    },
                    {
                        "name": "Brian Wheatman"
                    },
                    {
                        "name": "Prashant Pandey"
                    },
                    {
                        "name": "Helen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Helen Xu"
                },
                "author": "Helen Xu",
                "arxiv_doi": "10.1145/3754598.3754655",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3754598.3754655",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.21492v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21492v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Original paper was accepted into ICPP 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02930v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02930v4",
                "updated": "2025-08-04T02:47:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    2,
                    47,
                    35,
                    0,
                    216,
                    0
                ],
                "published": "2024-07-03T09:02:05Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    9,
                    2,
                    5,
                    2,
                    185,
                    0
                ],
                "title": "Timely Requesting for Time-Critical Content Users in Decentralized\n  F-RANs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timely Requesting for Time-Critical Content Users in Decentralized\n  F-RANs"
                },
                "summary": "With the rising demand for high-rate and timely communications, fog radio\naccess networks (F-RANs) offer a promising solution. This work investigates age\nof information (AoI) performance in F-RANs, consisting of multiple content\nusers (CUs), enhanced remote radio heads (eRRHs), and content providers (CPs).\nTime-critical CUs need rapid content updates from CPs but cannot communicate\ndirectly with them; instead, eRRHs act as intermediaries. CUs decide whether to\nrequest content from a CP and which eRRH to send the request to, while eRRHs\ndecide whether to command CPs to update content or use cached content. We study\ntwo general classes of policies: (i) oblivious policies, where decision-making\nis independent of historical information, and (ii) non-oblivious policies,\nwhere decisions are influenced by historical information. First, we obtain\nclosed-form expressions for the average AoI of eRRHs under both policy types.\nDue to the complexity of calculating closed-form expressions for CUs, we then\nderive general upper bounds for their average AoI. Next, we identify optimal\npolicies for both types. Under both optimal policies, each CU requests content\nfrom each CP at an equal rate, consolidating all requests to a single eRRH when\ndemand is low or resources are limited, and distributing requests evenly among\neRRHs when demand is high and resources are ample. eRRHs command content from\neach CP at an equal rate under an optimal oblivious policy, while prioritize\nthe CP with the highest age under an optimal non-oblivious policy. Our\nnumerical results validate these theoretical findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rising demand for high-rate and timely communications, fog radio\naccess networks (F-RANs) offer a promising solution. This work investigates age\nof information (AoI) performance in F-RANs, consisting of multiple content\nusers (CUs), enhanced remote radio heads (eRRHs), and content providers (CPs).\nTime-critical CUs need rapid content updates from CPs but cannot communicate\ndirectly with them; instead, eRRHs act as intermediaries. CUs decide whether to\nrequest content from a CP and which eRRH to send the request to, while eRRHs\ndecide whether to command CPs to update content or use cached content. We study\ntwo general classes of policies: (i) oblivious policies, where decision-making\nis independent of historical information, and (ii) non-oblivious policies,\nwhere decisions are influenced by historical information. First, we obtain\nclosed-form expressions for the average AoI of eRRHs under both policy types.\nDue to the complexity of calculating closed-form expressions for CUs, we then\nderive general upper bounds for their average AoI. Next, we identify optimal\npolicies for both types. Under both optimal policies, each CU requests content\nfrom each CP at an equal rate, consolidating all requests to a single eRRH when\ndemand is low or resources are limited, and distributing requests evenly among\neRRHs when demand is high and resources are ample. eRRHs command content from\neach CP at an equal rate under an optimal oblivious policy, while prioritize\nthe CP with the highest age under an optimal non-oblivious policy. Our\nnumerical results validate these theoretical findings."
                },
                "authors": [
                    {
                        "name": "Xingran Chen"
                    },
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Kun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Kun Yang"
                },
                "author": "Kun Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02930v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02930v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02252v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02252v2",
                "updated": "2025-08-04T02:17:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    2,
                    17,
                    56,
                    0,
                    216,
                    0
                ],
                "published": "2024-12-03T08:29:27Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    29,
                    27,
                    1,
                    338,
                    0
                ],
                "title": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity"
                },
                "summary": "The rapid expansion of context window sizes in Large Language Models~(LLMs)\nhas enabled them to tackle increasingly complex tasks involving lengthy\ndocuments. However, this progress comes at the cost of a substantial increase\nin memory usage during inference, primarily due to the linear growth of the\nkey-value~(KV) cache. Existing KV cache compression methods often discard less\nrelevant tokens, which can lead to significant performance degradation when\ncritical information is lost. In this paper, we propose \\textsc{PoD}~(Proximal\ntokens over Distant tokens), a novel KV cache compression framework that\nallocates memory according to token importance, retaining less important tokens\nin a more compact, shared form rather than discarding them entirely. Our\napproach is motivated by two key observations: (1) proximal tokens -- those at\nthe beginning and end of the context -- are significantly more important for\nnext-token prediction, and (2) attention scores for distant tokens are highly\nredundant across consecutive layers. Leveraging these insights, \\textsc{PoD}\npreserves the full KV cache for proximal tokens, while for distant tokens, it\nshares key states across layers. Since attention scores are determined by both\nqueries and keys, sharing key states enables multiple layers to reuse a single\nset of keys for distant tokens, substantially reducing KV cache memory without\ndiscarding essential context. We further introduce a lightweight post-training\nadaptation to enable the model to adjust to this new attention-sharing\nstructure. Extensive experiments on both synthetic~(Needle in a Haystack) and\nreal-world long-context benchmarks demonstrate that \\textsc{PoD} reduces KV\ncache memory usage by up to 35\\% without compromising performance. Our method\nis orthogonal to existing token-selection-based techniques and can be combined\nwith them for further KV cache compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid expansion of context window sizes in Large Language Models~(LLMs)\nhas enabled them to tackle increasingly complex tasks involving lengthy\ndocuments. However, this progress comes at the cost of a substantial increase\nin memory usage during inference, primarily due to the linear growth of the\nkey-value~(KV) cache. Existing KV cache compression methods often discard less\nrelevant tokens, which can lead to significant performance degradation when\ncritical information is lost. In this paper, we propose \\textsc{PoD}~(Proximal\ntokens over Distant tokens), a novel KV cache compression framework that\nallocates memory according to token importance, retaining less important tokens\nin a more compact, shared form rather than discarding them entirely. Our\napproach is motivated by two key observations: (1) proximal tokens -- those at\nthe beginning and end of the context -- are significantly more important for\nnext-token prediction, and (2) attention scores for distant tokens are highly\nredundant across consecutive layers. Leveraging these insights, \\textsc{PoD}\npreserves the full KV cache for proximal tokens, while for distant tokens, it\nshares key states across layers. Since attention scores are determined by both\nqueries and keys, sharing key states enables multiple layers to reuse a single\nset of keys for distant tokens, substantially reducing KV cache memory without\ndiscarding essential context. We further introduce a lightweight post-training\nadaptation to enable the model to adjust to this new attention-sharing\nstructure. Extensive experiments on both synthetic~(Needle in a Haystack) and\nreal-world long-context benchmarks demonstrate that \\textsc{PoD} reduces KV\ncache memory usage by up to 35\\% without compromising performance. Our method\nis orthogonal to existing token-selection-based techniques and can be combined\nwith them for further KV cache compression."
                },
                "authors": [
                    {
                        "name": "Da Ma"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Situo Zhang"
                    },
                    {
                        "name": "Yuxun Miao"
                    },
                    {
                        "name": "Su Zhu"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Hongshen Xu"
                    },
                    {
                        "name": "Hanqi Li"
                    },
                    {
                        "name": "Shuai Fan"
                    },
                    {
                        "name": "Lei Pan"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "14 pages, 7 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02252v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02252v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01898v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01898v1",
                "updated": "2025-08-03T19:16:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    3,
                    19,
                    16,
                    40,
                    6,
                    215,
                    0
                ],
                "published": "2025-08-03T19:16:40Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    19,
                    16,
                    40,
                    6,
                    215,
                    0
                ],
                "title": "Revenue Optimization in Wireless Video Caching Networks: A\n  Privacy-Preserving Two-Stage Solution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revenue Optimization in Wireless Video Caching Networks: A\n  Privacy-Preserving Two-Stage Solution"
                },
                "summary": "Video caching can significantly improve delivery efficiency and enhance\nquality of video streaming, which constitutes the majority of wireless\ncommunication traffic. Due to limited cache size, caching strategies must be\ndesigned to adapt to and dynamic user demand in order to maximize system\nrevenue. The system revenue depends on the benefits of delivering the requested\nvideos and costs for (a) transporting the files to the users and (b) cache\nreplacement. Since the cache content at any point in time impacts the\nreplacement costs in the future, demand predictions over multiple cache\nplacement slots become an important prerequisite for efficient cache planning.\nMotivated by this, we introduce a novel two-stage privacy-preserving solution\nfor revenue optimization in wireless video caching networks. First, we train a\nTransformer using privacy-preserving federated learning (FL) to predict\nmulti-slot future demands. Given that prediction results are never entirely\naccurate, especially for longer horizons, we further combine global content\npopularity with per-user prediction results to estimate the content demand\ndistribution. Then, in the second stage, we leverage these estimation results\nto find caching strategies that maximize the long-term system revenue. This\nlatter problem takes on the form of a multi-stage knapsack problem, which we\nthen transform to a integer linear program. Our extensive simulation results\ndemonstrate that (i) our FL solution delivers nearly identical performance to\nthat of the ideal centralized solution and outperforms other existing caching\nmethods, and (ii) our novel revenue optimization approach provides deeper\nsystem performance insights than traditional cache hit ratio (CHR)-based\noptimization approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video caching can significantly improve delivery efficiency and enhance\nquality of video streaming, which constitutes the majority of wireless\ncommunication traffic. Due to limited cache size, caching strategies must be\ndesigned to adapt to and dynamic user demand in order to maximize system\nrevenue. The system revenue depends on the benefits of delivering the requested\nvideos and costs for (a) transporting the files to the users and (b) cache\nreplacement. Since the cache content at any point in time impacts the\nreplacement costs in the future, demand predictions over multiple cache\nplacement slots become an important prerequisite for efficient cache planning.\nMotivated by this, we introduce a novel two-stage privacy-preserving solution\nfor revenue optimization in wireless video caching networks. First, we train a\nTransformer using privacy-preserving federated learning (FL) to predict\nmulti-slot future demands. Given that prediction results are never entirely\naccurate, especially for longer horizons, we further combine global content\npopularity with per-user prediction results to estimate the content demand\ndistribution. Then, in the second stage, we leverage these estimation results\nto find caching strategies that maximize the long-term system revenue. This\nlatter problem takes on the form of a multi-stage knapsack problem, which we\nthen transform to a integer linear program. Our extensive simulation results\ndemonstrate that (i) our FL solution delivers nearly identical performance to\nthat of the ideal centralized solution and outperforms other existing caching\nmethods, and (ii) our novel revenue optimization approach provides deeper\nsystem performance insights than traditional cache hit ratio (CHR)-based\noptimization approaches."
                },
                "authors": [
                    {
                        "name": "Yijing Zhang"
                    },
                    {
                        "name": "Md-Ferdous Pervej"
                    },
                    {
                        "name": "Andreas F. Molisch"
                    }
                ],
                "author_detail": {
                    "name": "Andreas F. Molisch"
                },
                "author": "Andreas F. Molisch",
                "arxiv_comment": "Under review for possible publication in the IEEE Transactions on\n  Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01898v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01898v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01875v1",
                "updated": "2025-08-03T18:15:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    3,
                    18,
                    15,
                    42,
                    6,
                    215,
                    0
                ],
                "published": "2025-08-03T18:15:42Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    18,
                    15,
                    42,
                    6,
                    215,
                    0
                ],
                "title": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding"
                },
                "summary": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios."
                },
                "authors": [
                    {
                        "name": "Haolin Yang"
                    },
                    {
                        "name": "Feilong Tang"
                    },
                    {
                        "name": "Linxiao Zhao"
                    },
                    {
                        "name": "Xiang An"
                    },
                    {
                        "name": "Ming Hu"
                    },
                    {
                        "name": "Huifa Li"
                    },
                    {
                        "name": "Xinlin Zhuang"
                    },
                    {
                        "name": "Boqian Wang"
                    },
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Xiaofeng Zhang"
                    },
                    {
                        "name": "Abdalla Swikir"
                    },
                    {
                        "name": "Junjun He"
                    },
                    {
                        "name": "Zongyuan Ge"
                    },
                    {
                        "name": "Imran Razzak"
                    }
                ],
                "author_detail": {
                    "name": "Imran Razzak"
                },
                "author": "Imran Razzak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16607v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16607v2",
                "updated": "2025-08-03T10:27:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    3,
                    10,
                    27,
                    19,
                    6,
                    215,
                    0
                ],
                "published": "2025-01-28T00:52:23Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    0,
                    52,
                    23,
                    1,
                    28,
                    0
                ],
                "title": "MCTS-SQL: Light-Weight LLMs can Master the Text-to-SQL through Monte\n  Carlo Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCTS-SQL: Light-Weight LLMs can Master the Text-to-SQL through Monte\n  Carlo Tree Search"
                },
                "summary": "Text-to-SQL is a fundamental yet challenging task in the NLP area, aiming at\ntranslating natural language questions into SQL queries. While recent advances\nin large language models have greatly improved performance, most existing\napproaches depend on models with tens of billions of parameters or costly APIs,\nlimiting their applicability in resource-constrained environments. For real\nworld, especially on edge devices, it is crucial for Text-to-SQL to ensure\ncost-effectiveness. Therefore, enabling the light-weight models for Text-to-SQL\nis of great practical significance. However, smaller LLMs often struggle with\ncomplicated user instruction, redundant schema linking or syntax correctness.\nTo address these challenges, we propose MCTS-SQL, a novel framework that uses\nMonte Carlo Tree Search to guide SQL generation through multi-step refinement.\nSince the light-weight models' weak performance of single-shot prediction, we\ngenerate better results through several trials with feedback. However, directly\napplying MCTS-based methods inevitably leads to significant time and\ncomputational overhead. Driven by this issue, we propose a token-level\nprefix-cache mechanism that stores prior information during iterations,\neffectively improved the execution speed. Experiments results on the SPIDER and\nBIRD benchmarks demonstrate the effectiveness of our approach. Using a small\nopen-source Qwen2.5-Coder-1.5B, our method outperforms ChatGPT-3.5. When\nleveraging a more powerful model Gemini 2.5 to explore the performance upper\nbound, we achieved results competitive with the SOTA. Our findings demonstrate\nthat even small models can be effectively deployed in practical Text-to-SQL\nsystems with the right strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL is a fundamental yet challenging task in the NLP area, aiming at\ntranslating natural language questions into SQL queries. While recent advances\nin large language models have greatly improved performance, most existing\napproaches depend on models with tens of billions of parameters or costly APIs,\nlimiting their applicability in resource-constrained environments. For real\nworld, especially on edge devices, it is crucial for Text-to-SQL to ensure\ncost-effectiveness. Therefore, enabling the light-weight models for Text-to-SQL\nis of great practical significance. However, smaller LLMs often struggle with\ncomplicated user instruction, redundant schema linking or syntax correctness.\nTo address these challenges, we propose MCTS-SQL, a novel framework that uses\nMonte Carlo Tree Search to guide SQL generation through multi-step refinement.\nSince the light-weight models' weak performance of single-shot prediction, we\ngenerate better results through several trials with feedback. However, directly\napplying MCTS-based methods inevitably leads to significant time and\ncomputational overhead. Driven by this issue, we propose a token-level\nprefix-cache mechanism that stores prior information during iterations,\neffectively improved the execution speed. Experiments results on the SPIDER and\nBIRD benchmarks demonstrate the effectiveness of our approach. Using a small\nopen-source Qwen2.5-Coder-1.5B, our method outperforms ChatGPT-3.5. When\nleveraging a more powerful model Gemini 2.5 to explore the performance upper\nbound, we achieved results competitive with the SOTA. Our findings demonstrate\nthat even small models can be effectively deployed in practical Text-to-SQL\nsystems with the right strategy."
                },
                "authors": [
                    {
                        "name": "Shuozhi Yuan"
                    },
                    {
                        "name": "Limin Chen"
                    },
                    {
                        "name": "Miaomiao Yuan"
                    },
                    {
                        "name": "Jin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jin Zhao"
                },
                "author": "Jin Zhao",
                "arxiv_comment": "15 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16607v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16607v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02751v1",
                "updated": "2025-08-03T09:15:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    3,
                    9,
                    15,
                    36,
                    6,
                    215,
                    0
                ],
                "published": "2025-08-03T09:15:36Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    9,
                    15,
                    36,
                    6,
                    215,
                    0
                ],
                "title": "SmallKV: Small Model Assisted Compensation of KV Cache Compression for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmallKV: Small Model Assisted Compensation of KV Cache Compression for\n  Efficient LLM Inference"
                },
                "summary": "KV cache eviction has emerged as an effective solution to alleviate resource\nconstraints faced by LLMs in long-context scenarios. However, existing\ntoken-level eviction methods often overlook two critical aspects: (1) their\nirreversible eviction strategy fails to adapt to dynamic attention patterns\nduring decoding (the saliency shift problem), and (2) they treat both\nmarginally important tokens and truly unimportant tokens equally, despite the\ncollective significance of marginal tokens to model performance (the marginal\ninformation over-compression problem). To address these issues, we design two\ncompensation mechanisms based on the high similarity of attention matrices\nbetween LLMs of different scales. We propose SmallKV, a small model assisted\ncompensation method for KV cache compression. SmallKV can maintain attention\nmatching between different-scale LLMs to: 1) assist the larger model in\nperceiving globally important information of attention; and 2) use the smaller\nmodel's attention scores to approximate those of marginal tokens in the larger\nmodel. Extensive experiments on benchmarks including GSM8K, BBH, MT-Bench, and\nLongBench demonstrate the effectiveness of SmallKV. Moreover, efficiency\nevaluations show that SmallKV achieves 1.75 - 2.56 times higher throughput than\nbaseline methods, highlighting its potential for efficient and performant LLM\ninference in resource constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache eviction has emerged as an effective solution to alleviate resource\nconstraints faced by LLMs in long-context scenarios. However, existing\ntoken-level eviction methods often overlook two critical aspects: (1) their\nirreversible eviction strategy fails to adapt to dynamic attention patterns\nduring decoding (the saliency shift problem), and (2) they treat both\nmarginally important tokens and truly unimportant tokens equally, despite the\ncollective significance of marginal tokens to model performance (the marginal\ninformation over-compression problem). To address these issues, we design two\ncompensation mechanisms based on the high similarity of attention matrices\nbetween LLMs of different scales. We propose SmallKV, a small model assisted\ncompensation method for KV cache compression. SmallKV can maintain attention\nmatching between different-scale LLMs to: 1) assist the larger model in\nperceiving globally important information of attention; and 2) use the smaller\nmodel's attention scores to approximate those of marginal tokens in the larger\nmodel. Extensive experiments on benchmarks including GSM8K, BBH, MT-Bench, and\nLongBench demonstrate the effectiveness of SmallKV. Moreover, efficiency\nevaluations show that SmallKV achieves 1.75 - 2.56 times higher throughput than\nbaseline methods, highlighting its potential for efficient and performant LLM\ninference in resource constrained environments."
                },
                "authors": [
                    {
                        "name": "Yi Zhao"
                    },
                    {
                        "name": "Yajuan Peng"
                    },
                    {
                        "name": "Cam-Tu Nguyen"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Xiaoliang Wang"
                    },
                    {
                        "name": "Hai Zhao"
                    },
                    {
                        "name": "Xiaoming Fu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoming Fu"
                },
                "author": "Xiaoming Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19718v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19718v2",
                "updated": "2025-08-02T23:59:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    23,
                    59,
                    11,
                    5,
                    214,
                    0
                ],
                "published": "2025-07-25T23:55:54Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    23,
                    55,
                    54,
                    4,
                    206,
                    0
                ],
                "title": "GSCache: Real-Time Radiance Caching for Volume Path Tracing using 3D\n  Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GSCache: Real-Time Radiance Caching for Volume Path Tracing using 3D\n  Gaussian Splatting"
                },
                "summary": "Real-time path tracing is rapidly becoming the standard for rendering in\nentertainment and professional applications. In scientific visualization,\nvolume rendering plays a crucial role in helping researchers analyze and\ninterpret complex 3D data. Recently, photorealistic rendering techniques have\ngained popularity in scientific visualization, yet they face significant\nchallenges. One of the most prominent issues is slow rendering performance and\nhigh pixel variance caused by Monte Carlo integration. In this work, we\nintroduce a novel radiance caching approach for path-traced volume rendering.\nOur method leverages advances in volumetric scene representation and adapts 3D\nGaussian splatting to function as a multi-level, path-space radiance cache.\nThis cache is designed to be trainable on the fly, dynamically adapting to\nchanges in scene parameters such as lighting configurations and transfer\nfunctions. By incorporating our cache, we achieve less noisy, higher-quality\nimages without increasing rendering costs. To evaluate our approach, we compare\nit against a baseline path tracer that supports uniform sampling and next-event\nestimation and the state-of-the-art for neural radiance caching. Through both\nquantitative and qualitative analyses, we demonstrate that our path-space\nradiance cache is a robust solution that is easy to integrate and significantly\nenhances the rendering quality of volumetric visualization applications while\nmaintaining comparable computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time path tracing is rapidly becoming the standard for rendering in\nentertainment and professional applications. In scientific visualization,\nvolume rendering plays a crucial role in helping researchers analyze and\ninterpret complex 3D data. Recently, photorealistic rendering techniques have\ngained popularity in scientific visualization, yet they face significant\nchallenges. One of the most prominent issues is slow rendering performance and\nhigh pixel variance caused by Monte Carlo integration. In this work, we\nintroduce a novel radiance caching approach for path-traced volume rendering.\nOur method leverages advances in volumetric scene representation and adapts 3D\nGaussian splatting to function as a multi-level, path-space radiance cache.\nThis cache is designed to be trainable on the fly, dynamically adapting to\nchanges in scene parameters such as lighting configurations and transfer\nfunctions. By incorporating our cache, we achieve less noisy, higher-quality\nimages without increasing rendering costs. To evaluate our approach, we compare\nit against a baseline path tracer that supports uniform sampling and next-event\nestimation and the state-of-the-art for neural radiance caching. Through both\nquantitative and qualitative analyses, we demonstrate that our path-space\nradiance cache is a robust solution that is easy to integrate and significantly\nenhances the rendering quality of volumetric visualization applications while\nmaintaining comparable computational efficiency."
                },
                "authors": [
                    {
                        "name": "David Bauer"
                    },
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "Hamid Gadirov"
                    },
                    {
                        "name": "Kwan-Liu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Liu Ma"
                },
                "author": "Kwan-Liu Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19718v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19718v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01488v1",
                "updated": "2025-08-02T21:00:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    21,
                    0,
                    55,
                    5,
                    214,
                    0
                ],
                "published": "2025-08-02T21:00:55Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    21,
                    0,
                    55,
                    5,
                    214,
                    0
                ],
                "title": "PESTO: Real-Time Pitch Estimation with Self-supervised\n  Transposition-equivariant Objective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PESTO: Real-Time Pitch Estimation with Self-supervised\n  Transposition-equivariant Objective"
                },
                "summary": "In this paper, we introduce PESTO, a self-supervised learning approach for\nsingle-pitch estimation using a Siamese architecture. Our model processes\nindividual frames of a Variable-$Q$ Transform (VQT) and predicts pitch\ndistributions. The neural network is designed to be equivariant to\ntranslations, notably thanks to a Toeplitz fully-connected layer. In addition,\nwe construct pitch-shifted pairs by translating and cropping the VQT frames and\ntrain our model with a novel class-based transposition-equivariant objective,\neliminating the need for annotated data. Thanks to this architecture and\ntraining objective, our model achieves remarkable performances while being very\nlightweight ($130$k parameters). Evaluations on music and speech datasets\n(MIR-1K, MDB-stem-synth, and PTDB) demonstrate that PESTO not only outperforms\nself-supervised baselines but also competes with supervised methods, exhibiting\nsuperior cross-dataset generalization. Finally, we enhance PESTO's practical\nutility by developing a streamable VQT implementation using cached\nconvolutions. Combined with our model's low latency (less than 10 ms) and\nminimal parameter count, this makes PESTO particularly suitable for real-time\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce PESTO, a self-supervised learning approach for\nsingle-pitch estimation using a Siamese architecture. Our model processes\nindividual frames of a Variable-$Q$ Transform (VQT) and predicts pitch\ndistributions. The neural network is designed to be equivariant to\ntranslations, notably thanks to a Toeplitz fully-connected layer. In addition,\nwe construct pitch-shifted pairs by translating and cropping the VQT frames and\ntrain our model with a novel class-based transposition-equivariant objective,\neliminating the need for annotated data. Thanks to this architecture and\ntraining objective, our model achieves remarkable performances while being very\nlightweight ($130$k parameters). Evaluations on music and speech datasets\n(MIR-1K, MDB-stem-synth, and PTDB) demonstrate that PESTO not only outperforms\nself-supervised baselines but also competes with supervised methods, exhibiting\nsuperior cross-dataset generalization. Finally, we enhance PESTO's practical\nutility by developing a streamable VQT implementation using cached\nconvolutions. Combined with our model's low latency (less than 10 ms) and\nminimal parameter count, this makes PESTO particularly suitable for real-time\napplications."
                },
                "authors": [
                    {
                        "name": "Alain Riou"
                    },
                    {
                        "name": "Bernardo Torres"
                    },
                    {
                        "name": "Ben Hayes"
                    },
                    {
                        "name": "Stefan Lattner"
                    },
                    {
                        "name": "Gatan Hadjeres"
                    },
                    {
                        "name": "Gal Richard"
                    },
                    {
                        "name": "Geoffroy Peeters"
                    }
                ],
                "author_detail": {
                    "name": "Geoffroy Peeters"
                },
                "author": "Geoffroy Peeters",
                "arxiv_comment": "Accepted to the Transactions of the International Society for Music\n  Information Retrieval",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01298v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01298v1",
                "updated": "2025-08-02T10:12:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    10,
                    12,
                    45,
                    5,
                    214,
                    0
                ],
                "published": "2025-08-02T10:12:45Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    10,
                    12,
                    45,
                    5,
                    214,
                    0
                ],
                "title": "Improving performance of content-centric networks via decentralized\n  coded caching for multi-level popularity and access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving performance of content-centric networks via decentralized\n  coded caching for multi-level popularity and access"
                },
                "summary": "Content-Centric Networking (CCN) offers a novel architectural paradigm that\nseeks to address the inherent limitations of the prevailing Internet Protocol\n(IP)-based networking model. In contrast to the host-centric communication\napproach of IP networks, CCN prioritizes content by enabling direct addressing\nand routing based on content identifiers. The potential performance\nimprovements of CCN can be further amplified through optimized management of\ncoded data storage and transmission strategies. Decentralized Coded Caching\n(DCC) emerges as a promising technique that harnesses the collective caching\npower of distributed network elements. By strategically pre-positioning\nfrequently accessed content closer to potential consumers during periods of low\nnetwork utilization, DCC has the potential to mitigate content transfer rates\nduring peak traffic periods. This paper proposes a series of fundamental\nmodifications to the CCN architecture by integrating DCC. The proposed\nframework incorporates differentiated coding strategies tailored to user access\nprivileges, thereby eliminating the overhead associated with queue-based\nsearching. Additionally, the framework facilitates recoding of uncoded data\nencountered along the content delivery path. These combined methodologies\ndemonstrably enhance network throughput, elevate cache hit ratios, and\nconsequently, reduce content delivery latency compared to conventional CCN\nimplementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content-Centric Networking (CCN) offers a novel architectural paradigm that\nseeks to address the inherent limitations of the prevailing Internet Protocol\n(IP)-based networking model. In contrast to the host-centric communication\napproach of IP networks, CCN prioritizes content by enabling direct addressing\nand routing based on content identifiers. The potential performance\nimprovements of CCN can be further amplified through optimized management of\ncoded data storage and transmission strategies. Decentralized Coded Caching\n(DCC) emerges as a promising technique that harnesses the collective caching\npower of distributed network elements. By strategically pre-positioning\nfrequently accessed content closer to potential consumers during periods of low\nnetwork utilization, DCC has the potential to mitigate content transfer rates\nduring peak traffic periods. This paper proposes a series of fundamental\nmodifications to the CCN architecture by integrating DCC. The proposed\nframework incorporates differentiated coding strategies tailored to user access\nprivileges, thereby eliminating the overhead associated with queue-based\nsearching. Additionally, the framework facilitates recoding of uncoded data\nencountered along the content delivery path. These combined methodologies\ndemonstrably enhance network throughput, elevate cache hit ratios, and\nconsequently, reduce content delivery latency compared to conventional CCN\nimplementations."
                },
                "authors": [
                    {
                        "name": "Azadeh Sadat Miraftab"
                    },
                    {
                        "name": "Ahmadreza Montazerolghaem"
                    },
                    {
                        "name": "Behrad Mahboobi"
                    }
                ],
                "author_detail": {
                    "name": "Behrad Mahboobi"
                },
                "author": "Behrad Mahboobi",
                "arxiv_doi": "10.1007/s10586-025-05256-6",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10586-025-05256-6",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.01298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01298v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01261v1",
                "updated": "2025-08-02T08:33:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    8,
                    33,
                    30,
                    5,
                    214,
                    0
                ],
                "published": "2025-08-02T08:33:30Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    8,
                    33,
                    30,
                    5,
                    214,
                    0
                ],
                "title": "Unifying Mixture of Experts and Multi-Head Latent Attention for\n  Efficient Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying Mixture of Experts and Multi-Head Latent Attention for\n  Efficient Language Models"
                },
                "summary": "We present MoE-MLA-RoPE, a novel architecture combination that combines\nMixture of Experts (MoE) with Multi-head Latent Attention (MLA) and Rotary\nPosition Embeddings (RoPE) for efficient language modeling. Our approach\naddresses the fundamental trade-off between model capacity and computational\nefficiency through three key innovations: (1) fine-grained expert routing with\n64 micro-experts and top-$k$ selection, enabling flexible specialization\nthrough 3.6 * 10^7 possible expert combinations; (2) shared expert isolation\nthat dedicates 2 always active experts for common patterns while routing to 6\nof 62 specialized experts; and (3) gradient-conflict-free load balancing that\nmaintains expert utilization without interfering with primary loss\noptimization.\n  Extensive experiments on models ranging from 17M to 202M parameters\ndemonstrate that MoE-MLA-RoPE with compression ratio r=d/2 achieves 68% KV\ncache memory reduction and 3.2x inference speedup while maintaining competitive\nperplexity (0.8% degradation). Compared to the parameters with 53.9M\nparameters, MoE-MLA-RoPE improves the validation loss by 6.9% over the vanilla\ntransformers while using 42% fewer active parameters per forward pass.\nFLOP-matched experiments reveal even larger gains: 11.1% improvement with 3.2x\ninference acceleration. Automated evaluation using GPT-4 as a judge confirms\nquality improvements in generation, with higher scores on coherence (8.1/10),\ncreativity (7.9/10) and grammatical correctness (8.2/10). Our results establish\nthat architectural novelty, not parameter scaling, defines the efficiency\nfrontier for resource-constrained language model deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present MoE-MLA-RoPE, a novel architecture combination that combines\nMixture of Experts (MoE) with Multi-head Latent Attention (MLA) and Rotary\nPosition Embeddings (RoPE) for efficient language modeling. Our approach\naddresses the fundamental trade-off between model capacity and computational\nefficiency through three key innovations: (1) fine-grained expert routing with\n64 micro-experts and top-$k$ selection, enabling flexible specialization\nthrough 3.6 * 10^7 possible expert combinations; (2) shared expert isolation\nthat dedicates 2 always active experts for common patterns while routing to 6\nof 62 specialized experts; and (3) gradient-conflict-free load balancing that\nmaintains expert utilization without interfering with primary loss\noptimization.\n  Extensive experiments on models ranging from 17M to 202M parameters\ndemonstrate that MoE-MLA-RoPE with compression ratio r=d/2 achieves 68% KV\ncache memory reduction and 3.2x inference speedup while maintaining competitive\nperplexity (0.8% degradation). Compared to the parameters with 53.9M\nparameters, MoE-MLA-RoPE improves the validation loss by 6.9% over the vanilla\ntransformers while using 42% fewer active parameters per forward pass.\nFLOP-matched experiments reveal even larger gains: 11.1% improvement with 3.2x\ninference acceleration. Automated evaluation using GPT-4 as a judge confirms\nquality improvements in generation, with higher scores on coherence (8.1/10),\ncreativity (7.9/10) and grammatical correctness (8.2/10). Our results establish\nthat architectural novelty, not parameter scaling, defines the efficiency\nfrontier for resource-constrained language model deployment."
                },
                "authors": [
                    {
                        "name": "Sushant Mehta"
                    },
                    {
                        "name": "Raj Dandekar"
                    },
                    {
                        "name": "Rajat Dandekar"
                    },
                    {
                        "name": "Sreedath Panat"
                    }
                ],
                "author_detail": {
                    "name": "Sreedath Panat"
                },
                "author": "Sreedath Panat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11972v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11972v2",
                "updated": "2025-08-02T06:50:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    6,
                    50,
                    59,
                    5,
                    214,
                    0
                ],
                "published": "2025-03-15T02:48:27Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    2,
                    48,
                    27,
                    5,
                    74,
                    0
                ],
                "title": "MoDM: Efficient Serving for Image Generation via Mixture-of-Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoDM: Efficient Serving for Image Generation via Mixture-of-Diffusion\n  Models"
                },
                "summary": "Diffusion-based text-to-image generation models trade latency for quality:\nsmall models are fast but generate lower-quality images, while large models\nproduce better images but are slow.\n  We present MoDM, a novel caching-based serving system for diffusion models\nthat dynamically balances latency and quality through a mixture of diffusion\nmodels. Unlike prior approaches that rely on model-specific internal features,\nMoDM caches final images, allowing seamless retrieval and reuse across multiple\ndiffusion model families.\n  This design enables adaptive serving by dynamically balancing latency and\nimage quality: using smaller models for cache-hit requests to reduce latency\nwhile reserving larger models for cache-miss requests to maintain quality.\nSmall model image quality is preserved using retrieved cached images.\n  We design a global monitor that optimally allocates GPU resources and\nbalances inference workload, ensuring high throughput while meeting\nservice-level objectives under varying request rates. Our evaluations show that\nMoDM significantly reduces average serving time by 2.5x while retaining image\nquality, making it a practical solution for scalable and resource-efficient\nmodel deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based text-to-image generation models trade latency for quality:\nsmall models are fast but generate lower-quality images, while large models\nproduce better images but are slow.\n  We present MoDM, a novel caching-based serving system for diffusion models\nthat dynamically balances latency and quality through a mixture of diffusion\nmodels. Unlike prior approaches that rely on model-specific internal features,\nMoDM caches final images, allowing seamless retrieval and reuse across multiple\ndiffusion model families.\n  This design enables adaptive serving by dynamically balancing latency and\nimage quality: using smaller models for cache-hit requests to reduce latency\nwhile reserving larger models for cache-miss requests to maintain quality.\nSmall model image quality is preserved using retrieved cached images.\n  We design a global monitor that optimally allocates GPU resources and\nbalances inference workload, ensuring high throughput while meeting\nservice-level objectives under varying request rates. Our evaluations show that\nMoDM significantly reduces average serving time by 2.5x while retaining image\nquality, making it a practical solution for scalable and resource-efficient\nmodel deployment."
                },
                "authors": [
                    {
                        "name": "Yuchen Xia"
                    },
                    {
                        "name": "Divyam Sharma"
                    },
                    {
                        "name": "Yichao Yuan"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Nishil Talati"
                    }
                ],
                "author_detail": {
                    "name": "Nishil Talati"
                },
                "author": "Nishil Talati",
                "arxiv_comment": "To appear in ASPLOS'26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11972v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11972v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01225v1",
                "updated": "2025-08-02T06:43:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    6,
                    43,
                    43,
                    5,
                    214,
                    0
                ],
                "published": "2025-08-02T06:43:43Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    6,
                    43,
                    43,
                    5,
                    214,
                    0
                ],
                "title": "Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of\n  Vision-Language Models"
                },
                "summary": "In zero-shot setting, test-time adaptation adjusts pre-trained models using\nunlabeled data from the test phase to enhance performance on unknown test\ndistributions. Existing cache-enhanced TTA methods rely on a low-entropy\ncriterion to select samples for prototype construction, assuming intra-class\ncompactness. However, low-entropy samples may be unreliable under distribution\nshifts, and the resulting prototypes may not ensure compact intra-class\ndistributions. This study identifies a positive correlation between\ncache-enhanced performance and intra-class compactness. Based on this\nobservation, we propose a Multi-Cache enhanced Prototype-based Test-Time\nAdaptation (MCP) featuring three caches: an entropy cache for initializing\nprototype representations with low-entropy samples, an align cache for\nintegrating visual and textual information to achieve compact intra-class\ndistributions, and a negative cache for prediction calibration using\nhigh-entropy samples. We further developed MCP++, a framework incorporating\ncross-modal prototype alignment and residual learning, introducing prototype\nresidual fine-tuning. Comparative and ablation experiments across 15 downstream\ntasks demonstrate that the proposed method and framework achieve\nstate-of-the-art generalization performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In zero-shot setting, test-time adaptation adjusts pre-trained models using\nunlabeled data from the test phase to enhance performance on unknown test\ndistributions. Existing cache-enhanced TTA methods rely on a low-entropy\ncriterion to select samples for prototype construction, assuming intra-class\ncompactness. However, low-entropy samples may be unreliable under distribution\nshifts, and the resulting prototypes may not ensure compact intra-class\ndistributions. This study identifies a positive correlation between\ncache-enhanced performance and intra-class compactness. Based on this\nobservation, we propose a Multi-Cache enhanced Prototype-based Test-Time\nAdaptation (MCP) featuring three caches: an entropy cache for initializing\nprototype representations with low-entropy samples, an align cache for\nintegrating visual and textual information to achieve compact intra-class\ndistributions, and a negative cache for prediction calibration using\nhigh-entropy samples. We further developed MCP++, a framework incorporating\ncross-modal prototype alignment and residual learning, introducing prototype\nresidual fine-tuning. Comparative and ablation experiments across 15 downstream\ntasks demonstrate that the proposed method and framework achieve\nstate-of-the-art generalization performance."
                },
                "authors": [
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Haotian Zhai"
                    },
                    {
                        "name": "Can Zhang"
                    },
                    {
                        "name": "Xiupeng Shi"
                    },
                    {
                        "name": "Ruirui Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruirui Li"
                },
                "author": "Ruirui Li",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19849v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19849v2",
                "updated": "2025-08-02T00:31:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    0,
                    31,
                    18,
                    5,
                    214,
                    0
                ],
                "published": "2025-05-26T11:35:04Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    11,
                    35,
                    4,
                    0,
                    146,
                    0
                ],
                "title": "HIT Model: A Hierarchical Interaction-Enhanced Two-Tower Model for\n  Pre-Ranking Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HIT Model: A Hierarchical Interaction-Enhanced Two-Tower Model for\n  Pre-Ranking Systems"
                },
                "summary": "Online display advertising platforms rely on pre-ranking systems to\nefficiently filter and prioritize candidate ads from large corpora, balancing\nrelevance to users with strict computational constraints. The prevailing\ntwo-tower architecture, though highly efficient due to its decoupled design and\npre-caching, suffers from cross-domain interaction and coarse similarity\nmetrics, undermining its capacity to model complex user-ad relationships. In\nthis study, we propose the Hierarchical Interaction-Enhanced Two-Tower (HIT)\nmodel, a new architecture that augments the two-tower paradigm with two key\ncomponents: $\\textit{generators}$ that pre-generate holistic vectors\nincorporating coarse-grained user-ad interactions through a dual-generator\nframework with a cosine-similarity-based generation loss as the training\nobjective, and $\\textit{multi-head representers}$ that project embeddings into\nmultiple latent subspaces to capture fine-grained, multi-faceted user interests\nand multi-dimensional ad attributes. This design enhances modeling\neffectiveness without compromising inference efficiency. Extensive experiments\non public datasets and large-scale online A/B testing on Tencent's advertising\nplatform demonstrate that HIT significantly outperforms several baselines in\nrelevance metrics, yielding a $1.66\\%$ increase in Gross Merchandise Volume and\na $1.55\\%$ improvement in Return on Investment, alongside similar serving\nlatency to the vanilla two-tower models. The HIT model has been successfully\ndeployed in Tencent's online display advertising system, serving billions of\nimpressions daily. The code is available at\nhttps://github.com/HarveyYang123/HIT_model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online display advertising platforms rely on pre-ranking systems to\nefficiently filter and prioritize candidate ads from large corpora, balancing\nrelevance to users with strict computational constraints. The prevailing\ntwo-tower architecture, though highly efficient due to its decoupled design and\npre-caching, suffers from cross-domain interaction and coarse similarity\nmetrics, undermining its capacity to model complex user-ad relationships. In\nthis study, we propose the Hierarchical Interaction-Enhanced Two-Tower (HIT)\nmodel, a new architecture that augments the two-tower paradigm with two key\ncomponents: $\\textit{generators}$ that pre-generate holistic vectors\nincorporating coarse-grained user-ad interactions through a dual-generator\nframework with a cosine-similarity-based generation loss as the training\nobjective, and $\\textit{multi-head representers}$ that project embeddings into\nmultiple latent subspaces to capture fine-grained, multi-faceted user interests\nand multi-dimensional ad attributes. This design enhances modeling\neffectiveness without compromising inference efficiency. Extensive experiments\non public datasets and large-scale online A/B testing on Tencent's advertising\nplatform demonstrate that HIT significantly outperforms several baselines in\nrelevance metrics, yielding a $1.66\\%$ increase in Gross Merchandise Volume and\na $1.55\\%$ improvement in Return on Investment, alongside similar serving\nlatency to the vanilla two-tower models. The HIT model has been successfully\ndeployed in Tencent's online display advertising system, serving billions of\nimpressions daily. The code is available at\nhttps://github.com/HarveyYang123/HIT_model."
                },
                "authors": [
                    {
                        "name": "Haoqiang Yang"
                    },
                    {
                        "name": "Congde Yuan"
                    },
                    {
                        "name": "Kun Bai"
                    },
                    {
                        "name": "Mengzhuo Guo"
                    },
                    {
                        "name": "Wei Yang"
                    },
                    {
                        "name": "Chao Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Chao Zhou"
                },
                "author": "Chao Zhou",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19849v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19849v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01051v1",
                "updated": "2025-08-01T20:08:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    20,
                    8,
                    52,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T20:08:52Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    20,
                    8,
                    52,
                    4,
                    213,
                    0
                ],
                "title": "QPP-RNG: A Conceptual Quantum System for True Randomness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QPP-RNG: A Conceptual Quantum System for True Randomness"
                },
                "summary": "We propose and experimentally demonstrate the \\emph{Quasi-Superposition\nQuantum-inspired System (QSQS)} -- a conceptual quantum system for randomness\ngeneration built on measuring two conjugate observables of a permutation\nsorting process: the deterministic permutation count $n_p$ and the\nfundamentally non-deterministic sorting time $t$. By analogy with quantum\nsystems, these observables are linked by an uncertainty-like constraint:\nalgorithmic determinism ensures structural uniformity, while system-level\nfluctuations introduce irreducible unpredictability. We realize this framework\nconcretely as \\emph{QPP-RNG}, a system-embedded, software-based true random\nnumber generator (TRNG). In QPP-RNG, real-time measurements of sorting time $t$\n-- shaped by CPU pipeline jitter, cache latency, and OS scheduling --\ndynamically reseed the PRNG driving the permutation sequence. Crucially, QSQS\ntransforms initially right-skewed raw distributions of $n_p$ and $t$ into\nnearly uniform outputs after modulo reduction, thanks to internal degeneracies\nthat collapse many distinct states into the same output symbol. Empirical\nresults show that as the repetition factor $m$ increases, output entropy\nconverges toward theoretical maxima: Shannon and min-entropy values approach 8\nbits, chi-squared statistics stabilize near ideal uniformity, and bell curves\nvisually confirm the flattening from skewed to uniform distributions. Beyond\npractical implications, QSQS unifies deterministic algorithmic processes with\nnon-deterministic physical fluctuations, offering a physics-based perspective\nfor engineering true randomness in post-quantum cryptographic systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose and experimentally demonstrate the \\emph{Quasi-Superposition\nQuantum-inspired System (QSQS)} -- a conceptual quantum system for randomness\ngeneration built on measuring two conjugate observables of a permutation\nsorting process: the deterministic permutation count $n_p$ and the\nfundamentally non-deterministic sorting time $t$. By analogy with quantum\nsystems, these observables are linked by an uncertainty-like constraint:\nalgorithmic determinism ensures structural uniformity, while system-level\nfluctuations introduce irreducible unpredictability. We realize this framework\nconcretely as \\emph{QPP-RNG}, a system-embedded, software-based true random\nnumber generator (TRNG). In QPP-RNG, real-time measurements of sorting time $t$\n-- shaped by CPU pipeline jitter, cache latency, and OS scheduling --\ndynamically reseed the PRNG driving the permutation sequence. Crucially, QSQS\ntransforms initially right-skewed raw distributions of $n_p$ and $t$ into\nnearly uniform outputs after modulo reduction, thanks to internal degeneracies\nthat collapse many distinct states into the same output symbol. Empirical\nresults show that as the repetition factor $m$ increases, output entropy\nconverges toward theoretical maxima: Shannon and min-entropy values approach 8\nbits, chi-squared statistics stabilize near ideal uniformity, and bell curves\nvisually confirm the flattening from skewed to uniform distributions. Beyond\npractical implications, QSQS unifies deterministic algorithmic processes with\nnon-deterministic physical fluctuations, offering a physics-based perspective\nfor engineering true randomness in post-quantum cryptographic systems."
                },
                "authors": [
                    {
                        "name": "Randy Kuang"
                    }
                ],
                "author_detail": {
                    "name": "Randy Kuang"
                },
                "author": "Randy Kuang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00647v1",
                "updated": "2025-08-01T14:05:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    14,
                    5,
                    44,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T14:05:44Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    14,
                    5,
                    44,
                    4,
                    213,
                    0
                ],
                "title": "Study of the HV power supply modules for the CUbesat Solar Polarimeter\n  (CUSP)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Study of the HV power supply modules for the CUbesat Solar Polarimeter\n  (CUSP)"
                },
                "summary": "The CUbesat Solar Polarimeter (CUSP) project is a CubeSat mission orbiting\nthe Earth aimed to measure the linear polarization of solar flares in the hard\nX-ray band by means of a Compton scattering polarimeter. CUSP will allow to\nstudy the magnetic reconnection and particle acceleration in the flaring\nmagnetic structures of our star. CUSP is a project in the framework of the\nAlcor Program of the Italian Space Agency aimed to develop new CubeSat\nmissions. CUSP undergoing the Phase B started in December 2024 that will last\nfor 12 month. The Compton polarimeter of the CUSP payload performs coincidence\nmeasurements between plastic scintilaltors and GaGG(Ce) crystals to derive the\npolarization of X-rays. These sensors are readout by Multi Anode\nPhotomultiplier Tubes (MAPMTs) and Avalanche Photodiodes (APDs) respectively.\nBoth sensors need an HV power supply up to -1~kV (for the MAPMT) and +500~V\n(for the APD). We tested precision regulated High Voltage DC/DC Converters by\nHVM Technology Inc. with Sub-Miniature Case Size\n($0.85''\\times0.85''\\times0.60''$) of the SMHV series. These modules are\ncompact and suited for CubeSat missions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The CUbesat Solar Polarimeter (CUSP) project is a CubeSat mission orbiting\nthe Earth aimed to measure the linear polarization of solar flares in the hard\nX-ray band by means of a Compton scattering polarimeter. CUSP will allow to\nstudy the magnetic reconnection and particle acceleration in the flaring\nmagnetic structures of our star. CUSP is a project in the framework of the\nAlcor Program of the Italian Space Agency aimed to develop new CubeSat\nmissions. CUSP undergoing the Phase B started in December 2024 that will last\nfor 12 month. The Compton polarimeter of the CUSP payload performs coincidence\nmeasurements between plastic scintilaltors and GaGG(Ce) crystals to derive the\npolarization of X-rays. These sensors are readout by Multi Anode\nPhotomultiplier Tubes (MAPMTs) and Avalanche Photodiodes (APDs) respectively.\nBoth sensors need an HV power supply up to -1~kV (for the MAPMT) and +500~V\n(for the APD). We tested precision regulated High Voltage DC/DC Converters by\nHVM Technology Inc. with Sub-Miniature Case Size\n($0.85''\\times0.85''\\times0.60''$) of the SMHV series. These modules are\ncompact and suited for CubeSat missions."
                },
                "authors": [
                    {
                        "name": "Alessandro Lacerenza"
                    },
                    {
                        "name": "Alda Rubini"
                    },
                    {
                        "name": "Andrea Alimenti"
                    },
                    {
                        "name": "Sergio Fabiani"
                    },
                    {
                        "name": "Ettore Del Monte"
                    },
                    {
                        "name": "Riccardo Campana"
                    },
                    {
                        "name": "Mauro Centrone"
                    },
                    {
                        "name": "Enrico Costa"
                    },
                    {
                        "name": "Nicolas De Angelis"
                    },
                    {
                        "name": "Giovanni De Cesare"
                    },
                    {
                        "name": "Sergio Di Cosimo"
                    },
                    {
                        "name": "Giuseppe Di Persio"
                    },
                    {
                        "name": "Abhay Kumar"
                    },
                    {
                        "name": "Pasqualino Loffredo"
                    },
                    {
                        "name": "Giovanni Lombardi"
                    },
                    {
                        "name": "Gabriele Minervini"
                    },
                    {
                        "name": "Fabio Muleri"
                    },
                    {
                        "name": "Paolo Romano"
                    },
                    {
                        "name": "Emanuele Scalise"
                    },
                    {
                        "name": "Enrico Silva"
                    },
                    {
                        "name": "Paolo Soffitta"
                    },
                    {
                        "name": "Davide Albanesi"
                    },
                    {
                        "name": "Ilaria Baffo"
                    },
                    {
                        "name": "Daniele Brienza"
                    },
                    {
                        "name": "Valerio Campamaggiore"
                    },
                    {
                        "name": "Giovanni Cucinella"
                    },
                    {
                        "name": "Andrea Curatolo"
                    },
                    {
                        "name": "Giulia de Iulis"
                    },
                    {
                        "name": "Andrea Del Re"
                    },
                    {
                        "name": "Vito Di Bari"
                    },
                    {
                        "name": "Simone Di Filippo"
                    },
                    {
                        "name": "Immacolata Donnarumma"
                    },
                    {
                        "name": "Pierluigi Fanelli"
                    },
                    {
                        "name": "Nicolas Gagliardi"
                    },
                    {
                        "name": "Paolo Leonetti"
                    },
                    {
                        "name": "Matteo Merge"
                    },
                    {
                        "name": "Dario Modenini"
                    },
                    {
                        "name": "Andrea Negri"
                    },
                    {
                        "name": "Daniele Pecorella"
                    },
                    {
                        "name": "Massimo Perelli"
                    },
                    {
                        "name": "Alice Ponti"
                    },
                    {
                        "name": "Francesca Sbop"
                    },
                    {
                        "name": "Paolo Tortora"
                    },
                    {
                        "name": "Alessandro Turchi"
                    },
                    {
                        "name": "Valerio Vagelli"
                    },
                    {
                        "name": "Emanuele Zaccagnino"
                    },
                    {
                        "name": "Alessandro Zambardi"
                    },
                    {
                        "name": "Costantino Zazza"
                    }
                ],
                "author_detail": {
                    "name": "Costantino Zazza"
                },
                "author": "Costantino Zazza",
                "arxiv_comment": "6 pages, 2 figures, SPIE Optics+Photonics 2025 proceeding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.space-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00629v1",
                "updated": "2025-08-01T13:40:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    40,
                    52,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T13:40:52Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    40,
                    52,
                    4,
                    213,
                    0
                ],
                "title": "Energy-Aware CPU Orchestration in O-RAN: A dApp-Driven Lightweight\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy-Aware CPU Orchestration in O-RAN: A dApp-Driven Lightweight\n  Approach"
                },
                "summary": "The transition toward softwarized Radio Access Networks (RANs), driven by the\nOpen RAN (O-RAN) paradigm, enables flexible, vendor-neutral deployments through\ndisaggregation and virtualization of base station functions. However, this\nshift introduces new challenges in managing CPU resources efficiently under\nstrict real-time constraints. In particular, the interplay between\nlatency-sensitive RAN workloads and general-purpose Operating System (OS)\nschedulers often leads to sub-optimal performance and unnecessary energy\nconsumption. This work proposes a lightweight, programmable distributed\napplication (dApp) deployed at the Distributed Unit (DU) level to dynamically\norchestrate CPU usage. The dApp operates in closed loop with the OS, leveraging\nthread-level telemetry like context switches, Instructions Per Cycle (IPC), and\ncache metrics, to adapt CPU thread affinity, core isolation, and frequency\nscaling in real time. Unlike existing solutions, it requires no access to\nproprietary RAN software, hardware-specific features, or kernel modifications.\nFully compliant with the O-RAN architecture and agnostic to the underlying RAN\nstack, the proposed solution introduces negligible overhead while improving\nenergy efficiency and CPU utilization. Experimental results using a\ncommercial-grade srsRAN deployment demonstrate consistent power savings without\ncompromising real-time processing performance, highlighting the potential of\nlow-latency dApps for fine-grained resource control in next-generation networks",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transition toward softwarized Radio Access Networks (RANs), driven by the\nOpen RAN (O-RAN) paradigm, enables flexible, vendor-neutral deployments through\ndisaggregation and virtualization of base station functions. However, this\nshift introduces new challenges in managing CPU resources efficiently under\nstrict real-time constraints. In particular, the interplay between\nlatency-sensitive RAN workloads and general-purpose Operating System (OS)\nschedulers often leads to sub-optimal performance and unnecessary energy\nconsumption. This work proposes a lightweight, programmable distributed\napplication (dApp) deployed at the Distributed Unit (DU) level to dynamically\norchestrate CPU usage. The dApp operates in closed loop with the OS, leveraging\nthread-level telemetry like context switches, Instructions Per Cycle (IPC), and\ncache metrics, to adapt CPU thread affinity, core isolation, and frequency\nscaling in real time. Unlike existing solutions, it requires no access to\nproprietary RAN software, hardware-specific features, or kernel modifications.\nFully compliant with the O-RAN architecture and agnostic to the underlying RAN\nstack, the proposed solution introduces negligible overhead while improving\nenergy efficiency and CPU utilization. Experimental results using a\ncommercial-grade srsRAN deployment demonstrate consistent power savings without\ncompromising real-time processing performance, highlighting the potential of\nlow-latency dApps for fine-grained resource control in next-generation networks"
                },
                "authors": [
                    {
                        "name": "Francisco Crespo"
                    },
                    {
                        "name": "Javier Villegas"
                    },
                    {
                        "name": "Carlos Baena"
                    },
                    {
                        "name": "Eduardo Baena"
                    },
                    {
                        "name": "Sergio Fortes"
                    },
                    {
                        "name": "Raquel Barco"
                    }
                ],
                "author_detail": {
                    "name": "Raquel Barco"
                },
                "author": "Raquel Barco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00616v1",
                "updated": "2025-08-01T13:25:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    25,
                    28,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T13:25:28Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    25,
                    28,
                    4,
                    213,
                    0
                ],
                "title": "Joint Association and Phase Shifts Design for UAV-mounted Stacked\n  Intelligent Metasurfaces-assisted Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Association and Phase Shifts Design for UAV-mounted Stacked\n  Intelligent Metasurfaces-assisted Communications"
                },
                "summary": "Stacked intelligent metasurfaces (SIMs) have emerged as a promising\ntechnology for realizing wave-domain signal processing, while the fixed SIMs\nwill limit the communication performance of the system compared to the mobile\nSIMs. In this work, we consider a UAV-mounted SIMs (UAV-SIMs) assisted\ncommunication system, where UAVs as base stations (BSs) can cache the data\nprocessed by SIMs, and also as mobile vehicles flexibly deploy SIMs to enhance\nthe communication performance. To this end, we formulate a UAV-SIM-based joint\noptimization problem (USBJOP) to comprehensively consider the association\nbetween UAV-SIMs and users, the locations of UAV-SIMs, and the phase shifts of\nUAV-SIMs, aiming to maximize the network capacity. Due to the non-convexity and\nNP-hardness of USBJOP, we decompose it into three sub-optimization problems,\nwhich are the association between UAV-SIMs and users optimization problem\n(AUUOP), the UAV location optimization problem (ULOP), and the UAV-SIM phase\nshifts optimization problem (USPSOP). Then, these three sub-optimization\nproblems are solved by an alternating optimization (AO) strategy. Specifically,\nAUUOP and ULOP are transformed to a convex form and then solved by the CVX\ntool, while we employ a layer-by-layer iterative optimization method for\nUSPSOP. Simulation results verify the effectiveness of the proposed strategy\nunder different simulation setups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stacked intelligent metasurfaces (SIMs) have emerged as a promising\ntechnology for realizing wave-domain signal processing, while the fixed SIMs\nwill limit the communication performance of the system compared to the mobile\nSIMs. In this work, we consider a UAV-mounted SIMs (UAV-SIMs) assisted\ncommunication system, where UAVs as base stations (BSs) can cache the data\nprocessed by SIMs, and also as mobile vehicles flexibly deploy SIMs to enhance\nthe communication performance. To this end, we formulate a UAV-SIM-based joint\noptimization problem (USBJOP) to comprehensively consider the association\nbetween UAV-SIMs and users, the locations of UAV-SIMs, and the phase shifts of\nUAV-SIMs, aiming to maximize the network capacity. Due to the non-convexity and\nNP-hardness of USBJOP, we decompose it into three sub-optimization problems,\nwhich are the association between UAV-SIMs and users optimization problem\n(AUUOP), the UAV location optimization problem (ULOP), and the UAV-SIM phase\nshifts optimization problem (USPSOP). Then, these three sub-optimization\nproblems are solved by an alternating optimization (AO) strategy. Specifically,\nAUUOP and ULOP are transformed to a convex form and then solved by the CVX\ntool, while we employ a layer-by-layer iterative optimization method for\nUSPSOP. Simulation results verify the effectiveness of the proposed strategy\nunder different simulation setups."
                },
                "authors": [
                    {
                        "name": "Mingzhe Fan"
                    },
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Hongyang Pan"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Jiancheng An"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Chau Yuen"
                    }
                ],
                "author_detail": {
                    "name": "Chau Yuen"
                },
                "author": "Chau Yuen",
                "arxiv_comment": "This papar has been submitted to the IEEE Global Communications\n  Conference. arXiv admin note: substantial text overlap with arXiv:2506.23488",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00412v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00412v1",
                "updated": "2025-08-01T08:10:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    8,
                    10,
                    54,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T08:10:54Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    8,
                    10,
                    54,
                    4,
                    213,
                    0
                ],
                "title": "Sortblock: Similarity-Aware Feature Reuse for Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sortblock: Similarity-Aware Feature Reuse for Diffusion Model"
                },
                "summary": "Diffusion Transformers (DiTs) have demonstrated remarkable generative\ncapabilities, particularly benefiting from Transformer architectures that\nenhance visual and artistic fidelity. However, their inherently sequential\ndenoising process results in high inference latency, limiting their deployment\nin real-time scenarios. Existing training-free acceleration approaches\ntypically reuse intermediate features at fixed timesteps or layers, overlooking\nthe evolving semantic focus across denoising stages and Transformer blocks.To\naddress this, we propose Sortblock, a training-free inference acceleration\nframework that dynamically caches block-wise features based on their similarity\nacross adjacent timesteps. By ranking the evolution of residuals, Sortblock\nadaptively determines a recomputation ratio, selectively skipping redundant\ncomputations while preserving generation quality. Furthermore, we incorporate a\nlightweight linear prediction mechanism to reduce accumulated errors in skipped\nblocks.Extensive experiments across various tasks and DiT architectures\ndemonstrate that Sortblock achieves over 2$\\times$ inference speedup with\nminimal degradation in output quality, offering an effective and generalizable\nsolution for accelerating diffusion-based generative models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have demonstrated remarkable generative\ncapabilities, particularly benefiting from Transformer architectures that\nenhance visual and artistic fidelity. However, their inherently sequential\ndenoising process results in high inference latency, limiting their deployment\nin real-time scenarios. Existing training-free acceleration approaches\ntypically reuse intermediate features at fixed timesteps or layers, overlooking\nthe evolving semantic focus across denoising stages and Transformer blocks.To\naddress this, we propose Sortblock, a training-free inference acceleration\nframework that dynamically caches block-wise features based on their similarity\nacross adjacent timesteps. By ranking the evolution of residuals, Sortblock\nadaptively determines a recomputation ratio, selectively skipping redundant\ncomputations while preserving generation quality. Furthermore, we incorporate a\nlightweight linear prediction mechanism to reduce accumulated errors in skipped\nblocks.Extensive experiments across various tasks and DiT architectures\ndemonstrate that Sortblock achieves over 2$\\times$ inference speedup with\nminimal degradation in output quality, offering an effective and generalizable\nsolution for accelerating diffusion-based generative models."
                },
                "authors": [
                    {
                        "name": "Hanqi Chen"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Xiaoliu Guan"
                    },
                    {
                        "name": "Lielin Jiang"
                    },
                    {
                        "name": "Guanzhong Wang"
                    },
                    {
                        "name": "Zeyu Chen"
                    },
                    {
                        "name": "Yi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Liu"
                },
                "author": "Yi Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00412v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00412v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23387v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23387v2",
                "updated": "2025-08-01T03:43:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    3,
                    43,
                    24,
                    4,
                    213,
                    0
                ],
                "published": "2025-07-31T10:02:26Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    10,
                    2,
                    26,
                    3,
                    212,
                    0
                ],
                "title": "SGEMM-cube: Emulating FP32 GEMM on Ascend NPUs Using FP16 Cube Units\n  with Precision Recovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SGEMM-cube: Emulating FP32 GEMM on Ascend NPUs Using FP16 Cube Units\n  with Precision Recovery"
                },
                "summary": "Low-precision matrix engines, such as FP16 cube, offer high throughput but\nlack support for full-precision computation. In this work, we propose\nSGEMM-cube, a high-performance algorithm for emulating FP32 general\nmatrix-matrix multiplication (GEMM) using only FP16 computation units on a\nrepresentative AI accelerator. The method decomposes each FP32 operand into two\nFP16 values and compensates for numerical errors through a tunable scaling\nstrategy. A detailed analysis of numerical errors, including underflow\nconditions and precision loss, guides the selection of scaling parameters to\npreserve up to 22 bits of mantissa accuracy. We further investigate the effect\nof computation order on accuracy and demonstrate that a term-wise accumulation\nscheme improves numerical stability over conventional FP32 GEMM in low-exponent\nregimes. Finally, a cache-aware blocking strategy and double-buffered pipeline\nare introduced to overlap memory transfers with computation, enabling\nSGEMM-cube to achieve up to 77\\% of the theoretical FP32-equivalent peak\nperformance on Ascend 910A NPU lacking native FP32 support. Extensive numerical\nexperiments confirm that our method not only recovers the accuracy of native\nFP32 GEMM but also exhibits superior numerical stability under certain\nconditions, due to its structured and error-aware computation order.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-precision matrix engines, such as FP16 cube, offer high throughput but\nlack support for full-precision computation. In this work, we propose\nSGEMM-cube, a high-performance algorithm for emulating FP32 general\nmatrix-matrix multiplication (GEMM) using only FP16 computation units on a\nrepresentative AI accelerator. The method decomposes each FP32 operand into two\nFP16 values and compensates for numerical errors through a tunable scaling\nstrategy. A detailed analysis of numerical errors, including underflow\nconditions and precision loss, guides the selection of scaling parameters to\npreserve up to 22 bits of mantissa accuracy. We further investigate the effect\nof computation order on accuracy and demonstrate that a term-wise accumulation\nscheme improves numerical stability over conventional FP32 GEMM in low-exponent\nregimes. Finally, a cache-aware blocking strategy and double-buffered pipeline\nare introduced to overlap memory transfers with computation, enabling\nSGEMM-cube to achieve up to 77\\% of the theoretical FP32-equivalent peak\nperformance on Ascend 910A NPU lacking native FP32 support. Extensive numerical\nexperiments confirm that our method not only recovers the accuracy of native\nFP32 GEMM but also exhibits superior numerical stability under certain\nconditions, due to its structured and error-aware computation order."
                },
                "authors": [
                    {
                        "name": "Weicheng Xue"
                    },
                    {
                        "name": "Baisong Xu"
                    },
                    {
                        "name": "Kai Yang"
                    },
                    {
                        "name": "Yongxiang Liu"
                    },
                    {
                        "name": "Dengdeng Fan"
                    },
                    {
                        "name": "Pengxiang Xu"
                    },
                    {
                        "name": "Yonghong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yonghong Tian"
                },
                "author": "Yonghong Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23387v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23387v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22746v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22746v2",
                "updated": "2025-08-01T03:37:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    3,
                    37,
                    42,
                    4,
                    213,
                    0
                ],
                "published": "2025-07-30T15:03:36Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    15,
                    3,
                    36,
                    2,
                    211,
                    0
                ],
                "title": "Next Tokens Denoising for Speech Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next Tokens Denoising for Speech Synthesis"
                },
                "summary": "While diffusion and autoregressive (AR) models have significantly advanced\ngenerative modeling, they each present distinct limitations. AR models, which\nrely on causal attention, cannot exploit future context and suffer from slow\ngeneration speeds. Conversely, diffusion models struggle with key-value (KV)\ncaching. To overcome these challenges, we introduce Dragon-FM, a novel\ntext-to-speech (TTS) design that unifies AR and flow-matching. This model\nprocesses 48 kHz audio codec tokens in chunks at a compact rate of 12.5 tokens\nper second. This design enables AR modeling across chunks, ensuring global\ncoherence, while parallel flow-matching within chunks facilitates fast\niterative denoising. Thus, the model leverages KV-cache across chunks and\nutilizes bidirectional context within each chunk. Furthermore, it bridges\ncontinuous and discrete feature modeling, demonstrating that continuous AR\nflow-matching can predict discrete tokens with finite scalar quantizers. This\nefficient codec and fast chunk-autoregressive architecture also make the model\nhighly effective for generating long-form content, such as podcasts.\nExperiments on podcast datasets demonstrate its capability to efficiently\ngenerate high-quality zero-shot podcasts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While diffusion and autoregressive (AR) models have significantly advanced\ngenerative modeling, they each present distinct limitations. AR models, which\nrely on causal attention, cannot exploit future context and suffer from slow\ngeneration speeds. Conversely, diffusion models struggle with key-value (KV)\ncaching. To overcome these challenges, we introduce Dragon-FM, a novel\ntext-to-speech (TTS) design that unifies AR and flow-matching. This model\nprocesses 48 kHz audio codec tokens in chunks at a compact rate of 12.5 tokens\nper second. This design enables AR modeling across chunks, ensuring global\ncoherence, while parallel flow-matching within chunks facilitates fast\niterative denoising. Thus, the model leverages KV-cache across chunks and\nutilizes bidirectional context within each chunk. Furthermore, it bridges\ncontinuous and discrete feature modeling, demonstrating that continuous AR\nflow-matching can predict discrete tokens with finite scalar quantizers. This\nefficient codec and fast chunk-autoregressive architecture also make the model\nhighly effective for generating long-form content, such as podcasts.\nExperiments on podcast datasets demonstrate its capability to efficiently\ngenerate high-quality zero-shot podcasts."
                },
                "authors": [
                    {
                        "name": "Yanqing Liu"
                    },
                    {
                        "name": "Ruiqing Xue"
                    },
                    {
                        "name": "Chong Zhang"
                    },
                    {
                        "name": "Yufei Liu"
                    },
                    {
                        "name": "Gang Wang"
                    },
                    {
                        "name": "Bohan Li"
                    },
                    {
                        "name": "Yao Qian"
                    },
                    {
                        "name": "Lei He"
                    },
                    {
                        "name": "Shujie Liu"
                    },
                    {
                        "name": "Sheng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Zhao"
                },
                "author": "Sheng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22746v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22746v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02659v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02659v2",
                "updated": "2025-07-31T21:00:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    21,
                    0,
                    28,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-03T14:20:41Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    20,
                    41,
                    3,
                    184,
                    0
                ],
                "title": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding"
                },
                "summary": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup."
                },
                "authors": [
                    {
                        "name": "Ramchalam Kinattinkara Ramakrishnan"
                    },
                    {
                        "name": "Zhaocong Yuan"
                    },
                    {
                        "name": "Shaojie Zhuo"
                    },
                    {
                        "name": "Chen Feng"
                    },
                    {
                        "name": "Yicheng Lin"
                    },
                    {
                        "name": "Chenzheng Su"
                    },
                    {
                        "name": "Xiaopeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaopeng Zhang"
                },
                "author": "Xiaopeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02659v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02659v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22701v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22701v2",
                "updated": "2025-07-31T16:21:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    16,
                    21,
                    3,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-30T14:10:16Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    14,
                    10,
                    16,
                    2,
                    211,
                    0
                ],
                "title": "SAM: A Stability-Aware Cache Manager for Multi-Tenant Embedded Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAM: A Stability-Aware Cache Manager for Multi-Tenant Embedded Databases"
                },
                "summary": "The co-location of multiple database instances on resource constrained edge\nnodes creates significant cache contention, where traditional schemes are\ninefficient and unstable under dynamic workloads. To address this, we present\nSAM(a Stability-Aware Manager), an autonomic cache manager that establishes\ndecision stability as a first-class design principle. It achieves this through\nits core control policy, AURA(Autonomic Utility-balancing Resource Allocator),\nwhich resolves the classic exploitation-exploration dilemma by synthesizing two\northogonal factors: the H-factor, representing proven historical efficiency\n(exploitation), and the V-factor, for estimated marginal gain (exploration).\nThrough this practical synthesis and adaptive control, SAM achieves sustained\nhigh performance with strategic stability and robustness in volatile\nconditions.\n  Extensive experiments against 14 diverse baselines demonstrate SAM's\nsuperiority. It achieves top-tier throughput while being uniquely resilient to\ncomplex workload shifts and adversarial workloads like cache pollution.\nFurthermore, its decision latency is highly scalable, remaining nearly constant\nas the system grows to 120 databases. Crucially, SAM achieves superior decision\nstability -- maintaining consistent optimization directions despite noise,\navoiding performance oscillations while ensuring predictable Quality of\nService. These results prove that a principled, stability-aware design is\nessential for sustained high performance in real-world, large-scale systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The co-location of multiple database instances on resource constrained edge\nnodes creates significant cache contention, where traditional schemes are\ninefficient and unstable under dynamic workloads. To address this, we present\nSAM(a Stability-Aware Manager), an autonomic cache manager that establishes\ndecision stability as a first-class design principle. It achieves this through\nits core control policy, AURA(Autonomic Utility-balancing Resource Allocator),\nwhich resolves the classic exploitation-exploration dilemma by synthesizing two\northogonal factors: the H-factor, representing proven historical efficiency\n(exploitation), and the V-factor, for estimated marginal gain (exploration).\nThrough this practical synthesis and adaptive control, SAM achieves sustained\nhigh performance with strategic stability and robustness in volatile\nconditions.\n  Extensive experiments against 14 diverse baselines demonstrate SAM's\nsuperiority. It achieves top-tier throughput while being uniquely resilient to\ncomplex workload shifts and adversarial workloads like cache pollution.\nFurthermore, its decision latency is highly scalable, remaining nearly constant\nas the system grows to 120 databases. Crucially, SAM achieves superior decision\nstability -- maintaining consistent optimization directions despite noise,\navoiding performance oscillations while ensuring predictable Quality of\nService. These results prove that a principled, stability-aware design is\nessential for sustained high performance in real-world, large-scale systems."
                },
                "authors": [
                    {
                        "name": "Haoran Zhang"
                    },
                    {
                        "name": "Decheng Zuo"
                    },
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Zhiyu Liang"
                    },
                    {
                        "name": "Hongzhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hongzhi Wang"
                },
                "author": "Hongzhi Wang",
                "arxiv_comment": "17 pages, 10 figures. An extended version of a paper under review at\n  the VLDB 2026 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22701v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22701v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.4; H.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23674v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23674v1",
                "updated": "2025-07-31T15:50:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    50,
                    57,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T15:50:57Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    50,
                    57,
                    3,
                    212,
                    0
                ],
                "title": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses"
                },
                "summary": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience."
                },
                "authors": [
                    {
                        "name": "Muhammad Taha Cheema"
                    },
                    {
                        "name": "Abeer Aamir"
                    },
                    {
                        "name": "Khawaja Gul Muhammad"
                    },
                    {
                        "name": "Naveed Anwar Bhatti"
                    },
                    {
                        "name": "Ihsan Ayyub Qazi"
                    },
                    {
                        "name": "Zafar Ayyub Qazi"
                    }
                ],
                "author_detail": {
                    "name": "Zafar Ayyub Qazi"
                },
                "author": "Zafar Ayyub Qazi",
                "arxiv_comment": "13 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23674v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21433v2",
                "updated": "2025-07-31T07:53:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    7,
                    53,
                    53,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-29T02:05:51Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    2,
                    5,
                    51,
                    1,
                    210,
                    0
                ],
                "title": "MemShare: Memory Efficient Inference for Large Reasoning Models through\n  KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemShare: Memory Efficient Inference for Large Reasoning Models through\n  KV Cache Reuse"
                },
                "summary": "Large Reasoning Models (LRMs) have achieved significant advances in\nmathematical reasoning and formal logic tasks. However, their tendency to\ngenerate lengthy chain-of-thought sequences leads to substantial memory\noverhead during inference. We observe that LRMs frequently produce highly\nsimilar intermediate reasoning steps, which correspond to similar KV cache\nstates across layers. Motivated by this observation, we propose MemShare, a\nnovel KV cache management approach that effectively reduces memory overhead.\nMemShare employs a collaborative filtering algorithm to efficiently identify\nreusable KV cache blocks and enables zero copy cache reuse to significantly\nreduce memory overhead, improve throughput while maintaining accuracy.\nExperimental results demonstrate that MemShare delivers up to 84.79\\%\nimprovement in throughput while maintaining better accuracy compared to\nexisting KV cache management methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models (LRMs) have achieved significant advances in\nmathematical reasoning and formal logic tasks. However, their tendency to\ngenerate lengthy chain-of-thought sequences leads to substantial memory\noverhead during inference. We observe that LRMs frequently produce highly\nsimilar intermediate reasoning steps, which correspond to similar KV cache\nstates across layers. Motivated by this observation, we propose MemShare, a\nnovel KV cache management approach that effectively reduces memory overhead.\nMemShare employs a collaborative filtering algorithm to efficiently identify\nreusable KV cache blocks and enables zero copy cache reuse to significantly\nreduce memory overhead, improve throughput while maintaining accuracy.\nExperimental results demonstrate that MemShare delivers up to 84.79\\%\nimprovement in throughput while maintaining better accuracy compared to\nexisting KV cache management methods."
                },
                "authors": [
                    {
                        "name": "Kaiwen Chen"
                    },
                    {
                        "name": "Xin Tan"
                    },
                    {
                        "name": "Minchen Yu"
                    },
                    {
                        "name": "Hong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Hong Xu"
                },
                "author": "Hong Xu",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01199v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01199v2",
                "updated": "2025-07-31T07:35:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    7,
                    35,
                    4,
                    3,
                    212,
                    0
                ],
                "published": "2025-03-03T05:52:02Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    5,
                    52,
                    2,
                    0,
                    62,
                    0
                ],
                "title": "LiteGS: A High-performance Framework to Train 3DGS in Subminutes via\n  System and Algorithm Codesign",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiteGS: A High-performance Framework to Train 3DGS in Subminutes via\n  System and Algorithm Codesign"
                },
                "summary": "3D Gaussian Splatting (3DGS) has emerged as promising alternative in 3D\nrepresentation. However, it still suffers from high training cost. This paper\nintroduces LiteGS, a high performance framework that systematically optimizes\nthe 3DGS training pipeline from multiple aspects. At the low-level computation\nlayer, we design a ``warp-based raster'' associated with two hardware-aware\noptimizations to significantly reduce gradient reduction overhead. At the\nmid-level data management layer, we introduce dynamic spatial sorting based on\nMorton coding to enable a performant ``Cluster-Cull-Compact'' pipeline and\nimprove data locality, therefore reducing cache misses. At the top-level\nalgorithm layer, we establish a new robust densification criterion based on the\nvariance of the opacity gradient, paired with a more stable opacity control\nmechanism, to achieve more precise parameter growth. Experimental results\ndemonstrate that LiteGS accelerates the original 3DGS training by up to 13.4x\nwith comparable or superior quality and surpasses the current SOTA in\nlightweight models by up to 1.4x speedup. For high-quality reconstruction\ntasks, LiteGS sets a new accuracy record and decreases the training time by an\norder of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Gaussian Splatting (3DGS) has emerged as promising alternative in 3D\nrepresentation. However, it still suffers from high training cost. This paper\nintroduces LiteGS, a high performance framework that systematically optimizes\nthe 3DGS training pipeline from multiple aspects. At the low-level computation\nlayer, we design a ``warp-based raster'' associated with two hardware-aware\noptimizations to significantly reduce gradient reduction overhead. At the\nmid-level data management layer, we introduce dynamic spatial sorting based on\nMorton coding to enable a performant ``Cluster-Cull-Compact'' pipeline and\nimprove data locality, therefore reducing cache misses. At the top-level\nalgorithm layer, we establish a new robust densification criterion based on the\nvariance of the opacity gradient, paired with a more stable opacity control\nmechanism, to achieve more precise parameter growth. Experimental results\ndemonstrate that LiteGS accelerates the original 3DGS training by up to 13.4x\nwith comparable or superior quality and surpasses the current SOTA in\nlightweight models by up to 1.4x speedup. For high-quality reconstruction\ntasks, LiteGS sets a new accuracy record and decreases the training time by an\norder of magnitude."
                },
                "authors": [
                    {
                        "name": "Kaimin Liao"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Luchao Wang"
                    },
                    {
                        "name": "Yaohua Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yaohua Tang"
                },
                "author": "Yaohua Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01199v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01199v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23292v1",
                "updated": "2025-07-31T07:10:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    7,
                    10,
                    39,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T07:10:39Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    7,
                    10,
                    39,
                    3,
                    212,
                    0
                ],
                "title": "SequenceLayers: Sequence Processing and Streaming Neural Networks Made\n  Easy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SequenceLayers: Sequence Processing and Streaming Neural Networks Made\n  Easy"
                },
                "summary": "We introduce a neural network layer API and library for sequence modeling,\ndesigned for easy creation of sequence models that can be executed both\nlayer-by-layer (e.g., teacher-forced training) and step-by-step (e.g.,\nautoregressive sampling). To achieve this, layers define an explicit\nrepresentation of their state over time (e.g., a Transformer KV cache, a\nconvolution buffer, an RNN hidden state), and a step method that evolves that\nstate, tested to give identical results to a stateless layer-wise invocation.\nThis and other aspects of the SequenceLayers contract enables complex models to\nbe immediately streamable, mitigates a wide range of common bugs arising in\nboth streaming and parallel sequence processing, and can be implemented in any\ndeep learning library. A composable and declarative API, along with a\ncomprehensive suite of layers and combinators, streamlines the construction of\nproduction-scale models from simple streamable components while preserving\nstrong correctness guarantees. Our current implementations of SequenceLayers\n(JAX, TensorFlow 2) are available at https://github.com/google/sequence-layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a neural network layer API and library for sequence modeling,\ndesigned for easy creation of sequence models that can be executed both\nlayer-by-layer (e.g., teacher-forced training) and step-by-step (e.g.,\nautoregressive sampling). To achieve this, layers define an explicit\nrepresentation of their state over time (e.g., a Transformer KV cache, a\nconvolution buffer, an RNN hidden state), and a step method that evolves that\nstate, tested to give identical results to a stateless layer-wise invocation.\nThis and other aspects of the SequenceLayers contract enables complex models to\nbe immediately streamable, mitigates a wide range of common bugs arising in\nboth streaming and parallel sequence processing, and can be implemented in any\ndeep learning library. A composable and declarative API, along with a\ncomprehensive suite of layers and combinators, streamlines the construction of\nproduction-scale models from simple streamable components while preserving\nstrong correctness guarantees. Our current implementations of SequenceLayers\n(JAX, TensorFlow 2) are available at https://github.com/google/sequence-layers."
                },
                "authors": [
                    {
                        "name": "RJ Skerry-Ryan"
                    },
                    {
                        "name": "Julian Salazar"
                    },
                    {
                        "name": "Soroosh Mariooryad"
                    },
                    {
                        "name": "David Kao"
                    },
                    {
                        "name": "Daisy Stanton"
                    },
                    {
                        "name": "Eric Battenberg"
                    },
                    {
                        "name": "Matt Shannon"
                    },
                    {
                        "name": "Ron J. Weiss"
                    },
                    {
                        "name": "Robin Scheibler"
                    },
                    {
                        "name": "Jonas Rothfuss"
                    },
                    {
                        "name": "Tom Bagby"
                    }
                ],
                "author_detail": {
                    "name": "Tom Bagby"
                },
                "author": "Tom Bagby",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07966v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07966v3",
                "updated": "2025-07-30T16:55:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    55,
                    33,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-10T17:47:40Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    47,
                    40,
                    3,
                    191,
                    0
                ],
                "title": "Scaling RL to Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling RL to Long Videos"
                },
                "summary": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B\nsupports processing up to 8,192 video frames per video, and configurable FPS\nsettings. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B\nsupports processing up to 8,192 video frames per video, and configurable FPS\nsettings. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames)."
                },
                "authors": [
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Baifeng Shi"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Hanrong Ye"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Xiaojuan Qi"
                    },
                    {
                        "name": "Sifei Liu"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Code at https://github.com/NVlabs/Long-RL and model at\n  https://huggingface.co/Efficient-Large-Model/LongVILA-R1-7B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07966v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07966v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22801v1",
                "updated": "2025-07-30T16:04:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    4,
                    1,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T16:04:01Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    4,
                    1,
                    2,
                    211,
                    0
                ],
                "title": "DSPE: Profit Maximization in Edge-Cloud Storage System using Dynamic\n  Space Partitioning with Erasure Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DSPE: Profit Maximization in Edge-Cloud Storage System using Dynamic\n  Space Partitioning with Erasure Code"
                },
                "summary": "Edge Storage Systems have emerged as a critical enabler of low latency data\naccess in modern cloud networks by bringing storage and computation closer to\nend users. However, the limited storage capacity of edge servers poses\nsignificant challenges in handling high volume and latency sensitive data\naccess requests, particularly under dynamic workloads. In this work, we propose\na profit driven framework that integrates three key mechanisms which are\ncollaborative caching, erasure coding, and elastic storage partitioning. Unlike\ntraditional replication, erasure coding enables space efficient redundancy,\nallowing data to be reconstructed from any subset of K out of K plus M coded\nblocks. We dynamically partition each edge server s storage into private and\npublic regions. The private region is further subdivided among access points\nbased on their incoming request rates, enabling adaptive control over data\nlocality and ownership. We design a data placement and replacement policy that\ndetermines how and where to store or evict coded data blocks to maximize data\naccess within deadlines. While the private region serves requests from local\nAPs, the public region handles cooperative storage requests from neighboring\nservers. Our proposed Dynamic Space Partitioning and Elastic caching strategy\nis evaluated on both synthetic and real world traces from Netflix and Spotify.\nExperimental results show that our method improves overall system profitability\nby approximately 5 to 8% compared to state of the art approaches under varied\nworkload conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Storage Systems have emerged as a critical enabler of low latency data\naccess in modern cloud networks by bringing storage and computation closer to\nend users. However, the limited storage capacity of edge servers poses\nsignificant challenges in handling high volume and latency sensitive data\naccess requests, particularly under dynamic workloads. In this work, we propose\na profit driven framework that integrates three key mechanisms which are\ncollaborative caching, erasure coding, and elastic storage partitioning. Unlike\ntraditional replication, erasure coding enables space efficient redundancy,\nallowing data to be reconstructed from any subset of K out of K plus M coded\nblocks. We dynamically partition each edge server s storage into private and\npublic regions. The private region is further subdivided among access points\nbased on their incoming request rates, enabling adaptive control over data\nlocality and ownership. We design a data placement and replacement policy that\ndetermines how and where to store or evict coded data blocks to maximize data\naccess within deadlines. While the private region serves requests from local\nAPs, the public region handles cooperative storage requests from neighboring\nservers. Our proposed Dynamic Space Partitioning and Elastic caching strategy\nis evaluated on both synthetic and real world traces from Netflix and Spotify.\nExperimental results show that our method improves overall system profitability\nby approximately 5 to 8% compared to state of the art approaches under varied\nworkload conditions."
                },
                "authors": [
                    {
                        "name": "Shubhradeep Roy"
                    },
                    {
                        "name": "Suvarthi Sarkar"
                    },
                    {
                        "name": "Vivek Verma"
                    },
                    {
                        "name": "Aryabartta Sahu"
                    }
                ],
                "author_detail": {
                    "name": "Aryabartta Sahu"
                },
                "author": "Aryabartta Sahu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22636v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22636v1",
                "updated": "2025-07-30T12:55:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    55,
                    55,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T12:55:55Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    55,
                    55,
                    2,
                    211,
                    0
                ],
                "title": "All-gluon amplitudes with off-shell recursion in multiplet bases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All-gluon amplitudes with off-shell recursion in multiplet bases"
                },
                "summary": "The efficient computation of color-summed QCD amplitudes at high parton\nmultiplicities remains a central challenge for precision collider predictions.\nExisting approaches using trace, color-flow, or adjoint bases suffer from\nnon-orthogonality, which complicates the color algebra and scales poorly with\nmultiplicity. In this work, we present an off-shell recursive framework for\ncomputing all-gluon tree-level amplitudes directly in orthogonal multiplet\nbases. Utilizing Wigner $6j$ coefficients, we construct an algorithm that\nbuilds multiplet-projected off-shell currents from lower-point currents. By\noptimizing the recursion through partial summation and caching, we find that\nthe computational complexity of calculating $n$-gluon color-summed squared\namplitudes scales as $\\mathcal{O}(17^n)$. This demonstrates the potential\ncompetitiveness of multiplet bases for high-multiplicity processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficient computation of color-summed QCD amplitudes at high parton\nmultiplicities remains a central challenge for precision collider predictions.\nExisting approaches using trace, color-flow, or adjoint bases suffer from\nnon-orthogonality, which complicates the color algebra and scales poorly with\nmultiplicity. In this work, we present an off-shell recursive framework for\ncomputing all-gluon tree-level amplitudes directly in orthogonal multiplet\nbases. Utilizing Wigner $6j$ coefficients, we construct an algorithm that\nbuilds multiplet-projected off-shell currents from lower-point currents. By\noptimizing the recursion through partial summation and caching, we find that\nthe computational complexity of calculating $n$-gluon color-summed squared\namplitudes scales as $\\mathcal{O}(17^n)$. This demonstrates the potential\ncompetitiveness of multiplet bases for high-multiplicity processes."
                },
                "authors": [
                    {
                        "name": "Oskar Bolinder"
                    },
                    {
                        "name": "Rikkert Frederix"
                    },
                    {
                        "name": "Malin Sjodahl"
                    }
                ],
                "author_detail": {
                    "name": "Malin Sjodahl"
                },
                "author": "Malin Sjodahl",
                "arxiv_comment": "15 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22636v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22636v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20984v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20984v2",
                "updated": "2025-07-30T06:29:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    6,
                    29,
                    40,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-28T16:45:14Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    45,
                    14,
                    0,
                    209,
                    0
                ],
                "title": "SmallThinker: A Family of Efficient Large Language Models Natively\n  Trained for Local Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmallThinker: A Family of Efficient Large Language Models Natively\n  Trained for Local Deployment"
                },
                "summary": "While frontier large language models (LLMs) continue to push capability\nboundaries, their deployment remains confined to GPU-powered cloud\ninfrastructure. We challenge this paradigm with SmallThinker, a family of LLMs\nnatively designed - not adapted - for the unique constraints of local devices:\nweak computational power, limited memory, and slow storage. Unlike traditional\napproaches that mainly compress existing models built for clouds, we architect\nSmallThinker from the ground up to thrive within these limitations. Our\ninnovation lies in a deployment-aware architecture that transforms constraints\ninto design principles. First, We introduce a two-level sparse structure\ncombining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward\nnetworks, drastically reducing computational demands without sacrificing model\ncapacity. Second, to conquer the I/O bottleneck of slow storage, we design a\npre-attention router that enables our co-designed inference engine to prefetch\nexpert parameters from storage while computing attention, effectively hiding\nstorage latency that would otherwise cripple on-device inference. Third, for\nmemory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to\nslash KV cache requirements. We release SmallThinker-4B-A0.6B and\nSmallThinker-21B-A3B, which achieve state-of-the-art performance scores and\neven outperform larger LLMs. Remarkably, our co-designed system mostly\neliminates the need for expensive GPU hardware: with Q4_0 quantization, both\nmodels exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB\nand 8GB of memory respectively. SmallThinker is publicly available at\nhf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and\nhf.co/PowerInfer/SmallThinker-21BA3B-Instruct.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While frontier large language models (LLMs) continue to push capability\nboundaries, their deployment remains confined to GPU-powered cloud\ninfrastructure. We challenge this paradigm with SmallThinker, a family of LLMs\nnatively designed - not adapted - for the unique constraints of local devices:\nweak computational power, limited memory, and slow storage. Unlike traditional\napproaches that mainly compress existing models built for clouds, we architect\nSmallThinker from the ground up to thrive within these limitations. Our\ninnovation lies in a deployment-aware architecture that transforms constraints\ninto design principles. First, We introduce a two-level sparse structure\ncombining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward\nnetworks, drastically reducing computational demands without sacrificing model\ncapacity. Second, to conquer the I/O bottleneck of slow storage, we design a\npre-attention router that enables our co-designed inference engine to prefetch\nexpert parameters from storage while computing attention, effectively hiding\nstorage latency that would otherwise cripple on-device inference. Third, for\nmemory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to\nslash KV cache requirements. We release SmallThinker-4B-A0.6B and\nSmallThinker-21B-A3B, which achieve state-of-the-art performance scores and\neven outperform larger LLMs. Remarkably, our co-designed system mostly\neliminates the need for expensive GPU hardware: with Q4_0 quantization, both\nmodels exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB\nand 8GB of memory respectively. SmallThinker is publicly available at\nhf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and\nhf.co/PowerInfer/SmallThinker-21BA3B-Instruct."
                },
                "authors": [
                    {
                        "name": "Yixin Song"
                    },
                    {
                        "name": "Zhenliang Xue"
                    },
                    {
                        "name": "Dongliang Wei"
                    },
                    {
                        "name": "Feiyang Chen"
                    },
                    {
                        "name": "Jianxiang Gao"
                    },
                    {
                        "name": "Junchen Liu"
                    },
                    {
                        "name": "Hangyu Liang"
                    },
                    {
                        "name": "Guangshuo Qin"
                    },
                    {
                        "name": "Chengrong Tian"
                    },
                    {
                        "name": "Bo Wen"
                    },
                    {
                        "name": "Longyu Zhao"
                    },
                    {
                        "name": "Xinrui Zheng"
                    },
                    {
                        "name": "Zeyu Mi"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20984v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20984v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19442v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19442v3",
                "updated": "2025-07-30T05:24:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    5,
                    24,
                    46,
                    2,
                    211,
                    0
                ],
                "published": "2024-12-27T04:17:57Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    17,
                    57,
                    4,
                    362,
                    0
                ],
                "title": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management"
                },
                "summary": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Tianhao Tang"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Nicole Hu"
                    },
                    {
                        "name": "Wei Dong"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "arxiv_comment": "Accepted to TMLR 2025. The revised version incorporates more papers\n  and has been further polished",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19442v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19442v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00904v1",
                "updated": "2025-07-29T03:08:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    29,
                    3,
                    8,
                    31,
                    1,
                    210,
                    0
                ],
                "published": "2025-07-29T03:08:31Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    3,
                    8,
                    31,
                    1,
                    210,
                    0
                ],
                "title": "Forecasting LLM Inference Performance via Hardware-Agnostic Analytical\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting LLM Inference Performance via Hardware-Agnostic Analytical\n  Modeling"
                },
                "summary": "Large language models (LLMs) have been increasingly deployed as local agents\non personal devices with CPUs, NPUs and integrated GPUs. However, forecasting\ninference performance on devices with such heterogeneity remains challenging\ndue to the dynamic compute and memory demands. Existing approaches rely on GPU\nbenchmarking or machine learning-based latency predictors, which are often\nhardware-specific and lack generalizability. To this end, we introduce LIFE, a\nlightweight and modular analytical framework that is comprised of modular\nanalytical model of operators, configurable to characterize LLM inference\nworkloads in a hardware and dataset-agnostic manner. LIFE characterizes the\ninfluence of software and model optimizations, such as quantization, KV cache\ncompression, LoRA adapters, chunked prefill, different attentions, and operator\nfusion, on performance metrics such as time-to-first-token (TTFT),\ntime-per-output-token (TPOT) and tokens-per-second (TPS). LIFE enables\nperformance forecasting using only hardware specifications, such as TOPS and\nmemory bandwidth, without requiring extensive dataset benchmarking. We validate\nLIFE's forecasting with inference on AMD Ryzen CPUs, NPUs, iGPUs and NVIDIA\nV100 GPUs, with Llama2-7B variants, demonstrating the utility of LIFE in\nforecasting LLM performance through lens of system efficiency to enable\nefficient LLM deployment across different hardware platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been increasingly deployed as local agents\non personal devices with CPUs, NPUs and integrated GPUs. However, forecasting\ninference performance on devices with such heterogeneity remains challenging\ndue to the dynamic compute and memory demands. Existing approaches rely on GPU\nbenchmarking or machine learning-based latency predictors, which are often\nhardware-specific and lack generalizability. To this end, we introduce LIFE, a\nlightweight and modular analytical framework that is comprised of modular\nanalytical model of operators, configurable to characterize LLM inference\nworkloads in a hardware and dataset-agnostic manner. LIFE characterizes the\ninfluence of software and model optimizations, such as quantization, KV cache\ncompression, LoRA adapters, chunked prefill, different attentions, and operator\nfusion, on performance metrics such as time-to-first-token (TTFT),\ntime-per-output-token (TPOT) and tokens-per-second (TPS). LIFE enables\nperformance forecasting using only hardware specifications, such as TOPS and\nmemory bandwidth, without requiring extensive dataset benchmarking. We validate\nLIFE's forecasting with inference on AMD Ryzen CPUs, NPUs, iGPUs and NVIDIA\nV100 GPUs, with Llama2-7B variants, demonstrating the utility of LIFE in\nforecasting LLM performance through lens of system efficiency to enable\nefficient LLM deployment across different hardware platforms."
                },
                "authors": [
                    {
                        "name": "Rajeev Patwari"
                    },
                    {
                        "name": "Ashish Sirasao"
                    },
                    {
                        "name": "Devleena Das"
                    }
                ],
                "author_detail": {
                    "name": "Devleena Das"
                },
                "author": "Devleena Das",
                "arxiv_comment": "10 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24358v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24358v2",
                "updated": "2025-07-28T20:44:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    20,
                    44,
                    23,
                    0,
                    209,
                    0
                ],
                "published": "2025-03-31T17:37:32Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    37,
                    32,
                    0,
                    90,
                    0
                ],
                "title": "SQuat: Subspace-orthogonal KV Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQuat: Subspace-orthogonal KV Cache Quantization"
                },
                "summary": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms."
                },
                "authors": [
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Ligong Han"
                    },
                    {
                        "name": "Kai Xu"
                    },
                    {
                        "name": "Akash Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "Akash Srivastava"
                },
                "author": "Akash Srivastava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24358v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24358v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.13349v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.13349v3",
                "updated": "2025-07-28T14:11:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    11,
                    53,
                    0,
                    209,
                    0
                ],
                "published": "2023-11-22T12:34:51Z",
                "published_parsed": [
                    2023,
                    11,
                    22,
                    12,
                    34,
                    51,
                    2,
                    326,
                    0
                ],
                "title": "REDS: Resource-Efficient Deep Subnetworks for Dynamic Resource\n  Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REDS: Resource-Efficient Deep Subnetworks for Dynamic Resource\n  Constraints"
                },
                "summary": "Deep learning models deployed on edge devices frequently encounter resource\nvariability, which arises from fluctuating energy levels, timing constraints,\nor prioritization of other critical tasks within the system. State-of-the-art\nmachine learning pipelines generate resource-agnostic models that are not\ncapable to adapt at runtime. In this work, we introduce Resource-Efficient Deep\nSubnetworks (REDS) to tackle model adaptation to variable resources. In\ncontrast to the state-of-the-art, REDS leverages structured sparsity\nconstructively by exploiting permutation invariance of neurons, which allows\nfor hardware-specific optimizations. Specifically, REDS achieves computational\nefficiency by (1) skipping sequential computational blocks identified by a\nnovel iterative knapsack optimizer, and (2) taking advantage of data cache by\nre-arranging the order of operations in REDS computational graph. REDS supports\nconventional deep networks frequently deployed on the edge and provides\ncomputational benefits even for small and simple networks. We evaluate REDS on\neight benchmark architectures trained on the Visual Wake Words, Google Speech\nCommands, Fashion-MNIST, CIFAR-10 and ImageNet-1K datasets, and test on four\noff-the-shelf mobile and embedded hardware platforms. We provide a theoretical\nresult and empirical evidence demonstrating REDS' outstanding performance in\nterms of submodels' test set accuracy, and demonstrate an adaptation time in\nresponse to dynamic resource constraints of under 40$\\mu$s, utilizing a\nfully-connected network on Arduino Nano 33 BLE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models deployed on edge devices frequently encounter resource\nvariability, which arises from fluctuating energy levels, timing constraints,\nor prioritization of other critical tasks within the system. State-of-the-art\nmachine learning pipelines generate resource-agnostic models that are not\ncapable to adapt at runtime. In this work, we introduce Resource-Efficient Deep\nSubnetworks (REDS) to tackle model adaptation to variable resources. In\ncontrast to the state-of-the-art, REDS leverages structured sparsity\nconstructively by exploiting permutation invariance of neurons, which allows\nfor hardware-specific optimizations. Specifically, REDS achieves computational\nefficiency by (1) skipping sequential computational blocks identified by a\nnovel iterative knapsack optimizer, and (2) taking advantage of data cache by\nre-arranging the order of operations in REDS computational graph. REDS supports\nconventional deep networks frequently deployed on the edge and provides\ncomputational benefits even for small and simple networks. We evaluate REDS on\neight benchmark architectures trained on the Visual Wake Words, Google Speech\nCommands, Fashion-MNIST, CIFAR-10 and ImageNet-1K datasets, and test on four\noff-the-shelf mobile and embedded hardware platforms. We provide a theoretical\nresult and empirical evidence demonstrating REDS' outstanding performance in\nterms of submodels' test set accuracy, and demonstrate an adaptation time in\nresponse to dynamic resource constraints of under 40$\\mu$s, utilizing a\nfully-connected network on Arduino Nano 33 BLE."
                },
                "authors": [
                    {
                        "name": "Francesco Corti"
                    },
                    {
                        "name": "Balz Maag"
                    },
                    {
                        "name": "Joachim Schauer"
                    },
                    {
                        "name": "Ulrich Pferschy"
                    },
                    {
                        "name": "Olga Saukh"
                    }
                ],
                "author_detail": {
                    "name": "Olga Saukh"
                },
                "author": "Olga Saukh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.13349v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.13349v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20677v1",
                "updated": "2025-07-28T09:59:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    59,
                    22,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T09:59:22Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    59,
                    22,
                    0,
                    209,
                    0
                ],
                "title": "Quantum Circuit Caches and Compressors for Low Latency, High Throughput\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Circuit Caches and Compressors for Low Latency, High Throughput\n  Computing"
                },
                "summary": "Utility-scale quantum programs contain operations on the order of $>10^{15}$\nwhich must be prepared and piped from a classical co-processor to the control\nunit of the quantum device. The latency of this process significantly increases\nwith the size of the program: existing high-level classical representations of\nquantum programs are typically memory intensive and do not na\\\"ively\nefficiently scale to the degree required to execute utility-scale programs in\nreal-time. To combat this limitation, we propose the utilization of high-level\nquantum circuit caches and compressors. The first save on the time associated\nwith repetitive tasks and sub-circuits, and the latter are useful for\nrepresenting the programs/circuits in memory-efficient formats. We present\nnumerical evidence that caches and compressors can offer five orders of\nmagnitude lower latencies during the automatic transpilation of extremely large\nquantum circuits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utility-scale quantum programs contain operations on the order of $>10^{15}$\nwhich must be prepared and piped from a classical co-processor to the control\nunit of the quantum device. The latency of this process significantly increases\nwith the size of the program: existing high-level classical representations of\nquantum programs are typically memory intensive and do not na\\\"ively\nefficiently scale to the degree required to execute utility-scale programs in\nreal-time. To combat this limitation, we propose the utilization of high-level\nquantum circuit caches and compressors. The first save on the time associated\nwith repetitive tasks and sub-circuits, and the latter are useful for\nrepresenting the programs/circuits in memory-efficient formats. We present\nnumerical evidence that caches and compressors can offer five orders of\nmagnitude lower latencies during the automatic transpilation of extremely large\nquantum circuits."
                },
                "authors": [
                    {
                        "name": "Ioana Moflic"
                    },
                    {
                        "name": "Alan Robertson"
                    },
                    {
                        "name": "Simon J. Devitt"
                    },
                    {
                        "name": "Alexandru Paler"
                    }
                ],
                "author_detail": {
                    "name": "Alexandru Paler"
                },
                "author": "Alexandru Paler",
                "arxiv_comment": "accepted at Q-CORE workshop of the QCE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20613v1",
                "updated": "2025-07-28T08:27:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    8,
                    27,
                    40,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T08:27:40Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    8,
                    27,
                    40,
                    0,
                    209,
                    0
                ],
                "title": "Enhancing Large Multimodal Models with Adaptive Sparsity and KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Large Multimodal Models with Adaptive Sparsity and KV Cache\n  Compression"
                },
                "summary": "Large multimodal models (LMMs) have advanced significantly by integrating\nvisual encoders with extensive language models, enabling robust reasoning\ncapabilities. However, compressing LMMs for deployment on edge devices remains\na critical challenge. In this work, we propose an adaptive search algorithm\nthat optimizes sparsity and KV cache compression to enhance LMM efficiency.\nUtilizing the Tree-structured Parzen Estimator, our method dynamically adjusts\npruning ratios and KV cache quantization bandwidth across different LMM layers,\nusing model performance as the optimization objective. This approach uniquely\ncombines pruning with key-value cache quantization and incorporates a fast\npruning technique that eliminates the need for additional fine-tuning or weight\nadjustments, achieving efficient compression without compromising accuracy.\nComprehensive evaluations on benchmark datasets, including LLaVA-1.5 7B and\n13B, demonstrate our method superiority over state-of-the-art techniques such\nas SparseGPT and Wanda across various compression levels. Notably, our\nframework automatic allocation of KV cache compression resources sets a new\nstandard in LMM optimization, delivering memory efficiency without sacrificing\nmuch performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large multimodal models (LMMs) have advanced significantly by integrating\nvisual encoders with extensive language models, enabling robust reasoning\ncapabilities. However, compressing LMMs for deployment on edge devices remains\na critical challenge. In this work, we propose an adaptive search algorithm\nthat optimizes sparsity and KV cache compression to enhance LMM efficiency.\nUtilizing the Tree-structured Parzen Estimator, our method dynamically adjusts\npruning ratios and KV cache quantization bandwidth across different LMM layers,\nusing model performance as the optimization objective. This approach uniquely\ncombines pruning with key-value cache quantization and incorporates a fast\npruning technique that eliminates the need for additional fine-tuning or weight\nadjustments, achieving efficient compression without compromising accuracy.\nComprehensive evaluations on benchmark datasets, including LLaVA-1.5 7B and\n13B, demonstrate our method superiority over state-of-the-art techniques such\nas SparseGPT and Wanda across various compression levels. Notably, our\nframework automatic allocation of KV cache compression resources sets a new\nstandard in LMM optimization, delivering memory efficiency without sacrificing\nmuch performance."
                },
                "authors": [
                    {
                        "name": "Te Zhang"
                    },
                    {
                        "name": "Yuheng Li"
                    },
                    {
                        "name": "Junxiang Wang"
                    },
                    {
                        "name": "Lujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Lujun Li"
                },
                "author": "Lujun Li",
                "arxiv_comment": "6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01760v2",
                "updated": "2025-07-28T04:25:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    4,
                    25,
                    58,
                    0,
                    209,
                    0
                ],
                "published": "2024-10-02T17:14:47Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    14,
                    47,
                    2,
                    276,
                    0
                ],
                "title": "Learning-Augmented Online Caching: New Upper Bounds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-Augmented Online Caching: New Upper Bounds"
                },
                "summary": "We address the problem of learning-augmented online caching in the scenario\nwhen each request is accompanied by a prediction of the next occurrence of the\nrequested page. We improve currently known bounds on the competitive ratio of\nthe BlindOracle algorithm, which evicts a page predicted to be requested last.\nWe also prove a lower bound on the competitive ratio of any randomized\nalgorithm and show that a combination of the BlindOracle with the Marker\nalgorithm achieves a competitive ratio that is optimal up to some constant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the problem of learning-augmented online caching in the scenario\nwhen each request is accompanied by a prediction of the next occurrence of the\nrequested page. We improve currently known bounds on the competitive ratio of\nthe BlindOracle algorithm, which evicts a page predicted to be requested last.\nWe also prove a lower bound on the competitive ratio of any randomized\nalgorithm and show that a combination of the BlindOracle with the Marker\nalgorithm achieves a competitive ratio that is optimal up to some constant."
                },
                "authors": [
                    {
                        "name": "Daniel Skachkov"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    },
                    {
                        "name": "Yuri Dorn"
                    },
                    {
                        "name": "Alexander Demin"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Demin"
                },
                "author": "Alexander Demin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08161v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08161v2",
                "updated": "2025-07-27T09:58:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    27,
                    9,
                    58,
                    25,
                    6,
                    208,
                    0
                ],
                "published": "2025-06-09T19:13:16Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    19,
                    13,
                    16,
                    0,
                    160,
                    0
                ],
                "title": "GATE: Geometry-Aware Trained Encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GATE: Geometry-Aware Trained Encoding"
                },
                "summary": "The encoding of input parameters is one of the fundamental building blocks of\nneural network algorithms. Its goal is to map the input data to a\nhigher-dimensional space, typically supported by trained feature vectors. The\nmapping is crucial for the efficiency and approximation quality of neural\nnetworks. We propose a novel geometry-aware encoding called GATE that stores\nfeature vectors on the surface of triangular meshes. Our encoding is suitable\nfor neural rendering-related algorithms, for example, neural radiance caching.\nIt also avoids limitations of previous hash-based encoding schemes, such as\nhash collisions, selection of resolution versus scene size, and divergent\nmemory access. Our approach decouples feature vector density from geometry\ndensity using mesh colors, while allowing for finer control over neural network\ntraining and adaptive level-of-detail.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The encoding of input parameters is one of the fundamental building blocks of\nneural network algorithms. Its goal is to map the input data to a\nhigher-dimensional space, typically supported by trained feature vectors. The\nmapping is crucial for the efficiency and approximation quality of neural\nnetworks. We propose a novel geometry-aware encoding called GATE that stores\nfeature vectors on the surface of triangular meshes. Our encoding is suitable\nfor neural rendering-related algorithms, for example, neural radiance caching.\nIt also avoids limitations of previous hash-based encoding schemes, such as\nhash collisions, selection of resolution versus scene size, and divergent\nmemory access. Our approach decouples feature vector density from geometry\ndensity using mesh colors, while allowing for finer control over neural network\ntraining and adaptive level-of-detail."
                },
                "authors": [
                    {
                        "name": "Jakub Bokansk"
                    },
                    {
                        "name": "Daniel Meister"
                    },
                    {
                        "name": "Carsten Benthin"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Benthin"
                },
                "author": "Carsten Benthin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08161v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08161v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20173v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20173v1",
                "updated": "2025-07-27T08:25:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    27,
                    8,
                    25,
                    8,
                    6,
                    208,
                    0
                ],
                "published": "2025-07-27T08:25:08Z",
                "published_parsed": [
                    2025,
                    7,
                    27,
                    8,
                    25,
                    8,
                    6,
                    208,
                    0
                ],
                "title": "High-Performance Parallel Optimization of the Fish School Behaviour on\n  the Setonix Platform Using OpenMP",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Performance Parallel Optimization of the Fish School Behaviour on\n  the Setonix Platform Using OpenMP"
                },
                "summary": "This paper presents an in-depth investigation into the high-performance\nparallel optimization of the Fish School Behaviour (FSB) algorithm on the\nSetonix supercomputing platform using the OpenMP framework. Given the\nincreasing demand for enhanced computational capabilities for complex,\nlarge-scale calculations across diverse domains, there's an imperative need for\noptimized parallel algorithms and computing structures. The FSB algorithm,\ninspired by nature's social behavior patterns, provides an ideal platform for\nparallelization due to its iterative and computationally intensive nature. This\nstudy leverages the capabilities of the Setonix platform and the OpenMP\nframework to analyze various aspects of multi-threading, such as thread counts,\nscheduling strategies, and OpenMP constructs, aiming to discern patterns and\nstrategies that can elevate program performance. Experiments were designed to\nrigorously test different configurations, and our results not only offer\ninsights for parallel optimization of FSB on Setonix but also provide valuable\nreferences for other parallel computational research using OpenMP. Looking\nforward, other factors, such as cache behavior and thread scheduling strategies\nat micro and macro levels, hold potential for further exploration and\noptimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an in-depth investigation into the high-performance\nparallel optimization of the Fish School Behaviour (FSB) algorithm on the\nSetonix supercomputing platform using the OpenMP framework. Given the\nincreasing demand for enhanced computational capabilities for complex,\nlarge-scale calculations across diverse domains, there's an imperative need for\noptimized parallel algorithms and computing structures. The FSB algorithm,\ninspired by nature's social behavior patterns, provides an ideal platform for\nparallelization due to its iterative and computationally intensive nature. This\nstudy leverages the capabilities of the Setonix platform and the OpenMP\nframework to analyze various aspects of multi-threading, such as thread counts,\nscheduling strategies, and OpenMP constructs, aiming to discern patterns and\nstrategies that can elevate program performance. Experiments were designed to\nrigorously test different configurations, and our results not only offer\ninsights for parallel optimization of FSB on Setonix but also provide valuable\nreferences for other parallel computational research using OpenMP. Looking\nforward, other factors, such as cache behavior and thread scheduling strategies\nat micro and macro levels, hold potential for further exploration and\noptimization."
                },
                "authors": [
                    {
                        "name": "Haitian Wang"
                    },
                    {
                        "name": "Long Qin"
                    }
                ],
                "author_detail": {
                    "name": "Long Qin"
                },
                "author": "Long Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20173v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20116v1",
                "updated": "2025-07-27T03:45:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    27,
                    3,
                    45,
                    7,
                    6,
                    208,
                    0
                ],
                "published": "2025-07-27T03:45:07Z",
                "published_parsed": [
                    2025,
                    7,
                    27,
                    3,
                    45,
                    7,
                    6,
                    208,
                    0
                ],
                "title": "Accelerating Containerized Service Delivery at the Network Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Containerized Service Delivery at the Network Edge"
                },
                "summary": "Efficient container image distribution is crucial for enabling machine\nlearning inference at the network edge, where resource limitations and dynamic\nnetwork conditions create significant challenges. In this paper, we present\nPeerSync, a decentralized P2P-based system designed to optimize image\ndistribution in edge environments. PeerSync employs a popularity- and\nnetwork-aware download engine that dynamically adapts to content popularity and\nreal-time network conditions using a sliding window mechanism. PeerSync further\nintegrates automated tracker election for rapid peer discovery and dynamic\ncache management for efficient storage utilization. We implement PeerSync with\n8000+ lines of Rust code and test its performance extensively on both physical\nedge devices and Docker-based emulations. Experimental results show that\nPeerSync delivers a remarkable speed increase of 2.72$\\times$, 1.79$\\times$,\nand 1.28$\\times$ compared to the Baseline, Dragonfly, and Kraken, respectively,\nwhile significantly reducing peak cross-network traffic by 90.72\\% under\ncongested and varying network conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient container image distribution is crucial for enabling machine\nlearning inference at the network edge, where resource limitations and dynamic\nnetwork conditions create significant challenges. In this paper, we present\nPeerSync, a decentralized P2P-based system designed to optimize image\ndistribution in edge environments. PeerSync employs a popularity- and\nnetwork-aware download engine that dynamically adapts to content popularity and\nreal-time network conditions using a sliding window mechanism. PeerSync further\nintegrates automated tracker election for rapid peer discovery and dynamic\ncache management for efficient storage utilization. We implement PeerSync with\n8000+ lines of Rust code and test its performance extensively on both physical\nedge devices and Docker-based emulations. Experimental results show that\nPeerSync delivers a remarkable speed increase of 2.72$\\times$, 1.79$\\times$,\nand 1.28$\\times$ compared to the Baseline, Dragonfly, and Kraken, respectively,\nwhile significantly reducing peak cross-network traffic by 90.72\\% under\ncongested and varying network conditions."
                },
                "authors": [
                    {
                        "name": "Yinuo Deng"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Dongjing Wang"
                    },
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Wenzhuo Qian"
                    },
                    {
                        "name": "Jianwei Yin"
                    },
                    {
                        "name": "Schahram Dustdar"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18300v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18300v3",
                "updated": "2025-07-27T00:40:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    27,
                    0,
                    40,
                    47,
                    6,
                    208,
                    0
                ],
                "published": "2025-05-23T18:46:10Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    18,
                    46,
                    10,
                    4,
                    143,
                    0
                ],
                "title": "Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient\n  Nonlinear MCMC on General Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient\n  Nonlinear MCMC on General Graphs"
                },
                "summary": "We propose a history-driven target (HDT) framework in Markov Chain Monte\nCarlo (MCMC) to improve any random walk algorithm on discrete state spaces,\nsuch as general undirected graphs, for efficient sampling from target\ndistribution $\\boldsymbol{\\mu}$. With broad applications in network science and\ndistributed optimization, recent innovations like the self-repellent random\nwalk (SRRW) achieve near-zero variance by prioritizing under-sampled states\nthrough transition kernel modifications based on past visit frequencies.\nHowever, SRRW's reliance on explicit computation of transition probabilities\nfor all neighbors at each step introduces substantial computational overhead,\nwhile its strict dependence on time-reversible Markov chains excludes advanced\nnon-reversible MCMC methods. To overcome these limitations, instead of direct\nmodification of transition kernel, HDT introduces a history-dependent target\ndistribution $\\boldsymbol{\\pi}[\\mathbf{x}]$ to replace the original target\n$\\boldsymbol{\\mu}$ in any graph sampler, where $\\mathbf{x}$ represents the\nempirical measure of past visits. This design preserves lightweight\nimplementation by requiring only local information between the current and\nproposed states and achieves compatibility with both reversible and\nnon-reversible MCMC samplers, while retaining unbiased samples with target\ndistribution $\\boldsymbol{\\mu}$ and near-zero variance performance. Extensive\nexperiments in graph sampling demonstrate consistent performance gains, and a\nmemory-efficient Least Recently Used (LRU) cache ensures scalability to large\ngeneral graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a history-driven target (HDT) framework in Markov Chain Monte\nCarlo (MCMC) to improve any random walk algorithm on discrete state spaces,\nsuch as general undirected graphs, for efficient sampling from target\ndistribution $\\boldsymbol{\\mu}$. With broad applications in network science and\ndistributed optimization, recent innovations like the self-repellent random\nwalk (SRRW) achieve near-zero variance by prioritizing under-sampled states\nthrough transition kernel modifications based on past visit frequencies.\nHowever, SRRW's reliance on explicit computation of transition probabilities\nfor all neighbors at each step introduces substantial computational overhead,\nwhile its strict dependence on time-reversible Markov chains excludes advanced\nnon-reversible MCMC methods. To overcome these limitations, instead of direct\nmodification of transition kernel, HDT introduces a history-dependent target\ndistribution $\\boldsymbol{\\pi}[\\mathbf{x}]$ to replace the original target\n$\\boldsymbol{\\mu}$ in any graph sampler, where $\\mathbf{x}$ represents the\nempirical measure of past visits. This design preserves lightweight\nimplementation by requiring only local information between the current and\nproposed states and achieves compatibility with both reversible and\nnon-reversible MCMC samplers, while retaining unbiased samples with target\ndistribution $\\boldsymbol{\\mu}$ and near-zero variance performance. Extensive\nexperiments in graph sampling demonstrate consistent performance gains, and a\nmemory-efficient Least Recently Used (LRU) cache ensures scalability to large\ngeneral graphs."
                },
                "authors": [
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Yi-Ting Ma"
                    },
                    {
                        "name": "Do Young Eun"
                    }
                ],
                "author_detail": {
                    "name": "Do Young Eun"
                },
                "author": "Do Young Eun",
                "arxiv_comment": "Accepted at ICML 2025 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18300v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18300v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20030v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20030v1",
                "updated": "2025-07-26T18:20:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    18,
                    20,
                    25,
                    5,
                    207,
                    0
                ],
                "published": "2025-07-26T18:20:25Z",
                "published_parsed": [
                    2025,
                    7,
                    26,
                    18,
                    20,
                    25,
                    5,
                    207,
                    0
                ],
                "title": "FAEDKV: Infinite-Window Fourier Transform for Unbiased KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAEDKV: Infinite-Window Fourier Transform for Unbiased KV Cache\n  Compression"
                },
                "summary": "The efficacy of Large Language Models (LLMs) in long-context tasks is often\nhampered by the substantial memory footprint and computational demands of the\nKey-Value (KV) cache. Current compression strategies, including token eviction\nand learned projections, frequently lead to biased representations -- either by\noveremphasizing recent/high-attention tokens or by repeatedly degrading\ninformation from earlier context -- and may require costly model retraining. We\npresent FAEDKV (Frequency-Adaptive Infinite-Window for KV cache), a novel,\ntraining-free KV cache compression framework that ensures unbiased information\nretention. FAEDKV operates by transforming the KV cache into the frequency\ndomain using a proposed Infinite-Window Fourier Transform (IWDFT). This\napproach allows for the equalized contribution of all tokens to the compressed\nrepresentation, effectively preserving both early and recent contextual\ninformation. A preliminary frequency ablation study identifies critical\nspectral components for layer-wise, targeted compression. Experiments on\nLongBench benchmark demonstrate FAEDKV's superiority over existing methods by\nup to 22\\%. In addition, our method shows superior, position-agnostic retrieval\naccuracy on the Needle-In-A-Haystack task compared to compression based\napproaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficacy of Large Language Models (LLMs) in long-context tasks is often\nhampered by the substantial memory footprint and computational demands of the\nKey-Value (KV) cache. Current compression strategies, including token eviction\nand learned projections, frequently lead to biased representations -- either by\noveremphasizing recent/high-attention tokens or by repeatedly degrading\ninformation from earlier context -- and may require costly model retraining. We\npresent FAEDKV (Frequency-Adaptive Infinite-Window for KV cache), a novel,\ntraining-free KV cache compression framework that ensures unbiased information\nretention. FAEDKV operates by transforming the KV cache into the frequency\ndomain using a proposed Infinite-Window Fourier Transform (IWDFT). This\napproach allows for the equalized contribution of all tokens to the compressed\nrepresentation, effectively preserving both early and recent contextual\ninformation. A preliminary frequency ablation study identifies critical\nspectral components for layer-wise, targeted compression. Experiments on\nLongBench benchmark demonstrate FAEDKV's superiority over existing methods by\nup to 22\\%. In addition, our method shows superior, position-agnostic retrieval\naccuracy on the Needle-In-A-Haystack task compared to compression based\napproaches."
                },
                "authors": [
                    {
                        "name": "Runchao Li"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Mu Sheng"
                    },
                    {
                        "name": "Xianxuan Long"
                    },
                    {
                        "name": "Haotian Yu"
                    },
                    {
                        "name": "Pan Li"
                    }
                ],
                "author_detail": {
                    "name": "Pan Li"
                },
                "author": "Pan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20030v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20030v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03140v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03140v2",
                "updated": "2025-07-26T15:25:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    15,
                    25,
                    22,
                    5,
                    207,
                    0
                ],
                "published": "2025-04-04T03:30:15Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    3,
                    30,
                    15,
                    4,
                    94,
                    0
                ],
                "title": "Model Reveals What to Cache: Profiling-Based Feature Reuse for Video\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Reveals What to Cache: Profiling-Based Feature Reuse for Video\n  Diffusion Models"
                },
                "summary": "Recent advances in diffusion models have demonstrated remarkable capabilities\nin video generation. However, the computational intensity remains a significant\nchallenge for practical applications. While feature caching has been proposed\nto reduce the computational burden of diffusion models, existing methods\ntypically overlook the heterogeneous significance of individual blocks,\nresulting in suboptimal reuse and degraded output quality. To this end, we\naddress this gap by introducing ProfilingDiT, a novel adaptive caching strategy\nthat explicitly disentangles foreground and background-focused blocks. Through\na systematic analysis of attention distributions in diffusion models, we reveal\na key observation: 1) Most layers exhibit a consistent preference for either\nforeground or background regions. 2) Predicted noise shows low inter-step\nsimilarity initially, which stabilizes as denoising progresses. This finding\ninspires us to formulate a selective caching strategy that preserves full\ncomputation for dynamic foreground elements while efficiently caching static\nbackground features. Our approach substantially reduces computational overhead\nwhile preserving visual fidelity. Extensive experiments demonstrate that our\nframework achieves significant acceleration (e.g., 2.01 times speedup for\nWan2.1) while maintaining visual fidelity across comprehensive quality metrics,\nestablishing a viable method for efficient video generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in diffusion models have demonstrated remarkable capabilities\nin video generation. However, the computational intensity remains a significant\nchallenge for practical applications. While feature caching has been proposed\nto reduce the computational burden of diffusion models, existing methods\ntypically overlook the heterogeneous significance of individual blocks,\nresulting in suboptimal reuse and degraded output quality. To this end, we\naddress this gap by introducing ProfilingDiT, a novel adaptive caching strategy\nthat explicitly disentangles foreground and background-focused blocks. Through\na systematic analysis of attention distributions in diffusion models, we reveal\na key observation: 1) Most layers exhibit a consistent preference for either\nforeground or background regions. 2) Predicted noise shows low inter-step\nsimilarity initially, which stabilizes as denoising progresses. This finding\ninspires us to formulate a selective caching strategy that preserves full\ncomputation for dynamic foreground elements while efficiently caching static\nbackground features. Our approach substantially reduces computational overhead\nwhile preserving visual fidelity. Extensive experiments demonstrate that our\nframework achieves significant acceleration (e.g., 2.01 times speedup for\nWan2.1) while maintaining visual fidelity across comprehensive quality metrics,\nestablishing a viable method for efficient video generation."
                },
                "authors": [
                    {
                        "name": "Xuran Ma"
                    },
                    {
                        "name": "Yexin Liu"
                    },
                    {
                        "name": "Yaofu Liu"
                    },
                    {
                        "name": "Xianfeng Wu"
                    },
                    {
                        "name": "Mingzhe Zheng"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Ser-Nam Lim"
                    },
                    {
                        "name": "Harry Yang"
                    }
                ],
                "author_detail": {
                    "name": "Harry Yang"
                },
                "author": "Harry Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03140v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03140v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09720v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09720v3",
                "updated": "2025-07-26T15:13:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    15,
                    13,
                    56,
                    5,
                    207,
                    0
                ],
                "published": "2025-02-13T19:11:40Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    11,
                    40,
                    3,
                    44,
                    0
                ],
                "title": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs"
                },
                "summary": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent works have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Metas SpinQuant (perplexity 7.3), OstQuant (7.3) and QuaRot\n(8.2). Comparisons on bigger models (up to 70B) and on various LLM evaluation\nbenchmarks confirm uniform superiority of NestQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent works have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Metas SpinQuant (perplexity 7.3), OstQuant (7.3) and QuaRot\n(8.2). Comparisons on bigger models (up to 70B) and on various LLM evaluation\nbenchmarks confirm uniform superiority of NestQuant."
                },
                "authors": [
                    {
                        "name": "Semyon Savkin"
                    },
                    {
                        "name": "Eitan Porat"
                    },
                    {
                        "name": "Or Ordentlich"
                    },
                    {
                        "name": "Yury Polyanskiy"
                    }
                ],
                "author_detail": {
                    "name": "Yury Polyanskiy"
                },
                "author": "Yury Polyanskiy",
                "arxiv_comment": "23 pages; Accepted at the 42nd International Conference on Machine\n  Learning (ICML 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09720v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09720v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20677v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20677v2",
                "updated": "2025-07-26T13:33:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    13,
                    33,
                    6,
                    5,
                    207,
                    0
                ],
                "published": "2024-12-30T03:05:45Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    5,
                    45,
                    0,
                    365,
                    0
                ],
                "title": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional performance across\ndiverse natural language processing tasks. However, as the model size and the\ninput sequence's length increase, the linearly increasing key-value (KV) cache\nsignificantly degrades inference throughput. Therefore, grouped-query attention\n(GQA), as an alternative to multi-head attention (MHA), has been widely\nintroduced into LLMs. In this work, we propose a cost-effective method for\nconverting MHA into GQA with any compression ratio of KV heads. The key point\nof our method lies in the application of Procrustes analysis to the attention\nheads, which enhances the similarity among attention heads while preserving\ncomputational invariance, thereby improving the model's post-training\nperformance. Subsequently, we employ $\\mathit{L_0}$ regularization to prune\nredundant parameters. The model after pruning can be adapted to the standard\nGQA framework. Experimental results show that our strategy can compress up to\n87.5\\% KV heads of LLaMA2-7B model and 75\\% KV heads of Sheared-LLaMA-1.3B with\nacceptable performance degradation. Our code is released at\nhttps://github.com/fpcsong/mha2gqa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional performance across\ndiverse natural language processing tasks. However, as the model size and the\ninput sequence's length increase, the linearly increasing key-value (KV) cache\nsignificantly degrades inference throughput. Therefore, grouped-query attention\n(GQA), as an alternative to multi-head attention (MHA), has been widely\nintroduced into LLMs. In this work, we propose a cost-effective method for\nconverting MHA into GQA with any compression ratio of KV heads. The key point\nof our method lies in the application of Procrustes analysis to the attention\nheads, which enhances the similarity among attention heads while preserving\ncomputational invariance, thereby improving the model's post-training\nperformance. Subsequently, we employ $\\mathit{L_0}$ regularization to prune\nredundant parameters. The model after pruning can be adapted to the standard\nGQA framework. Experimental results show that our strategy can compress up to\n87.5\\% KV heads of LLaMA2-7B model and 75\\% KV heads of Sheared-LLaMA-1.3B with\nacceptable performance degradation. Our code is released at\nhttps://github.com/fpcsong/mha2gqa."
                },
                "authors": [
                    {
                        "name": "Qingyun Jin"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Feng Zhou"
                    },
                    {
                        "name": "Zengchang Qin"
                    }
                ],
                "author_detail": {
                    "name": "Zengchang Qin"
                },
                "author": "Zengchang Qin",
                "arxiv_comment": "13 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20677v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20677v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19823v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19823v1",
                "updated": "2025-07-26T06:43:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    6,
                    43,
                    14,
                    5,
                    207,
                    0
                ],
                "published": "2025-07-26T06:43:14Z",
                "published_parsed": [
                    2025,
                    7,
                    26,
                    6,
                    43,
                    14,
                    5,
                    207,
                    0
                ],
                "title": "HCAttention: Extreme KV Cache Compression via Heterogeneous Attention\n  Computing for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HCAttention: Extreme KV Cache Compression via Heterogeneous Attention\n  Computing for LLMs"
                },
                "summary": "Processing long-context inputs with large language models presents a\nsignificant challenge due to the enormous memory requirements of the Key-Value\n(KV) cache during inference. Existing KV cache compression methods exhibit\nnoticeable performance degradation when memory is reduced by more than 85%.\nAdditionally, strategies that leverage GPU-CPU collaboration for approximate\nattention remain underexplored in this setting. We propose HCAttention, a\nheterogeneous attention computation framework that integrates key quantization,\nvalue offloading, and dynamic KV eviction to enable efficient inference under\nextreme memory constraints. The method is compatible with existing transformer\narchitectures and does not require model fine-tuning. Experimental results on\nthe LongBench benchmark demonstrate that our approach preserves the accuracy of\nfull-attention model while shrinking the KV cache memory footprint to 25% of\nits original size. Remarkably, it stays competitive with only 12.5% of the\ncache, setting a new state-of-the-art in LLM KV cache compression. To the best\nof our knowledge, HCAttention is the first to extend the Llama-3-8B model to\nprocess 4 million tokens on a single A100 GPU with 80GB memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long-context inputs with large language models presents a\nsignificant challenge due to the enormous memory requirements of the Key-Value\n(KV) cache during inference. Existing KV cache compression methods exhibit\nnoticeable performance degradation when memory is reduced by more than 85%.\nAdditionally, strategies that leverage GPU-CPU collaboration for approximate\nattention remain underexplored in this setting. We propose HCAttention, a\nheterogeneous attention computation framework that integrates key quantization,\nvalue offloading, and dynamic KV eviction to enable efficient inference under\nextreme memory constraints. The method is compatible with existing transformer\narchitectures and does not require model fine-tuning. Experimental results on\nthe LongBench benchmark demonstrate that our approach preserves the accuracy of\nfull-attention model while shrinking the KV cache memory footprint to 25% of\nits original size. Remarkably, it stays competitive with only 12.5% of the\ncache, setting a new state-of-the-art in LLM KV cache compression. To the best\nof our knowledge, HCAttention is the first to extend the Llama-3-8B model to\nprocess 4 million tokens on a single A100 GPU with 80GB memory."
                },
                "authors": [
                    {
                        "name": "Dongquan Yang"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Xiaotian Yu"
                    },
                    {
                        "name": "Xianbiao Qi"
                    },
                    {
                        "name": "Rong Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Rong Xiao"
                },
                "author": "Rong Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19823v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19823v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19427v1",
                "updated": "2025-07-25T16:53:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    16,
                    53,
                    13,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T16:53:13Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    16,
                    53,
                    13,
                    4,
                    206,
                    0
                ],
                "title": "Step-3 is Large yet Affordable: Model-system Co-design for\n  Cost-effective Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step-3 is Large yet Affordable: Model-system Co-design for\n  Cost-effective Decoding"
                },
                "summary": "Large language models (LLMs) face low hardware efficiency during decoding,\nespecially for long-context reasoning tasks. This paper introduces Step-3, a\n321B-parameter VLM with hardware-aware model-system co-design optimized for\nminimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel\nMulti-Matrix Factorization Attention (MFA) mechanism that significantly reduces\nboth KV cache size and computation while maintaining high attention\nexpressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed\ninference system that decouples attention and Feed-Forward Network (FFN) layers\ninto specialized subsystems. This co-design achieves unprecedented cost\nefficiency: Step-3 significantly reduces theoretical decoding costs compared\nwith models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at\nlonger context. Step-3 achieves low cost while activating 38B parameters per\ntoken (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that\nhardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are\ncritical to cost-effectiveness. We perform a head-to-head comparison with\nDeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs\nachieves a decoding throughput of up to 4,039 tokens per second per GPU under\n50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324\nin the same setup and sets a new Pareto frontier for LLM decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face low hardware efficiency during decoding,\nespecially for long-context reasoning tasks. This paper introduces Step-3, a\n321B-parameter VLM with hardware-aware model-system co-design optimized for\nminimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel\nMulti-Matrix Factorization Attention (MFA) mechanism that significantly reduces\nboth KV cache size and computation while maintaining high attention\nexpressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed\ninference system that decouples attention and Feed-Forward Network (FFN) layers\ninto specialized subsystems. This co-design achieves unprecedented cost\nefficiency: Step-3 significantly reduces theoretical decoding costs compared\nwith models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at\nlonger context. Step-3 achieves low cost while activating 38B parameters per\ntoken (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that\nhardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are\ncritical to cost-effectiveness. We perform a head-to-head comparison with\nDeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs\nachieves a decoding throughput of up to 4,039 tokens per second per GPU under\n50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324\nin the same setup and sets a new Pareto frontier for LLM decoding."
                },
                "authors": [
                    {
                        "name": "StepFun"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Bojun Wang"
                    },
                    {
                        "name": "Changyi Wan"
                    },
                    {
                        "name": "Guanzhe Huang"
                    },
                    {
                        "name": "Hanpeng Hu"
                    },
                    {
                        "name": "Haonan Jia"
                    },
                    {
                        "name": "Hao Nie"
                    },
                    {
                        "name": "Mingliang Li"
                    },
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Siyu Chen"
                    },
                    {
                        "name": "Song Yuan"
                    },
                    {
                        "name": "Wuxun Xie"
                    },
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Xing Chen"
                    },
                    {
                        "name": "Xingping Yang"
                    },
                    {
                        "name": "Xuelin Zhang"
                    },
                    {
                        "name": "Yanbo Yu"
                    },
                    {
                        "name": "Yaoyu Wang"
                    },
                    {
                        "name": "Yibo Zhu"
                    },
                    {
                        "name": "Yimin Jiang"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Yuanwei Lu"
                    },
                    {
                        "name": "Houyi Li"
                    },
                    {
                        "name": "Jingcheng Hu"
                    },
                    {
                        "name": "Ka Man Lo"
                    },
                    {
                        "name": "Ailin Huang"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Boyu Chen"
                    },
                    {
                        "name": "Changxin Miao"
                    },
                    {
                        "name": "Chang Lou"
                    },
                    {
                        "name": "Chen Hu"
                    },
                    {
                        "name": "Chen Xu"
                    },
                    {
                        "name": "Chenfeng Yu"
                    },
                    {
                        "name": "Chengyuan Yao"
                    },
                    {
                        "name": "Daokuan Lv"
                    },
                    {
                        "name": "Dapeng Shi"
                    },
                    {
                        "name": "Deshan Sun"
                    },
                    {
                        "name": "Ding Huang"
                    },
                    {
                        "name": "Dingyuan Hu"
                    },
                    {
                        "name": "Dongqing Pang"
                    },
                    {
                        "name": "Enle Liu"
                    },
                    {
                        "name": "Fajie Zhang"
                    },
                    {
                        "name": "Fanqi Wan"
                    },
                    {
                        "name": "Gulin Yan"
                    },
                    {
                        "name": "Han Zhang"
                    },
                    {
                        "name": "Han Zhou"
                    },
                    {
                        "name": "Hanghao Wu"
                    },
                    {
                        "name": "Hangyu Guo"
                    },
                    {
                        "name": "Hanqi Chen"
                    },
                    {
                        "name": "Hanshan Zhang"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Haocheng Zhang"
                    },
                    {
                        "name": "Haolong Yan"
                    },
                    {
                        "name": "Haoran Lv"
                    },
                    {
                        "name": "Haoran Wei"
                    },
                    {
                        "name": "Hebin Zhou"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Hongxin Li"
                    },
                    {
                        "name": "Hongyu Zhou"
                    },
                    {
                        "name": "Hongyuan Wang"
                    },
                    {
                        "name": "Huiyong Guo"
                    },
                    {
                        "name": "Jia Wang"
                    },
                    {
                        "name": "Jiahao Gong"
                    },
                    {
                        "name": "Jialing Xie"
                    },
                    {
                        "name": "Jian Zhou"
                    },
                    {
                        "name": "Jianjian Sun"
                    },
                    {
                        "name": "Jiaoren Wu"
                    },
                    {
                        "name": "Jiaran Zhang"
                    },
                    {
                        "name": "Jiayu Liu"
                    },
                    {
                        "name": "Jie Cheng"
                    },
                    {
                        "name": "Jie Luo"
                    },
                    {
                        "name": "Jie Yan"
                    },
                    {
                        "name": "Jie Yang"
                    },
                    {
                        "name": "Jieyi Hou"
                    },
                    {
                        "name": "Jinguang Zhang"
                    },
                    {
                        "name": "Jinlan Cao"
                    },
                    {
                        "name": "Jisheng Yin"
                    },
                    {
                        "name": "Junfeng Liu"
                    },
                    {
                        "name": "Junhao Huang"
                    },
                    {
                        "name": "Junzhe Lin"
                    },
                    {
                        "name": "Kaijun Tan"
                    },
                    {
                        "name": "Kaixiang Li"
                    },
                    {
                        "name": "Kang An"
                    },
                    {
                        "name": "Kangheng Lin"
                    },
                    {
                        "name": "Kenkun Liu"
                    },
                    {
                        "name": "Lei Yang"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Liangyu Chen"
                    },
                    {
                        "name": "Lieyu Shi"
                    },
                    {
                        "name": "Liguo Tan"
                    },
                    {
                        "name": "Lin Lin"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Lina Chen"
                    },
                    {
                        "name": "Liwen Huang"
                    },
                    {
                        "name": "Liying Shi"
                    },
                    {
                        "name": "Longlong Gu"
                    },
                    {
                        "name": "Mei Chen"
                    },
                    {
                        "name": "Mengqiang Ren"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Mingzhe Chen"
                    },
                    {
                        "name": "Na Wang"
                    },
                    {
                        "name": "Nan Wu"
                    },
                    {
                        "name": "Qi Han"
                    },
                    {
                        "name": "Qian Zhao"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Qianni Liu"
                    },
                    {
                        "name": "Qiaohui Chen"
                    },
                    {
                        "name": "Qiling Wu"
                    },
                    {
                        "name": "Qinglin He"
                    },
                    {
                        "name": "Qinyuan Tan"
                    },
                    {
                        "name": "Qiufeng Wang"
                    },
                    {
                        "name": "Qiuping Wu"
                    },
                    {
                        "name": "Qiuyan Liang"
                    },
                    {
                        "name": "Quan Sun"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Ruihang Miao"
                    },
                    {
                        "name": "Ruosi Wan"
                    },
                    {
                        "name": "Ruyan Guo"
                    },
                    {
                        "name": "Shangwu Zhong"
                    },
                    {
                        "name": "Shaoliang Pang"
                    },
                    {
                        "name": "Shengjie Fan"
                    },
                    {
                        "name": "Shijie Shang"
                    },
                    {
                        "name": "Shilei Jiang"
                    },
                    {
                        "name": "Shiliang Yang"
                    },
                    {
                        "name": "Shiming Hao"
                    },
                    {
                        "name": "Shuli Gao"
                    },
                    {
                        "name": "Siming Huang"
                    },
                    {
                        "name": "Siqi Liu"
                    },
                    {
                        "name": "Tiancheng Cao"
                    },
                    {
                        "name": "Tianhao Cheng"
                    },
                    {
                        "name": "Tianhao Peng"
                    },
                    {
                        "name": "Wang You"
                    },
                    {
                        "name": "Wei Ji"
                    },
                    {
                        "name": "Wen Sun"
                    },
                    {
                        "name": "Wenjin Deng"
                    },
                    {
                        "name": "Wenqing He"
                    },
                    {
                        "name": "Wenzhen Zheng"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Xiangwen Kong"
                    },
                    {
                        "name": "Xianzhen Luo"
                    },
                    {
                        "name": "Xiaobo Yang"
                    },
                    {
                        "name": "Xiaojia Liu"
                    },
                    {
                        "name": "Xiaoxiao Ren"
                    },
                    {
                        "name": "Xin Han"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Xin Wu"
                    },
                    {
                        "name": "Xu Zhao"
                    },
                    {
                        "name": "Yanan Wei"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Yangguang Li"
                    },
                    {
                        "name": "Yangshijie Xu"
                    },
                    {
                        "name": "Yanming Xu"
                    },
                    {
                        "name": "Yaqiang Shi"
                    },
                    {
                        "name": "Yeqing Shen"
                    },
                    {
                        "name": "Yi Yang"
                    },
                    {
                        "name": "Yifei Yang"
                    },
                    {
                        "name": "Yifeng Gong"
                    },
                    {
                        "name": "Yihan Chen"
                    },
                    {
                        "name": "Yijing Yang"
                    },
                    {
                        "name": "Yinmin Zhang"
                    },
                    {
                        "name": "Yizhuang Zhou"
                    },
                    {
                        "name": "Yuanhao Ding"
                    },
                    {
                        "name": "Yuantao Fan"
                    },
                    {
                        "name": "Yuanzhen Yang"
                    },
                    {
                        "name": "Yuchu Luo"
                    },
                    {
                        "name": "Yue Peng"
                    },
                    {
                        "name": "Yufan Lu"
                    },
                    {
                        "name": "Yuhang Deng"
                    },
                    {
                        "name": "Yuhe Yin"
                    },
                    {
                        "name": "Yujie Liu"
                    },
                    {
                        "name": "Yukun Chen"
                    },
                    {
                        "name": "Yuling Zhao"
                    },
                    {
                        "name": "Yun Mou"
                    },
                    {
                        "name": "Yunlong Li"
                    },
                    {
                        "name": "Yunzhou Ju"
                    },
                    {
                        "name": "Yusheng Li"
                    },
                    {
                        "name": "Yuxiang Yang"
                    },
                    {
                        "name": "Yuxiang Zhang"
                    },
                    {
                        "name": "Yuyang Chen"
                    },
                    {
                        "name": "Zejia Weng"
                    },
                    {
                        "name": "Zhe Xie"
                    },
                    {
                        "name": "Zheng Ge"
                    },
                    {
                        "name": "Zheng Gong"
                    },
                    {
                        "name": "Zhenyi Lu"
                    },
                    {
                        "name": "Zhewei Huang"
                    },
                    {
                        "name": "Zhichao Chang"
                    },
                    {
                        "name": "Zhiguo Huang"
                    },
                    {
                        "name": "Zhirui Wang"
                    },
                    {
                        "name": "Zidong Yang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Ziqi Wang"
                    },
                    {
                        "name": "Zixin Zhang"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Daxin Jiang"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhang"
                },
                "author": "Xiangyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19367v1",
                "updated": "2025-07-25T15:17:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    17,
                    29,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T15:17:29Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    17,
                    29,
                    4,
                    206,
                    0
                ],
                "title": "Empowering IoT Firmware Secure Update with Customization Rights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering IoT Firmware Secure Update with Customization Rights"
                },
                "summary": "Firmware updates remain the primary line of defense for IoT devices; however,\nthe update channel itself has become a well-established attack vector. Existing\ndefenses mainly focus on securing monolithic firmware images, leaving\nmodule-level customization -a growing user demand-largely unprotected and\ninsufficiently explored. To address this gap, we conduct a pilot study on the\nupdate workflows of 200 Linux-based IoT devices across 23 vendors, uncovering\nfive previously undocumented vulnerabilities caused by customization practices.\nA broader analysis of update-related CVEs from 2020 to 2024 reveals that over\nhalf originate from customization-induced issues. These findings highlight a\ncritical yet underexamined reality: as customization increases, so does the\nattack surface, while current defenses fail to keep pace. We propose IMUP\n(Integrity-Centric Modular Update Platform), the first framework to address two\nkey challenges: constructing a trustworthy cross-module integrity chain and\nscaling update performance under mass customization. IMUP combines three\ntechniques: per-module chameleon hashing for integrity, server-side\nproof-of-work offloading to reduce device overhead, and server-side caching to\nreuse module combinations, minimizing rebuild costs. Security analysis shows\nthat even when 95 percent of secret keys are exposed, forging a valid image\nincurs over 300 times the cost of the legitimate server. Experiments on\nheterogeneous IoT devices demonstrate that IMUP reduces server-side generation\ntime by 2.9 times and device downtime by 5.9 times compared to a\npackage-manager baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Firmware updates remain the primary line of defense for IoT devices; however,\nthe update channel itself has become a well-established attack vector. Existing\ndefenses mainly focus on securing monolithic firmware images, leaving\nmodule-level customization -a growing user demand-largely unprotected and\ninsufficiently explored. To address this gap, we conduct a pilot study on the\nupdate workflows of 200 Linux-based IoT devices across 23 vendors, uncovering\nfive previously undocumented vulnerabilities caused by customization practices.\nA broader analysis of update-related CVEs from 2020 to 2024 reveals that over\nhalf originate from customization-induced issues. These findings highlight a\ncritical yet underexamined reality: as customization increases, so does the\nattack surface, while current defenses fail to keep pace. We propose IMUP\n(Integrity-Centric Modular Update Platform), the first framework to address two\nkey challenges: constructing a trustworthy cross-module integrity chain and\nscaling update performance under mass customization. IMUP combines three\ntechniques: per-module chameleon hashing for integrity, server-side\nproof-of-work offloading to reduce device overhead, and server-side caching to\nreuse module combinations, minimizing rebuild costs. Security analysis shows\nthat even when 95 percent of secret keys are exposed, forging a valid image\nincurs over 300 times the cost of the legitimate server. Experiments on\nheterogeneous IoT devices demonstrate that IMUP reduces server-side generation\ntime by 2.9 times and device downtime by 5.9 times compared to a\npackage-manager baseline."
                },
                "authors": [
                    {
                        "name": "Weihao Chen"
                    },
                    {
                        "name": "Yansong Gao"
                    },
                    {
                        "name": "Boyu Kuang"
                    },
                    {
                        "name": "Jin B. Hong"
                    },
                    {
                        "name": "Yuqing Zhang"
                    },
                    {
                        "name": "Anmin Fu"
                    }
                ],
                "author_detail": {
                    "name": "Anmin Fu"
                },
                "author": "Anmin Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01802v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01802v2",
                "updated": "2025-07-24T19:44:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    19,
                    44,
                    36,
                    3,
                    205,
                    0
                ],
                "published": "2025-02-03T20:30:25Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    20,
                    30,
                    25,
                    0,
                    34,
                    0
                ],
                "title": "General kinetic ion induced electron emission model for metallic walls\n  applied to biased Z-pinch electrodes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General kinetic ion induced electron emission model for metallic walls\n  applied to biased Z-pinch electrodes"
                },
                "summary": "A kinetic ion induced electron emission (IIEE) model for general applications\nis developed to obtain the emitted electron energy spectrum for a distribution\nof ion impacts on a metallic surface. We assume an ionization cascade mechanism\nand use empirical models for the ion and electron stopping powers. The emission\nspectrum and the secondary electron yield (SEY) are validated for a variety of\nmaterials. The IIEE model is used to study the effect of IIEE on the\nplasma-material interactions of Z-pinch electrodes. Un-magnetized\nBoltzmann-Poisson simulations are performed for a Z-pinch plasma doubly bounded\nby two biased copper electrodes with and without IIEE at bias potentials from 0\nto 9 kV. At the anode, the SEY decreases from 0 to 1 kV, but then increases at\nhigher bias potentials. At the cathode, the SEY is much larger due to higher\nenergy ion bombardment and grows with bias potential. As the bias potential\nincreases, the emitted cathode electrons are accelerated to higher energies\ninto the domain collisionally heating the plasma. Above 1 kV, the heating is\nstrong enough to increase the plasma potential. Despite SEY greater than 1,\nonly a classical sheath forms as opposed to a space-charge limited or inverse\nsheath due to the emitted electron flux not reaching the space charge current\nsaturation limits. Furthermore, the current in the emissionless cases saturates\nto a value lower than experiment. With IIEE, the current does not saturate and\ncontinues to increase with the 4 kV case matching most closely with experiment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A kinetic ion induced electron emission (IIEE) model for general applications\nis developed to obtain the emitted electron energy spectrum for a distribution\nof ion impacts on a metallic surface. We assume an ionization cascade mechanism\nand use empirical models for the ion and electron stopping powers. The emission\nspectrum and the secondary electron yield (SEY) are validated for a variety of\nmaterials. The IIEE model is used to study the effect of IIEE on the\nplasma-material interactions of Z-pinch electrodes. Un-magnetized\nBoltzmann-Poisson simulations are performed for a Z-pinch plasma doubly bounded\nby two biased copper electrodes with and without IIEE at bias potentials from 0\nto 9 kV. At the anode, the SEY decreases from 0 to 1 kV, but then increases at\nhigher bias potentials. At the cathode, the SEY is much larger due to higher\nenergy ion bombardment and grows with bias potential. As the bias potential\nincreases, the emitted cathode electrons are accelerated to higher energies\ninto the domain collisionally heating the plasma. Above 1 kV, the heating is\nstrong enough to increase the plasma potential. Despite SEY greater than 1,\nonly a classical sheath forms as opposed to a space-charge limited or inverse\nsheath due to the emitted electron flux not reaching the space charge current\nsaturation limits. Furthermore, the current in the emissionless cases saturates\nto a value lower than experiment. With IIEE, the current does not saturate and\ncontinues to increase with the 4 kV case matching most closely with experiment."
                },
                "authors": [
                    {
                        "name": "Chirag R. Skolar"
                    },
                    {
                        "name": "Kolter Bradshaw"
                    },
                    {
                        "name": "Manaure Francisquez"
                    },
                    {
                        "name": "Lucio Murillo"
                    },
                    {
                        "name": "Vignesh Krishna Kumar"
                    },
                    {
                        "name": "Bhuvana Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Bhuvana Srinivasan"
                },
                "author": "Bhuvana Srinivasan",
                "arxiv_comment": "21 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01802v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01802v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16870v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16870v2",
                "updated": "2025-07-24T17:30:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    17,
                    30,
                    12,
                    3,
                    205,
                    0
                ],
                "published": "2025-03-21T05:58:18Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    5,
                    58,
                    18,
                    4,
                    80,
                    0
                ],
                "title": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs"
                },
                "summary": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B."
                },
                "authors": [
                    {
                        "name": "Anshumann"
                    },
                    {
                        "name": "Mohd Abbas Zaidi"
                    },
                    {
                        "name": "Akhil Kedia"
                    },
                    {
                        "name": "Jinwoo Ahn"
                    },
                    {
                        "name": "Taehwak Kwon"
                    },
                    {
                        "name": "Kangwook Lee"
                    },
                    {
                        "name": "Haejun Lee"
                    },
                    {
                        "name": "Joohyung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Joohyung Lee"
                },
                "author": "Joohyung Lee",
                "arxiv_comment": "Accepted as Oral paper at ACL 2025. Source code is available at\n  https://github.com/akhilkedia/RandomSamplingKD . Anshumann, Mohd Abbas Zaidi\n  and Akhil Kedia have Equal Contribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16870v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16870v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04704v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04704v2",
                "updated": "2025-07-24T16:25:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    25,
                    51,
                    3,
                    205,
                    0
                ],
                "published": "2025-04-07T03:22:15Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    3,
                    22,
                    15,
                    0,
                    97,
                    0
                ],
                "title": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important"
                },
                "summary": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modification of the inference infrastructure and\nsignificant computation overhead. Based on the fact that the Large Language\nmodels are autoregressive models, we propose LagKV, a KV compression strategy\nonly relying on straight forward comparison among KV themselves. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on RULER benchmark show that, our approach\noutperforms SnapKV and StreamingLLM in different compression ratios. Especially\nin the 64-digit passkey retrieval task, our method outperforms the attention\nweight based method $H_2O$ over $50\\%$ with same compression ratios. Our code\nis available at https://github.com/AI-Lab-China-Merchants-Bank/LagKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modification of the inference infrastructure and\nsignificant computation overhead. Based on the fact that the Large Language\nmodels are autoregressive models, we propose LagKV, a KV compression strategy\nonly relying on straight forward comparison among KV themselves. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on RULER benchmark show that, our approach\noutperforms SnapKV and StreamingLLM in different compression ratios. Especially\nin the 64-digit passkey retrieval task, our method outperforms the attention\nweight based method $H_2O$ over $50\\%$ with same compression ratios. Our code\nis available at https://github.com/AI-Lab-China-Merchants-Bank/LagKV."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "JiaMing Zhang"
                    },
                    {
                        "name": "Xiong Li"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04704v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04704v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18446v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18446v1",
                "updated": "2025-07-24T14:30:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    30,
                    48,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T14:30:48Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    30,
                    48,
                    3,
                    205,
                    0
                ],
                "title": "Streaming Sortformer: Speaker Cache-Based Online Speaker Diarization\n  with Arrival-Time Ordering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming Sortformer: Speaker Cache-Based Online Speaker Diarization\n  with Arrival-Time Ordering"
                },
                "summary": "This paper presents a streaming extension for the Sortformer speaker\ndiarization framework, whose key property is the arrival-time ordering of\noutput speakers. The proposed approach employs an Arrival-Order Speaker Cache\n(AOSC) to store frame-level acoustic embeddings of previously observed\nspeakers. Unlike conventional speaker-tracing buffers, AOSC orders embeddings\nby speaker index corresponding to their arrival time order, and is dynamically\nupdated by selecting frames with the highest scores based on the model's past\npredictions. Notably, the number of stored embeddings per speaker is determined\ndynamically by the update mechanism, ensuring efficient cache utilization and\nprecise speaker tracking. Experiments on benchmark datasets confirm the\neffectiveness and flexibility of our approach, even in low-latency setups.\nThese results establish Streaming Sortformer as a robust solution for real-time\nmulti-speaker tracking and a foundation for streaming multi-talker speech\nprocessing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a streaming extension for the Sortformer speaker\ndiarization framework, whose key property is the arrival-time ordering of\noutput speakers. The proposed approach employs an Arrival-Order Speaker Cache\n(AOSC) to store frame-level acoustic embeddings of previously observed\nspeakers. Unlike conventional speaker-tracing buffers, AOSC orders embeddings\nby speaker index corresponding to their arrival time order, and is dynamically\nupdated by selecting frames with the highest scores based on the model's past\npredictions. Notably, the number of stored embeddings per speaker is determined\ndynamically by the update mechanism, ensuring efficient cache utilization and\nprecise speaker tracking. Experiments on benchmark datasets confirm the\neffectiveness and flexibility of our approach, even in low-latency setups.\nThese results establish Streaming Sortformer as a robust solution for real-time\nmulti-speaker tracking and a foundation for streaming multi-talker speech\nprocessing."
                },
                "authors": [
                    {
                        "name": "Ivan Medennikov"
                    },
                    {
                        "name": "Taejin Park"
                    },
                    {
                        "name": "Weiqing Wang"
                    },
                    {
                        "name": "He Huang"
                    },
                    {
                        "name": "Kunal Dhawan"
                    },
                    {
                        "name": "Jinhan Wang"
                    },
                    {
                        "name": "Jagadeesh Balam"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Accepted to Interspeech 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18446v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18446v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18028v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18028v1",
                "updated": "2025-07-24T02:00:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    2,
                    0,
                    9,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T02:00:09Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    2,
                    0,
                    9,
                    3,
                    205,
                    0
                ],
                "title": "NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural\n  KV Database",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural\n  KV Database"
                },
                "summary": "Efficiently editing knowledge stored in large language models (LLMs) enables\nmodel updates without large-scale training. One possible solution is\nLocate-and-Edit (L\\&E), allowing simultaneous modifications of a massive number\nof facts. However, such editing may compromise the general abilities of LLMs\nand even result in forgetting edited facts when scaling up to thousands of\nedits. In this paper, we model existing linear L\\&E methods as querying a\nKey-Value (KV) database. From this perspective, we then propose NeuralDB, an\nediting framework that explicitly represents the edited facts as a neural KV\ndatabase equipped with a non-linear gated retrieval module, % In particular,\nour gated module only operates when inference involves the edited facts,\neffectively preserving the general abilities of LLMs. Comprehensive experiments\ninvolving the editing of 10,000 facts were conducted on the ZsRE and\nCounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results\ndemonstrate that NeuralDB not only excels in editing efficacy, generalization,\nspecificity, fluency, and consistency, but also preserves overall performance\nacross six representative text understanding and generation tasks. Further\nexperiments indicate that NeuralDB maintains its effectiveness even when scaled\nto 100,000 facts (\\textbf{50x} more than in prior work).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently editing knowledge stored in large language models (LLMs) enables\nmodel updates without large-scale training. One possible solution is\nLocate-and-Edit (L\\&E), allowing simultaneous modifications of a massive number\nof facts. However, such editing may compromise the general abilities of LLMs\nand even result in forgetting edited facts when scaling up to thousands of\nedits. In this paper, we model existing linear L\\&E methods as querying a\nKey-Value (KV) database. From this perspective, we then propose NeuralDB, an\nediting framework that explicitly represents the edited facts as a neural KV\ndatabase equipped with a non-linear gated retrieval module, % In particular,\nour gated module only operates when inference involves the edited facts,\neffectively preserving the general abilities of LLMs. Comprehensive experiments\ninvolving the editing of 10,000 facts were conducted on the ZsRE and\nCounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results\ndemonstrate that NeuralDB not only excels in editing efficacy, generalization,\nspecificity, fluency, and consistency, but also preserves overall performance\nacross six representative text understanding and generation tasks. Further\nexperiments indicate that NeuralDB maintains its effectiveness even when scaled\nto 100,000 facts (\\textbf{50x} more than in prior work)."
                },
                "authors": [
                    {
                        "name": "Weizhi Fei"
                    },
                    {
                        "name": "Hao Shi"
                    },
                    {
                        "name": "Jing Xu"
                    },
                    {
                        "name": "Jingchen Peng"
                    },
                    {
                        "name": "Jiazheng Li"
                    },
                    {
                        "name": "Jingzhao Zhang"
                    },
                    {
                        "name": "Bo Bai"
                    },
                    {
                        "name": "Wei Han"
                    },
                    {
                        "name": "Zhenyuan Chen"
                    },
                    {
                        "name": "Xueyan Niu"
                    }
                ],
                "author_detail": {
                    "name": "Xueyan Niu"
                },
                "author": "Xueyan Niu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18028v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18028v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17744v1",
                "updated": "2025-07-23T17:57:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    17,
                    57,
                    9,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-23T17:57:09Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    17,
                    57,
                    9,
                    2,
                    204,
                    0
                ],
                "title": "Yume: An Interactive World Generation Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yume: An Interactive World Generation Model"
                },
                "summary": "Yume aims to use images, text, or videos to create an interactive, realistic,\nand dynamic world, which allows exploration and control using peripheral\ndevices or neural signals. In this report, we present a preview version of\n\\method, which creates a dynamic world from an input image and allows\nexploration of the world using keyboard actions. To achieve this high-fidelity\nand interactive video world generation, we introduce a well-designed framework,\nwhich consists of four main components, including camera motion quantization,\nvideo generation architecture, advanced sampler, and model acceleration. First,\nwe quantize camera motions for stable training and user-friendly interaction\nusing keyboard inputs. Then, we introduce the Masked Video Diffusion\nTransformer~(MVDT) with a memory module for infinite video generation in an\nautoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM)\nand Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE)\nare introduced to the sampler for better visual quality and more precise\ncontrol. Moreover, we investigate model acceleration by synergistic\noptimization of adversarial distillation and caching mechanisms. We use the\nhigh-quality world exploration dataset \\sekai to train \\method, and it achieves\nremarkable results in diverse scenes and applications. All data, codebase, and\nmodel weights are available on https://github.com/stdstu12/YUME. Yume will\nupdate monthly to achieve its original goal. Project page:\nhttps://stdstu12.github.io/YUME-Project/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yume aims to use images, text, or videos to create an interactive, realistic,\nand dynamic world, which allows exploration and control using peripheral\ndevices or neural signals. In this report, we present a preview version of\n\\method, which creates a dynamic world from an input image and allows\nexploration of the world using keyboard actions. To achieve this high-fidelity\nand interactive video world generation, we introduce a well-designed framework,\nwhich consists of four main components, including camera motion quantization,\nvideo generation architecture, advanced sampler, and model acceleration. First,\nwe quantize camera motions for stable training and user-friendly interaction\nusing keyboard inputs. Then, we introduce the Masked Video Diffusion\nTransformer~(MVDT) with a memory module for infinite video generation in an\nautoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM)\nand Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE)\nare introduced to the sampler for better visual quality and more precise\ncontrol. Moreover, we investigate model acceleration by synergistic\noptimization of adversarial distillation and caching mechanisms. We use the\nhigh-quality world exploration dataset \\sekai to train \\method, and it achieves\nremarkable results in diverse scenes and applications. All data, codebase, and\nmodel weights are available on https://github.com/stdstu12/YUME. Yume will\nupdate monthly to achieve its original goal. Project page:\nhttps://stdstu12.github.io/YUME-Project/."
                },
                "authors": [
                    {
                        "name": "Xiaofeng Mao"
                    },
                    {
                        "name": "Shaoheng Lin"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Chuanhao Li"
                    },
                    {
                        "name": "Wenshuo Peng"
                    },
                    {
                        "name": "Tong He"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    },
                    {
                        "name": "Mingmin Chi"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kaipeng Zhang"
                },
                "author": "Kaipeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17647v1",
                "updated": "2025-07-23T16:09:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    16,
                    9,
                    10,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-23T16:09:10Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    16,
                    9,
                    10,
                    2,
                    204,
                    0
                ],
                "title": "SHINE: A Scalable HNSW Index in Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SHINE: A Scalable HNSW Index in Disaggregated Memory"
                },
                "summary": "Approximate nearest neighbor (ANN) search is a fundamental problem in\ncomputer science for which in-memory graph-based methods, such as Hierarchical\nNavigable Small World (HNSW), perform exceptionally well. To scale beyond\nbillions of high-dimensional vectors, the index must be distributed. The\ndisaggregated memory architecture physically separates compute and memory into\ntwo distinct hardware units and has become popular in modern data centers. Both\nunits are connected via RDMA networks that allow compute nodes to directly\naccess remote memory and perform all the computations, posing unique challenges\nfor disaggregated indexes.\n  In this work, we propose a scalable HNSW index for ANN search in\ndisaggregated memory. In contrast to existing distributed approaches, which\npartition the graph at the cost of accuracy, our method builds a\ngraph-preserving index that reaches the same accuracy as a single-machine HNSW.\nContinuously fetching high-dimensional vector data from remote memory leads to\nsevere network bandwidth limitations, which we overcome by employing an\nefficient caching mechanism. Since answering a single query involves processing\nnumerous unique graph nodes, caching alone is not sufficient to achieve high\nscalability. We logically combine the caches of the compute nodes to increase\nthe overall cache effectiveness and confirm the efficiency and scalability of\nour method in our evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate nearest neighbor (ANN) search is a fundamental problem in\ncomputer science for which in-memory graph-based methods, such as Hierarchical\nNavigable Small World (HNSW), perform exceptionally well. To scale beyond\nbillions of high-dimensional vectors, the index must be distributed. The\ndisaggregated memory architecture physically separates compute and memory into\ntwo distinct hardware units and has become popular in modern data centers. Both\nunits are connected via RDMA networks that allow compute nodes to directly\naccess remote memory and perform all the computations, posing unique challenges\nfor disaggregated indexes.\n  In this work, we propose a scalable HNSW index for ANN search in\ndisaggregated memory. In contrast to existing distributed approaches, which\npartition the graph at the cost of accuracy, our method builds a\ngraph-preserving index that reaches the same accuracy as a single-machine HNSW.\nContinuously fetching high-dimensional vector data from remote memory leads to\nsevere network bandwidth limitations, which we overcome by employing an\nefficient caching mechanism. Since answering a single query involves processing\nnumerous unique graph nodes, caching alone is not sufficient to achieve high\nscalability. We logically combine the caches of the compute nodes to increase\nthe overall cache effectiveness and confirm the efficiency and scalability of\nour method in our evaluation."
                },
                "authors": [
                    {
                        "name": "Manuel Widmoser"
                    },
                    {
                        "name": "Daniel Kocher"
                    },
                    {
                        "name": "Nikolaus Augsten"
                    }
                ],
                "author_detail": {
                    "name": "Nikolaus Augsten"
                },
                "author": "Nikolaus Augsten",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16242v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16242v2",
                "updated": "2025-07-23T15:59:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    15,
                    59,
                    38,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-22T05:26:28Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    26,
                    28,
                    1,
                    203,
                    0
                ],
                "title": "Toward a Lightweight and Robust Design for Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward a Lightweight and Robust Design for Caching"
                },
                "summary": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice."
                },
                "authors": [
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Jiaji Zhang"
                    },
                    {
                        "name": "Xueyan Tang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16242v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16242v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17554v1",
                "updated": "2025-07-23T14:43:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    14,
                    43,
                    22,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-23T14:43:22Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    14,
                    43,
                    22,
                    2,
                    204,
                    0
                ],
                "title": "An h-space Based Adversarial Attack for Protection Against Few-shot\n  Personalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An h-space Based Adversarial Attack for Protection Against Few-shot\n  Personalization"
                },
                "summary": "The versatility of diffusion models in generating customized images from few\nsamples raises significant privacy concerns, particularly regarding\nunauthorized modifications of private content. This concerning issue has\nrenewed the efforts in developing protection mechanisms based on adversarial\nattacks, which generate effective perturbations to poison diffusion models. Our\nwork is motivated by the observation that these models exhibit a high degree of\nabstraction within their semantic latent space (`h-space'), which encodes\ncritical high-level features for generating coherent and meaningful content. In\nthis paper, we propose a novel anti-customization approach, called HAAD\n(h-space based Adversarial Attack for Diffusion models), that leverages\nadversarial attacks to craft perturbations based on the h-space that can\nefficiently degrade the image generation process. Building upon HAAD, we\nfurther introduce a more efficient variant, HAAD-KV, that constructs\nperturbations solely based on the KV parameters of the h-space. This strategy\noffers a stronger protection, that is computationally less expensive. Despite\ntheir simplicity, our methods outperform state-of-the-art adversarial attacks,\nhighlighting their effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The versatility of diffusion models in generating customized images from few\nsamples raises significant privacy concerns, particularly regarding\nunauthorized modifications of private content. This concerning issue has\nrenewed the efforts in developing protection mechanisms based on adversarial\nattacks, which generate effective perturbations to poison diffusion models. Our\nwork is motivated by the observation that these models exhibit a high degree of\nabstraction within their semantic latent space (`h-space'), which encodes\ncritical high-level features for generating coherent and meaningful content. In\nthis paper, we propose a novel anti-customization approach, called HAAD\n(h-space based Adversarial Attack for Diffusion models), that leverages\nadversarial attacks to craft perturbations based on the h-space that can\nefficiently degrade the image generation process. Building upon HAAD, we\nfurther introduce a more efficient variant, HAAD-KV, that constructs\nperturbations solely based on the KV parameters of the h-space. This strategy\noffers a stronger protection, that is computationally less expensive. Despite\ntheir simplicity, our methods outperform state-of-the-art adversarial attacks,\nhighlighting their effectiveness."
                },
                "authors": [
                    {
                        "name": "Xide Xu"
                    },
                    {
                        "name": "Sandesh Kamath"
                    },
                    {
                        "name": "Muhammad Atif Butt"
                    },
                    {
                        "name": "Bogdan Raducanu"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Raducanu"
                },
                "author": "Bogdan Raducanu",
                "arxiv_comment": "32 pages, 15 figures. Accepted by ACM Multimedia 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23956v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23956v3",
                "updated": "2025-07-23T11:42:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    11,
                    42,
                    3,
                    2,
                    204,
                    0
                ],
                "published": "2025-03-31T11:13:18Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    13,
                    18,
                    0,
                    90,
                    0
                ],
                "title": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference"
                },
                "summary": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches."
                },
                "authors": [
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Hao Zou"
                    },
                    {
                        "name": "Bochen Wang"
                    },
                    {
                        "name": "Ye Xi"
                    },
                    {
                        "name": "Zhen Xie"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23956v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23956v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17411v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17411v1",
                "updated": "2025-07-23T11:12:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    11,
                    12,
                    8,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-23T11:12:08Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    11,
                    12,
                    8,
                    2,
                    204,
                    0
                ],
                "title": "Multiprocessor Scheduling with Memory Constraints: Fundamental\n  Properties and Finding Optimal Solutions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiprocessor Scheduling with Memory Constraints: Fundamental\n  Properties and Finding Optimal Solutions"
                },
                "summary": "We study the problem of scheduling a general computational DAG on multiple\nprocessors in a 2-level memory hierarchy. This setting is a natural\ngeneralization of several prominent models in the literature, and it\nsimultaneously captures workload balancing, communication, and data movement\ndue to cache size limitations. We first analyze the fundamental properties of\nthis problem from a theoretical perspective, such as its computational\ncomplexity. We also prove that optimizing parallelization and memory management\nseparately, as done in many applications, can result in a solution that is a\nlinear factor away from the optimum.\n  On the algorithmic side, we discuss a natural technique to represent and\nsolve the problem as an Integer Linear Program (ILP). We develop a holistic\nscheduling algorithm based on this approach, and we experimentally study its\nperformance and properties on a small benchmark of computational tasks. Our\nresults confirm that the ILP-based method can indeed find considerably better\nsolutions than a baseline which combines classical scheduling algorithms and\nmemory management policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of scheduling a general computational DAG on multiple\nprocessors in a 2-level memory hierarchy. This setting is a natural\ngeneralization of several prominent models in the literature, and it\nsimultaneously captures workload balancing, communication, and data movement\ndue to cache size limitations. We first analyze the fundamental properties of\nthis problem from a theoretical perspective, such as its computational\ncomplexity. We also prove that optimizing parallelization and memory management\nseparately, as done in many applications, can result in a solution that is a\nlinear factor away from the optimum.\n  On the algorithmic side, we discuss a natural technique to represent and\nsolve the problem as an Integer Linear Program (ILP). We develop a holistic\nscheduling algorithm based on this approach, and we experimentally study its\nperformance and properties on a small benchmark of computational tasks. Our\nresults confirm that the ILP-based method can indeed find considerably better\nsolutions than a baseline which combines classical scheduling algorithms and\nmemory management policies."
                },
                "authors": [
                    {
                        "name": "Pl Andrs Papp"
                    },
                    {
                        "name": "Toni Bhnlein"
                    },
                    {
                        "name": "A. N. Yzelman"
                    }
                ],
                "author_detail": {
                    "name": "A. N. Yzelman"
                },
                "author": "A. N. Yzelman",
                "arxiv_doi": "10.1145/3754598.3754676",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3754598.3754676",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.17411v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17411v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in the 54th International Conference on Parallel Processing\n  (ICPP 2025)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "90B35, 90C10, 68Q10, 68W10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13373v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13373v2",
                "updated": "2025-07-23T10:10:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    10,
                    10,
                    53,
                    2,
                    204,
                    0
                ],
                "published": "2024-11-20T14:52:36Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    52,
                    36,
                    2,
                    325,
                    0
                ],
                "title": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment"
                },
                "summary": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Nuclear\nPhysics (BINP), was based on a source of positive hydrogen ions, accelerated to\n50 keV and for a maximum ion current of 5 A. The beam could be modulated and\nthe maximum overall duration was 50 ms. With the upgrade of RFX-mod to the\npresent RFX-mod2 machine, the DNBI is being renovated to solve several power\nunits faults and improve the overall reliability of the system. The 50 kV power\nsupply is being improved, as well as the power supplies in the high voltage\ndeck and its insulation transformer. Magnetic field survival tests were\nperformed on the toroidal-core-based DC-DC converters that should power the\nelectronic boards in a reliable way. The control system, originally based on\nCAMAC technology, was redesigned to be fully replaced. This contribution\nreviews the technical criticalities emerged in the DNBI check-up and the new\nsolutions adopted to make the DNBI operative and more reliable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Nuclear\nPhysics (BINP), was based on a source of positive hydrogen ions, accelerated to\n50 keV and for a maximum ion current of 5 A. The beam could be modulated and\nthe maximum overall duration was 50 ms. With the upgrade of RFX-mod to the\npresent RFX-mod2 machine, the DNBI is being renovated to solve several power\nunits faults and improve the overall reliability of the system. The 50 kV power\nsupply is being improved, as well as the power supplies in the high voltage\ndeck and its insulation transformer. Magnetic field survival tests were\nperformed on the toroidal-core-based DC-DC converters that should power the\nelectronic boards in a reliable way. The control system, originally based on\nCAMAC technology, was redesigned to be fully replaced. This contribution\nreviews the technical criticalities emerged in the DNBI check-up and the new\nsolutions adopted to make the DNBI operative and more reliable."
                },
                "authors": [
                    {
                        "name": "Marco Barbisan"
                    },
                    {
                        "name": "Marco Boldrin"
                    },
                    {
                        "name": "Luca Cinnirella"
                    },
                    {
                        "name": "Bruno Laterza"
                    },
                    {
                        "name": "Alberto Maistrello"
                    },
                    {
                        "name": "Lionello Marrelli"
                    },
                    {
                        "name": "Federico Molon"
                    },
                    {
                        "name": "Simone Peruzzo"
                    },
                    {
                        "name": "Cesare Taliercio"
                    },
                    {
                        "name": "Marco Valisa"
                    },
                    {
                        "name": "Enrico Zampiva"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Zampiva"
                },
                "author": "Enrico Zampiva",
                "arxiv_doi": "10.1016/j.fusengdes.2025.115320",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.fusengdes.2025.115320",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.13373v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13373v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages (excl. highlights), 8 figures. Contribution to the 33rd\n  Symposium on Fusion Technology (SOFT), 22-27 September 2024. This is the\n  accepted manuscript for the \"Fusion Engineering and Design\" journal",
                "arxiv_journal_ref": "Fusion Engineering and Design, Volume 220, November 2025, 115320",
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16391v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16391v2",
                "updated": "2025-07-23T09:31:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    9,
                    31,
                    1,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-22T09:35:59Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    9,
                    35,
                    59,
                    1,
                    203,
                    0
                ],
                "title": "Ironman: Accelerating Oblivious Transfer Extension for\n  Privacy-Preserving AI with Near-Memory Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ironman: Accelerating Oblivious Transfer Extension for\n  Privacy-Preserving AI with Near-Memory Processing"
                },
                "summary": "With the wide application of machine learning (ML), privacy concerns arise\nwith user data as they may contain sensitive information. Privacy-preserving ML\n(PPML) based on cryptographic primitives has emerged as a promising solution in\nwhich an ML model is directly computed on the encrypted data to provide a\nformal privacy guarantee. However, PPML frameworks heavily rely on the\noblivious transfer (OT) primitive to compute nonlinear functions. OT mainly\ninvolves the computation of single-point correlated OT (SPCOT) and learning\nparity with noise (LPN) operations. As OT is still computed extensively on\ngeneral-purpose CPUs, it becomes the latency bottleneck of modern PPML\nframeworks.\n  In this paper, we propose a novel OT accelerator, dubbed Ironman, to\nsignificantly increase the efficiency of OT and the overall PPML framework. We\nobserve that SPCOT is computation-bounded, and thus propose a hardware-friendly\nSPCOT algorithm with a customized accelerator to improve SPCOT computation\nthroughput. In contrast, LPN is memory-bandwidth-bounded due to irregular\nmemory access patterns. Hence, we further leverage the near-memory processing\n(NMP) architecture equipped with memory-side cache and index sorting to improve\neffective memory bandwidth. With extensive experiments, we demonstrate Ironman\nachieves a 39.2-237.4 times improvement in OT throughput across different NMP\nconfigurations compared to the full-thread CPU implementation. For different\nPPML frameworks, Ironman demonstrates a 2.1-3.4 times reduction in end-to-end\nlatency for both CNN and Transformer models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the wide application of machine learning (ML), privacy concerns arise\nwith user data as they may contain sensitive information. Privacy-preserving ML\n(PPML) based on cryptographic primitives has emerged as a promising solution in\nwhich an ML model is directly computed on the encrypted data to provide a\nformal privacy guarantee. However, PPML frameworks heavily rely on the\noblivious transfer (OT) primitive to compute nonlinear functions. OT mainly\ninvolves the computation of single-point correlated OT (SPCOT) and learning\nparity with noise (LPN) operations. As OT is still computed extensively on\ngeneral-purpose CPUs, it becomes the latency bottleneck of modern PPML\nframeworks.\n  In this paper, we propose a novel OT accelerator, dubbed Ironman, to\nsignificantly increase the efficiency of OT and the overall PPML framework. We\nobserve that SPCOT is computation-bounded, and thus propose a hardware-friendly\nSPCOT algorithm with a customized accelerator to improve SPCOT computation\nthroughput. In contrast, LPN is memory-bandwidth-bounded due to irregular\nmemory access patterns. Hence, we further leverage the near-memory processing\n(NMP) architecture equipped with memory-side cache and index sorting to improve\neffective memory bandwidth. With extensive experiments, we demonstrate Ironman\nachieves a 39.2-237.4 times improvement in OT throughput across different NMP\nconfigurations compared to the full-thread CPU implementation. For different\nPPML frameworks, Ironman demonstrates a 2.1-3.4 times reduction in end-to-end\nlatency for both CNN and Transformer models."
                },
                "authors": [
                    {
                        "name": "Chenqi Lin"
                    },
                    {
                        "name": "Kang Yang"
                    },
                    {
                        "name": "Tianshi Xu"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Zhaohui Chen"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Mingyu Gao"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16391v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16391v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02634v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02634v4",
                "updated": "2025-07-23T08:07:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    8,
                    7,
                    19,
                    2,
                    204,
                    0
                ],
                "published": "2025-06-03T08:51:38Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    8,
                    51,
                    38,
                    1,
                    154,
                    0
                ],
                "title": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider"
                },
                "summary": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity."
                },
                "authors": [
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Jinbo Han"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Sijie Shen"
                    },
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Chenguang Fang"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by USENIX ATC'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02634v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02634v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17286v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17286v2",
                "updated": "2025-07-23T05:57:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    5,
                    57,
                    32,
                    2,
                    204,
                    0
                ],
                "published": "2025-06-15T07:19:33Z",
                "published_parsed": [
                    2025,
                    6,
                    15,
                    7,
                    19,
                    33,
                    6,
                    166,
                    0
                ],
                "title": "GTA: Grouped-head latenT Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTA: Grouped-head latenT Attention"
                },
                "summary": "Attention mechanisms underpin the success of large language models (LLMs),\nyet their substantial computational and memory overhead poses challenges for\noptimizing efficiency and performance. A critical bottleneck arises as KV cache\nand attention computations scale rapidly with text length, challenging\ndeployment on hardware with limited computational and memory resources. We\nobserve that attention mechanisms exhibit substantial redundancy, since the KV\ncache can be significantly compressed and attention maps across heads display\nhigh similarity, revealing that much of the computation and storage is\nunnecessary. Leveraging these insights, we propose \\textbf{G}rouped-Head\nLaten\\textbf{T} \\textbf{A}ttention (GTA), a novel attention mechanism that\nreduces memory usage and computational complexity while maintaining\nperformance. GTA comprises two components: (1) a shared attention map mechanism\nthat reuses attention scores across multiple heads, decreasing the key cache\nsize; and (2) a nonlinear value decoder with learned projections that\ncompresses the value cache into a latent space, further cutting memory needs.\nGTA cuts attention computation FLOPs by up to \\emph{62.5\\%} versus\nGrouped-Query Attention and shrink the KV cache by up to \\emph{70\\%}, all while\navoiding the extra overhead of Multi-Head Latent Attention to improve LLM\ndeployment efficiency. Consequently, GTA models achieve a \\emph{2x} increase in\nend-to-end inference speed, with prefill benefiting from reduced computational\ncost and decoding benefiting from the smaller cache footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention mechanisms underpin the success of large language models (LLMs),\nyet their substantial computational and memory overhead poses challenges for\noptimizing efficiency and performance. A critical bottleneck arises as KV cache\nand attention computations scale rapidly with text length, challenging\ndeployment on hardware with limited computational and memory resources. We\nobserve that attention mechanisms exhibit substantial redundancy, since the KV\ncache can be significantly compressed and attention maps across heads display\nhigh similarity, revealing that much of the computation and storage is\nunnecessary. Leveraging these insights, we propose \\textbf{G}rouped-Head\nLaten\\textbf{T} \\textbf{A}ttention (GTA), a novel attention mechanism that\nreduces memory usage and computational complexity while maintaining\nperformance. GTA comprises two components: (1) a shared attention map mechanism\nthat reuses attention scores across multiple heads, decreasing the key cache\nsize; and (2) a nonlinear value decoder with learned projections that\ncompresses the value cache into a latent space, further cutting memory needs.\nGTA cuts attention computation FLOPs by up to \\emph{62.5\\%} versus\nGrouped-Query Attention and shrink the KV cache by up to \\emph{70\\%}, all while\navoiding the extra overhead of Multi-Head Latent Attention to improve LLM\ndeployment efficiency. Consequently, GTA models achieve a \\emph{2x} increase in\nend-to-end inference speed, with prefill benefiting from reduced computational\ncost and decoding benefiting from the smaller cache footprint."
                },
                "authors": [
                    {
                        "name": "Luoyang Sun"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Jiwen Jiang"
                    },
                    {
                        "name": "Xinjian Wu"
                    },
                    {
                        "name": "Haifeng Zhang"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Lionel Ni"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17286v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17286v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11046v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11046v2",
                "updated": "2025-07-23T01:42:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    1,
                    42,
                    19,
                    2,
                    204,
                    0
                ],
                "published": "2025-02-16T09:08:36Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    9,
                    8,
                    36,
                    6,
                    47,
                    0
                ],
                "title": "Enabling Efficient Transaction Processing on CXL-Based Memory Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Efficient Transaction Processing on CXL-Based Memory Sharing"
                },
                "summary": "Transaction processing systems are the crux for modern data-center\napplications, yet current multi-node systems are slow due to network overheads.\nThis paper advocates for Compute Express Link (CXL) as a network alternative,\nwhich enables low-latency and cache-coherent shared memory accesses. However,\ndirectly adopting standard CXL primitives leads to performance degradation due\nto the high cost of maintaining cross-node cache coherence. To address the CXL\nchallenges, this paper introduces CtXnL, a software-hardware co-designed system\nthat implements a novel hybrid coherence primitive tailored to the loosely\ncoherent nature of transactional data. The core innovation of CtXnL is\nempowering transaction system developers with the ability to selectively\nachieve data coherence. Our evaluations on OLTP workloads demonstrate that\nCtXnL enhances performance, outperforming current network-based systems and\nachieves with up to 2.08x greater throughput than vanilla CXL memory sharing\narchitectures across universal transaction processing policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transaction processing systems are the crux for modern data-center\napplications, yet current multi-node systems are slow due to network overheads.\nThis paper advocates for Compute Express Link (CXL) as a network alternative,\nwhich enables low-latency and cache-coherent shared memory accesses. However,\ndirectly adopting standard CXL primitives leads to performance degradation due\nto the high cost of maintaining cross-node cache coherence. To address the CXL\nchallenges, this paper introduces CtXnL, a software-hardware co-designed system\nthat implements a novel hybrid coherence primitive tailored to the loosely\ncoherent nature of transactional data. The core innovation of CtXnL is\nempowering transaction system developers with the ability to selectively\nachieve data coherence. Our evaluations on OLTP workloads demonstrate that\nCtXnL enhances performance, outperforming current network-based systems and\nachieves with up to 2.08x greater throughput than vanilla CXL memory sharing\narchitectures across universal transaction processing policies."
                },
                "authors": [
                    {
                        "name": "Zhao Wang"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Cong Li"
                    },
                    {
                        "name": "Dimin Niu"
                    },
                    {
                        "name": "Tianchan Guan"
                    },
                    {
                        "name": "Zhaoyang Du"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11046v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11046v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17033v1",
                "updated": "2025-07-22T21:41:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    41,
                    43,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T21:41:43Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    41,
                    43,
                    1,
                    203,
                    0
                ],
                "title": "GATEBLEED: Exploiting On-Core Accelerator Power Gating for High\n  Performance & Stealthy Attacks on AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GATEBLEED: Exploiting On-Core Accelerator Power Gating for High\n  Performance & Stealthy Attacks on AI"
                },
                "summary": "As power consumption from AI training and inference continues to increase, AI\naccelerators are being integrated directly into the CPU. Intel's Advanced\nMatrix Extensions (AMX) is one such example, debuting on the 4th generation\nIntel Xeon Scalable CPU. We discover a timing side and covert channel,\nGATEBLEED, caused by the aggressive power gating utilized to keep the CPU\nwithin operating limits. We show that the GATEBLEED side channel is a threat to\nAI privacy as many ML models such as transformers and CNNs make critical\ncomputationally-heavy decisions based on private values like confidence\nthresholds and routing logits. Timing delays from selective powering down of\nAMX components mean that each matrix multiplication is a potential leakage\npoint when executed on the AMX accelerator. Our research identifies over a\ndozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,\nTensorFlow, etc.), revealing that they can leak sensitive and private\ninformation. GATEBLEED poses a risk for local and remote timing inference, even\nunder previous protective measures. GATEBLEED can be used as a high\nperformance, stealthy remote covert channel and a generic magnifier for timing\ntransmission channels, capable of bypassing traditional cache defenses to leak\narbitrary memory addresses and evading state of the art microarchitectural\nattack detectors under realistic network conditions and system configurations\nin which previous attacks fail. We implement an end-to-end microarchitectural\ninference attack on a transformer model optimized with Intel AMX, achieving a\nmembership inference accuracy of 81% and a precision of 0.89. In a CNN-based or\ntransformer-based mixture-of-experts model optimized with Intel AMX, we leak\nexpert choice with 100% accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As power consumption from AI training and inference continues to increase, AI\naccelerators are being integrated directly into the CPU. Intel's Advanced\nMatrix Extensions (AMX) is one such example, debuting on the 4th generation\nIntel Xeon Scalable CPU. We discover a timing side and covert channel,\nGATEBLEED, caused by the aggressive power gating utilized to keep the CPU\nwithin operating limits. We show that the GATEBLEED side channel is a threat to\nAI privacy as many ML models such as transformers and CNNs make critical\ncomputationally-heavy decisions based on private values like confidence\nthresholds and routing logits. Timing delays from selective powering down of\nAMX components mean that each matrix multiplication is a potential leakage\npoint when executed on the AMX accelerator. Our research identifies over a\ndozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,\nTensorFlow, etc.), revealing that they can leak sensitive and private\ninformation. GATEBLEED poses a risk for local and remote timing inference, even\nunder previous protective measures. GATEBLEED can be used as a high\nperformance, stealthy remote covert channel and a generic magnifier for timing\ntransmission channels, capable of bypassing traditional cache defenses to leak\narbitrary memory addresses and evading state of the art microarchitectural\nattack detectors under realistic network conditions and system configurations\nin which previous attacks fail. We implement an end-to-end microarchitectural\ninference attack on a transformer model optimized with Intel AMX, achieving a\nmembership inference accuracy of 81% and a precision of 0.89. In a CNN-based or\ntransformer-based mixture-of-experts model optimized with Intel AMX, we leak\nexpert choice with 100% accuracy."
                },
                "authors": [
                    {
                        "name": "Joshua Kalyanapu"
                    },
                    {
                        "name": "Farshad Dizani"
                    },
                    {
                        "name": "Darsh Asher"
                    },
                    {
                        "name": "Azam Ghanbari"
                    },
                    {
                        "name": "Rosario Cammarota"
                    },
                    {
                        "name": "Aydin Aysu"
                    },
                    {
                        "name": "Samira Mirbagher Ajorpaz"
                    }
                ],
                "author_detail": {
                    "name": "Samira Mirbagher Ajorpaz"
                },
                "author": "Samira Mirbagher Ajorpaz",
                "arxiv_comment": "Accepted at MICRO 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17029v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17029v1",
                "updated": "2025-07-22T21:33:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    33,
                    30,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T21:33:30Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    33,
                    30,
                    1,
                    203,
                    0
                ],
                "title": "StreamME: Simplify 3D Gaussian Avatar within Live Stream",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamME: Simplify 3D Gaussian Avatar within Live Stream"
                },
                "summary": "We propose StreamME, a method focuses on fast 3D avatar reconstruction. The\nStreamME synchronously records and reconstructs a head avatar from live video\nstreams without any pre-cached data, enabling seamless integration of the\nreconstructed appearance into downstream applications. This exceptionally fast\ntraining strategy, which we refer to as on-the-fly training, is central to our\napproach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminating\nthe reliance on MLPs in deformable 3DGS and relying solely on geometry, which\nsignificantly improves the adaptation speed to facial expression. To further\nensure high efficiency in on-the-fly training, we introduced a simplification\nstrategy based on primary points, which distributes the point clouds more\nsparsely across the facial surface, optimizing points number while maintaining\nrendering quality. Leveraging the on-the-fly training capabilities, our method\nprotects the facial privacy and reduces communication bandwidth in VR system or\nonline conference. Additionally, it can be directly applied to downstream\napplication such as animation, toonify, and relighting. Please refer to our\nproject page for more details: https://songluchuan.github.io/StreamME/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose StreamME, a method focuses on fast 3D avatar reconstruction. The\nStreamME synchronously records and reconstructs a head avatar from live video\nstreams without any pre-cached data, enabling seamless integration of the\nreconstructed appearance into downstream applications. This exceptionally fast\ntraining strategy, which we refer to as on-the-fly training, is central to our\napproach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminating\nthe reliance on MLPs in deformable 3DGS and relying solely on geometry, which\nsignificantly improves the adaptation speed to facial expression. To further\nensure high efficiency in on-the-fly training, we introduced a simplification\nstrategy based on primary points, which distributes the point clouds more\nsparsely across the facial surface, optimizing points number while maintaining\nrendering quality. Leveraging the on-the-fly training capabilities, our method\nprotects the facial privacy and reduces communication bandwidth in VR system or\nonline conference. Additionally, it can be directly applied to downstream\napplication such as animation, toonify, and relighting. Please refer to our\nproject page for more details: https://songluchuan.github.io/StreamME/."
                },
                "authors": [
                    {
                        "name": "Luchuan Song"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Zhan Xu"
                    },
                    {
                        "name": "Yi Zhou"
                    },
                    {
                        "name": "Deepali Aneja"
                    },
                    {
                        "name": "Chenliang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chenliang Xu"
                },
                "author": "Chenliang Xu",
                "arxiv_comment": "12 pages, 15 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17029v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17029v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16933v1",
                "updated": "2025-07-22T18:17:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    18,
                    17,
                    53,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T18:17:53Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    18,
                    17,
                    53,
                    1,
                    203,
                    0
                ],
                "title": "SiLQ: Simple Large Language Model Quantization-Aware Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SiLQ: Simple Large Language Model Quantization-Aware Training"
                },
                "summary": "Large language models can be quantized to reduce inference time latency,\nmodel size, and energy consumption, thereby delivering a better user experience\nat lower cost. A challenge exists to deliver quantized models with minimal loss\nof accuracy in reasonable time, and in particular to do so without requiring\nmechanisms incompatible with specialized inference accelerators. Here, we\ndemonstrate a simple, end-to-end quantization-aware training approach that,\nwith an increase in total model training budget of less than 0.1%, outperforms\nthe leading published quantization methods by large margins on several modern\nbenchmarks, with both base and instruct model variants. The approach easily\ngeneralizes across different model architectures, can be applied to\nactivations, cache, and weights, and requires the introduction of no additional\noperations to the model other than the quantization itself.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models can be quantized to reduce inference time latency,\nmodel size, and energy consumption, thereby delivering a better user experience\nat lower cost. A challenge exists to deliver quantized models with minimal loss\nof accuracy in reasonable time, and in particular to do so without requiring\nmechanisms incompatible with specialized inference accelerators. Here, we\ndemonstrate a simple, end-to-end quantization-aware training approach that,\nwith an increase in total model training budget of less than 0.1%, outperforms\nthe leading published quantization methods by large margins on several modern\nbenchmarks, with both base and instruct model variants. The approach easily\ngeneralizes across different model architectures, can be applied to\nactivations, cache, and weights, and requires the introduction of no additional\noperations to the model other than the quantization itself."
                },
                "authors": [
                    {
                        "name": "Steven K. Esser"
                    },
                    {
                        "name": "Jeffrey L. McKinstry"
                    },
                    {
                        "name": "Deepika Bablani"
                    },
                    {
                        "name": "Rathinakumar Appuswamy"
                    },
                    {
                        "name": "Dharmendra S. Modha"
                    }
                ],
                "author_detail": {
                    "name": "Dharmendra S. Modha"
                },
                "author": "Dharmendra S. Modha",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16784v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16784v1",
                "updated": "2025-07-22T17:30:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    17,
                    30,
                    4,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T17:30:04Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    17,
                    30,
                    4,
                    1,
                    203,
                    0
                ],
                "title": "Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning"
                },
                "summary": "To break the context limits of large language models (LLMs) that bottleneck\nreasoning accuracy and efficiency, we propose the Thread Inference Model (TIM),\na family of LLMs trained for recursive and decompositional problem solving, and\nTIMRUN, an inference runtime enabling long-horizon structured reasoning beyond\ncontext limits. Together, TIM hosted on TIMRUN supports virtually unlimited\nworking memory and multi-hop tool calls within a single language model\ninference, overcoming output limits, positional-embedding constraints, and\nGPU-memory bottlenecks. Performance is achieved by modeling natural language as\nreasoning trees measured by both length and depth instead of linear sequences.\nThe reasoning trees consist of tasks with thoughts, recursive subtasks, and\nconclusions based on the concept we proposed in Schroeder et al, 2025. During\ngeneration, we maintain a working memory that retains only the key-value states\nof the most relevant context tokens, selected by a rule-based subtask-pruning\nmechanism, enabling reuse of positional embeddings and GPU memory pages\nthroughout reasoning. Experimental results show that our system sustains high\ninference throughput, even when manipulating up to 90% of the KV cache in GPU\nmemory. It also delivers accurate reasoning on mathematical tasks and handles\ninformation retrieval challenges that require long-horizon reasoning and\nmulti-hop tool use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To break the context limits of large language models (LLMs) that bottleneck\nreasoning accuracy and efficiency, we propose the Thread Inference Model (TIM),\na family of LLMs trained for recursive and decompositional problem solving, and\nTIMRUN, an inference runtime enabling long-horizon structured reasoning beyond\ncontext limits. Together, TIM hosted on TIMRUN supports virtually unlimited\nworking memory and multi-hop tool calls within a single language model\ninference, overcoming output limits, positional-embedding constraints, and\nGPU-memory bottlenecks. Performance is achieved by modeling natural language as\nreasoning trees measured by both length and depth instead of linear sequences.\nThe reasoning trees consist of tasks with thoughts, recursive subtasks, and\nconclusions based on the concept we proposed in Schroeder et al, 2025. During\ngeneration, we maintain a working memory that retains only the key-value states\nof the most relevant context tokens, selected by a rule-based subtask-pruning\nmechanism, enabling reuse of positional embeddings and GPU memory pages\nthroughout reasoning. Experimental results show that our system sustains high\ninference throughput, even when manipulating up to 90% of the KV cache in GPU\nmemory. It also delivers accurate reasoning on mathematical tasks and handles\ninformation retrieval challenges that require long-horizon reasoning and\nmulti-hop tool use."
                },
                "authors": [
                    {
                        "name": "Hongyin Luo"
                    },
                    {
                        "name": "Nathaniel Morgan"
                    },
                    {
                        "name": "Tina Li"
                    },
                    {
                        "name": "Derek Zhao"
                    },
                    {
                        "name": "Ai Vy Ngo"
                    },
                    {
                        "name": "Philip Schroeder"
                    },
                    {
                        "name": "Lijie Yang"
                    },
                    {
                        "name": "Assaf Ben-Kish"
                    },
                    {
                        "name": "Jack O'Brien"
                    },
                    {
                        "name": "James Glass"
                    }
                ],
                "author_detail": {
                    "name": "James Glass"
                },
                "author": "James Glass",
                "arxiv_comment": "Research preview",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16784v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16784v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16768v1",
                "updated": "2025-07-22T17:13:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    17,
                    13,
                    47,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T17:13:47Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    17,
                    13,
                    47,
                    1,
                    203,
                    0
                ],
                "title": "WGRAMMAR: Leverage Prior Knowledge to Accelerate Structured Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WGRAMMAR: Leverage Prior Knowledge to Accelerate Structured Decoding"
                },
                "summary": "Structured decoding enables large language models (LLMs) to generate outputs\nin formats required by downstream systems, such as HTML or JSON. However,\nexisting methods suffer from efficiency bottlenecks due to grammar compilation,\nstate tracking, and mask creation. We observe that many real-world tasks embed\nstrong prior knowledge about output structure. Leveraging this, we propose a\ndecomposition of constraints into static and dynamic components -- precompiling\nstatic structures offline and instantiating dynamic arguments at runtime using\ngrammar snippets. Instead of relying on pushdown automata, we employ a\ncompositional set of operators to model regular formats, achieving lower\ntransition latency. We introduce wgrammar, a lightweight decoding engine that\nintegrates domain-aware simplification, constraint decomposition, and mask\ncaching, achieving up to 250x speedup over existing systems. wgrammar's source\ncode is publicly available at https://github.com/wrran/wgrammar.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured decoding enables large language models (LLMs) to generate outputs\nin formats required by downstream systems, such as HTML or JSON. However,\nexisting methods suffer from efficiency bottlenecks due to grammar compilation,\nstate tracking, and mask creation. We observe that many real-world tasks embed\nstrong prior knowledge about output structure. Leveraging this, we propose a\ndecomposition of constraints into static and dynamic components -- precompiling\nstatic structures offline and instantiating dynamic arguments at runtime using\ngrammar snippets. Instead of relying on pushdown automata, we employ a\ncompositional set of operators to model regular formats, achieving lower\ntransition latency. We introduce wgrammar, a lightweight decoding engine that\nintegrates domain-aware simplification, constraint decomposition, and mask\ncaching, achieving up to 250x speedup over existing systems. wgrammar's source\ncode is publicly available at https://github.com/wrran/wgrammar."
                },
                "authors": [
                    {
                        "name": "Ran Wang"
                    },
                    {
                        "name": "Xiaoxuan Liu"
                    },
                    {
                        "name": "Hao Ren"
                    },
                    {
                        "name": "Gang Chen"
                    },
                    {
                        "name": "Fanchao Qi"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.10131v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.10131v3",
                "updated": "2025-07-22T16:49:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    16,
                    49,
                    24,
                    1,
                    203,
                    0
                ],
                "published": "2022-12-20T09:58:39Z",
                "published_parsed": [
                    2022,
                    12,
                    20,
                    9,
                    58,
                    39,
                    1,
                    354,
                    0
                ],
                "title": "Hydra: Virtualized Multi-Language Runtime for High-Density Serverless\n  Platforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hydra: Virtualized Multi-Language Runtime for High-Density Serverless\n  Platforms"
                },
                "summary": "Serverless is an attractive computing model that offers seamless scalability\nand elasticity; it takes the infrastructure management burden away from users\nand enables a pay-as-you-use billing model. As a result, serverless is becoming\nincreasingly popular to support highly elastic and bursty workloads. However,\nexisting platforms are supported by bloated virtualization stacks, which,\ncombined with bursty and irregular invocations, lead to high memory and latency\noverheads.\n  To reduce the virtualization stack bloat, we propose Hydra, a virtualized\nmulti-language runtime and platform capable of hosting multiple sandboxes\nrunning concurrently. To fully leverage Hydra's virtualized runtime, we revisit\nthe existing serverless platform design to make it colocation-aware across\nowners and functions, and to feature a caching layer of pre-allocated Hydra\ninstances that can be used by different functions written in different\nlanguages to reduce cold starts. We also propose a snapshotting mechanism to\ncheckpoint and restore individual sandboxes.\n  By consolidating multiple serverless function invocations through Hydra, we\nimprove the overall function density (ops/GB-sec) by 2.41x on average compared\nto OpenWhisk runtimes, the state-of-the-art single-language runtimes used in\nmost serverless platforms, and by 1.43x on average compared to Knative runtimes\nsupporting invocation colocation within the same function. When reproducing the\nAzure Functions trace, our serverless platform operating Hydra instances\nreduces the overall memory footprint by 21.3-43.9% compared to operating\nOpenWhisk instances and by 14.5-30% compared to operating Knative instances.\nHydra eliminates cold starts thanks to the pool of pre-warmed runtime\ninstances, reducing p99 latency by 45.3-375.5x compared to OpenWhisk and by\n1.9-51.4x compared to Knative.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless is an attractive computing model that offers seamless scalability\nand elasticity; it takes the infrastructure management burden away from users\nand enables a pay-as-you-use billing model. As a result, serverless is becoming\nincreasingly popular to support highly elastic and bursty workloads. However,\nexisting platforms are supported by bloated virtualization stacks, which,\ncombined with bursty and irregular invocations, lead to high memory and latency\noverheads.\n  To reduce the virtualization stack bloat, we propose Hydra, a virtualized\nmulti-language runtime and platform capable of hosting multiple sandboxes\nrunning concurrently. To fully leverage Hydra's virtualized runtime, we revisit\nthe existing serverless platform design to make it colocation-aware across\nowners and functions, and to feature a caching layer of pre-allocated Hydra\ninstances that can be used by different functions written in different\nlanguages to reduce cold starts. We also propose a snapshotting mechanism to\ncheckpoint and restore individual sandboxes.\n  By consolidating multiple serverless function invocations through Hydra, we\nimprove the overall function density (ops/GB-sec) by 2.41x on average compared\nto OpenWhisk runtimes, the state-of-the-art single-language runtimes used in\nmost serverless platforms, and by 1.43x on average compared to Knative runtimes\nsupporting invocation colocation within the same function. When reproducing the\nAzure Functions trace, our serverless platform operating Hydra instances\nreduces the overall memory footprint by 21.3-43.9% compared to operating\nOpenWhisk instances and by 14.5-30% compared to operating Knative instances.\nHydra eliminates cold starts thanks to the pool of pre-warmed runtime\ninstances, reducing p99 latency by 45.3-375.5x compared to OpenWhisk and by\n1.9-51.4x compared to Knative."
                },
                "authors": [
                    {
                        "name": "Serhii Ivanenko"
                    },
                    {
                        "name": "Vasyl Lanko"
                    },
                    {
                        "name": "Rudi Horn"
                    },
                    {
                        "name": "Vojin Jovanovic"
                    },
                    {
                        "name": "Rodrigo Bruno"
                    }
                ],
                "author_detail": {
                    "name": "Rodrigo Bruno"
                },
                "author": "Rodrigo Bruno",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2212.10131v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.10131v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16243v1",
                "updated": "2025-07-22T05:34:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    34,
                    3,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T05:34:03Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    34,
                    3,
                    1,
                    203,
                    0
                ],
                "title": "Genus Zero Kashiwara-Vergne Solutions from Braids",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genus Zero Kashiwara-Vergne Solutions from Braids"
                },
                "summary": "Using the language of moperads-monoids in the category of right modules over\nan operad-we reinterpret the Alekseev-Enriquez-Torossian construction of\nKashiwara-Vergne (KV) solutions from associators. We show that any isomorphism\nbetween the moperad of parenthesized braids with a frozen strand and the\nmoperad of chord diagrams gives rise to a family of genus zero KV solutions\noperadically generated by a single classical KV solution. We show that the\nGrothendieck-Teichm\\\"uller module groups act on the latter, intertwining the\nactions of the KV symmetry groups. In the other direction, we show that any\nsymmetric KV solution gives rise to a morphism from the moperad of\nparenthesized braids with a frozen strand to the moperad of tangential\nautomorphisms of free Lie algebras. This morphism factors through the moperad\nof chord diagrams if and only if the associated KV associator is a Drinfeld\nassociator.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using the language of moperads-monoids in the category of right modules over\nan operad-we reinterpret the Alekseev-Enriquez-Torossian construction of\nKashiwara-Vergne (KV) solutions from associators. We show that any isomorphism\nbetween the moperad of parenthesized braids with a frozen strand and the\nmoperad of chord diagrams gives rise to a family of genus zero KV solutions\noperadically generated by a single classical KV solution. We show that the\nGrothendieck-Teichm\\\"uller module groups act on the latter, intertwining the\nactions of the KV symmetry groups. In the other direction, we show that any\nsymmetric KV solution gives rise to a morphism from the moperad of\nparenthesized braids with a frozen strand to the moperad of tangential\nautomorphisms of free Lie algebras. This morphism factors through the moperad\nof chord diagrams if and only if the associated KV associator is a Drinfeld\nassociator."
                },
                "authors": [
                    {
                        "name": "Zsuzsanna Dancso"
                    },
                    {
                        "name": "Iva Halacheva"
                    },
                    {
                        "name": "Guillaume Laplante-Anfossi"
                    },
                    {
                        "name": "Marcy Robertson"
                    },
                    {
                        "name": "Chandan Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandan Singh"
                },
                "author": "Chandan Singh",
                "arxiv_comment": "comments welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.AT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.AT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.QA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "18M60, 17B, 55",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16217v1",
                "updated": "2025-07-22T04:21:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    4,
                    21,
                    3,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T04:21:03Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    4,
                    21,
                    3,
                    1,
                    203,
                    0
                ],
                "title": "Towards Compute-Optimal Many-Shot In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Compute-Optimal Many-Shot In-Context Learning"
                },
                "summary": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL."
                },
                "authors": [
                    {
                        "name": "Shahriar Golchin"
                    },
                    {
                        "name": "Yanfei Chen"
                    },
                    {
                        "name": "Rujun Han"
                    },
                    {
                        "name": "Manan Gandhi"
                    },
                    {
                        "name": "Tianli Yu"
                    },
                    {
                        "name": "Swaroop Mishra"
                    },
                    {
                        "name": "Mihai Surdeanu"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    },
                    {
                        "name": "Chen-Yu Lee"
                    },
                    {
                        "name": "Tomas Pfister"
                    }
                ],
                "author_detail": {
                    "name": "Tomas Pfister"
                },
                "author": "Tomas Pfister",
                "arxiv_comment": "Final version; accepted at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10789v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10789v2",
                "updated": "2025-07-21T19:31:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    19,
                    31,
                    37,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-14T20:38:09Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    20,
                    38,
                    9,
                    0,
                    195,
                    0
                ],
                "title": "Dissecting the NVIDIA Blackwell Architecture with Microbenchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting the NVIDIA Blackwell Architecture with Microbenchmarks"
                },
                "summary": "The rapid development in scientific research provides a need for more compute\npower, which is partly being solved by GPUs. This paper presents a\nmicroarchitectural analysis of the modern NVIDIA Blackwell architecture by\nstudying GPU performance\n  features with thought through microbenchmarks. We unveil key subsystems,\nincluding the memory hierarchy, SM execution\n  pipelines, and the SM sub-core units, including the 5th generation tensor\ncores supporting FP4 and FP6 precisions.\n  To understand the different key features of the NVIDIA GPU, we study latency,\nthroughput, cache behavior, and scheduling\n  details, revealing subtle tuning metrics in the design of Blackwell. To\ndevelop a comprehensive analysis, we compare the\n  Blackwell architecture with the previous Hopper architecture by using the\nGeForce RTX 5080 and H100 PCIe, respectively. We\n  evaluate and compare results, presenting both generational improvements and\nperformance regressions. Additionally, we\n  investigate the role of power efficiency and energy consumption under varied\nworkloads. Our findings provide actionable insights\n  for application developers, compiler writers, and performance engineers to\noptimize workloads on Blackwell-based platforms,\n  and contribute new data to the growing research on GPU architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development in scientific research provides a need for more compute\npower, which is partly being solved by GPUs. This paper presents a\nmicroarchitectural analysis of the modern NVIDIA Blackwell architecture by\nstudying GPU performance\n  features with thought through microbenchmarks. We unveil key subsystems,\nincluding the memory hierarchy, SM execution\n  pipelines, and the SM sub-core units, including the 5th generation tensor\ncores supporting FP4 and FP6 precisions.\n  To understand the different key features of the NVIDIA GPU, we study latency,\nthroughput, cache behavior, and scheduling\n  details, revealing subtle tuning metrics in the design of Blackwell. To\ndevelop a comprehensive analysis, we compare the\n  Blackwell architecture with the previous Hopper architecture by using the\nGeForce RTX 5080 and H100 PCIe, respectively. We\n  evaluate and compare results, presenting both generational improvements and\nperformance regressions. Additionally, we\n  investigate the role of power efficiency and energy consumption under varied\nworkloads. Our findings provide actionable insights\n  for application developers, compiler writers, and performance engineers to\noptimize workloads on Blackwell-based platforms,\n  and contribute new data to the growing research on GPU architectures."
                },
                "authors": [
                    {
                        "name": "Aaron Jarmusch"
                    },
                    {
                        "name": "Nathan Graddon"
                    },
                    {
                        "name": "Sunita Chandrasekaran"
                    }
                ],
                "author_detail": {
                    "name": "Sunita Chandrasekaran"
                },
                "author": "Sunita Chandrasekaran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10789v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10789v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14003v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14003v2",
                "updated": "2025-07-21T19:05:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    19,
                    5,
                    1,
                    0,
                    202,
                    0
                ],
                "published": "2024-10-17T20:11:34Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    20,
                    11,
                    34,
                    3,
                    291,
                    0
                ],
                "title": "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time\n  Systems"
                },
                "summary": "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\nmemory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial for high performance. To support high MLP, shared last-level caches\n(LLCs) are divided into multiple banks, allowing parallel access. However,\nuneven distribution of cache requests from the cores, especially when requests\nfrom multiple cores are concentrated on a single bank, can result in\nsignificant contention affecting all cores that access the cache. Such cache\nbank contention can even be maliciously induced -- known as cache bank-aware\ndenial-of-service (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked shared LLC based multicore real-time systems. By regulating\nbandwidth on a per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache accesses to non-contended banks, thus improving overall\nperformance (throughput) without compromising isolation benefits of throttling.\nWe implement our approach on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively using both synthetic and real-world workloads.\nOur evaluation results show that the proposed per-bank regulation approach\neffectively protects real-time tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\times$ performance improvement for the\nthrottled benign best-effort tasks compared to prior bank-oblivious bandwidth\nthrottling approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\nmemory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial for high performance. To support high MLP, shared last-level caches\n(LLCs) are divided into multiple banks, allowing parallel access. However,\nuneven distribution of cache requests from the cores, especially when requests\nfrom multiple cores are concentrated on a single bank, can result in\nsignificant contention affecting all cores that access the cache. Such cache\nbank contention can even be maliciously induced -- known as cache bank-aware\ndenial-of-service (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked shared LLC based multicore real-time systems. By regulating\nbandwidth on a per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache accesses to non-contended banks, thus improving overall\nperformance (throughput) without compromising isolation benefits of throttling.\nWe implement our approach on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively using both synthetic and real-world workloads.\nOur evaluation results show that the proposed per-bank regulation approach\neffectively protects real-time tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\times$ performance improvement for the\nthrottled benign best-effort tasks compared to prior bank-oblivious bandwidth\nthrottling approaches."
                },
                "authors": [
                    {
                        "name": "Connor Sullivan"
                    },
                    {
                        "name": "Alex Manley"
                    },
                    {
                        "name": "Mohammad Alian"
                    },
                    {
                        "name": "Heechul Yun"
                    }
                ],
                "author_detail": {
                    "name": "Heechul Yun"
                },
                "author": "Heechul Yun",
                "arxiv_doi": "10.1109/RTSS62706.2024.00036",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/RTSS62706.2024.00036",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.14003v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14003v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Update to Fig. 11: The previous version used mismatched cache\n  capacities between the 2-bank and 4-bank configurations in the simulation\n  setup. This has been corrected to ensure both configurations have equal total\n  cache capacity. As a result, the specific numerical results in Fig. 11 have\n  changed. However, the overall trend shown in Fig. 11 and key findings of the\n  paper remain consistent",
                "arxiv_journal_ref": "IEEE Real-Time Systems Symposium (RTSS), 2024, pp. 336-348",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18974v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18974v3",
                "updated": "2025-07-21T14:50:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    14,
                    50,
                    41,
                    0,
                    202,
                    0
                ],
                "published": "2025-03-22T06:14:33Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    6,
                    14,
                    33,
                    5,
                    81,
                    0
                ],
                "title": "An Efficient Frequency-Based Approach for Maximal Square Detection in\n  Binary Matrices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Frequency-Based Approach for Maximal Square Detection in\n  Binary Matrices"
                },
                "summary": "Detecting maximal square submatrices of ones in binary matrices is a\nfundamental problem with applications in computer vision and pattern\nrecognition. While the standard dynamic programming (DP) solution achieves\noptimal asymptotic complexity, its practical performance suffers from repeated\nminimum operations and inefficient memory access patterns that degrade cache\nutilization. To address these limitations, we introduce a novel frequency-based\nalgorithm that employs a greedy approach to track the columnar continuity of\nones through an adaptive frequency array and a dynamic thresholding mechanism.\nExtensive benchmarking demonstrates that the frequency-based algorithm achieves\nfaster performance than the standard DP in 100% of test cases with an average\nspeedup of 3.32x, a maximum speedup of 4.60x, and a minimum speedup of 2.31x\nacross matrices up to 5000x5000 with densities from 0.1 to 0.9. The algorithm's\naverage speedup exceeds 2.5x for all densities and rises to over 3.5x for\ndensities of 0.7 and higher across all matrix sizes. These results demonstrate\nthat the frequency-based approach is a superior alternative to standard DP and\nopens new possibilities for efficient matrix analysis in performance-critical\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting maximal square submatrices of ones in binary matrices is a\nfundamental problem with applications in computer vision and pattern\nrecognition. While the standard dynamic programming (DP) solution achieves\noptimal asymptotic complexity, its practical performance suffers from repeated\nminimum operations and inefficient memory access patterns that degrade cache\nutilization. To address these limitations, we introduce a novel frequency-based\nalgorithm that employs a greedy approach to track the columnar continuity of\nones through an adaptive frequency array and a dynamic thresholding mechanism.\nExtensive benchmarking demonstrates that the frequency-based algorithm achieves\nfaster performance than the standard DP in 100% of test cases with an average\nspeedup of 3.32x, a maximum speedup of 4.60x, and a minimum speedup of 2.31x\nacross matrices up to 5000x5000 with densities from 0.1 to 0.9. The algorithm's\naverage speedup exceeds 2.5x for all densities and rises to over 3.5x for\ndensities of 0.7 and higher across all matrix sizes. These results demonstrate\nthat the frequency-based approach is a superior alternative to standard DP and\nopens new possibilities for efficient matrix analysis in performance-critical\napplications."
                },
                "authors": [
                    {
                        "name": "Swastik Bhandari"
                    }
                ],
                "author_detail": {
                    "name": "Swastik Bhandari"
                },
                "author": "Swastik Bhandari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18974v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18974v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10524v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10524v2",
                "updated": "2025-07-21T07:45:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    7,
                    45,
                    14,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-14T17:49:00Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    49,
                    0,
                    0,
                    195,
                    0
                ],
                "title": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation"
                },
                "summary": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to decrease prefill latency and memory\nfootprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms\na new Pareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to decrease prefill latency and memory\nfootprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms\na new Pareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost."
                },
                "authors": [
                    {
                        "name": "Sangmin Bae"
                    },
                    {
                        "name": "Yujin Kim"
                    },
                    {
                        "name": "Reza Bayat"
                    },
                    {
                        "name": "Sungnyun Kim"
                    },
                    {
                        "name": "Jiyoun Ha"
                    },
                    {
                        "name": "Tal Schuster"
                    },
                    {
                        "name": "Adam Fisch"
                    },
                    {
                        "name": "Hrayr Harutyunyan"
                    },
                    {
                        "name": "Ziwei Ji"
                    },
                    {
                        "name": "Aaron Courville"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "36 pages, 9 figures, 14 tables, codes at\n  https://github.com/raymin0223/mixture_of_recursions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10524v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10524v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09025v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09025v2",
                "updated": "2025-07-20T03:49:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    20,
                    3,
                    49,
                    3,
                    6,
                    201,
                    0
                ],
                "published": "2025-07-11T21:19:18Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    21,
                    19,
                    18,
                    4,
                    192,
                    0
                ],
                "title": "Lizard: An Efficient Linearization Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lizard: An Efficient Linearization Framework for Large Language Models"
                },
                "summary": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\narchitectures for infinite-context generation. Transformer-based LLMs face\nsignificant memory and computational bottlenecks as context lengths increase,\ndue to the quadratic complexity of softmax attention and the growing key-value\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\nattention mechanism that closely approximates softmax attention while\npreserving the output quality. Unlike previous linearization methods, which are\noften limited by fixed model structures and therefore exclude gating\nmechanisms, Lizard incorporates a gating module inspired by recent\nstate-of-the-art linear models. This enables adaptive memory control, supports\nconstant-memory inference, offers strong length generalization, and allows more\nflexible model design. Lizard combines gated linear attention for global\ncontext compression with sliding window attention enhanced by meta memory,\nforming a hybrid mechanism that captures both long-range dependencies and\nfine-grained local interactions. Moreover, we introduce a hardware-aware\nalgorithm that accelerates the training speed of our models. Extensive\nexperiments show that Lizard achieves near-lossless recovery of the teacher\nmodel's performance across standard language modeling tasks, while\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\nbenchmark, Lizard improves over prior models by 18 points and shows significant\nimprovements on associative recall tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\narchitectures for infinite-context generation. Transformer-based LLMs face\nsignificant memory and computational bottlenecks as context lengths increase,\ndue to the quadratic complexity of softmax attention and the growing key-value\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\nattention mechanism that closely approximates softmax attention while\npreserving the output quality. Unlike previous linearization methods, which are\noften limited by fixed model structures and therefore exclude gating\nmechanisms, Lizard incorporates a gating module inspired by recent\nstate-of-the-art linear models. This enables adaptive memory control, supports\nconstant-memory inference, offers strong length generalization, and allows more\nflexible model design. Lizard combines gated linear attention for global\ncontext compression with sliding window attention enhanced by meta memory,\nforming a hybrid mechanism that captures both long-range dependencies and\nfine-grained local interactions. Moreover, we introduce a hardware-aware\nalgorithm that accelerates the training speed of our models. Extensive\nexperiments show that Lizard achieves near-lossless recovery of the teacher\nmodel's performance across standard language modeling tasks, while\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\nbenchmark, Lizard improves over prior models by 18 points and shows significant\nimprovements on associative recall tasks."
                },
                "authors": [
                    {
                        "name": "Chien Van Nguyen"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Hanieh Deilamsalehy"
                    },
                    {
                        "name": "Puneet Mathur"
                    },
                    {
                        "name": "Viet Dac Lai"
                    },
                    {
                        "name": "Haoliang Wang"
                    },
                    {
                        "name": "Jayakumar Subramanian"
                    },
                    {
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "name": "Trung Bui"
                    },
                    {
                        "name": "Nikos Vlassis"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Thien Huu Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Thien Huu Nguyen"
                },
                "author": "Thien Huu Nguyen",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09025v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09025v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11092v2",
                "updated": "2025-07-19T17:46:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    19,
                    17,
                    46,
                    19,
                    5,
                    200,
                    0
                ],
                "published": "2025-06-05T19:47:22Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    19,
                    47,
                    22,
                    3,
                    156,
                    0
                ],
                "title": "Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing\n  Multi-Turn Planning and Tool Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing\n  Multi-Turn Planning and Tool Adaptation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has significantly advanced large\nlanguage models (LLMs) by grounding their outputs in external tools and\nknowledge sources. However, existing RAG systems are typically constrained to\nstatic, single-turn interactions with fixed toolsets, making them ill-suited\nfor dynamic domains such as healthcare and smart homes, where user intent,\navailable tools, and contextual factors evolve over time. We present Dynamic\nContext Tuning (DCT), a lightweight framework that extends RAG to support\nmulti-turn dialogue and evolving tool environments without requiring\nretraining. DCT integrates an attention-based context cache to track relevant\npast information, LoRA-based retrieval to dynamically select domain-specific\ntools, and efficient context compression to maintain inputs within LLM context\nlimits. Experiments on both synthetic and real-world benchmarks show that DCT\nimproves plan accuracy by 14% and reduces hallucinations by 37%, while matching\nGPT-4 performance at significantly lower cost. Furthermore, DCT generalizes to\npreviously unseen tools, enabling scalable and adaptable AI assistants across a\nwide range of dynamic environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has significantly advanced large\nlanguage models (LLMs) by grounding their outputs in external tools and\nknowledge sources. However, existing RAG systems are typically constrained to\nstatic, single-turn interactions with fixed toolsets, making them ill-suited\nfor dynamic domains such as healthcare and smart homes, where user intent,\navailable tools, and contextual factors evolve over time. We present Dynamic\nContext Tuning (DCT), a lightweight framework that extends RAG to support\nmulti-turn dialogue and evolving tool environments without requiring\nretraining. DCT integrates an attention-based context cache to track relevant\npast information, LoRA-based retrieval to dynamically select domain-specific\ntools, and efficient context compression to maintain inputs within LLM context\nlimits. Experiments on both synthetic and real-world benchmarks show that DCT\nimproves plan accuracy by 14% and reduces hallucinations by 37%, while matching\nGPT-4 performance at significantly lower cost. Furthermore, DCT generalizes to\npreviously unseen tools, enabling scalable and adaptable AI assistants across a\nwide range of dynamic environments."
                },
                "authors": [
                    {
                        "name": "Jubin Abhishek Soni"
                    },
                    {
                        "name": "Amit Anand"
                    },
                    {
                        "name": "Rajesh Kumar Pandey"
                    },
                    {
                        "name": "Aniket Abhishek Soni"
                    }
                ],
                "author_detail": {
                    "name": "Aniket Abhishek Soni"
                },
                "author": "Aniket Abhishek Soni",
                "arxiv_comment": "We are withdrawing the submission in order to thoroughly revise the\n  work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17772v1",
                "updated": "2025-07-19T17:02:15Z",
                "updated_parsed": [
                    2025,
                    7,
                    19,
                    17,
                    2,
                    15,
                    5,
                    200,
                    0
                ],
                "published": "2025-07-19T17:02:15Z",
                "published_parsed": [
                    2025,
                    7,
                    19,
                    17,
                    2,
                    15,
                    5,
                    200,
                    0
                ],
                "title": "Caching Techniques for Reducing the Communication Cost of Federated\n  Learning in IoT Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching Techniques for Reducing the Communication Cost of Federated\n  Learning in IoT Environments"
                },
                "summary": "Federated Learning (FL) allows multiple distributed devices to jointly train\na shared model without centralizing data, but communication cost remains a\nmajor bottleneck, especially in resource-constrained environments. This paper\nintroduces caching strategies - FIFO, LRU, and Priority-Based - to reduce\nunnecessary model update transmissions. By selectively forwarding significant\nupdates, our approach lowers bandwidth usage while maintaining model accuracy.\nExperiments on CIFAR-10 and medical datasets show reduced communication with\nminimal accuracy loss. Results confirm that intelligent caching improves\nscalability, memory efficiency, and supports reliable FL in edge IoT networks,\nmaking it practical for deployment in smart cities, healthcare, and other\nlatency-sensitive applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) allows multiple distributed devices to jointly train\na shared model without centralizing data, but communication cost remains a\nmajor bottleneck, especially in resource-constrained environments. This paper\nintroduces caching strategies - FIFO, LRU, and Priority-Based - to reduce\nunnecessary model update transmissions. By selectively forwarding significant\nupdates, our approach lowers bandwidth usage while maintaining model accuracy.\nExperiments on CIFAR-10 and medical datasets show reduced communication with\nminimal accuracy loss. Results confirm that intelligent caching improves\nscalability, memory efficiency, and supports reliable FL in edge IoT networks,\nmaking it practical for deployment in smart cities, healthcare, and other\nlatency-sensitive applications."
                },
                "authors": [
                    {
                        "name": "Ahmad Alhonainy"
                    },
                    {
                        "name": "Praveen Rao"
                    }
                ],
                "author_detail": {
                    "name": "Praveen Rao"
                },
                "arxiv_affiliation": "University of Missouri, USA",
                "author": "Praveen Rao",
                "arxiv_comment": "Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16002v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16002v3",
                "updated": "2025-07-19T07:41:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    19,
                    7,
                    41,
                    3,
                    5,
                    200,
                    0
                ],
                "published": "2025-02-21T23:34:29Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    23,
                    34,
                    29,
                    4,
                    52,
                    0
                ],
                "title": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse"
                },
                "summary": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we investigate a\nnew strategy to eliminate such inefficiency, where the KV cache of each\ndocument is precomputed independently. During inference, the KV caches of\nretrieved documents are concatenated, allowing the model to reuse cached\nrepresentations instead of recomputing them. To mitigate the performance\ndegradation when using KV caches computed independently for each document,\nKVLink introduces two key techniques: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, and using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments. Experiments across 7 datasets demonstrate that KVLink improves\nquestion answering accuracy by an average of 4% over state-of-the-art methods.\nFurthermore, by leveraging precomputed KV caches, our approach reduces\ntime-to-first-token by up to 96% compared to standard LLM inference, making it\na scalable and efficient solution for context reuse. Additionally, KVLink can\nbe combined with KV cache compression to further save cache loading and storage\noverhead while outperforming the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we investigate a\nnew strategy to eliminate such inefficiency, where the KV cache of each\ndocument is precomputed independently. During inference, the KV caches of\nretrieved documents are concatenated, allowing the model to reuse cached\nrepresentations instead of recomputing them. To mitigate the performance\ndegradation when using KV caches computed independently for each document,\nKVLink introduces two key techniques: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, and using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments. Experiments across 7 datasets demonstrate that KVLink improves\nquestion answering accuracy by an average of 4% over state-of-the-art methods.\nFurthermore, by leveraging precomputed KV caches, our approach reduces\ntime-to-first-token by up to 96% compared to standard LLM inference, making it\na scalable and efficient solution for context reuse. Additionally, KVLink can\nbe combined with KV cache compression to further save cache loading and storage\noverhead while outperforming the baselines."
                },
                "authors": [
                    {
                        "name": "Jingbo Yang"
                    },
                    {
                        "name": "Bairu Hou"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Yujia Bao"
                    },
                    {
                        "name": "Shiyu Chang"
                    }
                ],
                "author_detail": {
                    "name": "Shiyu Chang"
                },
                "author": "Shiyu Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16002v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16002v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.04703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04703v1",
                "updated": "2025-08-06T17:59:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    59,
                    50,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T17:59:50Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    59,
                    50,
                    2,
                    218,
                    0
                ],
                "title": "Stochastic Taylor expansion via Poisson point processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic Taylor expansion via Poisson point processes"
                },
                "summary": "We generalize Taylor's theorem by introducing a stochastic formulation based\non an underlying Poisson point process model. We utilize this approach to\npropose a novel non-linear regression framework and perform statistical\ninference of the model parameters. Theoretical properties of the proposed\nestimator are also proven, including its convergence, uniformly almost surely,\nto the true function. The theory is presented for the univariate and\nmultivariate cases, and we exemplify the proposed methodology using several\nexamples via simulations and an application to stock market data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We generalize Taylor's theorem by introducing a stochastic formulation based\non an underlying Poisson point process model. We utilize this approach to\npropose a novel non-linear regression framework and perform statistical\ninference of the model parameters. Theoretical properties of the proposed\nestimator are also proven, including its convergence, uniformly almost surely,\nto the true function. The theory is presented for the univariate and\nmultivariate cases, and we exemplify the proposed methodology using several\nexamples via simulations and an application to stock market data."
                },
                "authors": [
                    {
                        "name": "Weichao Wu"
                    },
                    {
                        "name": "Athanasios C. Micheas"
                    }
                ],
                "author_detail": {
                    "name": "Athanasios C. Micheas"
                },
                "author": "Athanasios C. Micheas",
                "arxiv_comment": "44 pages, 10 figures, 7 tables, Stochastic formulation of Taylor's\n  theorem, function approximation, non-linear regression model",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04698v1",
                "updated": "2025-08-06T17:58:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    58,
                    26,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T17:58:26Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    58,
                    26,
                    2,
                    218,
                    0
                ],
                "title": "FaST: Feature-aware Sampling and Tuning for Personalized Preference\n  Alignment with Limited Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FaST: Feature-aware Sampling and Tuning for Personalized Preference\n  Alignment with Limited Data"
                },
                "summary": "LLM-powered conversational assistants are often deployed in a\none-size-fits-all manner, which fails to accommodate individual user\npreferences. Recently, LLM personalization -- tailoring models to align with\nspecific user preferences -- has gained increasing attention as a way to bridge\nthis gap. In this work, we specifically focus on a practical yet challenging\nsetting where only a small set of preference annotations can be collected per\nuser -- a problem we define as Personalized Preference Alignment with Limited\nData (PPALLI). To support research in this area, we introduce two datasets --\nDnD and ELIP -- and benchmark a variety of alignment techniques on them. We\nfurther propose FaST, a highly parameter-efficient approach that leverages\nhigh-level features automatically discovered from the data, achieving the best\noverall performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-powered conversational assistants are often deployed in a\none-size-fits-all manner, which fails to accommodate individual user\npreferences. Recently, LLM personalization -- tailoring models to align with\nspecific user preferences -- has gained increasing attention as a way to bridge\nthis gap. In this work, we specifically focus on a practical yet challenging\nsetting where only a small set of preference annotations can be collected per\nuser -- a problem we define as Personalized Preference Alignment with Limited\nData (PPALLI). To support research in this area, we introduce two datasets --\nDnD and ELIP -- and benchmark a variety of alignment techniques on them. We\nfurther propose FaST, a highly parameter-efficient approach that leverages\nhigh-level features automatically discovered from the data, achieving the best\noverall performance."
                },
                "authors": [
                    {
                        "name": "Thibaut Thonet"
                    },
                    {
                        "name": "Germn Kruszewski"
                    },
                    {
                        "name": "Jos Rozen"
                    },
                    {
                        "name": "Pierre Erbacher"
                    },
                    {
                        "name": "Marc Dymetman"
                    }
                ],
                "author_detail": {
                    "name": "Marc Dymetman"
                },
                "author": "Marc Dymetman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.11270v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.11270v4",
                "updated": "2025-08-06T17:57:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    57,
                    46,
                    2,
                    218,
                    0
                ],
                "published": "2023-07-20T23:56:38Z",
                "published_parsed": [
                    2023,
                    7,
                    20,
                    23,
                    56,
                    38,
                    3,
                    201,
                    0
                ],
                "title": "The Role of r-Modes in Pulsar Spin-down, Pulsar Timing, and\n  Gravitational Waves",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Role of r-Modes in Pulsar Spin-down, Pulsar Timing, and\n  Gravitational Waves"
                },
                "summary": "We investigate the role of r-mode oscillations in pulsar spin-down and their\nimplications for gravitational wave emission and pulsar timing analysis. Using\na non-linear differential framework that includes r-mode contributions, we\nderive time-dependent solutions for rotational frequency and period evolution.\nThese expressions are validated using observational data from the Crab pulsar\nwith high precision. By analytically fitting braking indices and spin-down\ncoefficients, we link measurable pulsar properties to gravitational wave\nsignatures. Furthermore, we present closed-form expressions for neutron star\ncompactness and tidal deformability using Lambert W and Lambert-Tsallis\nfunctions, enabling model-independent inferences from r-mode gravitational wave\nfrequencies. Our results show that incorporating r-modes significantly improves\nthe accuracy of spin-down models and continuous wave detectability,\nparticularly through the inclusion of high-order frequency terms. This\nframework supports the modeling of timing residuals, glitch quantification, and\ngravitational wave constraints. Our findings have direct relevance for data\nanalysis in ongoing and future gravitational wave observatories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the role of r-mode oscillations in pulsar spin-down and their\nimplications for gravitational wave emission and pulsar timing analysis. Using\na non-linear differential framework that includes r-mode contributions, we\nderive time-dependent solutions for rotational frequency and period evolution.\nThese expressions are validated using observational data from the Crab pulsar\nwith high precision. By analytically fitting braking indices and spin-down\ncoefficients, we link measurable pulsar properties to gravitational wave\nsignatures. Furthermore, we present closed-form expressions for neutron star\ncompactness and tidal deformability using Lambert W and Lambert-Tsallis\nfunctions, enabling model-independent inferences from r-mode gravitational wave\nfrequencies. Our results show that incorporating r-modes significantly improves\nthe accuracy of spin-down models and continuous wave detectability,\nparticularly through the inclusion of high-order frequency terms. This\nframework supports the modeling of timing residuals, glitch quantification, and\ngravitational wave constraints. Our findings have direct relevance for data\nanalysis in ongoing and future gravitational wave observatories."
                },
                "authors": [
                    {
                        "name": "Xiyuan Li"
                    },
                    {
                        "name": "Shahram Abbassi"
                    },
                    {
                        "name": "Varenya Upadhyaya"
                    },
                    {
                        "name": "Xiyang Zhang"
                    },
                    {
                        "name": "S. R. Valluri"
                    }
                ],
                "author_detail": {
                    "name": "S. R. Valluri"
                },
                "author": "S. R. Valluri",
                "arxiv_comment": "Accepted for publication in the Journal of High Energy Astrophysics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.11270v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.11270v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00873v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00873v2",
                "updated": "2025-08-06T17:57:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    57,
                    1,
                    2,
                    218,
                    0
                ],
                "published": "2024-10-01T17:09:01Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    17,
                    9,
                    1,
                    1,
                    275,
                    0
                ],
                "title": "Aligning Human and LLM Judgments: Insights from EvalAssist on\n  Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Human and LLM Judgments: Insights from EvalAssist on\n  Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"
                },
                "summary": "Evaluation of large language model (LLM) outputs requires users to make\ncritical judgments about the best outputs across various configurations. This\nprocess is costly and takes time given the large amounts of data. LLMs are\nincreasingly used as evaluators to filter training data, evaluate model\nperformance or assist human evaluators with detailed assessments. To support\nthis process, effective front-end tools are critical for evaluation. Two common\napproaches for using LLMs as evaluators are direct assessment and pairwise\ncomparison. In our study with machine learning practitioners (n=15), each\ncompleting 6 tasks yielding 131 evaluations, we explore how task-related\nfactors and assessment strategies influence criteria refinement and user\nperceptions. Findings show that users performed more evaluations with direct\nassessment by making criteria task-specific, modifying judgments, and changing\nthe evaluator model. We conclude with recommendations for how systems can\nbetter support interactions in LLM-assisted evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation of large language model (LLM) outputs requires users to make\ncritical judgments about the best outputs across various configurations. This\nprocess is costly and takes time given the large amounts of data. LLMs are\nincreasingly used as evaluators to filter training data, evaluate model\nperformance or assist human evaluators with detailed assessments. To support\nthis process, effective front-end tools are critical for evaluation. Two common\napproaches for using LLMs as evaluators are direct assessment and pairwise\ncomparison. In our study with machine learning practitioners (n=15), each\ncompleting 6 tasks yielding 131 evaluations, we explore how task-related\nfactors and assessment strategies influence criteria refinement and user\nperceptions. Findings show that users performed more evaluations with direct\nassessment by making criteria task-specific, modifying judgments, and changing\nthe evaluator model. We conclude with recommendations for how systems can\nbetter support interactions in LLM-assisted evaluations."
                },
                "authors": [
                    {
                        "name": "Zahra Ashktorab"
                    },
                    {
                        "name": "Michael Desmond"
                    },
                    {
                        "name": "Qian Pan"
                    },
                    {
                        "name": "James M. Johnson"
                    },
                    {
                        "name": "Martin Santillan Cooper"
                    },
                    {
                        "name": "Elizabeth M. Daly"
                    },
                    {
                        "name": "Rahul Nair"
                    },
                    {
                        "name": "Tejaswini Pedapati"
                    },
                    {
                        "name": "Hyo Jin Do"
                    },
                    {
                        "name": "Werner Geyer"
                    }
                ],
                "author_detail": {
                    "name": "Werner Geyer"
                },
                "author": "Werner Geyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00873v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00873v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13415v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13415v2",
                "updated": "2025-08-06T17:45:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    45,
                    47,
                    2,
                    218,
                    0
                ],
                "published": "2024-11-20T16:02:14Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    16,
                    2,
                    14,
                    2,
                    325,
                    0
                ],
                "title": "Harnessing Large Language Models for Group POI Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Large Language Models for Group POI Recommendations"
                },
                "summary": "The rapid proliferation of Location-Based Social Networks (LBSNs) has\nunderscored the importance of Point-of-Interest (POI) recommendation systems in\nenhancing user experiences. While individual POI recommendation methods\nleverage users' check-in histories to provide personalized suggestions, they\nstruggle to address scenarios requiring group decision-making. Group POI\nrecommendation systems aim to satisfy the collective preferences of multiple\nusers, but existing approaches face two major challenges: diverse group\npreferences and extreme data sparsity in group check-in data. To overcome these\nchallenges, we propose LLMGPR, a novel framework that leverages large language\nmodels (LLMs) for group POI recommendations. LLMGPR introduces\nsemantic-enhanced POI tokens and incorporates rich contextual information to\nmodel the diverse and complex dynamics of group decision-making. To further\nenhance its capabilities, we developed a sequencing adapter using Quantized\nLow-Rank Adaptation (QLoRA), which aligns LLMs with group POI recommendation\ntasks. To address the issue of sparse group check-in data, LLMGPR employs an\naggregation adapter that integrates individual representations into meaningful\ngroup representations. Additionally, a self-supervised learning (SSL) task is\ndesigned to predict the purposes of check-in sequences (e.g., business trips\nand family vacations), thereby enriching group representations with deeper\nsemantic insights. Extensive experiments demonstrate the effectiveness of\nLLMGPR, showcasing its ability to significantly enhance the accuracy and\nrobustness of group POI recommendations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid proliferation of Location-Based Social Networks (LBSNs) has\nunderscored the importance of Point-of-Interest (POI) recommendation systems in\nenhancing user experiences. While individual POI recommendation methods\nleverage users' check-in histories to provide personalized suggestions, they\nstruggle to address scenarios requiring group decision-making. Group POI\nrecommendation systems aim to satisfy the collective preferences of multiple\nusers, but existing approaches face two major challenges: diverse group\npreferences and extreme data sparsity in group check-in data. To overcome these\nchallenges, we propose LLMGPR, a novel framework that leverages large language\nmodels (LLMs) for group POI recommendations. LLMGPR introduces\nsemantic-enhanced POI tokens and incorporates rich contextual information to\nmodel the diverse and complex dynamics of group decision-making. To further\nenhance its capabilities, we developed a sequencing adapter using Quantized\nLow-Rank Adaptation (QLoRA), which aligns LLMs with group POI recommendation\ntasks. To address the issue of sparse group check-in data, LLMGPR employs an\naggregation adapter that integrates individual representations into meaningful\ngroup representations. Additionally, a self-supervised learning (SSL) task is\ndesigned to predict the purposes of check-in sequences (e.g., business trips\nand family vacations), thereby enriching group representations with deeper\nsemantic insights. Extensive experiments demonstrate the effectiveness of\nLLMGPR, showcasing its ability to significantly enhance the accuracy and\nrobustness of group POI recommendations."
                },
                "authors": [
                    {
                        "name": "Jing Long"
                    },
                    {
                        "name": "Liang Qu"
                    },
                    {
                        "name": "Junliang Yu"
                    },
                    {
                        "name": "Tong Chen"
                    },
                    {
                        "name": "Quoc Viet Hung Nguyen"
                    },
                    {
                        "name": "Hongzhi Yin"
                    }
                ],
                "author_detail": {
                    "name": "Hongzhi Yin"
                },
                "author": "Hongzhi Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13415v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13415v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04679v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04679v1",
                "updated": "2025-08-06T17:45:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    45,
                    11,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T17:45:11Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    45,
                    11,
                    2,
                    218,
                    0
                ],
                "title": "MisVisFix: An Interactive Dashboard for Detecting, Explaining, and\n  Correcting Misleading Visualizations using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MisVisFix: An Interactive Dashboard for Detecting, Explaining, and\n  Correcting Misleading Visualizations using Large Language Models"
                },
                "summary": "Misleading visualizations pose a significant challenge to accurate data\ninterpretation. While recent research has explored the use of Large Language\nModels (LLMs) for detecting such misinformation, practical tools that also\nsupport explanation and correction remain limited. We present MisVisFix, an\ninteractive dashboard that leverages both Claude and GPT models to support the\nfull workflow of detecting, explaining, and correcting misleading\nvisualizations. MisVisFix correctly identifies 96% of visualization issues and\naddresses all 74 known visualization misinformation types, classifying them as\nmajor, minor, or potential concerns. It provides detailed explanations,\nactionable suggestions, and automatically generates corrected charts. An\ninteractive chat interface allows users to ask about specific chart elements or\nrequest modifications. The dashboard adapts to newly emerging misinformation\nstrategies through targeted user interactions. User studies with visualization\nexperts and developers of fact-checking tools show that MisVisFix accurately\nidentifies issues and offers useful suggestions for improvement. By\ntransforming LLM-based detection into an accessible, interactive platform,\nMisVisFix advances visualization literacy and supports more trustworthy data\ncommunication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Misleading visualizations pose a significant challenge to accurate data\ninterpretation. While recent research has explored the use of Large Language\nModels (LLMs) for detecting such misinformation, practical tools that also\nsupport explanation and correction remain limited. We present MisVisFix, an\ninteractive dashboard that leverages both Claude and GPT models to support the\nfull workflow of detecting, explaining, and correcting misleading\nvisualizations. MisVisFix correctly identifies 96% of visualization issues and\naddresses all 74 known visualization misinformation types, classifying them as\nmajor, minor, or potential concerns. It provides detailed explanations,\nactionable suggestions, and automatically generates corrected charts. An\ninteractive chat interface allows users to ask about specific chart elements or\nrequest modifications. The dashboard adapts to newly emerging misinformation\nstrategies through targeted user interactions. User studies with visualization\nexperts and developers of fact-checking tools show that MisVisFix accurately\nidentifies issues and offers useful suggestions for improvement. By\ntransforming LLM-based detection into an accessible, interactive platform,\nMisVisFix advances visualization literacy and supports more trustworthy data\ncommunication."
                },
                "authors": [
                    {
                        "name": "Amit Kumar Das"
                    },
                    {
                        "name": "Klaus Mueller"
                    }
                ],
                "author_detail": {
                    "name": "Klaus Mueller"
                },
                "author": "Klaus Mueller",
                "arxiv_comment": "11 pages, 6 figures. Accepted at IEEE VIS: Visualization & Visual\n  Analytics 2025 conference, November 2-7, 2025, Vienna, Austria",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04679v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04676v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04676v1",
                "updated": "2025-08-06T17:42:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    42,
                    22,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T17:42:22Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    42,
                    22,
                    2,
                    218,
                    0
                ],
                "title": "GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via\n  General Samples Replay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via\n  General Samples Replay"
                },
                "summary": "The continual learning capability of large language models (LLMs) is crucial\nfor advancing artificial general intelligence. However, continual fine-tuning\nLLMs across various domains often suffers from catastrophic forgetting,\ncharacterized by: 1) significant forgetting of their general capabilities, and\n2) sharp performance declines in previously learned tasks. To simultaneously\naddress both issues in a simple yet stable manner, we propose General Sample\nReplay (GeRe), a framework that use usual pretraining texts for efficient\nanti-forgetting. Beyond revisiting the most prevalent replay-based practices\nunder GeRe, we further leverage neural states to introduce a enhanced\nactivation states constrained optimization method using threshold-based margin\n(TM) loss, which maintains activation state consistency during replay learning.\nWe are the first to validate that a small, fixed set of pre-collected general\nreplay samples is sufficient to resolve both concerns--retaining general\ncapabilities while promoting overall performance across sequential tasks.\nIndeed, the former can inherently facilitate the latter. Through controlled\nexperiments, we systematically compare TM with different replay strategies\nunder the GeRe framework, including vanilla label fitting, logit imitation via\nKL divergence and feature imitation via L1/L2 losses. Results demonstrate that\nTM consistently improves performance and exhibits better robustness. Our work\npaves the way for efficient replay of LLMs for the future. Our code and data\nare available at https://github.com/Qznan/GeRe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The continual learning capability of large language models (LLMs) is crucial\nfor advancing artificial general intelligence. However, continual fine-tuning\nLLMs across various domains often suffers from catastrophic forgetting,\ncharacterized by: 1) significant forgetting of their general capabilities, and\n2) sharp performance declines in previously learned tasks. To simultaneously\naddress both issues in a simple yet stable manner, we propose General Sample\nReplay (GeRe), a framework that use usual pretraining texts for efficient\nanti-forgetting. Beyond revisiting the most prevalent replay-based practices\nunder GeRe, we further leverage neural states to introduce a enhanced\nactivation states constrained optimization method using threshold-based margin\n(TM) loss, which maintains activation state consistency during replay learning.\nWe are the first to validate that a small, fixed set of pre-collected general\nreplay samples is sufficient to resolve both concerns--retaining general\ncapabilities while promoting overall performance across sequential tasks.\nIndeed, the former can inherently facilitate the latter. Through controlled\nexperiments, we systematically compare TM with different replay strategies\nunder the GeRe framework, including vanilla label fitting, logit imitation via\nKL divergence and feature imitation via L1/L2 losses. Results demonstrate that\nTM consistently improves performance and exhibits better robustness. Our work\npaves the way for efficient replay of LLMs for the future. Our code and data\nare available at https://github.com/Qznan/GeRe."
                },
                "authors": [
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Shuoran Jiang"
                    },
                    {
                        "name": "Mengchen Zhao"
                    },
                    {
                        "name": "Yuefeng Li"
                    },
                    {
                        "name": "Yang Fan"
                    },
                    {
                        "name": "Xiangping Wu"
                    },
                    {
                        "name": "Qingcai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Qingcai Chen"
                },
                "author": "Qingcai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04676v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04676v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06362v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06362v2",
                "updated": "2025-08-06T17:41:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    41,
                    48,
                    2,
                    218,
                    0
                ],
                "published": "2025-03-09T00:02:10Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    0,
                    2,
                    10,
                    6,
                    68,
                    0
                ],
                "title": "Adaptive Audio-Visual Speech Recognition via Matryoshka-Based Multimodal\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Audio-Visual Speech Recognition via Matryoshka-Based Multimodal\n  LLMs"
                },
                "summary": "Audio-Visual Speech Recognition (AVSR) leverages audio and visual modalities\nto improve robustness in noisy environments. Recent advances in Large Language\nModels (LLMs) show strong performance in speech recognition, including AVSR.\nHowever, the long speech representations lead to high computational costs for\nLLMs. Prior methods compress inputs before feeding them to LLMs, but high\ncompression often harms accuracy. To address this, we propose Llama-MTSK, the\nfirst Matryoshka-based Multimodal LLM for AVSR, which flexibly adapts\naudio-visual token allocation under varying compute constraints. Inspired by\nMatryoshka Representation Learning, our model encodes representations at\nmultiple granularities with a single architecture, avoiding the need for\nseparate models. For efficient fine-tuning, we introduce three LoRA-based\nstrategies using global and scale-specific modules. Evaluations on major AVSR\ndatasets show Llama-MTSK matches or outperforms models trained at fixed\ncompression levels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio-Visual Speech Recognition (AVSR) leverages audio and visual modalities\nto improve robustness in noisy environments. Recent advances in Large Language\nModels (LLMs) show strong performance in speech recognition, including AVSR.\nHowever, the long speech representations lead to high computational costs for\nLLMs. Prior methods compress inputs before feeding them to LLMs, but high\ncompression often harms accuracy. To address this, we propose Llama-MTSK, the\nfirst Matryoshka-based Multimodal LLM for AVSR, which flexibly adapts\naudio-visual token allocation under varying compute constraints. Inspired by\nMatryoshka Representation Learning, our model encodes representations at\nmultiple granularities with a single architecture, avoiding the need for\nseparate models. For efficient fine-tuning, we introduce three LoRA-based\nstrategies using global and scale-specific modules. Evaluations on major AVSR\ndatasets show Llama-MTSK matches or outperforms models trained at fixed\ncompression levels."
                },
                "authors": [
                    {
                        "name": "Umberto Cappellazzo"
                    },
                    {
                        "name": "Minsu Kim"
                    },
                    {
                        "name": "Stavros Petridis"
                    }
                ],
                "author_detail": {
                    "name": "Stavros Petridis"
                },
                "author": "Stavros Petridis",
                "arxiv_comment": "Accepted to IEEE ASRU 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06362v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06362v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04675v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04675v1",
                "updated": "2025-08-06T17:41:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    41,
                    44,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T17:41:44Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    41,
                    44,
                    2,
                    218,
                    0
                ],
                "title": "Water Detection in the Interstellar Object 3I/ATLAS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Water Detection in the Interstellar Object 3I/ATLAS"
                },
                "summary": "We report the first detection of water activity in the third confirmed\ninterstellar object, 3I/ATLAS, based on ultraviolet imaging with the \\emph{Neil\nGehrels-Swift Observatory}. Observations acquired with the Ultraviolet/Optical\nTelescope on 2025 July 31st - Aug 1st revealed OH (A$^2\\Sigma$ -- X$^2\\Pi$)\nemission near 3085~\\AA. The water production rate results highly depend on the\nreddening assumption. For a reddening of 38.6\\% between 5437.8~\\AA\\ and\n3325.7~\\AA, the water production rate is $(1.35 \\pm 0.27) \\times 10^{27} $\nmolecules\\,s$^{-1}$ (40~kg\\,s$^{-1}$) at a heliocentric distance of 3.51~au.\nThis places 3I/ATLAS among the few comets with confirmed OH emission beyond\n3~au, where water ice sublimation is typically inefficient. The inferred\nproduction rate is consistent with an active area of at least 19~km$^2$,\nassuming equilibrium sublimation. Based on current upper limits of the nucleus'\nradius, this requires that over 20\\% of the surface is active, which is larger\nthan activity levels observed in most solar system comets. Contemporaneous\nnear-infrared spectroscopy indicates the presence of large icy grains in the\ncoma, which may serve as an extended source of water vapor. The detection of OH\nemission prior to any CN detection is unusual and may reflect differences in\ngrain-driven outgassing or volatile inventory compared to typical comets. While\nsimilar behavior has been observed in solar system comets, the mechanisms\ncontrolling distant activity and the storage and release of volatiles remain\npoorly understood. If 3I/ATLAS' coma continues to be dominated by H$_2$O,\nsupporting the early and low-metallicity formation hypothesis, the derived\nlarge size of the nucleus could be indicative of a key knowledge gap in\nlow-metallicity system planetesimal formation and loss mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report the first detection of water activity in the third confirmed\ninterstellar object, 3I/ATLAS, based on ultraviolet imaging with the \\emph{Neil\nGehrels-Swift Observatory}. Observations acquired with the Ultraviolet/Optical\nTelescope on 2025 July 31st - Aug 1st revealed OH (A$^2\\Sigma$ -- X$^2\\Pi$)\nemission near 3085~\\AA. The water production rate results highly depend on the\nreddening assumption. For a reddening of 38.6\\% between 5437.8~\\AA\\ and\n3325.7~\\AA, the water production rate is $(1.35 \\pm 0.27) \\times 10^{27} $\nmolecules\\,s$^{-1}$ (40~kg\\,s$^{-1}$) at a heliocentric distance of 3.51~au.\nThis places 3I/ATLAS among the few comets with confirmed OH emission beyond\n3~au, where water ice sublimation is typically inefficient. The inferred\nproduction rate is consistent with an active area of at least 19~km$^2$,\nassuming equilibrium sublimation. Based on current upper limits of the nucleus'\nradius, this requires that over 20\\% of the surface is active, which is larger\nthan activity levels observed in most solar system comets. Contemporaneous\nnear-infrared spectroscopy indicates the presence of large icy grains in the\ncoma, which may serve as an extended source of water vapor. The detection of OH\nemission prior to any CN detection is unusual and may reflect differences in\ngrain-driven outgassing or volatile inventory compared to typical comets. While\nsimilar behavior has been observed in solar system comets, the mechanisms\ncontrolling distant activity and the storage and release of volatiles remain\npoorly understood. If 3I/ATLAS' coma continues to be dominated by H$_2$O,\nsupporting the early and low-metallicity formation hypothesis, the derived\nlarge size of the nucleus could be indicative of a key knowledge gap in\nlow-metallicity system planetesimal formation and loss mechanisms."
                },
                "authors": [
                    {
                        "name": "Zexi Xing"
                    },
                    {
                        "name": "Shawn Oset"
                    },
                    {
                        "name": "John Noonan"
                    },
                    {
                        "name": "Dennis Bodewits"
                    }
                ],
                "author_detail": {
                    "name": "Dennis Bodewits"
                },
                "author": "Dennis Bodewits",
                "arxiv_comment": "Submitted to ApJL, 9 pages, 2 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04675v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04675v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04664v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04664v1",
                "updated": "2025-08-06T17:32:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    32,
                    58,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T17:32:58Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    32,
                    58,
                    2,
                    218,
                    0
                ],
                "title": "Sculptor: Empowering LLMs with Cognitive Agency via Active Context\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sculptor: Empowering LLMs with Cognitive Agency via Active Context\n  Management"
                },
                "summary": "Large Language Models (LLMs) suffer from significant performance degradation\nwhen processing long contexts due to proactive interference, where irrelevant\ninformation in earlier parts of the context disrupts reasoning and memory\nrecall. While most research focuses on external memory systems to augment LLMs'\ncapabilities, we propose a complementary approach: empowering LLMs with Active\nContext Management (ACM) tools to actively sculpt their internal working\nmemory. We introduce Sculptor, a framework that equips LLMs with three\ncategories of tools: (1) context fragmentation, (2) summary, hide, and restore,\nand (3) intelligent search. Our approach enables LLMs to proactively manage\ntheir attention and working memory, analogous to how humans selectively focus\non relevant information while filtering out distractions. Experimental\nevaluation on information-sparse benchmarks-PI-LLM (proactive interference) and\nNeedleBench Multi-Needle Reasoning-demonstrates that Sculptor significantly\nimproves performance even without specific training, leveraging LLMs' inherent\ntool calling generalization capabilities. By enabling Active Context\nManagement, Sculptor not only mitigates proactive interference but also\nprovides a cognitive foundation for more reliable reasoning across diverse\nlong-context tasks-highlighting that explicit context-control strategies,\nrather than merely larger token windows, are key to robustness at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) suffer from significant performance degradation\nwhen processing long contexts due to proactive interference, where irrelevant\ninformation in earlier parts of the context disrupts reasoning and memory\nrecall. While most research focuses on external memory systems to augment LLMs'\ncapabilities, we propose a complementary approach: empowering LLMs with Active\nContext Management (ACM) tools to actively sculpt their internal working\nmemory. We introduce Sculptor, a framework that equips LLMs with three\ncategories of tools: (1) context fragmentation, (2) summary, hide, and restore,\nand (3) intelligent search. Our approach enables LLMs to proactively manage\ntheir attention and working memory, analogous to how humans selectively focus\non relevant information while filtering out distractions. Experimental\nevaluation on information-sparse benchmarks-PI-LLM (proactive interference) and\nNeedleBench Multi-Needle Reasoning-demonstrates that Sculptor significantly\nimproves performance even without specific training, leveraging LLMs' inherent\ntool calling generalization capabilities. By enabling Active Context\nManagement, Sculptor not only mitigates proactive interference but also\nprovides a cognitive foundation for more reliable reasoning across diverse\nlong-context tasks-highlighting that explicit context-control strategies,\nrather than merely larger token windows, are key to robustness at scale."
                },
                "authors": [
                    {
                        "name": "Mo Li"
                    },
                    {
                        "name": "L. H. Xu"
                    },
                    {
                        "name": "Qitai Tan"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Yunxin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yunxin Liu"
                },
                "author": "Yunxin Liu",
                "arxiv_comment": "Preprint. Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04664v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04664v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04663v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04663v1",
                "updated": "2025-08-06T17:30:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    30,
                    44,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T17:30:44Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    30,
                    44,
                    2,
                    218,
                    0
                ],
                "title": "HierarchicalPrune: Position-Aware Compression for Large-Scale Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HierarchicalPrune: Position-Aware Compression for Large-Scale Diffusion\n  Models"
                },
                "summary": "State-of-the-art text-to-image diffusion models (DMs) achieve remarkable\nquality, yet their massive parameter scale (8-11B) poses significant challenges\nfor inferences on resource-constrained devices. In this paper, we present\nHierarchicalPrune, a novel compression framework grounded in a key observation:\nDM blocks exhibit distinct functional hierarchies, where early blocks establish\nsemantic structures while later blocks handle texture refinements.\nHierarchicalPrune synergistically combines three techniques: (1) Hierarchical\nPosition Pruning, which identifies and removes less essential later blocks\nbased on position hierarchy; (2) Positional Weight Preservation, which\nsystematically protects early model portions that are essential for semantic\nstructural integrity; and (3) Sensitivity-Guided Distillation, which adjusts\nknowledge-transfer intensity based on our discovery of block-wise sensitivity\nvariations. As a result, our framework brings billion-scale diffusion models\ninto a range more suitable for on-device inference, while preserving the\nquality of the output images. Specifically, when combined with INT4 weight\nquantisation, HierarchicalPrune achieves 77.5-80.4% memory footprint reduction\n(e.g., from 15.8 GB to 3.2 GB) and 27.9-38.0% latency reduction, measured on\nserver and consumer grade GPUs, with the minimum drop of 2.6% in GenEval score\nand 7% in HPSv2 score compared to the original model. Last but not least, our\ncomprehensive user study with 85 participants demonstrates that\nHierarchicalPrune maintains perceptual quality comparable to the original model\nwhile significantly outperforming prior works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art text-to-image diffusion models (DMs) achieve remarkable\nquality, yet their massive parameter scale (8-11B) poses significant challenges\nfor inferences on resource-constrained devices. In this paper, we present\nHierarchicalPrune, a novel compression framework grounded in a key observation:\nDM blocks exhibit distinct functional hierarchies, where early blocks establish\nsemantic structures while later blocks handle texture refinements.\nHierarchicalPrune synergistically combines three techniques: (1) Hierarchical\nPosition Pruning, which identifies and removes less essential later blocks\nbased on position hierarchy; (2) Positional Weight Preservation, which\nsystematically protects early model portions that are essential for semantic\nstructural integrity; and (3) Sensitivity-Guided Distillation, which adjusts\nknowledge-transfer intensity based on our discovery of block-wise sensitivity\nvariations. As a result, our framework brings billion-scale diffusion models\ninto a range more suitable for on-device inference, while preserving the\nquality of the output images. Specifically, when combined with INT4 weight\nquantisation, HierarchicalPrune achieves 77.5-80.4% memory footprint reduction\n(e.g., from 15.8 GB to 3.2 GB) and 27.9-38.0% latency reduction, measured on\nserver and consumer grade GPUs, with the minimum drop of 2.6% in GenEval score\nand 7% in HPSv2 score compared to the original model. Last but not least, our\ncomprehensive user study with 85 participants demonstrates that\nHierarchicalPrune maintains perceptual quality comparable to the original model\nwhile significantly outperforming prior works."
                },
                "authors": [
                    {
                        "name": "Young D. Kwon"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Sijia Li"
                    },
                    {
                        "name": "Da Li"
                    },
                    {
                        "name": "Sourav Bhattacharya"
                    },
                    {
                        "name": "Stylianos I. Venieris"
                    }
                ],
                "author_detail": {
                    "name": "Stylianos I. Venieris"
                },
                "author": "Stylianos I. Venieris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04663v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04663v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02520v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02520v4",
                "updated": "2025-08-07T05:19:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    5,
                    19,
                    42,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-04T15:30:57Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    30,
                    57,
                    0,
                    216,
                    0
                ],
                "title": "xDeepServe: Model-as-a-Service on Huawei CloudMatrix384",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xDeepServe: Model-as-a-Service on Huawei CloudMatrix384"
                },
                "summary": "The rise of scaled-out LLMs and scaled-up SuperPods signals a new era in\nlarge-scale AI infrastructure. LLMs continue to scale out via MoE, as seen in\nrecent models like DeepSeek, Kimi, and Qwen. In parallel, AI hardware is\nscaling up, with Huawei's CloudMatrix384 SuperPod offering hundreds of GB/s\nhigh-speed interconnects. Running large MoE models on SuperPod-scale hardware\nbrings new challenges. It requires new execution models, scalable scheduling,\nefficient expert load balancing, and elimination of single points of failure.\nThis paper presents xDeepServe, Huawei Cloud's LLM serving system designed for\nSuperPod-scale infrastructure. At its core is Transformerless, a disaggregated\narchitecture that decomposes transformer models into modular units--attention,\nfeedforward, and MoE--executed independently on NPUs connected via high-speed\nfabric. We implement this design in two forms: disaggregated prefill-decode and\ndisaggregated MoE-attention. This fully disaggregated setup enables independent\nscaling of compute and memory without sacrificing performance. To support this\narchitecture, we propose XCCL, a communication library that leverages\nCloudMatrix384's global shared memory to implement efficient point-to-point and\nall-to-all primitives. We also extend our serving engine FlowServe with\nsystem-level techniques, enabling scalable inference across hundreds of NPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of scaled-out LLMs and scaled-up SuperPods signals a new era in\nlarge-scale AI infrastructure. LLMs continue to scale out via MoE, as seen in\nrecent models like DeepSeek, Kimi, and Qwen. In parallel, AI hardware is\nscaling up, with Huawei's CloudMatrix384 SuperPod offering hundreds of GB/s\nhigh-speed interconnects. Running large MoE models on SuperPod-scale hardware\nbrings new challenges. It requires new execution models, scalable scheduling,\nefficient expert load balancing, and elimination of single points of failure.\nThis paper presents xDeepServe, Huawei Cloud's LLM serving system designed for\nSuperPod-scale infrastructure. At its core is Transformerless, a disaggregated\narchitecture that decomposes transformer models into modular units--attention,\nfeedforward, and MoE--executed independently on NPUs connected via high-speed\nfabric. We implement this design in two forms: disaggregated prefill-decode and\ndisaggregated MoE-attention. This fully disaggregated setup enables independent\nscaling of compute and memory without sacrificing performance. To support this\narchitecture, we propose XCCL, a communication library that leverages\nCloudMatrix384's global shared memory to implement efficient point-to-point and\nall-to-all primitives. We also extend our serving engine FlowServe with\nsystem-level techniques, enabling scalable inference across hundreds of NPUs."
                },
                "authors": [
                    {
                        "name": "Ao Xiao"
                    },
                    {
                        "name": "Bangzheng He"
                    },
                    {
                        "name": "Baoquan Zhang"
                    },
                    {
                        "name": "Baoxing Huai"
                    },
                    {
                        "name": "Bingji Wang"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Bo Xu"
                    },
                    {
                        "name": "Boyi Hou"
                    },
                    {
                        "name": "Chan Yang"
                    },
                    {
                        "name": "Changhong Liu"
                    },
                    {
                        "name": "Cheng Cui"
                    },
                    {
                        "name": "Chenyu Zhu"
                    },
                    {
                        "name": "Cong Feng"
                    },
                    {
                        "name": "Daohui Wang"
                    },
                    {
                        "name": "Dayun Lin"
                    },
                    {
                        "name": "Duo Zhao"
                    },
                    {
                        "name": "Fengshao Zou"
                    },
                    {
                        "name": "Fu Wang"
                    },
                    {
                        "name": "Gangqiang Zhang"
                    },
                    {
                        "name": "Gengyuan Dan"
                    },
                    {
                        "name": "Guanjie Chen"
                    },
                    {
                        "name": "Guodong Guan"
                    },
                    {
                        "name": "Guodong Yang"
                    },
                    {
                        "name": "Haifeng Li"
                    },
                    {
                        "name": "Haipei Zhu"
                    },
                    {
                        "name": "Hao Feng"
                    },
                    {
                        "name": "Hao Huang"
                    },
                    {
                        "name": "Hao Xu"
                    },
                    {
                        "name": "Hengrui Ma"
                    },
                    {
                        "name": "Hengtao Fan"
                    },
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Jiang Liu"
                    },
                    {
                        "name": "Jiang Xu"
                    },
                    {
                        "name": "Jie Meng"
                    },
                    {
                        "name": "Jinhan Xin"
                    },
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Juwei Chen"
                    },
                    {
                        "name": "Lan Yu"
                    },
                    {
                        "name": "Lanxin Miao"
                    },
                    {
                        "name": "Liang Liu"
                    },
                    {
                        "name": "Linan Jing"
                    },
                    {
                        "name": "Lu Zhou"
                    },
                    {
                        "name": "Meina Han"
                    },
                    {
                        "name": "Mingkun Deng"
                    },
                    {
                        "name": "Mingyu Deng"
                    },
                    {
                        "name": "Naitian Deng"
                    },
                    {
                        "name": "Nizhong Lin"
                    },
                    {
                        "name": "Peihan Zhao"
                    },
                    {
                        "name": "Peng Pan"
                    },
                    {
                        "name": "Pengfei Shen"
                    },
                    {
                        "name": "Ping Li"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Qin Zhang"
                    },
                    {
                        "name": "Qingrong Xia"
                    },
                    {
                        "name": "Qingyi Zhang"
                    },
                    {
                        "name": "Qunchao Fu"
                    },
                    {
                        "name": "Ren Guo"
                    },
                    {
                        "name": "Ruimin Gao"
                    },
                    {
                        "name": "Shaochun Li"
                    },
                    {
                        "name": "Sheng Long"
                    },
                    {
                        "name": "Shentian Li"
                    },
                    {
                        "name": "Shining Wan"
                    },
                    {
                        "name": "Shuai Shen"
                    },
                    {
                        "name": "Shuangfu Zeng"
                    },
                    {
                        "name": "Shuming Jing"
                    },
                    {
                        "name": "Siqi Yang"
                    },
                    {
                        "name": "Song Zhang"
                    },
                    {
                        "name": "Tao Xu"
                    },
                    {
                        "name": "Tianlin Du"
                    },
                    {
                        "name": "Ting Chen"
                    },
                    {
                        "name": "Wanxu Wu"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Weinan Tong"
                    },
                    {
                        "name": "Weiwei Chen"
                    },
                    {
                        "name": "Wen Peng"
                    },
                    {
                        "name": "Wenli Zhou"
                    },
                    {
                        "name": "Wenquan Yang"
                    },
                    {
                        "name": "Wenxin Liang"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Xiaoli Zhou"
                    },
                    {
                        "name": "Xin Jin"
                    },
                    {
                        "name": "Xinyu Duan"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yalong Shan"
                    },
                    {
                        "name": "Yang Gan"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Yi Deng"
                    },
                    {
                        "name": "Yi Zheng"
                    },
                    {
                        "name": "Yingfei Zheng"
                    },
                    {
                        "name": "Yiyun Zheng"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Yong Gao"
                    },
                    {
                        "name": "Yongqiang Yang"
                    },
                    {
                        "name": "Yuanjin Gong"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Yuetao Chen"
                    },
                    {
                        "name": "Yukun Zhu"
                    },
                    {
                        "name": "Yulong He"
                    },
                    {
                        "name": "Yusu Zhao"
                    },
                    {
                        "name": "Yuyan Wu"
                    },
                    {
                        "name": "Zenan Zhang"
                    },
                    {
                        "name": "Zhaojin Zhuo"
                    },
                    {
                        "name": "Zhaoyang Ji"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Zhenhua Yang"
                    },
                    {
                        "name": "Zhenli Sheng"
                    },
                    {
                        "name": "Zhibin Yu"
                    },
                    {
                        "name": "Zhigang Ji"
                    },
                    {
                        "name": "Zhihao Ren"
                    },
                    {
                        "name": "Zhipeng Bian"
                    },
                    {
                        "name": "Zhixia Liu"
                    },
                    {
                        "name": "Zhiyu Dong"
                    },
                    {
                        "name": "Zhonghua Li"
                    },
                    {
                        "name": "Zhou Yu"
                    },
                    {
                        "name": "Zhuoming Shen"
                    },
                    {
                        "name": "Zhuwei Peng"
                    },
                    {
                        "name": "Zi Ye"
                    },
                    {
                        "name": "Zihao Xiang"
                    },
                    {
                        "name": "Zimin Fu"
                    },
                    {
                        "name": "Zixuan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zixuan Zhang"
                },
                "author": "Zixuan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02520v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02520v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04655v1",
                "updated": "2025-08-06T17:19:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    19,
                    10,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T17:19:10Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    19,
                    10,
                    2,
                    218,
                    0
                ],
                "title": "X-SAM: From Segment Anything to Any Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-SAM: From Segment Anything to Any Segmentation"
                },
                "summary": "Large Language Models (LLMs) demonstrate strong capabilities in broad\nknowledge representation, yet they are inherently deficient in pixel-level\nperceptual understanding. Although the Segment Anything Model (SAM) represents\na significant advancement in visual-prompt-driven image segmentation, it\nexhibits notable limitations in multi-mask prediction and category-specific\nsegmentation tasks, and it cannot integrate all segmentation tasks within a\nunified model architecture. To address these limitations, we present X-SAM, a\nstreamlined Multimodal Large Language Model (MLLM) framework that extends the\nsegmentation paradigm from \\textit{segment anything} to \\textit{any\nsegmentation}. Specifically, we introduce a novel unified framework that\nenables more advanced pixel-level perceptual comprehension for MLLMs.\nFurthermore, we propose a new segmentation task, termed Visual GrounDed (VGD)\nsegmentation, which segments all instance objects with interactive visual\nprompts and empowers MLLMs with visual grounded, pixel-wise interpretative\ncapabilities. To enable effective training on diverse data sources, we present\na unified training strategy that supports co-training across multiple datasets.\nExperimental results demonstrate that X-SAM achieves state-of-the-art\nperformance on a wide range of image segmentation benchmarks, highlighting its\nefficiency for multimodal, pixel-level visual understanding. Code is available\nat https://github.com/wanghao9610/X-SAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate strong capabilities in broad\nknowledge representation, yet they are inherently deficient in pixel-level\nperceptual understanding. Although the Segment Anything Model (SAM) represents\na significant advancement in visual-prompt-driven image segmentation, it\nexhibits notable limitations in multi-mask prediction and category-specific\nsegmentation tasks, and it cannot integrate all segmentation tasks within a\nunified model architecture. To address these limitations, we present X-SAM, a\nstreamlined Multimodal Large Language Model (MLLM) framework that extends the\nsegmentation paradigm from \\textit{segment anything} to \\textit{any\nsegmentation}. Specifically, we introduce a novel unified framework that\nenables more advanced pixel-level perceptual comprehension for MLLMs.\nFurthermore, we propose a new segmentation task, termed Visual GrounDed (VGD)\nsegmentation, which segments all instance objects with interactive visual\nprompts and empowers MLLMs with visual grounded, pixel-wise interpretative\ncapabilities. To enable effective training on diverse data sources, we present\na unified training strategy that supports co-training across multiple datasets.\nExperimental results demonstrate that X-SAM achieves state-of-the-art\nperformance on a wide range of image segmentation benchmarks, highlighting its\nefficiency for multimodal, pixel-level visual understanding. Code is available\nat https://github.com/wanghao9610/X-SAM."
                },
                "authors": [
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Limeng Qiao"
                    },
                    {
                        "name": "Zequn Jie"
                    },
                    {
                        "name": "Zhijian Huang"
                    },
                    {
                        "name": "Chengjian Feng"
                    },
                    {
                        "name": "Qingfang Zheng"
                    },
                    {
                        "name": "Lin Ma"
                    },
                    {
                        "name": "Xiangyuan Lan"
                    },
                    {
                        "name": "Xiaodan Liang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodan Liang"
                },
                "author": "Xiaodan Liang",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04652v1",
                "updated": "2025-08-06T17:18:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    18,
                    25,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T17:18:25Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    18,
                    25,
                    2,
                    218,
                    0
                ],
                "title": "LLM Collaboration With Multi-Agent Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Collaboration With Multi-Agent Reinforcement Learning"
                },
                "summary": "A large amount of work has been done in Multi-Agent Systems (MAS) for\nmodeling and solving problems with multiple interacting agents. However, most\nLLMs are pretrained independently and not specifically optimized for\ncoordination. Existing LLM fine-tuning frameworks rely on individual rewards,\nwhich require complex reward designs for each agent to encourage collaboration.\nTo address these challenges, we model LLM collaboration as a cooperative\nMulti-Agent Reinforcement Learning (MARL) problem. We develop a multi-agent,\nmulti-turn algorithm, Multi-Agent Group Relative Policy Optimization (MAGRPO),\nto solve it, building on current RL approaches for LLMs as well as MARL\ntechniques. Our experiments on LLM writing and coding collaboration demonstrate\nthat fine-tuning MAS with MAGRPO enables agents to generate high-quality\nresponses efficiently through effective cooperation. Our approach opens the\ndoor to using other MARL methods for LLMs and highlights the associated\nchallenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A large amount of work has been done in Multi-Agent Systems (MAS) for\nmodeling and solving problems with multiple interacting agents. However, most\nLLMs are pretrained independently and not specifically optimized for\ncoordination. Existing LLM fine-tuning frameworks rely on individual rewards,\nwhich require complex reward designs for each agent to encourage collaboration.\nTo address these challenges, we model LLM collaboration as a cooperative\nMulti-Agent Reinforcement Learning (MARL) problem. We develop a multi-agent,\nmulti-turn algorithm, Multi-Agent Group Relative Policy Optimization (MAGRPO),\nto solve it, building on current RL approaches for LLMs as well as MARL\ntechniques. Our experiments on LLM writing and coding collaboration demonstrate\nthat fine-tuning MAS with MAGRPO enables agents to generate high-quality\nresponses efficiently through effective cooperation. Our approach opens the\ndoor to using other MARL methods for LLMs and highlights the associated\nchallenges."
                },
                "authors": [
                    {
                        "name": "Shuo Liu"
                    },
                    {
                        "name": "Zeyu Liang"
                    },
                    {
                        "name": "Xueguang Lyu"
                    },
                    {
                        "name": "Christopher Amato"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Amato"
                },
                "author": "Christopher Amato",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04647v1",
                "updated": "2025-08-06T17:15:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    15,
                    3,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T17:15:03Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    15,
                    3,
                    2,
                    218,
                    0
                ],
                "title": "Stochastic Calculus for Pathwise Observables of Markov-Jump Processes:\n  Unification of Diffusion and Jump Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic Calculus for Pathwise Observables of Markov-Jump Processes:\n  Unification of Diffusion and Jump Dynamics"
                },
                "summary": "Path-wise observables--functionals of stochastic trajectories--are at the\nheart of time-average statistical mechanics and are central to thermodynamic\ninequalities such as uncertainty relations, speed limits, and\ncorrelation-bounds. They provide a means of thermodynamic inference in the\ntypical situation, when not all dissipative degrees of freedom in a system are\nexperimentally accessible. So far, theories focusing on path-wise observables\nhave been developing in two major directions, diffusion processes and\nMarkov-jump dynamics, in a virtually disjoint manner. Moreover, even the\nrespective results for diffusion and jump dynamics were derived with a\npatchwork of different approaches that are predominantly indirect. Stochastic\ncalculus was recently shown to provide a direct approach to path-wise\nobservables of diffusion processes, while a corresponding framework for jump\ndynamics remained elusive. In our work we develop, in an exact parallelism with\ncontinuous-space diffusion, a complete stochastic calculus for path-wise\nobservables of Markov-jump processes. We formulate a \"Langevin equation\" for\njump processes, define general path-wise observables, and establish their\ncovariation structure, whereby we fully account for transients and\ntime-inhomogeneous dynamics. We prove the known kinds of thermodynamic\ninequalities in their most general form and discus saturation conditions. We\ndetermine the response of path-wise observables to general (incl. thermal)\nperturbations and carry out the continuum limit to achieve the complete\nunification of diffusion and jump dynamics. Our results open new avenues in the\ndirection of discrete-state analogs of generative diffusion models and the\nlearning of stochastic thermodynamics from fluctuating trajectories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Path-wise observables--functionals of stochastic trajectories--are at the\nheart of time-average statistical mechanics and are central to thermodynamic\ninequalities such as uncertainty relations, speed limits, and\ncorrelation-bounds. They provide a means of thermodynamic inference in the\ntypical situation, when not all dissipative degrees of freedom in a system are\nexperimentally accessible. So far, theories focusing on path-wise observables\nhave been developing in two major directions, diffusion processes and\nMarkov-jump dynamics, in a virtually disjoint manner. Moreover, even the\nrespective results for diffusion and jump dynamics were derived with a\npatchwork of different approaches that are predominantly indirect. Stochastic\ncalculus was recently shown to provide a direct approach to path-wise\nobservables of diffusion processes, while a corresponding framework for jump\ndynamics remained elusive. In our work we develop, in an exact parallelism with\ncontinuous-space diffusion, a complete stochastic calculus for path-wise\nobservables of Markov-jump processes. We formulate a \"Langevin equation\" for\njump processes, define general path-wise observables, and establish their\ncovariation structure, whereby we fully account for transients and\ntime-inhomogeneous dynamics. We prove the known kinds of thermodynamic\ninequalities in their most general form and discus saturation conditions. We\ndetermine the response of path-wise observables to general (incl. thermal)\nperturbations and carry out the continuum limit to achieve the complete\nunification of diffusion and jump dynamics. Our results open new avenues in the\ndirection of discrete-state analogs of generative diffusion models and the\nlearning of stochastic thermodynamics from fluctuating trajectories."
                },
                "authors": [
                    {
                        "name": "Lars Torbjrn Stutzer"
                    },
                    {
                        "name": "Cai Dieball"
                    },
                    {
                        "name": "Alja Godec"
                    }
                ],
                "author_detail": {
                    "name": "Alja Godec"
                },
                "author": "Alja Godec",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04634v1",
                "updated": "2025-08-06T17:02:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    2,
                    1,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T17:02:01Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    2,
                    1,
                    2,
                    218,
                    0
                ],
                "title": "VirtLab: An AI-Powered System for Flexible, Customizable, and\n  Large-scale Team Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VirtLab: An AI-Powered System for Flexible, Customizable, and\n  Large-scale Team Simulations"
                },
                "summary": "Simulating how team members collaborate within complex environments using\nAgentic AI is a promising approach to explore hypotheses grounded in social\nscience theories and study team behaviors. We introduce VirtLab, a\nuser-friendly, customizable, multi-agent, and scalable team simulation system\nthat enables testing teams with LLM-based agents in spatial and temporal\nsettings. This system addresses the current frameworks' design and technical\nlimitations that do not consider flexible simulation scenarios and spatial\nsettings. VirtLab contains a simulation engine and a web interface that enables\nboth technical and non-technical users to formulate, run, and analyze team\nsimulations without programming. We demonstrate the system's utility by\ncomparing ground truth data with simulated scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating how team members collaborate within complex environments using\nAgentic AI is a promising approach to explore hypotheses grounded in social\nscience theories and study team behaviors. We introduce VirtLab, a\nuser-friendly, customizable, multi-agent, and scalable team simulation system\nthat enables testing teams with LLM-based agents in spatial and temporal\nsettings. This system addresses the current frameworks' design and technical\nlimitations that do not consider flexible simulation scenarios and spatial\nsettings. VirtLab contains a simulation engine and a web interface that enables\nboth technical and non-technical users to formulate, run, and analyze team\nsimulations without programming. We demonstrate the system's utility by\ncomparing ground truth data with simulated scenarios."
                },
                "authors": [
                    {
                        "name": "Mohammed Almutairi"
                    },
                    {
                        "name": "Charles Chiang"
                    },
                    {
                        "name": "Haoze Guo"
                    },
                    {
                        "name": "Matthew Belcher"
                    },
                    {
                        "name": "Nandini Banerjee"
                    },
                    {
                        "name": "Maria Milkowski"
                    },
                    {
                        "name": "Svitlana Volkova"
                    },
                    {
                        "name": "Daniel Nguyen"
                    },
                    {
                        "name": "Tim Weninger"
                    },
                    {
                        "name": "Michael Yankoski"
                    },
                    {
                        "name": "Trenton W. Ford"
                    },
                    {
                        "name": "Diego Gomez-Zara"
                    }
                ],
                "author_detail": {
                    "name": "Diego Gomez-Zara"
                },
                "author": "Diego Gomez-Zara",
                "arxiv_doi": "10.1145/3746058.3758994",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746058.3758994",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.04634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "5 pages, 2 figures, UIST 2025",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04632v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04632v2",
                "updated": "2025-08-07T11:30:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    11,
                    30,
                    20,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-06T17:00:54Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    0,
                    54,
                    2,
                    218,
                    0
                ],
                "title": "IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with\n  Verifiable Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with\n  Verifiable Rewards"
                },
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) improves instruction\nfollowing capabilities of large language models (LLMs), but suffers from\ntraining inefficiency due to inadequate difficulty assessment. Moreover, RLVR\nis prone to over-optimization, where LLMs exploit verification shortcuts\nwithout aligning to the actual intent of user instructions. We introduce\nInstruction Following Decorator (IFDecorator}, a framework that wraps RLVR\ntraining into a robust and sample-efficient pipeline. It consists of three\ncomponents: (1) a cooperative-adversarial data flywheel that co-evolves\ninstructions and hybrid verifications, generating progressively more\nchallenging instruction-verification pairs; (2) IntentCheck, a bypass module\nenforcing intent alignment; and (3) trip wires, a diagnostic mechanism that\ndetects reward hacking via trap instructions, which trigger and capture\nshortcut exploitation behaviors. Our Qwen2.5-32B-Instruct-IFDecorator achieves\n87.43% accuracy on IFEval, outperforming larger proprietary models such as\nGPT-4o. Additionally, we demonstrate substantial improvements on FollowBench\nwhile preserving general capabilities. Our trip wires show significant\nreductions in reward hacking rates. We will release models, code, and data for\nfuture research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR) improves instruction\nfollowing capabilities of large language models (LLMs), but suffers from\ntraining inefficiency due to inadequate difficulty assessment. Moreover, RLVR\nis prone to over-optimization, where LLMs exploit verification shortcuts\nwithout aligning to the actual intent of user instructions. We introduce\nInstruction Following Decorator (IFDecorator}, a framework that wraps RLVR\ntraining into a robust and sample-efficient pipeline. It consists of three\ncomponents: (1) a cooperative-adversarial data flywheel that co-evolves\ninstructions and hybrid verifications, generating progressively more\nchallenging instruction-verification pairs; (2) IntentCheck, a bypass module\nenforcing intent alignment; and (3) trip wires, a diagnostic mechanism that\ndetects reward hacking via trap instructions, which trigger and capture\nshortcut exploitation behaviors. Our Qwen2.5-32B-Instruct-IFDecorator achieves\n87.43% accuracy on IFEval, outperforming larger proprietary models such as\nGPT-4o. Additionally, we demonstrate substantial improvements on FollowBench\nwhile preserving general capabilities. Our trip wires show significant\nreductions in reward hacking rates. We will release models, code, and data for\nfuture research."
                },
                "authors": [
                    {
                        "name": "Xu Guo"
                    },
                    {
                        "name": "Tianyi Liang"
                    },
                    {
                        "name": "Tong Jian"
                    },
                    {
                        "name": "Xiaogui Yang"
                    },
                    {
                        "name": "Ling-I Wu"
                    },
                    {
                        "name": "Chenhui Li"
                    },
                    {
                        "name": "Zhihui Lu"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "arxiv_comment": "7 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04632v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04632v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04449v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04449v2",
                "updated": "2025-08-06T16:57:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    57,
                    39,
                    2,
                    218,
                    0
                ],
                "published": "2024-12-05T18:58:03Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    58,
                    3,
                    3,
                    340,
                    0
                ],
                "title": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay"
                },
                "summary": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. In this paper, we propose p-MoD, an efficient MLLM\narchitecture that significantly reduces training and inference costs while\nmaintaining model performance. The majority of computation in MLLMs stems from\nthe overwhelming volume of vision tokens processed by the transformer-based\nLLM. Accordingly, we leverage the Mixture-of-Depths (MoD) mechanism, where each\nLLM layer selects essential vision tokens to process while skipping redundant\nones. However, integrating MoD into MLLMs is non-trivial. To address the\nchallenges of training and inference stability as well as limited training\ndata, we adapt the MoD module with two novel designs: tanh-gated weight\nnormalization (TanhNorm) and symmetric token reweighting (STRing). Moreover, we\nobserve that vision tokens exhibit higher redundancy in deeper layers and thus\ndesign a progressive ratio decay (PRD) strategy, which gradually reduces the\ntoken retention ratio layer by layer, employing a shifted cosine schedule. This\ncrucial design fully unleashes the potential of MoD, significantly boosting the\nefficiency and performance of our models. Extensive experiments on two baseline\nmodels across 15 benchmarks show that our model matches or even surpasses the\nperformance of corresponding baselines, while requiring only 55.6% TFLOPs and\n53.7% KV cache storage during inference, and 77.7% GPU hours during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. In this paper, we propose p-MoD, an efficient MLLM\narchitecture that significantly reduces training and inference costs while\nmaintaining model performance. The majority of computation in MLLMs stems from\nthe overwhelming volume of vision tokens processed by the transformer-based\nLLM. Accordingly, we leverage the Mixture-of-Depths (MoD) mechanism, where each\nLLM layer selects essential vision tokens to process while skipping redundant\nones. However, integrating MoD into MLLMs is non-trivial. To address the\nchallenges of training and inference stability as well as limited training\ndata, we adapt the MoD module with two novel designs: tanh-gated weight\nnormalization (TanhNorm) and symmetric token reweighting (STRing). Moreover, we\nobserve that vision tokens exhibit higher redundancy in deeper layers and thus\ndesign a progressive ratio decay (PRD) strategy, which gradually reduces the\ntoken retention ratio layer by layer, employing a shifted cosine schedule. This\ncrucial design fully unleashes the potential of MoD, significantly boosting the\nefficiency and performance of our models. Extensive experiments on two baseline\nmodels across 15 benchmarks show that our model matches or even surpasses the\nperformance of corresponding baselines, while requiring only 55.6% TFLOPs and\n53.7% KV cache storage during inference, and 77.7% GPU hours during training."
                },
                "authors": [
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Desen Meng"
                    },
                    {
                        "name": "Zhengming Zhang"
                    },
                    {
                        "name": "Zhenpeng Huang"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Limin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Limin Wang"
                },
                "author": "Limin Wang",
                "arxiv_comment": "Accepted by ICCV 2025; Code released at\n  https://github.com/MCG-NJU/p-MoD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04449v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04449v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04626v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04626v1",
                "updated": "2025-08-06T16:51:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    51,
                    38,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T16:51:38Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    51,
                    38,
                    2,
                    218,
                    0
                ],
                "title": "P-Aligner: Enabling Pre-Alignment of Language Models via Principled\n  Instruction Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "P-Aligner: Enabling Pre-Alignment of Language Models via Principled\n  Instruction Synthesis"
                },
                "summary": "Large Language Models (LLMs) are expected to produce safe, helpful, and\nhonest content during interaction with human users, but they frequently fail to\nalign with such values when given flawed instructions, e.g., missing context,\nambiguous directives, or inappropriate tone, leaving substantial room for\nimprovement along multiple dimensions. A cost-effective yet high-impact way is\nto pre-align instructions before the model begins decoding. Existing approaches\neither rely on prohibitive test-time search costs or end-to-end model rewrite,\nwhich is powered by a customized training corpus with unclear objectives. In\nthis work, we demonstrate that the goal of efficient and effective preference\nalignment can be achieved by P-Aligner, a lightweight module generating\ninstructions that preserve the original intents while being expressed in a more\nhuman-preferred form. P-Aligner is trained on UltraPrompt, a new dataset\nsynthesized via a proposed principle-guided pipeline using Monte-Carlo Tree\nSearch, which systematically explores the space of candidate instructions that\nare closely tied to human preference. Experiments across different methods show\nthat P-Aligner generally outperforms strong baselines across various models and\nbenchmarks, including average win-rate gains of 28.35% and 8.69% on GPT-4-turbo\nand Gemma-2-SimPO, respectively. Further analyses validate its effectiveness\nand efficiency through multiple perspectives, including data quality, search\nstrategies, iterative deployment, and time overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are expected to produce safe, helpful, and\nhonest content during interaction with human users, but they frequently fail to\nalign with such values when given flawed instructions, e.g., missing context,\nambiguous directives, or inappropriate tone, leaving substantial room for\nimprovement along multiple dimensions. A cost-effective yet high-impact way is\nto pre-align instructions before the model begins decoding. Existing approaches\neither rely on prohibitive test-time search costs or end-to-end model rewrite,\nwhich is powered by a customized training corpus with unclear objectives. In\nthis work, we demonstrate that the goal of efficient and effective preference\nalignment can be achieved by P-Aligner, a lightweight module generating\ninstructions that preserve the original intents while being expressed in a more\nhuman-preferred form. P-Aligner is trained on UltraPrompt, a new dataset\nsynthesized via a proposed principle-guided pipeline using Monte-Carlo Tree\nSearch, which systematically explores the space of candidate instructions that\nare closely tied to human preference. Experiments across different methods show\nthat P-Aligner generally outperforms strong baselines across various models and\nbenchmarks, including average win-rate gains of 28.35% and 8.69% on GPT-4-turbo\nand Gemma-2-SimPO, respectively. Further analyses validate its effectiveness\nand efficiency through multiple perspectives, including data quality, search\nstrategies, iterative deployment, and time overhead."
                },
                "authors": [
                    {
                        "name": "Feifan Song"
                    },
                    {
                        "name": "Bofei Gao"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Weimin Xiong"
                    },
                    {
                        "name": "Yuyang Song"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Houfeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Houfeng Wang"
                },
                "author": "Houfeng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04626v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00222v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00222v3",
                "updated": "2025-08-06T16:36:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    36,
                    42,
                    2,
                    218,
                    0
                ],
                "published": "2025-07-31T23:55:29Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    23,
                    55,
                    29,
                    3,
                    212,
                    0
                ],
                "title": "RL-PLUS: Countering Capability Boundary Collapse of LLMs in\n  Reinforcement Learning with Hybrid-policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RL-PLUS: Countering Capability Boundary Collapse of LLMs in\n  Reinforcement Learning with Hybrid-policy Optimization"
                },
                "summary": "Reinforcement Learning with Verifiable Reward (RLVR) has significantly\nadvanced the complex reasoning abilities of Large Language Models (LLMs).\nHowever, it struggles to break through the inherent capability boundaries of\nthe base LLM, due to its essentially on-policy strategy coupled with LLM's\nimmense action space and sparse reward. Critically, RLVR can lead to the\ncapability boundary collapse, narrowing the LLM's problem-solving scope. To\naddress this problem, we propose RL-PLUS, a novel hybrid-policy optimization\napproach for LLMs that synergizes internal exploitation with external data to\nachieve stronger reasoning capabilities and surpass the boundaries of base\nmodels. RL-PLUS integrates two core components, i.e., Multiple Importance\nSampling to address distributional mismatch from external data, and\nExploration-Based Advantage Function to guide the model towards high-value,\nunexplored reasoning paths. We provide both theoretical analysis and extensive\nexperiments to demonstrate the superiority and generalizability of our\napproach. Compared with existing RLVR methods, RL-PLUS achieves 1)\nstate-of-the-art performance on six math reasoning benchmarks; 2) superior\nperformance on six out-of-distribution reasoning tasks; 3) consistent and\nsignificant gains across diverse model families, with average relative\nimprovements up to 69.2\\%. Moreover, the analysis of Pass@k curves indicates\nthat RL-PLUS effectively resolves the capability boundary collapse problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Reward (RLVR) has significantly\nadvanced the complex reasoning abilities of Large Language Models (LLMs).\nHowever, it struggles to break through the inherent capability boundaries of\nthe base LLM, due to its essentially on-policy strategy coupled with LLM's\nimmense action space and sparse reward. Critically, RLVR can lead to the\ncapability boundary collapse, narrowing the LLM's problem-solving scope. To\naddress this problem, we propose RL-PLUS, a novel hybrid-policy optimization\napproach for LLMs that synergizes internal exploitation with external data to\nachieve stronger reasoning capabilities and surpass the boundaries of base\nmodels. RL-PLUS integrates two core components, i.e., Multiple Importance\nSampling to address distributional mismatch from external data, and\nExploration-Based Advantage Function to guide the model towards high-value,\nunexplored reasoning paths. We provide both theoretical analysis and extensive\nexperiments to demonstrate the superiority and generalizability of our\napproach. Compared with existing RLVR methods, RL-PLUS achieves 1)\nstate-of-the-art performance on six math reasoning benchmarks; 2) superior\nperformance on six out-of-distribution reasoning tasks; 3) consistent and\nsignificant gains across diverse model families, with average relative\nimprovements up to 69.2\\%. Moreover, the analysis of Pass@k curves indicates\nthat RL-PLUS effectively resolves the capability boundary collapse problem."
                },
                "authors": [
                    {
                        "name": "Yihong Dong"
                    },
                    {
                        "name": "Xue Jiang"
                    },
                    {
                        "name": "Yongding Tao"
                    },
                    {
                        "name": "Huanyu Liu"
                    },
                    {
                        "name": "Kechi Zhang"
                    },
                    {
                        "name": "Lili Mou"
                    },
                    {
                        "name": "Rongyu Cao"
                    },
                    {
                        "name": "Yingwei Ma"
                    },
                    {
                        "name": "Jue Chen"
                    },
                    {
                        "name": "Binhua Li"
                    },
                    {
                        "name": "Zhi Jin"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Yongbin Li"
                    },
                    {
                        "name": "Ge Li"
                    }
                ],
                "author_detail": {
                    "name": "Ge Li"
                },
                "author": "Ge Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00222v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00222v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04604v1",
                "updated": "2025-08-06T16:24:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    24,
                    17,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T16:24:17Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    24,
                    17,
                    2,
                    218,
                    0
                ],
                "title": "TURA: Tool-Augmented Unified Retrieval Agent for AI Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TURA: Tool-Augmented Unified Retrieval Agent for AI Search"
                },
                "summary": "The advent of Large Language Models (LLMs) is transforming search engines\ninto conversational AI search products, primarily using Retrieval-Augmented\nGeneration (RAG) on web corpora. However, this paradigm has significant\nindustrial limitations. Traditional RAG approaches struggle with real-time\nneeds and structured queries that require accessing dynamically generated\ncontent like ticket availability or inventory. Limited to indexing static\npages, search engines cannot perform the interactive queries needed for such\ntime-sensitive data. Academic research has focused on optimizing RAG for static\ncontent, overlooking complex intents and the need for dynamic sources like\ndatabases and real-time APIs. To bridge this gap, we introduce TURA\n(Tool-Augmented Unified Retrieval Agent for AI Search), a novel three-stage\nframework that combines RAG with agentic tool-use to access both static content\nand dynamic, real-time information. TURA has three key components: an\nIntent-Aware Retrieval module to decompose queries and retrieve information\nsources encapsulated as Model Context Protocol (MCP) Servers, a DAG-based Task\nPlanner that models task dependencies as a Directed Acyclic Graph (DAG) for\noptimal parallel execution, and a lightweight Distilled Agent Executor for\nefficient tool calling. TURA is the first architecture to systematically bridge\nthe gap between static RAG and dynamic information sources for a world-class AI\nsearch product. Serving tens of millions of users, it leverages an agentic\nframework to deliver robust, real-time answers while meeting the low-latency\ndemands of a large-scale industrial system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) is transforming search engines\ninto conversational AI search products, primarily using Retrieval-Augmented\nGeneration (RAG) on web corpora. However, this paradigm has significant\nindustrial limitations. Traditional RAG approaches struggle with real-time\nneeds and structured queries that require accessing dynamically generated\ncontent like ticket availability or inventory. Limited to indexing static\npages, search engines cannot perform the interactive queries needed for such\ntime-sensitive data. Academic research has focused on optimizing RAG for static\ncontent, overlooking complex intents and the need for dynamic sources like\ndatabases and real-time APIs. To bridge this gap, we introduce TURA\n(Tool-Augmented Unified Retrieval Agent for AI Search), a novel three-stage\nframework that combines RAG with agentic tool-use to access both static content\nand dynamic, real-time information. TURA has three key components: an\nIntent-Aware Retrieval module to decompose queries and retrieve information\nsources encapsulated as Model Context Protocol (MCP) Servers, a DAG-based Task\nPlanner that models task dependencies as a Directed Acyclic Graph (DAG) for\noptimal parallel execution, and a lightweight Distilled Agent Executor for\nefficient tool calling. TURA is the first architecture to systematically bridge\nthe gap between static RAG and dynamic information sources for a world-class AI\nsearch product. Serving tens of millions of users, it leverages an agentic\nframework to deliver robust, real-time answers while meeting the low-latency\ndemands of a large-scale industrial system."
                },
                "authors": [
                    {
                        "name": "Zhejun Zhao"
                    },
                    {
                        "name": "Yuehu Dong"
                    },
                    {
                        "name": "Alley Liu"
                    },
                    {
                        "name": "Lixue Zheng"
                    },
                    {
                        "name": "Pingsheng Liu"
                    },
                    {
                        "name": "Dongdong Shen"
                    },
                    {
                        "name": "Long Xia"
                    },
                    {
                        "name": "Jiashu Zhao"
                    },
                    {
                        "name": "Dawei Yin"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Yin"
                },
                "author": "Dawei Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15877v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15877v3",
                "updated": "2025-08-07T01:02:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    1,
                    2,
                    42,
                    3,
                    219,
                    0
                ],
                "published": "2024-05-24T18:40:20Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    18,
                    40,
                    20,
                    4,
                    145,
                    0
                ],
                "title": "Basis Selection: Low-Rank Decomposition of Pretrained Large Language\n  Models for Target Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Basis Selection: Low-Rank Decomposition of Pretrained Large Language\n  Models for Target Applications"
                },
                "summary": "Large language models (LLMs) significantly enhance the performance of various\napplications, but they are computationally intensive and energy-demanding. This\nmakes it challenging to deploy them on devices with limited resources, such as\npersonal computers and mobile/wearable devices, and results in substantial\ninference costs in resource-rich environments like cloud servers. To extend the\nuse of LLMs, we introduce a low-rank decomposition approach to effectively\ncompress these models, tailored to the requirements of specific applications.\nWe observe that LLMs pretrained on general datasets contain many redundant\ncomponents not needed for particular applications. Our method focuses on\nidentifying and removing these redundant parts, retaining only the necessary\nelements for the target applications. Specifically, we represent the weight\nmatrices of LLMs as a linear combination of base components. We then prune the\nirrelevant bases and enhance the model with new bases beneficial for specific\napplications. Deep compression results on the Llama 2-7b and -13B models,\nconducted on target applications including mathematical reasoning and code\ngeneration, show that our method significantly reduces model size while\nmaintaining comparable accuracy to state-of-the-art low-rank compression\ntechniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) significantly enhance the performance of various\napplications, but they are computationally intensive and energy-demanding. This\nmakes it challenging to deploy them on devices with limited resources, such as\npersonal computers and mobile/wearable devices, and results in substantial\ninference costs in resource-rich environments like cloud servers. To extend the\nuse of LLMs, we introduce a low-rank decomposition approach to effectively\ncompress these models, tailored to the requirements of specific applications.\nWe observe that LLMs pretrained on general datasets contain many redundant\ncomponents not needed for particular applications. Our method focuses on\nidentifying and removing these redundant parts, retaining only the necessary\nelements for the target applications. Specifically, we represent the weight\nmatrices of LLMs as a linear combination of base components. We then prune the\nirrelevant bases and enhance the model with new bases beneficial for specific\napplications. Deep compression results on the Llama 2-7b and -13B models,\nconducted on target applications including mathematical reasoning and code\ngeneration, show that our method significantly reduces model size while\nmaintaining comparable accuracy to state-of-the-art low-rank compression\ntechniques."
                },
                "authors": [
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Daniel Agyei Asante"
                    },
                    {
                        "name": "Changsheng Zhao"
                    },
                    {
                        "name": "Ernie Chang"
                    },
                    {
                        "name": "Yangyang Shi"
                    },
                    {
                        "name": "Vikas Chandra"
                    }
                ],
                "author_detail": {
                    "name": "Vikas Chandra"
                },
                "author": "Vikas Chandra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15877v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15877v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04597v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04597v1",
                "updated": "2025-08-06T16:16:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    16,
                    58,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T16:16:58Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    16,
                    58,
                    2,
                    218,
                    0
                ],
                "title": "Pseudo Depth Meets Gaussian: A Feed-forward RGB SLAM Baseline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pseudo Depth Meets Gaussian: A Feed-forward RGB SLAM Baseline"
                },
                "summary": "Incrementally recovering real-sized 3D geometry from a pose-free RGB stream\nis a challenging task in 3D reconstruction, requiring minimal assumptions on\ninput data. Existing methods can be broadly categorized into end-to-end and\nvisual SLAM-based approaches, both of which either struggle with long sequences\nor depend on slow test-time optimization and depth sensors. To address this, we\nfirst integrate a depth estimator into an RGB-D SLAM system, but this approach\nis hindered by inaccurate geometric details in predicted depth. Through further\ninvestigation, we find that 3D Gaussian mapping can effectively solve this\nproblem. Building on this, we propose an online 3D reconstruction method using\n3D Gaussian-based SLAM, combined with a feed-forward recurrent prediction\nmodule to directly infer camera pose from optical flow. This approach replaces\nslow test-time optimization with fast network inference, significantly\nimproving tracking speed. Additionally, we introduce a local graph rendering\ntechnique to enhance robustness in feed-forward pose prediction. Experimental\nresults on the Replica and TUM-RGBD datasets, along with a real-world\ndeployment demonstration, show that our method achieves performance on par with\nthe state-of-the-art SplaTAM, while reducing tracking time by more than 90\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incrementally recovering real-sized 3D geometry from a pose-free RGB stream\nis a challenging task in 3D reconstruction, requiring minimal assumptions on\ninput data. Existing methods can be broadly categorized into end-to-end and\nvisual SLAM-based approaches, both of which either struggle with long sequences\nor depend on slow test-time optimization and depth sensors. To address this, we\nfirst integrate a depth estimator into an RGB-D SLAM system, but this approach\nis hindered by inaccurate geometric details in predicted depth. Through further\ninvestigation, we find that 3D Gaussian mapping can effectively solve this\nproblem. Building on this, we propose an online 3D reconstruction method using\n3D Gaussian-based SLAM, combined with a feed-forward recurrent prediction\nmodule to directly infer camera pose from optical flow. This approach replaces\nslow test-time optimization with fast network inference, significantly\nimproving tracking speed. Additionally, we introduce a local graph rendering\ntechnique to enhance robustness in feed-forward pose prediction. Experimental\nresults on the Replica and TUM-RGBD datasets, along with a real-world\ndeployment demonstration, show that our method achieves performance on par with\nthe state-of-the-art SplaTAM, while reducing tracking time by more than 90\\%."
                },
                "authors": [
                    {
                        "name": "Linqing Zhao"
                    },
                    {
                        "name": "Xiuwei Xu"
                    },
                    {
                        "name": "Yirui Wang"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Wenzhao Zheng"
                    },
                    {
                        "name": "Yansong Tang"
                    },
                    {
                        "name": "Haibin Yan"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "IROS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04597v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04597v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03388v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03388v2",
                "updated": "2025-08-06T16:16:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    16,
                    17,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-05T12:40:55Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    12,
                    40,
                    55,
                    1,
                    217,
                    0
                ],
                "title": "Neutralizing Token Aggregation via Information Augmentation for\n  Efficient Test-Time Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neutralizing Token Aggregation via Information Augmentation for\n  Efficient Test-Time Adaptation"
                },
                "summary": "Test-Time Adaptation (TTA) has emerged as an effective solution for adapting\nVision Transformers (ViT) to distribution shifts without additional training\ndata. However, existing TTA methods often incur substantial computational\noverhead, limiting their applicability in resource-constrained real-world\nscenarios. To reduce inference cost, plug-and-play token aggregation methods\nmerge redundant tokens in ViTs to reduce total processed tokens. Albeit\nefficient, it suffers from significant performance degradation when directly\nintegrated with existing TTA methods. We formalize this problem as Efficient\nTest-Time Adaptation (ETTA), seeking to preserve the adaptation capability of\nTTA while reducing inference latency. In this paper, we first provide a\ntheoretical analysis from a novel mutual information perspective, showing that\ntoken aggregation inherently leads to information loss, which cannot be fully\nmitigated by conventional norm-tuning-based TTA methods. Guided by this\ninsight, we propose to \\textbf{N}eutralize Token \\textbf{A}ggregation\n\\textbf{v}ia \\textbf{I}nformation \\textbf{A}ugmentation (\\textbf{NAVIA}).\nSpecifically, we directly augment the [CLS] token embedding and incorporate\nadaptive biases into the [CLS] token in shallow layers of ViTs. We\ntheoretically demonstrate that these augmentations, when optimized via entropy\nminimization, recover the information lost due to token aggregation. Extensive\nexperiments across various out-of-distribution benchmarks demonstrate that\nNAVIA significantly outperforms state-of-the-art methods by over 2.5\\%, while\nachieving an inference latency reduction of more than 20\\%, effectively\naddressing the ETTA challenge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-Time Adaptation (TTA) has emerged as an effective solution for adapting\nVision Transformers (ViT) to distribution shifts without additional training\ndata. However, existing TTA methods often incur substantial computational\noverhead, limiting their applicability in resource-constrained real-world\nscenarios. To reduce inference cost, plug-and-play token aggregation methods\nmerge redundant tokens in ViTs to reduce total processed tokens. Albeit\nefficient, it suffers from significant performance degradation when directly\nintegrated with existing TTA methods. We formalize this problem as Efficient\nTest-Time Adaptation (ETTA), seeking to preserve the adaptation capability of\nTTA while reducing inference latency. In this paper, we first provide a\ntheoretical analysis from a novel mutual information perspective, showing that\ntoken aggregation inherently leads to information loss, which cannot be fully\nmitigated by conventional norm-tuning-based TTA methods. Guided by this\ninsight, we propose to \\textbf{N}eutralize Token \\textbf{A}ggregation\n\\textbf{v}ia \\textbf{I}nformation \\textbf{A}ugmentation (\\textbf{NAVIA}).\nSpecifically, we directly augment the [CLS] token embedding and incorporate\nadaptive biases into the [CLS] token in shallow layers of ViTs. We\ntheoretically demonstrate that these augmentations, when optimized via entropy\nminimization, recover the information lost due to token aggregation. Extensive\nexperiments across various out-of-distribution benchmarks demonstrate that\nNAVIA significantly outperforms state-of-the-art methods by over 2.5\\%, while\nachieving an inference latency reduction of more than 20\\%, effectively\naddressing the ETTA challenge."
                },
                "authors": [
                    {
                        "name": "Yizhe Xiong"
                    },
                    {
                        "name": "Zihan Zhou"
                    },
                    {
                        "name": "Yiwen Liang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Tianxiang Hao"
                    },
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "9 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03388v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03388v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2211.00114v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2211.00114v2",
                "updated": "2025-08-06T16:10:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    10,
                    41,
                    2,
                    218,
                    0
                ],
                "published": "2022-10-31T19:47:26Z",
                "published_parsed": [
                    2022,
                    10,
                    31,
                    19,
                    47,
                    26,
                    0,
                    304,
                    0
                ],
                "title": "Bayesian MI-LASSO for Variable Selection on Multiply-Imputed Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian MI-LASSO for Variable Selection on Multiply-Imputed Data"
                },
                "summary": "Multiple imputation is widely used for handling missing data in real-world\napplications. For variable selection on multiply-imputed datasets, however, if\nselection is performed on each imputed dataset separately, it can result in\ndifferent sets of selected variables across datasets. MI-LASSO, one of the most\ncommonly used approaches to this problem, regards the same variable across all\nseparate imputed datasets as a group variable and exploits the group LASSO to\nyield a consistent variable selection across all the multiply-imputed datasets.\nIn this paper, we extend MI-LASSO to a Bayesian framework and propose four\nBayesian MI-LASSO models for variable selection on multiply-imputed data,\nincluding three shrinkage prior-based and one Spike-Slab prior-based methods.\nTo further support robust variable selection, we develop a four-step projection\npredictive variable selection procedure that avoids ad hoc thresholding and\nfacilitates valid post-selection inference. Simulation studies showed that the\nBayesian MI-LASSO outperformed MI-LASSO and other alternative approaches,\nachieving higher specificity and lower mean squared error across a range of\nsettings. We further demonstrated these methods via a case study using a\nmultiply-imputed dataset from the University of Michigan Dioxin Exposure Study.\nThe R package BMIselect is available on CRAN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple imputation is widely used for handling missing data in real-world\napplications. For variable selection on multiply-imputed datasets, however, if\nselection is performed on each imputed dataset separately, it can result in\ndifferent sets of selected variables across datasets. MI-LASSO, one of the most\ncommonly used approaches to this problem, regards the same variable across all\nseparate imputed datasets as a group variable and exploits the group LASSO to\nyield a consistent variable selection across all the multiply-imputed datasets.\nIn this paper, we extend MI-LASSO to a Bayesian framework and propose four\nBayesian MI-LASSO models for variable selection on multiply-imputed data,\nincluding three shrinkage prior-based and one Spike-Slab prior-based methods.\nTo further support robust variable selection, we develop a four-step projection\npredictive variable selection procedure that avoids ad hoc thresholding and\nfacilitates valid post-selection inference. Simulation studies showed that the\nBayesian MI-LASSO outperformed MI-LASSO and other alternative approaches,\nachieving higher specificity and lower mean squared error across a range of\nsettings. We further demonstrated these methods via a case study using a\nmultiply-imputed dataset from the University of Michigan Dioxin Exposure Study.\nThe R package BMIselect is available on CRAN."
                },
                "authors": [
                    {
                        "name": "Jungang Zou"
                    },
                    {
                        "name": "Sijian Wang"
                    },
                    {
                        "name": "Qixuan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Qixuan Chen"
                },
                "author": "Qixuan Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2211.00114v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2211.00114v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19404v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19404v2",
                "updated": "2025-08-06T16:08:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    8,
                    52,
                    2,
                    218,
                    0
                ],
                "published": "2024-09-28T16:53:38Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    16,
                    53,
                    38,
                    5,
                    272,
                    0
                ],
                "title": "Incorporation of model accuracy in gravitational wave Bayesian inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incorporation of model accuracy in gravitational wave Bayesian inference"
                },
                "summary": "Inferring the properties of colliding black holes from gravitational-wave\nobservations is subject to systematic errors arising from modelling\nuncertainties. Although the accuracy of each model can be calculated through\ncomparison to theoretical expectations from general relativity, Bayesian\nanalyses are yet to incorporate this information. As such, a mixture model is\ntypically used where results obtained with different gravitational-wave models\nare combined with either equal weight, or based on their relative Bayesian\nevidence. In this work we present a novel method to incorporate the accuracy of\nmultiple models in gravitational-wave Bayesian analyses. By analysing simulated\ngravitational-wave signals in zero-noise, we show that our technique uses\n$30\\%$ less computational resources, and more faithfully recovers the true\nparameters than existing techniques. We further apply our method to a real\ngravitational-wave signal and, when assuming the binary black hole hypothesis,\ndemonstrate that the source of GW191109_010717 has unequal component masses,\nwith the primary having a $69\\%$ probability that it lies above the maximum\nblack hole mass from stellar collapse. We envisage that this method will become\nan essential tool within ground-based gravitational-wave astronomy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring the properties of colliding black holes from gravitational-wave\nobservations is subject to systematic errors arising from modelling\nuncertainties. Although the accuracy of each model can be calculated through\ncomparison to theoretical expectations from general relativity, Bayesian\nanalyses are yet to incorporate this information. As such, a mixture model is\ntypically used where results obtained with different gravitational-wave models\nare combined with either equal weight, or based on their relative Bayesian\nevidence. In this work we present a novel method to incorporate the accuracy of\nmultiple models in gravitational-wave Bayesian analyses. By analysing simulated\ngravitational-wave signals in zero-noise, we show that our technique uses\n$30\\%$ less computational resources, and more faithfully recovers the true\nparameters than existing techniques. We further apply our method to a real\ngravitational-wave signal and, when assuming the binary black hole hypothesis,\ndemonstrate that the source of GW191109_010717 has unequal component masses,\nwith the primary having a $69\\%$ probability that it lies above the maximum\nblack hole mass from stellar collapse. We envisage that this method will become\nan essential tool within ground-based gravitational-wave astronomy."
                },
                "authors": [
                    {
                        "name": "Charlie Hoy"
                    },
                    {
                        "name": "Sarp Akcay"
                    },
                    {
                        "name": "Jake Mac Uilliam"
                    },
                    {
                        "name": "Jonathan E. Thompson"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan E. Thompson"
                },
                "author": "Jonathan E. Thompson",
                "arxiv_doi": "10.1038/s41550-025-02579-7",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1038/s41550-025-02579-7",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.19404v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19404v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "17 pages, 3 figures, 6 supplementary figures. Matches version\n  published in Nature Astronomy",
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04585v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04585v2",
                "updated": "2025-08-07T03:38:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    3,
                    38,
                    23,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-06T16:08:22Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    8,
                    22,
                    2,
                    218,
                    0
                ],
                "title": "UniTalker: Conversational Speech-Visual Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniTalker: Conversational Speech-Visual Synthesis"
                },
                "summary": "Conversational Speech Synthesis (CSS) is a key task in the user-agent\ninteraction area, aiming to generate more expressive and empathetic speech for\nusers. However, it is well-known that \"listening\" and \"eye contact\" play\ncrucial roles in conveying emotions during real-world interpersonal\ncommunication. Existing CSS research is limited to perceiving only text and\nspeech within the dialogue context, which restricts its effectiveness.\nMoreover, speech-only responses further constrain the interactive experience.\nTo address these limitations, we introduce a Conversational Speech-Visual\nSynthesis (CSVS) task as an extension of traditional CSS. By leveraging\nmultimodal dialogue context, it provides users with coherent audiovisual\nresponses. To this end, we develop a CSVS system named UniTalker, which is a\nunified model that seamlessly integrates multimodal perception and multimodal\nrendering capabilities. Specifically, it leverages a large-scale language model\nto comprehensively understand multimodal cues in the dialogue context,\nincluding speaker, text, speech, and the talking-face animations. After that,\nit employs multi-task sequence prediction to first infer the target utterance's\nemotion and then generate empathetic speech and natural talking-face\nanimations. To ensure that the generated speech-visual content remains\nconsistent in terms of emotion, content, and duration, we introduce three key\noptimizations: 1) Designing a specialized neural landmark codec to tokenize and\nreconstruct facial expression sequences. 2) Proposing a bimodal speech-visual\nhard alignment decoding strategy. 3) Applying emotion-guided rendering during\nthe generation stage. Comprehensive objective and subjective experiments\ndemonstrate that our model synthesizes more empathetic speech and provides\nusers with more natural and emotionally consistent talking-face animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational Speech Synthesis (CSS) is a key task in the user-agent\ninteraction area, aiming to generate more expressive and empathetic speech for\nusers. However, it is well-known that \"listening\" and \"eye contact\" play\ncrucial roles in conveying emotions during real-world interpersonal\ncommunication. Existing CSS research is limited to perceiving only text and\nspeech within the dialogue context, which restricts its effectiveness.\nMoreover, speech-only responses further constrain the interactive experience.\nTo address these limitations, we introduce a Conversational Speech-Visual\nSynthesis (CSVS) task as an extension of traditional CSS. By leveraging\nmultimodal dialogue context, it provides users with coherent audiovisual\nresponses. To this end, we develop a CSVS system named UniTalker, which is a\nunified model that seamlessly integrates multimodal perception and multimodal\nrendering capabilities. Specifically, it leverages a large-scale language model\nto comprehensively understand multimodal cues in the dialogue context,\nincluding speaker, text, speech, and the talking-face animations. After that,\nit employs multi-task sequence prediction to first infer the target utterance's\nemotion and then generate empathetic speech and natural talking-face\nanimations. To ensure that the generated speech-visual content remains\nconsistent in terms of emotion, content, and duration, we introduce three key\noptimizations: 1) Designing a specialized neural landmark codec to tokenize and\nreconstruct facial expression sequences. 2) Proposing a bimodal speech-visual\nhard alignment decoding strategy. 3) Applying emotion-guided rendering during\nthe generation stage. Comprehensive objective and subjective experiments\ndemonstrate that our model synthesizes more empathetic speech and provides\nusers with more natural and emotionally consistent talking-face animations."
                },
                "authors": [
                    {
                        "name": "Yifan Hu"
                    },
                    {
                        "name": "Rui Liu"
                    },
                    {
                        "name": "Yi Ren"
                    },
                    {
                        "name": "Xiang Yin"
                    },
                    {
                        "name": "Haizhou Li"
                    }
                ],
                "author_detail": {
                    "name": "Haizhou Li"
                },
                "author": "Haizhou Li",
                "arxiv_comment": "15 pages, 8 figures, Accepted by ACM MM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04585v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04585v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04583v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04583v1",
                "updated": "2025-08-06T16:07:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    7,
                    29,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T16:07:29Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    7,
                    29,
                    2,
                    218,
                    0
                ],
                "title": "Measuring the Carbon Footprint of Cryptographic Privacy-Enhancing\n  Technologies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring the Carbon Footprint of Cryptographic Privacy-Enhancing\n  Technologies"
                },
                "summary": "Privacy-enhancing technologies (PETs) have attracted significant attention in\nresponse to privacy regulations, driving the development of applications that\nprioritize user data protection. At the same time, the information and\ncommunication technology (ICT) sector faces growing pressure to reduce its\nenvironmental footprint, particularly its carbon emissions. While numerous\nstudies have assessed the energy footprint of various ICT applications, the\nenvironmental footprint of cryptographic PETs remains largely unexplored.\n  Our work addresses this gap by proposing a standardized methodology for\nevaluating the carbon footprint of PETs. To demonstrate this methodology, we\nfocus on PETs supporting client-server applications as they are the simplest to\ndeploy. In particular, we measure the energy consumption and carbon footprint\nincrease induced by five cryptographic PETs (compared to their non-private\nequivalent): HTTPS web browsing, encrypted machine learning (ML) inference,\nencrypted ML training, encrypted databases, and encrypted emails. Our findings\nreveal significant variability in carbon footprint increases, ranging from a\ntwofold increase in HTTPS web browsing to a 100,000-fold increase in encrypted\nML.\n  Our study provides essential data to help decision-makers assess\nprivacy-carbon trade-offs in such applications. Finally, we outline key\nresearch directions for developing PETs that balance strong privacy protection\nwith environmental sustainability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-enhancing technologies (PETs) have attracted significant attention in\nresponse to privacy regulations, driving the development of applications that\nprioritize user data protection. At the same time, the information and\ncommunication technology (ICT) sector faces growing pressure to reduce its\nenvironmental footprint, particularly its carbon emissions. While numerous\nstudies have assessed the energy footprint of various ICT applications, the\nenvironmental footprint of cryptographic PETs remains largely unexplored.\n  Our work addresses this gap by proposing a standardized methodology for\nevaluating the carbon footprint of PETs. To demonstrate this methodology, we\nfocus on PETs supporting client-server applications as they are the simplest to\ndeploy. In particular, we measure the energy consumption and carbon footprint\nincrease induced by five cryptographic PETs (compared to their non-private\nequivalent): HTTPS web browsing, encrypted machine learning (ML) inference,\nencrypted ML training, encrypted databases, and encrypted emails. Our findings\nreveal significant variability in carbon footprint increases, ranging from a\ntwofold increase in HTTPS web browsing to a 100,000-fold increase in encrypted\nML.\n  Our study provides essential data to help decision-makers assess\nprivacy-carbon trade-offs in such applications. Finally, we outline key\nresearch directions for developing PETs that balance strong privacy protection\nwith environmental sustainability."
                },
                "authors": [
                    {
                        "name": "Marc Damie"
                    },
                    {
                        "name": "Mihai Pop"
                    },
                    {
                        "name": "Merijn Posthuma"
                    }
                ],
                "author_detail": {
                    "name": "Merijn Posthuma"
                },
                "author": "Merijn Posthuma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04583v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04583v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16781v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16781v2",
                "updated": "2025-08-06T16:07:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    7,
                    6,
                    2,
                    218,
                    0
                ],
                "published": "2025-02-24T02:16:37Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    2,
                    16,
                    37,
                    0,
                    55,
                    0
                ],
                "title": "Evaluating Robustness of LLMs in Question Answering on Multilingual\n  Noisy OCR Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Robustness of LLMs in Question Answering on Multilingual\n  Noisy OCR Data"
                },
                "summary": "Optical Character Recognition (OCR) plays a crucial role in digitizing\nhistorical and multilingual documents, yet OCR errors - imperfect extraction of\ntext, including character insertion, deletion, and substitution can\nsignificantly impact downstream tasks like question-answering (QA). In this\nwork, we conduct a comprehensive analysis of how OCR-induced noise affects the\nperformance of Multilingual QA Systems. To support this analysis, we introduce\na multilingual QA dataset MultiOCR-QA, comprising 50K question-answer pairs\nacross three languages, English, French, and German. The dataset is curated\nfrom OCR-ed historical documents, which include different levels and types of\nOCR noise. We then evaluate how different state-of-the-art Large Language\nmodels (LLMs) perform under different error conditions, focusing on three major\nOCR error types. Our findings show that QA systems are highly prone to\nOCR-induced errors and perform poorly on noisy OCR text. By comparing model\nperformance on clean versus noisy texts, we provide insights into the\nlimitations of current approaches and emphasize the need for more\nnoise-resilient QA systems in historical digitization contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optical Character Recognition (OCR) plays a crucial role in digitizing\nhistorical and multilingual documents, yet OCR errors - imperfect extraction of\ntext, including character insertion, deletion, and substitution can\nsignificantly impact downstream tasks like question-answering (QA). In this\nwork, we conduct a comprehensive analysis of how OCR-induced noise affects the\nperformance of Multilingual QA Systems. To support this analysis, we introduce\na multilingual QA dataset MultiOCR-QA, comprising 50K question-answer pairs\nacross three languages, English, French, and German. The dataset is curated\nfrom OCR-ed historical documents, which include different levels and types of\nOCR noise. We then evaluate how different state-of-the-art Large Language\nmodels (LLMs) perform under different error conditions, focusing on three major\nOCR error types. Our findings show that QA systems are highly prone to\nOCR-induced errors and perform poorly on noisy OCR text. By comparing model\nperformance on clean versus noisy texts, we provide insights into the\nlimitations of current approaches and emphasize the need for more\nnoise-resilient QA systems in historical digitization contexts."
                },
                "authors": [
                    {
                        "name": "Bhawna Piryani"
                    },
                    {
                        "name": "Jamshid Mozafari"
                    },
                    {
                        "name": "Abdelrahman Abdallah"
                    },
                    {
                        "name": "Antoine Doucet"
                    },
                    {
                        "name": "Adam Jatowt"
                    }
                ],
                "author_detail": {
                    "name": "Adam Jatowt"
                },
                "author": "Adam Jatowt",
                "arxiv_comment": "Accepted at CIKM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16781v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16781v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04581v1",
                "updated": "2025-08-06T16:06:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    6,
                    43,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T16:06:43Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    6,
                    43,
                    2,
                    218,
                    0
                ],
                "title": "Share Your Attention: Transformer Weight Sharing via Matrix-based\n  Dictionary Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Share Your Attention: Transformer Weight Sharing via Matrix-based\n  Dictionary Learning"
                },
                "summary": "Large language models (LLMs) have revolutionized AI applications, yet their\nhigh computational and memory demands hinder their widespread deployment.\nExisting compression techniques focus on intra-block optimizations (e.g.\nlow-rank approximation, attention head pruning), while the repetitive layered\nstructure of transformers implies significant inter-block redundancy - a\ndimension largely unexplored beyond key-value (KV) caching. Inspired by\ndictionary learning in CNNs, we propose a framework for structured weight\nsharing across transformer layers. Our approach decomposes attention projection\nmatrices into shared dictionary atoms, reducing the attention module's\nparameters by 66.7% while achieving on-par performance. Unlike complex methods\nrequiring distillation or architectural changes, MASA (Matrix Atom Sharing in\nAttention) operates as a drop-in replacement - trained with standard optimizers\n- and represents each layer's weights as linear combinations of shared matrix\natoms. Experiments across scales (100M-700M parameters) show that MASA achieves\nbetter benchmark accuracy and perplexity than grouped-query attention (GQA),\nlow-rank baselines and recently proposed Repeat-all-over/Sequential sharing at\ncomparable parameter budgets. Ablation studies confirm robustness to the\ndictionary size and the efficacy of shared representations in capturing\ncross-layer statistical regularities. Extending to Vision Transformers (ViT),\nMASA matches performance metrics on image classification and detection tasks\nwith 66.7% fewer attention parameters. By combining dictionary learning\nstrategies with transformer efficiency, MASA offers a scalable blueprint for\nparameter-efficient models without sacrificing performance. Finally, we\ninvestigate the possibility of employing MASA on pretrained LLMs to reduce\ntheir number of parameters without experiencing any significant drop in their\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized AI applications, yet their\nhigh computational and memory demands hinder their widespread deployment.\nExisting compression techniques focus on intra-block optimizations (e.g.\nlow-rank approximation, attention head pruning), while the repetitive layered\nstructure of transformers implies significant inter-block redundancy - a\ndimension largely unexplored beyond key-value (KV) caching. Inspired by\ndictionary learning in CNNs, we propose a framework for structured weight\nsharing across transformer layers. Our approach decomposes attention projection\nmatrices into shared dictionary atoms, reducing the attention module's\nparameters by 66.7% while achieving on-par performance. Unlike complex methods\nrequiring distillation or architectural changes, MASA (Matrix Atom Sharing in\nAttention) operates as a drop-in replacement - trained with standard optimizers\n- and represents each layer's weights as linear combinations of shared matrix\natoms. Experiments across scales (100M-700M parameters) show that MASA achieves\nbetter benchmark accuracy and perplexity than grouped-query attention (GQA),\nlow-rank baselines and recently proposed Repeat-all-over/Sequential sharing at\ncomparable parameter budgets. Ablation studies confirm robustness to the\ndictionary size and the efficacy of shared representations in capturing\ncross-layer statistical regularities. Extending to Vision Transformers (ViT),\nMASA matches performance metrics on image classification and detection tasks\nwith 66.7% fewer attention parameters. By combining dictionary learning\nstrategies with transformer efficiency, MASA offers a scalable blueprint for\nparameter-efficient models without sacrificing performance. Finally, we\ninvestigate the possibility of employing MASA on pretrained LLMs to reduce\ntheir number of parameters without experiencing any significant drop in their\nperformance."
                },
                "authors": [
                    {
                        "name": "Magauiya Zhussip"
                    },
                    {
                        "name": "Dmitriy Shopkhoev"
                    },
                    {
                        "name": "Ammar Ali"
                    },
                    {
                        "name": "Stamatios Lefkimmiatis"
                    }
                ],
                "author_detail": {
                    "name": "Stamatios Lefkimmiatis"
                },
                "author": "Stamatios Lefkimmiatis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16136v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16136v2",
                "updated": "2025-08-06T16:02:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    2,
                    25,
                    2,
                    218,
                    0
                ],
                "published": "2025-07-22T01:11:26Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    1,
                    11,
                    26,
                    1,
                    203,
                    0
                ],
                "title": "SDBench: A Comprehensive Benchmark Suite for Speaker Diarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SDBench: A Comprehensive Benchmark Suite for Speaker Diarization"
                },
                "summary": "Even state-of-the-art speaker diarization systems exhibit high variance in\nerror rates across different datasets, representing numerous use cases and\ndomains. Furthermore, comparing across systems requires careful application of\nbest practices such as dataset splits and metric definitions to allow for\napples-to-apples comparison. We propose SDBench (Speaker Diarization\nBenchmark), an open-source benchmark suite that integrates 13 diverse datasets\nwith built-in tooling for consistent and fine-grained analysis of speaker\ndiarization performance for various on-device and server-side systems. SDBench\nenables reproducible evaluation and easy integration of new systems over time.\nTo demonstrate the efficacy of SDBench, we built SpeakerKit, an inference\nefficiency-focused system built on top of Pyannote v3. SDBench enabled rapid\nexecution of ablation studies that led to SpeakerKit being 9.6x faster than\nPyannote v3 while achieving comparable error rates. We benchmark 6\nstate-of-the-art systems including Deepgram, AWS Transcribe, and Pyannote AI\nAPI, revealing important trade-offs between accuracy and speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Even state-of-the-art speaker diarization systems exhibit high variance in\nerror rates across different datasets, representing numerous use cases and\ndomains. Furthermore, comparing across systems requires careful application of\nbest practices such as dataset splits and metric definitions to allow for\napples-to-apples comparison. We propose SDBench (Speaker Diarization\nBenchmark), an open-source benchmark suite that integrates 13 diverse datasets\nwith built-in tooling for consistent and fine-grained analysis of speaker\ndiarization performance for various on-device and server-side systems. SDBench\nenables reproducible evaluation and easy integration of new systems over time.\nTo demonstrate the efficacy of SDBench, we built SpeakerKit, an inference\nefficiency-focused system built on top of Pyannote v3. SDBench enabled rapid\nexecution of ablation studies that led to SpeakerKit being 9.6x faster than\nPyannote v3 while achieving comparable error rates. We benchmark 6\nstate-of-the-art systems including Deepgram, AWS Transcribe, and Pyannote AI\nAPI, revealing important trade-offs between accuracy and speed."
                },
                "authors": [
                    {
                        "name": "Eduardo Pacheco"
                    },
                    {
                        "name": "Atila Orhon"
                    },
                    {
                        "name": "Berkin Durmus"
                    },
                    {
                        "name": "Blaise Munyampirwa"
                    },
                    {
                        "name": "Andrey Leonov"
                    }
                ],
                "author_detail": {
                    "name": "Andrey Leonov"
                },
                "author": "Andrey Leonov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16136v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16136v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07722v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07722v6",
                "updated": "2025-08-06T15:51:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    15,
                    51,
                    18,
                    2,
                    218,
                    0
                ],
                "published": "2025-04-10T13:15:52Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    13,
                    15,
                    52,
                    3,
                    100,
                    0
                ],
                "title": "A Relative Ignorability Framework for Decision-Relevant Observability in\n  Control Theory and Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Relative Ignorability Framework for Decision-Relevant Observability in\n  Control Theory and Reinforcement Learning"
                },
                "summary": "Sequential decision-making systems routinely operate with missing or\nincomplete data. Classical reinforcement learning theory, which is commonly\nused to solve sequential decision problems, assumes Markovian observability,\nwhich may not hold under partial observability. Causal inference paradigms\nformalise ignorability of missingness. We show these views can be unified and\ngeneralized in order to guarantee Q-learning convergence even when the Markov\nproperty fails. To do so, we introduce the concept of relative ignorability.\nRelative ignorability is a graphical-causal criterion which refines the\nrequirements for accurate decision-making based on incomplete data. Theoretical\nresults and simulations both reveal that non-Markovian stochastic processes\nwhose missingness is relatively ignorable with respect to causal estimands can\nstill be optimized using standard Reinforcement Learning algorithms. These\nresults expand the theoretical foundations of safe, data-efficient AI to\nreal-world environments where complete information is unattainable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential decision-making systems routinely operate with missing or\nincomplete data. Classical reinforcement learning theory, which is commonly\nused to solve sequential decision problems, assumes Markovian observability,\nwhich may not hold under partial observability. Causal inference paradigms\nformalise ignorability of missingness. We show these views can be unified and\ngeneralized in order to guarantee Q-learning convergence even when the Markov\nproperty fails. To do so, we introduce the concept of relative ignorability.\nRelative ignorability is a graphical-causal criterion which refines the\nrequirements for accurate decision-making based on incomplete data. Theoretical\nresults and simulations both reveal that non-Markovian stochastic processes\nwhose missingness is relatively ignorable with respect to causal estimands can\nstill be optimized using standard Reinforcement Learning algorithms. These\nresults expand the theoretical foundations of safe, data-efficient AI to\nreal-world environments where complete information is unattainable."
                },
                "authors": [
                    {
                        "name": "MaryLena Bleile"
                    },
                    {
                        "name": "Minh-Nhat Phung"
                    },
                    {
                        "name": "Minh-Binh Tran"
                    }
                ],
                "author_detail": {
                    "name": "Minh-Binh Tran"
                },
                "author": "Minh-Binh Tran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07722v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07722v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "60G",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04563v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04563v1",
                "updated": "2025-08-06T15:49:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    15,
                    49,
                    26,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T15:49:26Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    15,
                    49,
                    26,
                    2,
                    218,
                    0
                ],
                "title": "SID: Benchmarking Guided Instruction Capabilities in STEM Education with\n  a Socratic Interdisciplinary Dialogues Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SID: Benchmarking Guided Instruction Capabilities in STEM Education with\n  a Socratic Interdisciplinary Dialogues Dataset"
                },
                "summary": "Fostering students' abilities for knowledge integration and transfer in\ncomplex problem-solving scenarios is a core objective of modern education, and\ninterdisciplinary STEM is a key pathway to achieve this, yet it requires expert\nguidance that is difficult to scale. While LLMs offer potential in this regard,\ntheir true capability for guided instruction remains unclear due to the lack of\nan effective evaluation benchmark. To address this, we introduce SID, the first\nbenchmark designed to systematically evaluate the higher-order guidance\ncapabilities of LLMs in multi-turn, interdisciplinary Socratic dialogues. Our\ncontributions include a large-scale dataset of 10,000 dialogue turns across 48\ncomplex STEM projects, a novel annotation schema for capturing deep pedagogical\nfeatures, and a new suite of evaluation metrics (e.g., X-SRG). Baseline\nexperiments confirm that even state-of-the-art LLMs struggle to execute\neffective guided dialogues that lead students to achieve knowledge integration\nand transfer. This highlights the critical value of our benchmark in driving\nthe development of more pedagogically-aware LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fostering students' abilities for knowledge integration and transfer in\ncomplex problem-solving scenarios is a core objective of modern education, and\ninterdisciplinary STEM is a key pathway to achieve this, yet it requires expert\nguidance that is difficult to scale. While LLMs offer potential in this regard,\ntheir true capability for guided instruction remains unclear due to the lack of\nan effective evaluation benchmark. To address this, we introduce SID, the first\nbenchmark designed to systematically evaluate the higher-order guidance\ncapabilities of LLMs in multi-turn, interdisciplinary Socratic dialogues. Our\ncontributions include a large-scale dataset of 10,000 dialogue turns across 48\ncomplex STEM projects, a novel annotation schema for capturing deep pedagogical\nfeatures, and a new suite of evaluation metrics (e.g., X-SRG). Baseline\nexperiments confirm that even state-of-the-art LLMs struggle to execute\neffective guided dialogues that lead students to achieve knowledge integration\nand transfer. This highlights the critical value of our benchmark in driving\nthe development of more pedagogically-aware LLMs."
                },
                "authors": [
                    {
                        "name": "Mei Jiang"
                    },
                    {
                        "name": "Houping Yue"
                    },
                    {
                        "name": "Bingdong Li"
                    },
                    {
                        "name": "Hao Hao"
                    },
                    {
                        "name": "Ying Qian"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Aimin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Aimin Zhou"
                },
                "author": "Aimin Zhou",
                "arxiv_comment": "26 pages, 20 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04563v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04563v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03905v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03905v4",
                "updated": "2025-08-07T03:40:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    3,
                    40,
                    52,
                    3,
                    219,
                    0
                ],
                "published": "2025-07-05T05:36:26Z",
                "published_parsed": [
                    2025,
                    7,
                    5,
                    5,
                    36,
                    26,
                    5,
                    186,
                    0
                ],
                "title": "EchoMimicV3: 1.3B Parameters are All You Need for Unified Multi-Modal\n  and Multi-Task Human Animation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EchoMimicV3: 1.3B Parameters are All You Need for Unified Multi-Modal\n  and Multi-Task Human Animation"
                },
                "summary": "Recent work on human animation usually incorporates large-scale video models,\nthereby achieving more vivid performance. However, the practical use of such\nmethods is hindered by the slow inference speed and high computational demands.\nMoreover, traditional work typically employs separate models for each animation\ntask, increasing costs in multi-task scenarios and worsening the dilemma. To\naddress these limitations, we introduce EchoMimicV3, an efficient framework\nthat unifies multi-task and multi-modal human animation. At the core of\nEchoMimicV3 lies a threefold design: a Soup-of-Tasks paradigm, a Soup-of-Modals\nparadigm, and a novel training and inference strategy. The Soup-of-Tasks\nleverages multi-task mask inputs and a counter-intuitive task allocation\nstrategy to achieve multi-task gains without multi-model pains. Meanwhile, the\nSoup-of-Modals introduces a Coupled-Decoupled Multi-Modal Cross Attention\nmodule to inject multi-modal conditions, complemented by a Multi-Modal Timestep\nPhase-aware Dynamical Allocation mechanism to modulate multi-modal mixtures.\nBesides, we propose Negative Direct Preference Optimization, Phase-aware\nNegative Classifier-Free Guidance (CFG), and Long Video CFG, which ensure\nstable training and inference. Extensive experiments and analyses demonstrate\nthat EchoMimicV3, with a minimal model size of 1.3 billion parameters, achieves\ncompetitive performance in both quantitative and qualitative evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work on human animation usually incorporates large-scale video models,\nthereby achieving more vivid performance. However, the practical use of such\nmethods is hindered by the slow inference speed and high computational demands.\nMoreover, traditional work typically employs separate models for each animation\ntask, increasing costs in multi-task scenarios and worsening the dilemma. To\naddress these limitations, we introduce EchoMimicV3, an efficient framework\nthat unifies multi-task and multi-modal human animation. At the core of\nEchoMimicV3 lies a threefold design: a Soup-of-Tasks paradigm, a Soup-of-Modals\nparadigm, and a novel training and inference strategy. The Soup-of-Tasks\nleverages multi-task mask inputs and a counter-intuitive task allocation\nstrategy to achieve multi-task gains without multi-model pains. Meanwhile, the\nSoup-of-Modals introduces a Coupled-Decoupled Multi-Modal Cross Attention\nmodule to inject multi-modal conditions, complemented by a Multi-Modal Timestep\nPhase-aware Dynamical Allocation mechanism to modulate multi-modal mixtures.\nBesides, we propose Negative Direct Preference Optimization, Phase-aware\nNegative Classifier-Free Guidance (CFG), and Long Video CFG, which ensure\nstable training and inference. Extensive experiments and analyses demonstrate\nthat EchoMimicV3, with a minimal model size of 1.3 billion parameters, achieves\ncompetitive performance in both quantitative and qualitative evaluations."
                },
                "authors": [
                    {
                        "name": "Rang Meng"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Weipeng Wu"
                    },
                    {
                        "name": "Ruobing Zheng"
                    },
                    {
                        "name": "Yuming Li"
                    },
                    {
                        "name": "Chenguang Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chenguang Ma"
                },
                "author": "Chenguang Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03905v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03905v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15434v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15434v3",
                "updated": "2025-08-06T15:35:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    15,
                    35,
                    58,
                    2,
                    218,
                    0
                ],
                "published": "2025-02-21T13:01:26Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    1,
                    26,
                    4,
                    52,
                    0
                ],
                "title": "Mixup Model Merge: Enhancing Model Merging Performance through\n  Randomized Linear Interpolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixup Model Merge: Enhancing Model Merging Performance through\n  Randomized Linear Interpolation"
                },
                "summary": "Model merging aims to integrate multiple task-specific models into a unified\nmodel that inherits the capabilities of the task-specific models, without\nadditional training. Existing model merging methods often lack consideration of\nthe varying contribution ratios of different task-specific models to the final\nmerged model. In this paper, we propose Mixup Model Merge (M3), a simple yet\neffective method inspired by the randomized linear interpolation strategy from\nthe Mixup data augmentation technique. M3 performs randomized linear\ninterpolation in parameter space between two task-specific LLMs, where\ninterpolation coefficients are sampled from a Beta distribution to explore\ndiverse contribution ratios. This controllable randomness allows M3 to\noutperform standard equal-ratio merging by discovering better contribution\nratio combinations. Extensive experiments show that M3 significantly (1)\nimproves merged LLM performance across tasks, (2) enhances out-of-distribution\nand adversarial robustness, (3) outperforms the positive effects of the\nsparsification method DARE on model merging and can be further combined with\nDARE to achieve superior results, and (4) balances exploration efficiency and\ndiversity in contribution ratios by tuning the Beta distribution's shape\nparameters. The code is provided in the supplementary materials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model merging aims to integrate multiple task-specific models into a unified\nmodel that inherits the capabilities of the task-specific models, without\nadditional training. Existing model merging methods often lack consideration of\nthe varying contribution ratios of different task-specific models to the final\nmerged model. In this paper, we propose Mixup Model Merge (M3), a simple yet\neffective method inspired by the randomized linear interpolation strategy from\nthe Mixup data augmentation technique. M3 performs randomized linear\ninterpolation in parameter space between two task-specific LLMs, where\ninterpolation coefficients are sampled from a Beta distribution to explore\ndiverse contribution ratios. This controllable randomness allows M3 to\noutperform standard equal-ratio merging by discovering better contribution\nratio combinations. Extensive experiments show that M3 significantly (1)\nimproves merged LLM performance across tasks, (2) enhances out-of-distribution\nand adversarial robustness, (3) outperforms the positive effects of the\nsparsification method DARE on model merging and can be further combined with\nDARE to achieve superior results, and (4) balances exploration efficiency and\ndiversity in contribution ratios by tuning the Beta distribution's shape\nparameters. The code is provided in the supplementary materials."
                },
                "authors": [
                    {
                        "name": "Yue Zhou"
                    },
                    {
                        "name": "Yi Chang"
                    },
                    {
                        "name": "Yuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Wu"
                },
                "author": "Yuan Wu",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15434v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15434v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14194v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14194v4",
                "updated": "2025-08-06T15:34:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    15,
                    34,
                    10,
                    2,
                    218,
                    0
                ],
                "published": "2025-04-19T06:12:33Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    6,
                    12,
                    33,
                    5,
                    109,
                    0
                ],
                "title": "Meta-rater: A Multi-dimensional Data Selection Method for Pre-training\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meta-rater: A Multi-dimensional Data Selection Method for Pre-training\n  Language Models"
                },
                "summary": "The composition of pre-training datasets for large language models (LLMs)\nremains largely undisclosed, hindering transparency and efforts to optimize\ndata quality, a critical driver of model performance. Current data selection\nmethods, such as natural language quality assessments, diversity-based filters,\nand classifier-based approaches, are limited by single-dimensional evaluation\nor redundancy-focused strategies. To address these gaps, we propose four\ndimensions to evaluate data quality: professionalism, readability, reasoning,\nand cleanliness. We further introduce Meta-rater,a multi-dimensional data\nselection method that integrates these dimensions with existing quality metrics\nthrough learned optimal weightings. Meta-rater employs proxy models to train a\nregression model that predicts validation loss, enabling the identification of\noptimal combinations of quality scores. Experiments demonstrate that Meta-rater\ndoubles convergence speed for 1.3B parameter models and improves downstream\ntask performance by 3.23, with advantages that scale to models as large as 7.2B\nparameters. Our work establishes that holistic, multi-dimensional quality\nintegration significantly outperforms conventional single-dimension approaches,\noffering a scalable paradigm for enhancing pre-training efficiency and model\ncapability. To advance future research, we release scripts, data, and models at\nhttps://github.com/opendatalab/Meta-rater.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The composition of pre-training datasets for large language models (LLMs)\nremains largely undisclosed, hindering transparency and efforts to optimize\ndata quality, a critical driver of model performance. Current data selection\nmethods, such as natural language quality assessments, diversity-based filters,\nand classifier-based approaches, are limited by single-dimensional evaluation\nor redundancy-focused strategies. To address these gaps, we propose four\ndimensions to evaluate data quality: professionalism, readability, reasoning,\nand cleanliness. We further introduce Meta-rater,a multi-dimensional data\nselection method that integrates these dimensions with existing quality metrics\nthrough learned optimal weightings. Meta-rater employs proxy models to train a\nregression model that predicts validation loss, enabling the identification of\noptimal combinations of quality scores. Experiments demonstrate that Meta-rater\ndoubles convergence speed for 1.3B parameter models and improves downstream\ntask performance by 3.23, with advantages that scale to models as large as 7.2B\nparameters. Our work establishes that holistic, multi-dimensional quality\nintegration significantly outperforms conventional single-dimension approaches,\noffering a scalable paradigm for enhancing pre-training efficiency and model\ncapability. To advance future research, we release scripts, data, and models at\nhttps://github.com/opendatalab/Meta-rater."
                },
                "authors": [
                    {
                        "name": "Xinlin Zhuang"
                    },
                    {
                        "name": "Jiahui Peng"
                    },
                    {
                        "name": "Ren Ma"
                    },
                    {
                        "name": "Yinfan Wang"
                    },
                    {
                        "name": "Tianyi Bai"
                    },
                    {
                        "name": "Xingjian Wei"
                    },
                    {
                        "name": "Jiantao Qiu"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Ying Qian"
                    },
                    {
                        "name": "Conghui He"
                    }
                ],
                "author_detail": {
                    "name": "Conghui He"
                },
                "author": "Conghui He",
                "arxiv_comment": "ACL 2025 Best Theme Paper Award",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14194v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14194v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03174v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03174v2",
                "updated": "2025-08-06T15:28:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    15,
                    28,
                    4,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-05T07:33:48Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    7,
                    33,
                    48,
                    1,
                    217,
                    0
                ],
                "title": "InqEduAgent: Adaptive AI Learning Partners with Gaussian Process\n  Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InqEduAgent: Adaptive AI Learning Partners with Gaussian Process\n  Augmentation"
                },
                "summary": "Collaborative partnership matters in inquiry-oriented education. However,\nmost study partners are selected either rely on experience-based assignments\nwith little scientific planning or build on rule-based machine assistants,\nencountering difficulties in knowledge expansion and inadequate flexibility.\nThis paper proposes an LLM-empowered agent model for simulating and selecting\nlearning partners tailored to inquiry-oriented learning, named InqEduAgent.\nGenerative agents are designed to capture cognitive and evaluative features of\nlearners in real-world scenarios. Then, an adaptive matching algorithm with\nGaussian process augmentation is formulated to identify patterns within prior\nknowledge. Optimal learning-partner matches are provided for learners facing\ndifferent exercises. The experimental results show the optimal performance of\nInqEduAgent in most knowledge-learning scenarios and LLM environment with\ndifferent levels of capabilities. This study promotes the intelligent\nallocation of human-based learning partners and the formulation of AI-based\nlearning partners. The code, data, and appendix are publicly available at\nhttps://github.com/InqEduAgent/InqEduAgent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative partnership matters in inquiry-oriented education. However,\nmost study partners are selected either rely on experience-based assignments\nwith little scientific planning or build on rule-based machine assistants,\nencountering difficulties in knowledge expansion and inadequate flexibility.\nThis paper proposes an LLM-empowered agent model for simulating and selecting\nlearning partners tailored to inquiry-oriented learning, named InqEduAgent.\nGenerative agents are designed to capture cognitive and evaluative features of\nlearners in real-world scenarios. Then, an adaptive matching algorithm with\nGaussian process augmentation is formulated to identify patterns within prior\nknowledge. Optimal learning-partner matches are provided for learners facing\ndifferent exercises. The experimental results show the optimal performance of\nInqEduAgent in most knowledge-learning scenarios and LLM environment with\ndifferent levels of capabilities. This study promotes the intelligent\nallocation of human-based learning partners and the formulation of AI-based\nlearning partners. The code, data, and appendix are publicly available at\nhttps://github.com/InqEduAgent/InqEduAgent."
                },
                "authors": [
                    {
                        "name": "Tian-Fang Zhao"
                    },
                    {
                        "name": "Wen-Xi Yang"
                    },
                    {
                        "name": "Guan Liu"
                    },
                    {
                        "name": "Liang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Yang"
                },
                "author": "Liang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03174v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03174v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06850v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06850v4",
                "updated": "2025-08-06T15:27:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    15,
                    27,
                    3,
                    2,
                    218,
                    0
                ],
                "published": "2025-07-09T13:54:58Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    13,
                    54,
                    58,
                    2,
                    190,
                    0
                ],
                "title": "The Dark Side of LLMs: Agent-based Attacks for Complete Computer\n  Takeover",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Dark Side of LLMs: Agent-based Attacks for Complete Computer\n  Takeover"
                },
                "summary": "The rapid adoption of Large Language Model (LLM) agents and multi-agent\nsystems enables remarkable capabilities in natural language processing and\ngeneration. However, these systems introduce unprecedented security\nvulnerabilities that extend beyond traditional content generation attacks to\nsystem-level compromise. This paper presents a comprehensive evaluation of the\nsecurity of LLMs used as reasoning engines within autonomous agents,\nhighlighting how they can be exploited as attack vectors capable of achieving\ncomplete computer takeover. We focus on how different attack surfaces and trust\nboundaries - Direct Prompt Injection, RAG Backdoor, and Inter Agent Trust - can\nbe leveraged to orchestrate such takeovers. We demonstrate that adversaries can\neffectively coerce popular LLMs (including GPT-4, Claude-4 and Gemini-2.5) into\nautonomously installing and executing malware on victim machines. Our\nevaluation of 18 state-of-the-art LLMs reveals an alarming scenario: 94.4% of\nmodels succumb to Direct Prompt Injection and 83.3% are vulnerable to the more\nstealth and evasive RAG Backdoor Attack. Notably, we tested trust boundaries\nwithin multi-agent systems, where LLM agents interact and influence each other,\nand we revealed a critical security flaw: LLMs which successfully resist direct\ninjection or RAG backdoor will execute identical payloads when requested by\npeer agents. Our findings show that 100.0% of tested LLMs can be compromised\nthrough Inter-Agent Trust Exploitation attacks and that every model exhibits\ncontext-dependent security behaviors that create exploitable blind spots. Our\nresults also highlight the need to increase awareness and research on the\nsecurity risks of LLMs, showing a paradigm shift in cybersecurity threats,\nwhere AI tools themselves become sophisticated attack vectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid adoption of Large Language Model (LLM) agents and multi-agent\nsystems enables remarkable capabilities in natural language processing and\ngeneration. However, these systems introduce unprecedented security\nvulnerabilities that extend beyond traditional content generation attacks to\nsystem-level compromise. This paper presents a comprehensive evaluation of the\nsecurity of LLMs used as reasoning engines within autonomous agents,\nhighlighting how they can be exploited as attack vectors capable of achieving\ncomplete computer takeover. We focus on how different attack surfaces and trust\nboundaries - Direct Prompt Injection, RAG Backdoor, and Inter Agent Trust - can\nbe leveraged to orchestrate such takeovers. We demonstrate that adversaries can\neffectively coerce popular LLMs (including GPT-4, Claude-4 and Gemini-2.5) into\nautonomously installing and executing malware on victim machines. Our\nevaluation of 18 state-of-the-art LLMs reveals an alarming scenario: 94.4% of\nmodels succumb to Direct Prompt Injection and 83.3% are vulnerable to the more\nstealth and evasive RAG Backdoor Attack. Notably, we tested trust boundaries\nwithin multi-agent systems, where LLM agents interact and influence each other,\nand we revealed a critical security flaw: LLMs which successfully resist direct\ninjection or RAG backdoor will execute identical payloads when requested by\npeer agents. Our findings show that 100.0% of tested LLMs can be compromised\nthrough Inter-Agent Trust Exploitation attacks and that every model exhibits\ncontext-dependent security behaviors that create exploitable blind spots. Our\nresults also highlight the need to increase awareness and research on the\nsecurity risks of LLMs, showing a paradigm shift in cybersecurity threats,\nwhere AI tools themselves become sophisticated attack vectors."
                },
                "authors": [
                    {
                        "name": "Matteo Lupinacci"
                    },
                    {
                        "name": "Francesco Aurelio Pironti"
                    },
                    {
                        "name": "Francesco Blefari"
                    },
                    {
                        "name": "Francesco Romeo"
                    },
                    {
                        "name": "Luigi Arena"
                    },
                    {
                        "name": "Angelo Furfaro"
                    }
                ],
                "author_detail": {
                    "name": "Angelo Furfaro"
                },
                "author": "Angelo Furfaro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06850v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06850v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11773v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11773v3",
                "updated": "2025-08-06T15:26:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    15,
                    26,
                    58,
                    2,
                    218,
                    0
                ],
                "published": "2025-06-13T13:31:08Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    31,
                    8,
                    4,
                    164,
                    0
                ],
                "title": "AgentSense: Virtual Sensor Data Generation Using LLM Agents in Simulated\n  Home Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentSense: Virtual Sensor Data Generation Using LLM Agents in Simulated\n  Home Environments"
                },
                "summary": "A major challenge in developing robust and generalizable Human Activity\nRecognition (HAR) systems for smart homes is the lack of large and diverse\nlabeled datasets. Variations in home layouts, sensor configurations, and\nindividual behaviors further exacerbate this issue. To address this, we\nleverage the idea of embodied AI agents-virtual agents that perceive and act\nwithin simulated environments guided by internal world models. We introduce\nAgentSense, a virtual data generation pipeline in which agents live out daily\nroutines in simulated smart homes, with behavior guided by Large Language\nModels (LLMs). The LLM generates diverse synthetic personas and realistic\nroutines grounded in the environment, which are then decomposed into\nfine-grained actions. These actions are executed in an extended version of the\nVirtualHome simulator, which we augment with virtual ambient sensors that\nrecord the agents' activities. Our approach produces rich, privacy-preserving\nsensor data that reflects real-world diversity. We evaluate AgentSense on five\nreal HAR datasets. Models pretrained on the generated data consistently\noutperform baselines, especially in low-resource settings. Furthermore,\ncombining the generated virtual sensor data with a small amount of real data\nachieves performance comparable to training on full real-world datasets. These\nresults highlight the potential of using LLM-guided embodied agents for\nscalable and cost-effective sensor data generation in HAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A major challenge in developing robust and generalizable Human Activity\nRecognition (HAR) systems for smart homes is the lack of large and diverse\nlabeled datasets. Variations in home layouts, sensor configurations, and\nindividual behaviors further exacerbate this issue. To address this, we\nleverage the idea of embodied AI agents-virtual agents that perceive and act\nwithin simulated environments guided by internal world models. We introduce\nAgentSense, a virtual data generation pipeline in which agents live out daily\nroutines in simulated smart homes, with behavior guided by Large Language\nModels (LLMs). The LLM generates diverse synthetic personas and realistic\nroutines grounded in the environment, which are then decomposed into\nfine-grained actions. These actions are executed in an extended version of the\nVirtualHome simulator, which we augment with virtual ambient sensors that\nrecord the agents' activities. Our approach produces rich, privacy-preserving\nsensor data that reflects real-world diversity. We evaluate AgentSense on five\nreal HAR datasets. Models pretrained on the generated data consistently\noutperform baselines, especially in low-resource settings. Furthermore,\ncombining the generated virtual sensor data with a small amount of real data\nachieves performance comparable to training on full real-world datasets. These\nresults highlight the potential of using LLM-guided embodied agents for\nscalable and cost-effective sensor data generation in HAR."
                },
                "authors": [
                    {
                        "name": "Zikang Leng"
                    },
                    {
                        "name": "Megha Thukral"
                    },
                    {
                        "name": "Yaqi Liu"
                    },
                    {
                        "name": "Hrudhai Rajasekhar"
                    },
                    {
                        "name": "Shruthi K. Hiremath"
                    },
                    {
                        "name": "Jiaman He"
                    },
                    {
                        "name": "Thomas Pltz"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Pltz"
                },
                "author": "Thomas Pltz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11773v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11773v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03440v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03440v3",
                "updated": "2025-08-07T06:38:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    6,
                    38,
                    21,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-05T13:38:33Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    13,
                    38,
                    33,
                    1,
                    217,
                    0
                ],
                "title": "LLMs are Single-threaded Reasoners: Demystifying the Working Mechanism\n  of Soft Thinking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are Single-threaded Reasoners: Demystifying the Working Mechanism\n  of Soft Thinking"
                },
                "summary": "Human cognition naturally engages with abstract and fluid concepts, whereas\nexisting reasoning models often rely on generating discrete tokens, potentially\nconstraining their expressive capabilities. Recent advancements aim to address\nthis limitation by enabling large language models (LLMs) to generate soft,\nabstract tokens, thus facilitating reasoning within a continuous concept space.\nThis paper explores the `Soft Thinking' capabilities of various LLMs by\nexamining the models' internal behavior using a suite of probing techniques.\nContrary to the common belief that Soft Thinking enables the simultaneous\nexploration of diverse reasoning paths, our findings reveal that LLMs\npredominantly rely on the most influential component of the soft inputs during\nsubsequent decoding steps. This reliance hinders the exploration of different\nreasoning paths and reduces vanilla Soft Thinking to a form of greedy decoding,\nobscuring the advantage of transmitting more information through Soft Tokens.\nTo tackle this issue, we explore sampling strategies to introduce\n\\emph{randomness}, employing methods such as Dirichlet resampling and the\nGumbel-Softmax trick. Our experiments demonstrate that incorporating randomness\ncan alleviate the limitations of vanilla approaches and unleash the potential\nof Soft Thinking. Notably, the Gumbel-Softmax trick provides adequate\nrandomness with controlled smoothness, resulting in superior performance across\neight reasoning benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human cognition naturally engages with abstract and fluid concepts, whereas\nexisting reasoning models often rely on generating discrete tokens, potentially\nconstraining their expressive capabilities. Recent advancements aim to address\nthis limitation by enabling large language models (LLMs) to generate soft,\nabstract tokens, thus facilitating reasoning within a continuous concept space.\nThis paper explores the `Soft Thinking' capabilities of various LLMs by\nexamining the models' internal behavior using a suite of probing techniques.\nContrary to the common belief that Soft Thinking enables the simultaneous\nexploration of diverse reasoning paths, our findings reveal that LLMs\npredominantly rely on the most influential component of the soft inputs during\nsubsequent decoding steps. This reliance hinders the exploration of different\nreasoning paths and reduces vanilla Soft Thinking to a form of greedy decoding,\nobscuring the advantage of transmitting more information through Soft Tokens.\nTo tackle this issue, we explore sampling strategies to introduce\n\\emph{randomness}, employing methods such as Dirichlet resampling and the\nGumbel-Softmax trick. Our experiments demonstrate that incorporating randomness\ncan alleviate the limitations of vanilla approaches and unleash the potential\nof Soft Thinking. Notably, the Gumbel-Softmax trick provides adequate\nrandomness with controlled smoothness, resulting in superior performance across\neight reasoning benchmarks."
                },
                "authors": [
                    {
                        "name": "Chnhung Wu"
                    },
                    {
                        "name": "Jinliang Lu"
                    },
                    {
                        "name": "Zixuan Ren"
                    },
                    {
                        "name": "Gangqiang Hu"
                    },
                    {
                        "name": "Zhi Wu"
                    },
                    {
                        "name": "Dai Dai"
                    },
                    {
                        "name": "Hua Wu"
                    }
                ],
                "author_detail": {
                    "name": "Hua Wu"
                },
                "author": "Hua Wu",
                "arxiv_comment": "11 pages, 7 figures, working in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03440v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03440v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04531v1",
                "updated": "2025-08-06T15:13:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    15,
                    13,
                    24,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T15:13:24Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    15,
                    13,
                    24,
                    2,
                    218,
                    0
                ],
                "title": "Unveiling the Landscape of Clinical Depression Assessment: From\n  Behavioral Signatures to Psychiatric Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling the Landscape of Clinical Depression Assessment: From\n  Behavioral Signatures to Psychiatric Reasoning"
                },
                "summary": "Depression is a widespread mental disorder that affects millions worldwide.\nWhile automated depression assessment shows promise, most studies rely on\nlimited or non-clinically validated data, and often prioritize complex model\ndesign over real-world effectiveness. In this paper, we aim to unveil the\nlandscape of clinical depression assessment. We introduce C-MIND, a clinical\nneuropsychiatric multimodal diagnosis dataset collected over two years from\nreal hospital visits. Each participant completes three structured psychiatric\ntasks and receives a final diagnosis from expert clinicians, with informative\naudio, video, transcript, and functional near-infrared spectroscopy (fNIRS)\nsignals recorded. Using C-MIND, we first analyze behavioral signatures relevant\nto diagnosis. We train a range of classical models to quantify how different\ntasks and modalities contribute to diagnostic performance, and dissect the\neffectiveness of their combinations. We then explore whether LLMs can perform\npsychiatric reasoning like clinicians and identify their clear limitations in\nrealistic clinical settings. In response, we propose to guide the reasoning\nprocess with clinical expertise and consistently improves LLM diagnostic\nperformance by up to 10% in Macro-F1 score. We aim to build an infrastructure\nfor clinical depression assessment from both data and algorithmic perspectives,\nenabling C-MIND to facilitate grounded and reliable research for mental\nhealthcare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Depression is a widespread mental disorder that affects millions worldwide.\nWhile automated depression assessment shows promise, most studies rely on\nlimited or non-clinically validated data, and often prioritize complex model\ndesign over real-world effectiveness. In this paper, we aim to unveil the\nlandscape of clinical depression assessment. We introduce C-MIND, a clinical\nneuropsychiatric multimodal diagnosis dataset collected over two years from\nreal hospital visits. Each participant completes three structured psychiatric\ntasks and receives a final diagnosis from expert clinicians, with informative\naudio, video, transcript, and functional near-infrared spectroscopy (fNIRS)\nsignals recorded. Using C-MIND, we first analyze behavioral signatures relevant\nto diagnosis. We train a range of classical models to quantify how different\ntasks and modalities contribute to diagnostic performance, and dissect the\neffectiveness of their combinations. We then explore whether LLMs can perform\npsychiatric reasoning like clinicians and identify their clear limitations in\nrealistic clinical settings. In response, we propose to guide the reasoning\nprocess with clinical expertise and consistently improves LLM diagnostic\nperformance by up to 10% in Macro-F1 score. We aim to build an infrastructure\nfor clinical depression assessment from both data and algorithmic perspectives,\nenabling C-MIND to facilitate grounded and reliable research for mental\nhealthcare."
                },
                "authors": [
                    {
                        "name": "Zhuang Chen"
                    },
                    {
                        "name": "Guanqun Bi"
                    },
                    {
                        "name": "Wen Zhang"
                    },
                    {
                        "name": "Jiawei Hu"
                    },
                    {
                        "name": "Aoyun Wang"
                    },
                    {
                        "name": "Xiyao Xiao"
                    },
                    {
                        "name": "Kun Feng"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04530v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04530v2",
                "updated": "2025-08-07T06:14:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    6,
                    14,
                    50,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-06T15:12:05Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    15,
                    12,
                    5,
                    2,
                    218,
                    0
                ],
                "title": "Balancing Stylization and Truth via Disentangled Representation Steering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Balancing Stylization and Truth via Disentangled Representation Steering"
                },
                "summary": "Generating stylized large language model (LLM) responses via representation\nediting is a promising way for fine-grained output control. However, there\nexists an inherent trade-off: imposing a distinctive style often degrades\ntruthfulness. Existing representation editing methods, by naively injecting\nstyle signals, overlook this collateral impact and frequently contaminate the\nmodel's core truthfulness representations, resulting in reduced answer\ncorrectness. We term this phenomenon stylization-induced truthfulness collapse.\nWe attribute this issue to latent coupling between style and truth directions\nin certain key attention heads, and propose StyliTruth, a mechanism that\npreserves stylization while keeping truthfulness intact. StyliTruth separates\nthe style-relevant and truth-relevant subspaces in the model's representation\nspace via an orthogonal deflation process. This decomposition enables\nindependent control of style and truth in their own subspaces, minimizing\ninterference. By designing adaptive, token-level steering vectors within each\nsubspace, we dynamically and precisely control the generation process to\nmaintain both stylistic fidelity and truthfulness. We validate our method on\nmultiple styles and languages. Extensive experiments and analyses show that\nStyliTruth significantly reduces stylization-induced truthfulness collapse and\noutperforms existing inference-time intervention methods in balancing style\nadherence with truthfulness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating stylized large language model (LLM) responses via representation\nediting is a promising way for fine-grained output control. However, there\nexists an inherent trade-off: imposing a distinctive style often degrades\ntruthfulness. Existing representation editing methods, by naively injecting\nstyle signals, overlook this collateral impact and frequently contaminate the\nmodel's core truthfulness representations, resulting in reduced answer\ncorrectness. We term this phenomenon stylization-induced truthfulness collapse.\nWe attribute this issue to latent coupling between style and truth directions\nin certain key attention heads, and propose StyliTruth, a mechanism that\npreserves stylization while keeping truthfulness intact. StyliTruth separates\nthe style-relevant and truth-relevant subspaces in the model's representation\nspace via an orthogonal deflation process. This decomposition enables\nindependent control of style and truth in their own subspaces, minimizing\ninterference. By designing adaptive, token-level steering vectors within each\nsubspace, we dynamically and precisely control the generation process to\nmaintain both stylistic fidelity and truthfulness. We validate our method on\nmultiple styles and languages. Extensive experiments and analyses show that\nStyliTruth significantly reduces stylization-induced truthfulness collapse and\noutperforms existing inference-time intervention methods in balancing style\nadherence with truthfulness."
                },
                "authors": [
                    {
                        "name": "Chenglei Shen"
                    },
                    {
                        "name": "Zhongxiang Sun"
                    },
                    {
                        "name": "Teng Shi"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Jun Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Xu"
                },
                "author": "Jun Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04530v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04530v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15787v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15787v4",
                "updated": "2025-08-06T15:09:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    15,
                    9,
                    52,
                    2,
                    218,
                    0
                ],
                "published": "2025-06-18T18:10:30Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    18,
                    10,
                    30,
                    2,
                    169,
                    0
                ],
                "title": "SLR: Automated Synthesis for Scalable Logical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLR: Automated Synthesis for Scalable Logical Reasoning"
                },
                "summary": "We introduce SLR, an end-to-end framework for systematic evaluation and\ntraining of Large Language Models (LLMs) via Scalable Logical Reasoning. Given\na user's task specification, SLR automatically synthesizes (i) an instruction\nprompt for an inductive reasoning task, (ii) a validation program, executable\non model outputs to provide verifiable rewards, and (iii) the latent\nground-truth rule. This process is fully automated, scalable, requires no human\nannotations, and offers precise control over task difficulty. Using SLR, we\ncreate SLR-Bench, a benchmark comprising 19k prompts organized into 20\ncurriculum levels that progressively increase in relational, arithmetic, and\nrecursive complexity. Large-scale evaluation reveals that contemporary LLMs\nreadily produce syntactically valid rules, yet often fail at correct logical\ninference. Recent reasoning LLMs demonstrate improved performance but incur\nvery high test-time computation, with costs exceeding $300 for just 1,000\nprompts. Finally, curriculum learning via SLR doubles Llama-3-8B accuracy on\nSLR-Bench, achieving parity with Gemini-Flash-Thinking at a fraction of\ncomputational cost. Moreover, these reasoning capabilities generalize to a wide\nrange of established benchmarks, underscoring the effectiveness of SLR for\ndownstream reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SLR, an end-to-end framework for systematic evaluation and\ntraining of Large Language Models (LLMs) via Scalable Logical Reasoning. Given\na user's task specification, SLR automatically synthesizes (i) an instruction\nprompt for an inductive reasoning task, (ii) a validation program, executable\non model outputs to provide verifiable rewards, and (iii) the latent\nground-truth rule. This process is fully automated, scalable, requires no human\nannotations, and offers precise control over task difficulty. Using SLR, we\ncreate SLR-Bench, a benchmark comprising 19k prompts organized into 20\ncurriculum levels that progressively increase in relational, arithmetic, and\nrecursive complexity. Large-scale evaluation reveals that contemporary LLMs\nreadily produce syntactically valid rules, yet often fail at correct logical\ninference. Recent reasoning LLMs demonstrate improved performance but incur\nvery high test-time computation, with costs exceeding $300 for just 1,000\nprompts. Finally, curriculum learning via SLR doubles Llama-3-8B accuracy on\nSLR-Bench, achieving parity with Gemini-Flash-Thinking at a fraction of\ncomputational cost. Moreover, these reasoning capabilities generalize to a wide\nrange of established benchmarks, underscoring the effectiveness of SLR for\ndownstream reasoning."
                },
                "authors": [
                    {
                        "name": "Lukas Helff"
                    },
                    {
                        "name": "Ahmad Omar"
                    },
                    {
                        "name": "Felix Friedrich"
                    },
                    {
                        "name": "Antonia Wst"
                    },
                    {
                        "name": "Hikaru Shindo"
                    },
                    {
                        "name": "Rupert Mitchell"
                    },
                    {
                        "name": "Tim Woydt"
                    },
                    {
                        "name": "Patrick Schramowski"
                    },
                    {
                        "name": "Wolfgang Stammer"
                    },
                    {
                        "name": "Kristian Kersting"
                    }
                ],
                "author_detail": {
                    "name": "Kristian Kersting"
                },
                "author": "Kristian Kersting",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15787v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15787v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04524v1",
                "updated": "2025-08-06T15:08:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    15,
                    8,
                    16,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T15:08:16Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    15,
                    8,
                    16,
                    2,
                    218,
                    0
                ],
                "title": "RAIDX: A Retrieval-Augmented Generation and GRPO Reinforcement Learning\n  Framework for Explainable Deepfake Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAIDX: A Retrieval-Augmented Generation and GRPO Reinforcement Learning\n  Framework for Explainable Deepfake Detection"
                },
                "summary": "The rapid advancement of AI-generation models has enabled the creation of\nhyperrealistic imagery, posing ethical risks through widespread misinformation.\nCurrent deepfake detection methods, categorized as face specific detectors or\ngeneral AI-generated detectors, lack transparency by framing detection as a\nclassification task without explaining decisions. While several LLM-based\napproaches offer explainability, they suffer from coarse-grained analyses and\ndependency on labor-intensive annotations. This paper introduces RAIDX\n(Retrieval-Augmented Image Deepfake Detection and Explainability), a novel\ndeepfake detection framework integrating Retrieval-Augmented Generation (RAG)\nand Group Relative Policy Optimization (GRPO) to enhance detection accuracy and\ndecision explainability. Specifically, RAIDX leverages RAG to incorporate\nexternal knowledge for improved detection accuracy and employs GRPO to\nautonomously generate fine-grained textual explanations and saliency maps,\neliminating the need for extensive manual annotations. Experiments on multiple\nbenchmarks demonstrate RAIDX's effectiveness in identifying real or fake, and\nproviding interpretable rationales in both textual descriptions and saliency\nmaps, achieving state-of-the-art detection performance while advancing\ntransparency in deepfake identification. RAIDX represents the first unified\nframework to synergize RAG and GRPO, addressing critical gaps in accuracy and\nexplainability. Our code and models will be publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of AI-generation models has enabled the creation of\nhyperrealistic imagery, posing ethical risks through widespread misinformation.\nCurrent deepfake detection methods, categorized as face specific detectors or\ngeneral AI-generated detectors, lack transparency by framing detection as a\nclassification task without explaining decisions. While several LLM-based\napproaches offer explainability, they suffer from coarse-grained analyses and\ndependency on labor-intensive annotations. This paper introduces RAIDX\n(Retrieval-Augmented Image Deepfake Detection and Explainability), a novel\ndeepfake detection framework integrating Retrieval-Augmented Generation (RAG)\nand Group Relative Policy Optimization (GRPO) to enhance detection accuracy and\ndecision explainability. Specifically, RAIDX leverages RAG to incorporate\nexternal knowledge for improved detection accuracy and employs GRPO to\nautonomously generate fine-grained textual explanations and saliency maps,\neliminating the need for extensive manual annotations. Experiments on multiple\nbenchmarks demonstrate RAIDX's effectiveness in identifying real or fake, and\nproviding interpretable rationales in both textual descriptions and saliency\nmaps, achieving state-of-the-art detection performance while advancing\ntransparency in deepfake identification. RAIDX represents the first unified\nframework to synergize RAG and GRPO, addressing critical gaps in accuracy and\nexplainability. Our code and models will be publicly available."
                },
                "authors": [
                    {
                        "name": "Tianxiao Li"
                    },
                    {
                        "name": "Zhenglin Huang"
                    },
                    {
                        "name": "Haiquan Wen"
                    },
                    {
                        "name": "Yiwei He"
                    },
                    {
                        "name": "Shuchang Lyu"
                    },
                    {
                        "name": "Baoyuan Wu"
                    },
                    {
                        "name": "Guangliang Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Guangliang Cheng"
                },
                "author": "Guangliang Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.07543v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.07543v5",
                "updated": "2025-08-06T15:00:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    15,
                    0,
                    18,
                    2,
                    218,
                    0
                ],
                "published": "2023-10-11T14:47:51Z",
                "published_parsed": [
                    2023,
                    10,
                    11,
                    14,
                    47,
                    51,
                    2,
                    284,
                    0
                ],
                "title": "Ordinal Characterization of Similarity Judgments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ordinal Characterization of Similarity Judgments"
                },
                "summary": "Characterizing judgments of similarity within a perceptual or semantic\ndomain, and making inferences about the underlying structure of this domain\nfrom these judgments, has an increasingly important role in cognitive and\nsystems neuroscience. We present a new framework for this purpose that makes\nlimited assumptions about how perceptual distances are converted into\nsimilarity judgments. The approach starts from a dataset of empirical judgments\nof relative similarities: the fraction of times that a subject chooses one of\ntwo comparison stimuli to be more similar to a reference stimulus. These\nempirical judgments provide Bayesian estimates of underling choice\nprobabilities. From these estimates, we derive indices that characterize the\nset of judgments in three ways: compatibility with a symmetric dis-similarity,\ncompatibility with an ultrametric space, and compatibility with an additive\ntree. Each of the indices is derived from rank-order relationships among the\nchoice probabilities that, as we show, are necessary and sufficient for local\nconsistency with the three respective characteristics. We illustrate this\napproach with simulations and example psychophysical datasets of dis-similarity\njudgments in several visual domains and provide code that implements the\nanalyses at https://github.com/jvlab/simrank.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing judgments of similarity within a perceptual or semantic\ndomain, and making inferences about the underlying structure of this domain\nfrom these judgments, has an increasingly important role in cognitive and\nsystems neuroscience. We present a new framework for this purpose that makes\nlimited assumptions about how perceptual distances are converted into\nsimilarity judgments. The approach starts from a dataset of empirical judgments\nof relative similarities: the fraction of times that a subject chooses one of\ntwo comparison stimuli to be more similar to a reference stimulus. These\nempirical judgments provide Bayesian estimates of underling choice\nprobabilities. From these estimates, we derive indices that characterize the\nset of judgments in three ways: compatibility with a symmetric dis-similarity,\ncompatibility with an ultrametric space, and compatibility with an additive\ntree. Each of the indices is derived from rank-order relationships among the\nchoice probabilities that, as we show, are necessary and sufficient for local\nconsistency with the three respective characteristics. We illustrate this\napproach with simulations and example psychophysical datasets of dis-similarity\njudgments in several visual domains and provide code that implements the\nanalyses at https://github.com/jvlab/simrank."
                },
                "authors": [
                    {
                        "name": "Jonathan D. Victor"
                    },
                    {
                        "name": "Guillermo Aguilar"
                    },
                    {
                        "name": "Suniyya A. Waraich"
                    }
                ],
                "author_detail": {
                    "name": "Suniyya A. Waraich"
                },
                "author": "Suniyya A. Waraich",
                "arxiv_doi": "10.46298/mna.12457",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.46298/mna.12457",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.07543v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.07543v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "64 pages, 16 figures + 7 supplementary figures. Final version\n  accepted for publication in Mathematical Neuroscience and Applications",
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "91E30 (Primary) 62P15, 92-08, 92-10, 51-08 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02663v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02663v2",
                "updated": "2025-08-06T14:55:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    55,
                    31,
                    2,
                    218,
                    0
                ],
                "published": "2025-07-03T14:24:26Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    24,
                    26,
                    3,
                    184,
                    0
                ],
                "title": "Think How to Think: Mitigating Overthinking with Autonomous Difficulty\n  Cognition in Large Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think How to Think: Mitigating Overthinking with Autonomous Difficulty\n  Cognition in Large Reasoning Models"
                },
                "summary": "Recent Large Reasoning Models (LRMs) excel at complex reasoning tasks but\noften suffer from overthinking, generating overly long and redundant reasoning\ntrajectories. To explore its essence, our empirical analysis reveals that LRMs\nare primarily limited to recognizing task properties (i.e., difficulty levels)\nlike humans before solving the problem, leading to a one-size-fits-all\nreasoning process. Inspired by this, a pressing and natural question emerges:\nCan we explicitly bootstrap such ability to alleviate overthinking in LRMs? In\nthis paper, we propose Think-How-to-Think (TH2T), a novel two-stage fine-tuning\nstrategy that progressively inspires LRMs' difficulty cognition and redundancy\ncognition of LRMs. Specifically, we first inject difficulty hypnosis into\noutput prefixes to guide the model toward adaptive reasoning depth, trained on\na hybrid dataset mixing short and long reasoning paths. Then, we incorporate\nredundancy hypnosis, which supervises the intermediate reasoning steps to\nidentify and eliminate unnecessary reasoning patterns. Experiments on\n7B/14B/32B models demonstrate that TH2T significantly reduces inference costs\nby over 70% on easy tasks and 40% on hard tasks while maintaining performance\nstability. The resulting outputs exhibit clear signs of difficulty-aware\ncapabilities and reduced redundancy (e.g., reflection and looping).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Large Reasoning Models (LRMs) excel at complex reasoning tasks but\noften suffer from overthinking, generating overly long and redundant reasoning\ntrajectories. To explore its essence, our empirical analysis reveals that LRMs\nare primarily limited to recognizing task properties (i.e., difficulty levels)\nlike humans before solving the problem, leading to a one-size-fits-all\nreasoning process. Inspired by this, a pressing and natural question emerges:\nCan we explicitly bootstrap such ability to alleviate overthinking in LRMs? In\nthis paper, we propose Think-How-to-Think (TH2T), a novel two-stage fine-tuning\nstrategy that progressively inspires LRMs' difficulty cognition and redundancy\ncognition of LRMs. Specifically, we first inject difficulty hypnosis into\noutput prefixes to guide the model toward adaptive reasoning depth, trained on\na hybrid dataset mixing short and long reasoning paths. Then, we incorporate\nredundancy hypnosis, which supervises the intermediate reasoning steps to\nidentify and eliminate unnecessary reasoning patterns. Experiments on\n7B/14B/32B models demonstrate that TH2T significantly reduces inference costs\nby over 70% on easy tasks and 40% on hard tasks while maintaining performance\nstability. The resulting outputs exhibit clear signs of difficulty-aware\ncapabilities and reduced redundancy (e.g., reflection and looping)."
                },
                "authors": [
                    {
                        "name": "Yongjiang Liu"
                    },
                    {
                        "name": "Haoxi Li"
                    },
                    {
                        "name": "Xiaosong Ma"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Song Guo"
                    }
                ],
                "author_detail": {
                    "name": "Song Guo"
                },
                "author": "Song Guo",
                "arxiv_comment": "19 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02663v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02663v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22716v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22716v2",
                "updated": "2025-08-06T14:53:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    53,
                    31,
                    2,
                    218,
                    0
                ],
                "published": "2025-07-30T14:29:44Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    14,
                    29,
                    44,
                    2,
                    211,
                    0
                ],
                "title": "From Sufficiency to Reflection: Reinforcement-Guided Thinking Quality in\n  Retrieval-Augmented Reasoning for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Sufficiency to Reflection: Reinforcement-Guided Thinking Quality in\n  Retrieval-Augmented Reasoning for LLMs"
                },
                "summary": "Reinforcement learning-based retrieval-augmented generation (RAG) methods\nenhance the reasoning abilities of large language models (LLMs). However, most\nrely only on final-answer rewards, overlooking intermediate reasoning quality.\nThis paper analyzes existing RAG reasoning models and identifies three main\nfailure patterns: (1) information insufficiency, meaning the model fails to\nretrieve adequate support; (2) faulty reasoning, where logical or content-level\nflaws appear despite sufficient information; and (3) answer-reasoning\ninconsistency, where a valid reasoning chain leads to a mismatched final\nanswer. We propose TIRESRAG-R1, a novel framework using a\nthink-retrieve-reflect process and a multi-dimensional reward system to improve\nreasoning and stability. TIRESRAG-R1 introduces: (1) a sufficiency reward to\nencourage thorough retrieval; (2) a reasoning quality reward to assess the\nrationality and accuracy of the reasoning chain; and (3) a reflection reward to\ndetect and revise errors. It also employs a difficulty-aware reweighting\nstrategy and training sample filtering to boost performance on complex tasks.\nExperiments on four multi-hop QA datasets show that TIRESRAG-R1 outperforms\nprior RAG methods and generalizes well to single-hop tasks. The code and data\nare available at: https://github.com/probe2/TIRESRAG-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning-based retrieval-augmented generation (RAG) methods\nenhance the reasoning abilities of large language models (LLMs). However, most\nrely only on final-answer rewards, overlooking intermediate reasoning quality.\nThis paper analyzes existing RAG reasoning models and identifies three main\nfailure patterns: (1) information insufficiency, meaning the model fails to\nretrieve adequate support; (2) faulty reasoning, where logical or content-level\nflaws appear despite sufficient information; and (3) answer-reasoning\ninconsistency, where a valid reasoning chain leads to a mismatched final\nanswer. We propose TIRESRAG-R1, a novel framework using a\nthink-retrieve-reflect process and a multi-dimensional reward system to improve\nreasoning and stability. TIRESRAG-R1 introduces: (1) a sufficiency reward to\nencourage thorough retrieval; (2) a reasoning quality reward to assess the\nrationality and accuracy of the reasoning chain; and (3) a reflection reward to\ndetect and revise errors. It also employs a difficulty-aware reweighting\nstrategy and training sample filtering to boost performance on complex tasks.\nExperiments on four multi-hop QA datasets show that TIRESRAG-R1 outperforms\nprior RAG methods and generalizes well to single-hop tasks. The code and data\nare available at: https://github.com/probe2/TIRESRAG-R1."
                },
                "authors": [
                    {
                        "name": "Jie He"
                    },
                    {
                        "name": "Victor Gutirrez-Basulto"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Z. Pan"
                },
                "author": "Jeff Z. Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22716v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22716v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04495v1",
                "updated": "2025-08-06T14:44:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    44,
                    23,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T14:44:23Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    44,
                    23,
                    2,
                    218,
                    0
                ],
                "title": "Causal Reflection with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Reflection with Language Models"
                },
                "summary": "While LLMs exhibit impressive fluency and factual recall, they struggle with\nrobust causal reasoning, often relying on spurious correlations and brittle\npatterns. Similarly, traditional Reinforcement Learning agents also lack causal\nunderstanding, optimizing for rewards without modeling why actions lead to\noutcomes. We introduce Causal Reflection, a framework that explicitly models\ncausality as a dynamic function over state, action, time, and perturbation,\nenabling agents to reason about delayed and nonlinear effects. Additionally, we\ndefine a formal Reflect mechanism that identifies mismatches between predicted\nand observed outcomes and generates causal hypotheses to revise the agent's\ninternal model. In this architecture, LLMs serve not as black-box reasoners,\nbut as structured inference engines translating formal causal outputs into\nnatural language explanations and counterfactuals. Our framework lays the\ntheoretical groundwork for Causal Reflective agents that can adapt,\nself-correct, and communicate causal understanding in evolving environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While LLMs exhibit impressive fluency and factual recall, they struggle with\nrobust causal reasoning, often relying on spurious correlations and brittle\npatterns. Similarly, traditional Reinforcement Learning agents also lack causal\nunderstanding, optimizing for rewards without modeling why actions lead to\noutcomes. We introduce Causal Reflection, a framework that explicitly models\ncausality as a dynamic function over state, action, time, and perturbation,\nenabling agents to reason about delayed and nonlinear effects. Additionally, we\ndefine a formal Reflect mechanism that identifies mismatches between predicted\nand observed outcomes and generates causal hypotheses to revise the agent's\ninternal model. In this architecture, LLMs serve not as black-box reasoners,\nbut as structured inference engines translating formal causal outputs into\nnatural language explanations and counterfactuals. Our framework lays the\ntheoretical groundwork for Causal Reflective agents that can adapt,\nself-correct, and communicate causal understanding in evolving environments."
                },
                "authors": [
                    {
                        "name": "Abi Aryan"
                    },
                    {
                        "name": "Zac Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zac Liu"
                },
                "author": "Zac Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.13330v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.13330v3",
                "updated": "2025-08-06T14:41:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    41,
                    42,
                    2,
                    218,
                    0
                ],
                "published": "2024-01-24T09:48:12Z",
                "published_parsed": [
                    2024,
                    1,
                    24,
                    9,
                    48,
                    12,
                    2,
                    24,
                    0
                ],
                "title": "NACHOS: Neural Architecture Search for Hardware Constrained Early Exit\n  Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NACHOS: Neural Architecture Search for Hardware Constrained Early Exit\n  Neural Networks"
                },
                "summary": "Early Exit Neural Networks (EENNs) endow astandard Deep Neural Network (DNN)\nwith Early Exit Classifiers (EECs), to provide predictions at intermediate\npoints of the processing when enough confidence in classification is achieved.\nThis leads to many benefits in terms of effectiveness and efficiency.\nCurrently, the design of EENNs is carried out manually by experts, a complex\nand time-consuming task that requires accounting for many aspects, including\nthe correct placement, the thresholding, and the computational overhead of the\nEECs. For this reason, the research is exploring the use of Neural Architecture\nSearch (NAS) to automatize the design of EENNs. Currently, few comprehensive\nNAS solutions for EENNs have been proposed in the literature, and a fully\nautomated, joint design strategy taking into consideration both the backbone\nand the EECs remains an open problem. To this end, this work presents Neural\nArchitecture Search for Hardware Constrained Early Exit Neural Networks\n(NACHOS), the first NAS framework for the design of optimal EENNs satisfying\nconstraints on the accuracy and the number of Multiply and Accumulate (MAC)\noperations performed by the EENNs at inference time. In particular, this\nprovides the joint design of backbone and EECs to select a set of admissible\n(i.e., respecting the constraints) Pareto Optimal Solutions in terms of best\ntradeoff between the accuracy and number of MACs. The results show that the\nmodels designed by NACHOS are competitive with the state-of-the-art EENNs.\nAdditionally, this work investigates the effectiveness of two novel\nregularization terms designed for the optimization of the auxiliary classifiers\nof the EENN",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early Exit Neural Networks (EENNs) endow astandard Deep Neural Network (DNN)\nwith Early Exit Classifiers (EECs), to provide predictions at intermediate\npoints of the processing when enough confidence in classification is achieved.\nThis leads to many benefits in terms of effectiveness and efficiency.\nCurrently, the design of EENNs is carried out manually by experts, a complex\nand time-consuming task that requires accounting for many aspects, including\nthe correct placement, the thresholding, and the computational overhead of the\nEECs. For this reason, the research is exploring the use of Neural Architecture\nSearch (NAS) to automatize the design of EENNs. Currently, few comprehensive\nNAS solutions for EENNs have been proposed in the literature, and a fully\nautomated, joint design strategy taking into consideration both the backbone\nand the EECs remains an open problem. To this end, this work presents Neural\nArchitecture Search for Hardware Constrained Early Exit Neural Networks\n(NACHOS), the first NAS framework for the design of optimal EENNs satisfying\nconstraints on the accuracy and the number of Multiply and Accumulate (MAC)\noperations performed by the EENNs at inference time. In particular, this\nprovides the joint design of backbone and EECs to select a set of admissible\n(i.e., respecting the constraints) Pareto Optimal Solutions in terms of best\ntradeoff between the accuracy and number of MACs. The results show that the\nmodels designed by NACHOS are competitive with the state-of-the-art EENNs.\nAdditionally, this work investigates the effectiveness of two novel\nregularization terms designed for the optimization of the auxiliary classifiers\nof the EENN"
                },
                "authors": [
                    {
                        "name": "Matteo Gambella"
                    },
                    {
                        "name": "Jary Pomponi"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Manuel Roveri"
                    }
                ],
                "author_detail": {
                    "name": "Manuel Roveri"
                },
                "author": "Manuel Roveri",
                "arxiv_doi": "10.1109/TNNLS.2025.3588558",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TNNLS.2025.3588558",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.13330v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.13330v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in IEEE Transactions on Neural Networks and Learning\n  Systems (TNNLS) 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04482v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04482v1",
                "updated": "2025-08-06T14:33:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    33,
                    45,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T14:33:45Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    33,
                    45,
                    2,
                    218,
                    0
                ],
                "title": "OS Agents: A Survey on MLLM-based Agents for General Computing Devices\n  Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OS Agents: A Survey on MLLM-based Agents for General Computing Devices\n  Use"
                },
                "summary": "The dream to create AI assistants as capable and versatile as the fictional\nJ.A.R.V.I.S from Iron Man has long captivated imaginations. With the evolution\nof (multi-modal) large language models ((M)LLMs), this dream is closer to\nreality, as (M)LLM-based Agents using computing devices (e.g., computers and\nmobile phones) by operating within the environments and interfaces (e.g.,\nGraphical User Interface (GUI)) provided by operating systems (OS) to automate\ntasks have significantly advanced. This paper presents a comprehensive survey\nof these advanced agents, designated as OS Agents. We begin by elucidating the\nfundamentals of OS Agents, exploring their key components including the\nenvironment, observation space, and action space, and outlining essential\ncapabilities such as understanding, planning, and grounding. We then examine\nmethodologies for constructing OS Agents, focusing on domain-specific\nfoundation models and agent frameworks. A detailed review of evaluation\nprotocols and benchmarks highlights how OS Agents are assessed across diverse\ntasks. Finally, we discuss current challenges and identify promising directions\nfor future research, including safety and privacy, personalization and\nself-evolution. This survey aims to consolidate the state of OS Agents\nresearch, providing insights to guide both academic inquiry and industrial\ndevelopment. An open-source GitHub repository is maintained as a dynamic\nresource to foster further innovation in this field. We present a 9-page\nversion of our work, accepted by ACL 2025, to provide a concise overview to the\ndomain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The dream to create AI assistants as capable and versatile as the fictional\nJ.A.R.V.I.S from Iron Man has long captivated imaginations. With the evolution\nof (multi-modal) large language models ((M)LLMs), this dream is closer to\nreality, as (M)LLM-based Agents using computing devices (e.g., computers and\nmobile phones) by operating within the environments and interfaces (e.g.,\nGraphical User Interface (GUI)) provided by operating systems (OS) to automate\ntasks have significantly advanced. This paper presents a comprehensive survey\nof these advanced agents, designated as OS Agents. We begin by elucidating the\nfundamentals of OS Agents, exploring their key components including the\nenvironment, observation space, and action space, and outlining essential\ncapabilities such as understanding, planning, and grounding. We then examine\nmethodologies for constructing OS Agents, focusing on domain-specific\nfoundation models and agent frameworks. A detailed review of evaluation\nprotocols and benchmarks highlights how OS Agents are assessed across diverse\ntasks. Finally, we discuss current challenges and identify promising directions\nfor future research, including safety and privacy, personalization and\nself-evolution. This survey aims to consolidate the state of OS Agents\nresearch, providing insights to guide both academic inquiry and industrial\ndevelopment. An open-source GitHub repository is maintained as a dynamic\nresource to foster further innovation in this field. We present a 9-page\nversion of our work, accepted by ACL 2025, to provide a concise overview to the\ndomain."
                },
                "authors": [
                    {
                        "name": "Xueyu Hu"
                    },
                    {
                        "name": "Tao Xiong"
                    },
                    {
                        "name": "Biao Yi"
                    },
                    {
                        "name": "Zishu Wei"
                    },
                    {
                        "name": "Ruixuan Xiao"
                    },
                    {
                        "name": "Yurun Chen"
                    },
                    {
                        "name": "Jiasheng Ye"
                    },
                    {
                        "name": "Meiling Tao"
                    },
                    {
                        "name": "Xiangxin Zhou"
                    },
                    {
                        "name": "Ziyu Zhao"
                    },
                    {
                        "name": "Yuhuai Li"
                    },
                    {
                        "name": "Shengze Xu"
                    },
                    {
                        "name": "Shenzhi Wang"
                    },
                    {
                        "name": "Xinchen Xu"
                    },
                    {
                        "name": "Shuofei Qiao"
                    },
                    {
                        "name": "Zhaokai Wang"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Tieyong Zeng"
                    },
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Jiwei Li"
                    },
                    {
                        "name": "Yuchen Eleanor Jiang"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Keting Yin"
                    },
                    {
                        "name": "Zhou Zhao"
                    },
                    {
                        "name": "Hongxia Yang"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Shengyu Zhang"
                    },
                    {
                        "name": "Fei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Wu"
                },
                "author": "Fei Wu",
                "arxiv_comment": "ACL 2025 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04482v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04482v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04474v1",
                "updated": "2025-08-06T14:25:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    25,
                    5,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T14:25:05Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    25,
                    5,
                    2,
                    218,
                    0
                ],
                "title": "TRAIL: Joint Inference and Refinement of Knowledge Graphs with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRAIL: Joint Inference and Refinement of Knowledge Graphs with Large\n  Language Models"
                },
                "summary": "Recent advances in large language models (LLMs) have unlocked powerful\nreasoning and decision-making capabilities. However, their inherent dependence\non static parametric memory fundamentally limits their adaptability, factual\naccuracy, and interpretability in knowledge-intensive scenarios. Knowledge\ngraphs (KGs), as structured repositories of explicit relational knowledge,\noffer a promising approach for augmenting LLMs with external, interpretable\nmemory. Nevertheless, most existing methods that combine LLMs with KGs treat\nreasoning and knowledge updating as separate processes, resulting in suboptimal\nutilization of new information and hindering real-time updates. In this work,\nwe propose TRAIL: a novel, unified framework for Thinking, Reasoning, And\nIncremental Learning that couples joint inference and dynamic KG refinement\nwith large language models. TRAIL enables LLM agents to iteratively explore,\nupdate, and refine knowledge graphs during the reasoning process, employing a\nconfidence-driven mechanism for the generation, validation, and pruning of new\nfacts. This plug-and-play architecture facilitates seamless integration with\nvarious LLMs, supporting continual adaptation without the need for retraining.\nExtensive experiments on multiple benchmarks demonstrate that TRAIL outperforms\nexisting KG-augmented and retrieval-augmented LLM baselines by 3% to 13%. More\nimportantly, these results represent a significant step toward developing\nadaptive, memory-augmented language models capable of continual learning and\nreliable, transparent reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have unlocked powerful\nreasoning and decision-making capabilities. However, their inherent dependence\non static parametric memory fundamentally limits their adaptability, factual\naccuracy, and interpretability in knowledge-intensive scenarios. Knowledge\ngraphs (KGs), as structured repositories of explicit relational knowledge,\noffer a promising approach for augmenting LLMs with external, interpretable\nmemory. Nevertheless, most existing methods that combine LLMs with KGs treat\nreasoning and knowledge updating as separate processes, resulting in suboptimal\nutilization of new information and hindering real-time updates. In this work,\nwe propose TRAIL: a novel, unified framework for Thinking, Reasoning, And\nIncremental Learning that couples joint inference and dynamic KG refinement\nwith large language models. TRAIL enables LLM agents to iteratively explore,\nupdate, and refine knowledge graphs during the reasoning process, employing a\nconfidence-driven mechanism for the generation, validation, and pruning of new\nfacts. This plug-and-play architecture facilitates seamless integration with\nvarious LLMs, supporting continual adaptation without the need for retraining.\nExtensive experiments on multiple benchmarks demonstrate that TRAIL outperforms\nexisting KG-augmented and retrieval-augmented LLM baselines by 3% to 13%. More\nimportantly, these results represent a significant step toward developing\nadaptive, memory-augmented language models capable of continual learning and\nreliable, transparent reasoning."
                },
                "authors": [
                    {
                        "name": "Xinkui Zhao"
                    },
                    {
                        "name": "Haode Li"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Guanjie Cheng"
                    },
                    {
                        "name": "Yueshen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Yueshen Xu"
                },
                "author": "Yueshen Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20951v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20951v3",
                "updated": "2025-08-07T01:25:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    1,
                    25,
                    26,
                    3,
                    219,
                    0
                ],
                "published": "2025-05-27T09:45:00Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    9,
                    45,
                    0,
                    1,
                    147,
                    0
                ],
                "title": "DSOcc: Leveraging Depth Awareness and Semantic Aid to Boost Camera-Based\n  3D Semantic Occupancy Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DSOcc: Leveraging Depth Awareness and Semantic Aid to Boost Camera-Based\n  3D Semantic Occupancy Prediction"
                },
                "summary": "Camera-based 3D semantic occupancy prediction offers an efficient and\ncost-effective solution for perceiving surrounding scenes in autonomous\ndriving. However, existing works rely on explicit occupancy state inference,\nleading to numerous incorrect feature assignments, and insufficient samples\nrestrict the learning of occupancy class inference. To address these\nchallenges, we propose leveraging Depth awareness and Semantic aid to boost\ncamera-based 3D semantic Occupancy prediction (DSOcc). We jointly perform\noccupancy state and occupancy class inference, where soft occupancy confidence\nis calculated by non-learning method and multiplied with image features to make\nvoxels aware of depth, enabling adaptive implicit occupancy state inference.\nInstead of enhancing feature learning, we directly utilize well-trained image\nsemantic segmentation and fuse multiple frames with their occupancy\nprobabilities to aid occupancy class inference, thereby enhancing robustness.\nExperimental results demonstrate that DSOcc achieves state-of-the-art\nperformance on the SemanticKITTI dataset among camera-based methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Camera-based 3D semantic occupancy prediction offers an efficient and\ncost-effective solution for perceiving surrounding scenes in autonomous\ndriving. However, existing works rely on explicit occupancy state inference,\nleading to numerous incorrect feature assignments, and insufficient samples\nrestrict the learning of occupancy class inference. To address these\nchallenges, we propose leveraging Depth awareness and Semantic aid to boost\ncamera-based 3D semantic Occupancy prediction (DSOcc). We jointly perform\noccupancy state and occupancy class inference, where soft occupancy confidence\nis calculated by non-learning method and multiplied with image features to make\nvoxels aware of depth, enabling adaptive implicit occupancy state inference.\nInstead of enhancing feature learning, we directly utilize well-trained image\nsemantic segmentation and fuse multiple frames with their occupancy\nprobabilities to aid occupancy class inference, thereby enhancing robustness.\nExperimental results demonstrate that DSOcc achieves state-of-the-art\nperformance on the SemanticKITTI dataset among camera-based methods."
                },
                "authors": [
                    {
                        "name": "Naiyu Fang"
                    },
                    {
                        "name": "Zheyuan Zhou"
                    },
                    {
                        "name": "Kang Wang"
                    },
                    {
                        "name": "Ruibo Li"
                    },
                    {
                        "name": "Lemiao Qiu"
                    },
                    {
                        "name": "Shuyou Zhang"
                    },
                    {
                        "name": "Zhe Wang"
                    },
                    {
                        "name": "Guosheng Lin"
                    }
                ],
                "author_detail": {
                    "name": "Guosheng Lin"
                },
                "author": "Guosheng Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20951v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20951v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04462v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04462v1",
                "updated": "2025-08-06T14:02:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    2,
                    10,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T14:02:10Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    2,
                    10,
                    2,
                    218,
                    0
                ],
                "title": "CARD: Cache-Assisted Parallel Speculative Decoding for Efficient Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CARD: Cache-Assisted Parallel Speculative Decoding for Efficient Large\n  Language Model Inference"
                },
                "summary": "Speculative decoding (SD), where an extra draft model first provides multiple\ndraft tokens and the original target model then verifies these tokens in\nparallel, has shown great power for LLM inference acceleration. However,\nexisting SD methods must adhere to the 'draft-then-verify' paradigm, which\nforces drafting and verification processes to execute sequentially during SD,\nresulting in inefficient inference performance and limiting the size of the\ndraft model. Furthermore, once a single token in the candidate sequence is\nrejected during the drafting process, all subsequent candidate tokens must be\ndiscarded, leading to inefficient drafting. To address these challenges, we\npropose a cache-based parallel speculative decoding framework employing a\n'query-and-correct' paradigm. Specifically, CARD decouples drafting and\nverification: the draft model generates candidate tokens to populate a shared\ncache, while the target model concurrently rectifies the draft model's\ngeneration direction. This effectively enables the target model to perform\ninference at speed approaching that of the draft model. Our approach achieves\nup to 4.83 speedup over vanilla decoding without requiring fine-tuning of\neither the draft or target models. Our code is available at\nhttps://github.com/hunzhizi/CARD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding (SD), where an extra draft model first provides multiple\ndraft tokens and the original target model then verifies these tokens in\nparallel, has shown great power for LLM inference acceleration. However,\nexisting SD methods must adhere to the 'draft-then-verify' paradigm, which\nforces drafting and verification processes to execute sequentially during SD,\nresulting in inefficient inference performance and limiting the size of the\ndraft model. Furthermore, once a single token in the candidate sequence is\nrejected during the drafting process, all subsequent candidate tokens must be\ndiscarded, leading to inefficient drafting. To address these challenges, we\npropose a cache-based parallel speculative decoding framework employing a\n'query-and-correct' paradigm. Specifically, CARD decouples drafting and\nverification: the draft model generates candidate tokens to populate a shared\ncache, while the target model concurrently rectifies the draft model's\ngeneration direction. This effectively enables the target model to perform\ninference at speed approaching that of the draft model. Our approach achieves\nup to 4.83 speedup over vanilla decoding without requiring fine-tuning of\neither the draft or target models. Our code is available at\nhttps://github.com/hunzhizi/CARD."
                },
                "authors": [
                    {
                        "name": "Enyu Zhou"
                    },
                    {
                        "name": "Kai Sheng"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Xin He"
                    }
                ],
                "author_detail": {
                    "name": "Xin He"
                },
                "author": "Xin He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04462v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04462v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04460v1",
                "updated": "2025-08-06T13:59:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    59,
                    17,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T13:59:17Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    59,
                    17,
                    2,
                    218,
                    0
                ],
                "title": "From \"Aha Moments\" to Controllable Thinking: Toward Meta-Cognitive\n  Reasoning in Large Reasoning Models via Decoupled Reasoning and Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From \"Aha Moments\" to Controllable Thinking: Toward Meta-Cognitive\n  Reasoning in Large Reasoning Models via Decoupled Reasoning and Control"
                },
                "summary": "Large Reasoning Models (LRMs) have demonstrated a latent capacity for complex\nreasoning by spontaneously exhibiting cognitive behaviors such as step-by-step\nreasoning, reflection, and backtracking, commonly referred to as \"Aha Moments\".\nHowever, such emergent behaviors remain unregulated and uncontrolled, often\nresulting in overthinking, where the model continues generating redundant\nreasoning content even after reaching reliable conclusions. This leads to\nexcessive computational costs and increased latency, limiting the practical\ndeployment of LRMs. The root cause lies in the absence of intrinsic regulatory\nmechanisms, as current models are unable to monitor and adaptively manage their\nreasoning process to determine when to continue, backtrack, or terminate. To\naddress this issue, we propose the Meta-cognitive Reasoning Framework (MERA),\nwhich explicitly decouples the thinking process into distinct reasoning and\ncontrol components, thereby enabling the independent optimization of control\nstrategies. Specifically, MERA incorporates a takeover-based data construction\nmechanism that identifies critical decision points during reasoning and\ndelegates the creation of control signals to auxiliary LLMs, thereby enabling\nthe construction of high-quality reasoning-control data. Additionally, a\nstructured reasoning-control separation is implemented via supervised\nfine-tuning, enabling the model to generate explicit traces and acquire initial\nmeta-cognitive control capabilities. Finally, MERA employs Control-Segment\nPolicy Optimization (CSPO), which combines segment-wise Group Relative Policy\nOptimization (GRPO) with a control-masking mechanism to optimize control\nbehavior learning while minimizing interference from irrelevant content.\nExperiments on various reasoning benchmarks demonstrate that models trained\nwith MERA enhance both reasoning efficiency and accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models (LRMs) have demonstrated a latent capacity for complex\nreasoning by spontaneously exhibiting cognitive behaviors such as step-by-step\nreasoning, reflection, and backtracking, commonly referred to as \"Aha Moments\".\nHowever, such emergent behaviors remain unregulated and uncontrolled, often\nresulting in overthinking, where the model continues generating redundant\nreasoning content even after reaching reliable conclusions. This leads to\nexcessive computational costs and increased latency, limiting the practical\ndeployment of LRMs. The root cause lies in the absence of intrinsic regulatory\nmechanisms, as current models are unable to monitor and adaptively manage their\nreasoning process to determine when to continue, backtrack, or terminate. To\naddress this issue, we propose the Meta-cognitive Reasoning Framework (MERA),\nwhich explicitly decouples the thinking process into distinct reasoning and\ncontrol components, thereby enabling the independent optimization of control\nstrategies. Specifically, MERA incorporates a takeover-based data construction\nmechanism that identifies critical decision points during reasoning and\ndelegates the creation of control signals to auxiliary LLMs, thereby enabling\nthe construction of high-quality reasoning-control data. Additionally, a\nstructured reasoning-control separation is implemented via supervised\nfine-tuning, enabling the model to generate explicit traces and acquire initial\nmeta-cognitive control capabilities. Finally, MERA employs Control-Segment\nPolicy Optimization (CSPO), which combines segment-wise Group Relative Policy\nOptimization (GRPO) with a control-masking mechanism to optimize control\nbehavior learning while minimizing interference from irrelevant content.\nExperiments on various reasoning benchmarks demonstrate that models trained\nwith MERA enhance both reasoning efficiency and accuracy."
                },
                "authors": [
                    {
                        "name": "Rui Ha"
                    },
                    {
                        "name": "Chaozhuo Li"
                    },
                    {
                        "name": "Rui Pu"
                    },
                    {
                        "name": "Sen Su"
                    }
                ],
                "author_detail": {
                    "name": "Sen Su"
                },
                "author": "Sen Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04453v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04453v1",
                "updated": "2025-08-06T13:54:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    54,
                    49,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T13:54:49Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    54,
                    49,
                    2,
                    218,
                    0
                ],
                "title": "Boosting Visual Knowledge-Intensive Training for LVLMs Through\n  Causality-Driven Visual Object Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Visual Knowledge-Intensive Training for LVLMs Through\n  Causality-Driven Visual Object Completion"
                },
                "summary": "Large Vision-Language Models (LVLMs) have experienced significant\nadvancements in recent years. However, their performance still falls short in\ntasks requiring deep visual perception, such as identifying subtle differences\nbetween images. A potential cause is the scarcity of visual knowledge in\npopular instruction-tuning corpora, resulting in inadequate visual perception\nand reasoning capabilities. To address this challenge, we introduce a\nself-improvement framework grounded in a novel visual knowledge-intensive task,\n\\underline{C}ausality-driven \\underline{V}isual object \\underline{C}ompletion\n(CVC). This task requires LVLMs to infer the masked object in an image based on\nits \\textit{causal} relationships with the other visible information. We first\nobtain rich examples cheaply through our automated instance construction\npipeline, without relying on sophisticated LVLMs (\\textit{e.g.}, GPT-4V) or\nhuman assistance. Then, LVLMs effectively self-improve through trial and error\nlearning using these created instances. Our experiments demonstrate substantial\ngains across four challenging specialized tasks and four widely-used\ncomprehensive benchmarks. Especially on specialized tasks, our method achieves\nan average improvement of 5.4\\% and 4.0\\% compared to the corresponding\nbaselines when utilizing LLaVA-1.5-7B and LLaVA-1.5-13B, respectively. The code\nis available at https://github.com/XMUDeepLIT/CVC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs) have experienced significant\nadvancements in recent years. However, their performance still falls short in\ntasks requiring deep visual perception, such as identifying subtle differences\nbetween images. A potential cause is the scarcity of visual knowledge in\npopular instruction-tuning corpora, resulting in inadequate visual perception\nand reasoning capabilities. To address this challenge, we introduce a\nself-improvement framework grounded in a novel visual knowledge-intensive task,\n\\underline{C}ausality-driven \\underline{V}isual object \\underline{C}ompletion\n(CVC). This task requires LVLMs to infer the masked object in an image based on\nits \\textit{causal} relationships with the other visible information. We first\nobtain rich examples cheaply through our automated instance construction\npipeline, without relying on sophisticated LVLMs (\\textit{e.g.}, GPT-4V) or\nhuman assistance. Then, LVLMs effectively self-improve through trial and error\nlearning using these created instances. Our experiments demonstrate substantial\ngains across four challenging specialized tasks and four widely-used\ncomprehensive benchmarks. Especially on specialized tasks, our method achieves\nan average improvement of 5.4\\% and 4.0\\% compared to the corresponding\nbaselines when utilizing LLaVA-1.5-7B and LLaVA-1.5-13B, respectively. The code\nis available at https://github.com/XMUDeepLIT/CVC."
                },
                "authors": [
                    {
                        "name": "Qingguo Hu"
                    },
                    {
                        "name": "Ante Wang"
                    },
                    {
                        "name": "Jia Song"
                    },
                    {
                        "name": "Delai Qiu"
                    },
                    {
                        "name": "Qingsong Liu"
                    },
                    {
                        "name": "Jinsong Su"
                    }
                ],
                "author_detail": {
                    "name": "Jinsong Su"
                },
                "author": "Jinsong Su",
                "arxiv_comment": "Accepted by IJCAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04453v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04453v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04451v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04451v1",
                "updated": "2025-08-06T13:52:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    52,
                    0,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T13:52:00Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    52,
                    0,
                    2,
                    218,
                    0
                ],
                "title": "Automatic LLM Red Teaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic LLM Red Teaming"
                },
                "summary": "Red teaming is critical for identifying vulnerabilities and building trust in\ncurrent LLMs. However, current automated methods for Large Language Models\n(LLMs) rely on brittle prompt templates or single-turn attacks, failing to\ncapture the complex, interactive nature of real-world adversarial dialogues. We\npropose a novel paradigm: training an AI to strategically `break' another AI.\nBy formalizing red teaming as a Markov Decision Process (MDP) and employing a\nhierarchical Reinforcement Learning (RL) framework, we effectively address the\ninherent sparse reward and long-horizon challenges. Our generative agent learns\ncoherent, multi-turn attack strategies through a fine-grained, token-level harm\nreward, enabling it to uncover subtle vulnerabilities missed by existing\nbaselines. This approach sets a new state-of-the-art, fundamentally reframing\nLLM red teaming as a dynamic, trajectory-based process (rather than a one-step\ntest) essential for robust AI deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Red teaming is critical for identifying vulnerabilities and building trust in\ncurrent LLMs. However, current automated methods for Large Language Models\n(LLMs) rely on brittle prompt templates or single-turn attacks, failing to\ncapture the complex, interactive nature of real-world adversarial dialogues. We\npropose a novel paradigm: training an AI to strategically `break' another AI.\nBy formalizing red teaming as a Markov Decision Process (MDP) and employing a\nhierarchical Reinforcement Learning (RL) framework, we effectively address the\ninherent sparse reward and long-horizon challenges. Our generative agent learns\ncoherent, multi-turn attack strategies through a fine-grained, token-level harm\nreward, enabling it to uncover subtle vulnerabilities missed by existing\nbaselines. This approach sets a new state-of-the-art, fundamentally reframing\nLLM red teaming as a dynamic, trajectory-based process (rather than a one-step\ntest) essential for robust AI deployment."
                },
                "authors": [
                    {
                        "name": "Roman Belaire"
                    },
                    {
                        "name": "Arunesh Sinha"
                    },
                    {
                        "name": "Pradeep Varakantham"
                    }
                ],
                "author_detail": {
                    "name": "Pradeep Varakantham"
                },
                "author": "Pradeep Varakantham",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04451v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04451v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04448v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04448v1",
                "updated": "2025-08-06T13:48:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    48,
                    38,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T13:48:38Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    48,
                    38,
                    2,
                    218,
                    0
                ],
                "title": "Large Language Models Versus Static Code Analysis Tools: A Systematic\n  Benchmark for Vulnerability Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Versus Static Code Analysis Tools: A Systematic\n  Benchmark for Vulnerability Detection"
                },
                "summary": "Modern software relies on a multitude of automated testing and quality\nassurance tools to prevent errors, bugs and potential vulnerabilities. This\nstudy sets out to provide a head-to-head, quantitative and qualitative\nevaluation of six automated approaches: three industry-standard rule-based\nstatic code-analysis tools (SonarQube, CodeQL and Snyk Code) and three\nstate-of-the-art large language models hosted on the GitHub Models platform\n(GPT-4.1, Mistral Large and DeepSeek V3). Using a curated suite of ten\nreal-world C# projects that embed 63 vulnerabilities across common categories\nsuch as SQL injection, hard-coded secrets and outdated dependencies, we measure\nclassical detection accuracy (precision, recall, F-score), analysis latency,\nand the developer effort required to vet true positives. The language-based\nscanners achieve higher mean F-1 scores,0.797, 0.753 and 0.750, than their\nstatic counterparts, which score 0.260, 0.386 and 0.546, respectively. LLMs'\nadvantage originates from superior recall, confirming an ability to reason\nacross broader code contexts. However, this benefit comes with substantial\ntrade-offs: DeepSeek V3 exhibits the highest false-positive ratio, all language\nmodels mislocate issues at line-or-column granularity due to tokenisation\nartefacts. Overall, language models successfully rival traditional static\nanalysers in finding real vulnerabilities. Still, their noisier output and\nimprecise localisation limit their standalone use in safety-critical audits. We\ntherefore recommend a hybrid pipeline: employ language models early in\ndevelopment for broad, context-aware triage, while reserving deterministic\nrule-based scanners for high-assurance verification. The open benchmark and\nJSON-based result harness released with this paper lay a foundation for\nreproducible, practitioner-centric research into next-generation automated code\nsecurity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software relies on a multitude of automated testing and quality\nassurance tools to prevent errors, bugs and potential vulnerabilities. This\nstudy sets out to provide a head-to-head, quantitative and qualitative\nevaluation of six automated approaches: three industry-standard rule-based\nstatic code-analysis tools (SonarQube, CodeQL and Snyk Code) and three\nstate-of-the-art large language models hosted on the GitHub Models platform\n(GPT-4.1, Mistral Large and DeepSeek V3). Using a curated suite of ten\nreal-world C# projects that embed 63 vulnerabilities across common categories\nsuch as SQL injection, hard-coded secrets and outdated dependencies, we measure\nclassical detection accuracy (precision, recall, F-score), analysis latency,\nand the developer effort required to vet true positives. The language-based\nscanners achieve higher mean F-1 scores,0.797, 0.753 and 0.750, than their\nstatic counterparts, which score 0.260, 0.386 and 0.546, respectively. LLMs'\nadvantage originates from superior recall, confirming an ability to reason\nacross broader code contexts. However, this benefit comes with substantial\ntrade-offs: DeepSeek V3 exhibits the highest false-positive ratio, all language\nmodels mislocate issues at line-or-column granularity due to tokenisation\nartefacts. Overall, language models successfully rival traditional static\nanalysers in finding real vulnerabilities. Still, their noisier output and\nimprecise localisation limit their standalone use in safety-critical audits. We\ntherefore recommend a hybrid pipeline: employ language models early in\ndevelopment for broad, context-aware triage, while reserving deterministic\nrule-based scanners for high-assurance verification. The open benchmark and\nJSON-based result harness released with this paper lay a foundation for\nreproducible, practitioner-centric research into next-generation automated code\nsecurity."
                },
                "authors": [
                    {
                        "name": "Damian Gnieciak"
                    },
                    {
                        "name": "Tomasz Szandala"
                    }
                ],
                "author_detail": {
                    "name": "Tomasz Szandala"
                },
                "author": "Tomasz Szandala",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04448v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04448v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13928v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13928v3",
                "updated": "2025-08-06T13:47:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    47,
                    10,
                    2,
                    218,
                    0
                ],
                "published": "2024-10-17T17:56:01Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    56,
                    1,
                    3,
                    291,
                    0
                ],
                "title": "Automatically Interpreting Millions of Features in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically Interpreting Millions of Features in Large Language Models"
                },
                "summary": "While the activations of neurons in deep neural networks usually do not have\na simple human-understandable interpretation, sparse autoencoders (SAEs) can be\nused to transform these activations into a higher-dimensional latent space\nwhich may be more easily interpretable. However, these SAEs can have millions\nof distinct latent features, making it infeasible for humans to manually\ninterpret each one. In this work, we build an open-source automated pipeline to\ngenerate and evaluate natural language explanations for SAE features using\nLLMs. We test our framework on SAEs of varying sizes, activation functions, and\nlosses, trained on two different open-weight LLMs. We introduce five new\ntechniques to score the quality of explanations that are cheaper to run than\nthe previous state of the art. One of these techniques, intervention scoring,\nevaluates the interpretability of the effects of intervening on a feature,\nwhich we find explains features that are not recalled by existing methods. We\npropose guidelines for generating better explanations that remain valid for a\nbroader set of activating contexts, and discuss pitfalls with existing scoring\ntechniques. We use our explanations to measure the semantic similarity of\nindependently trained SAEs, and find that SAEs trained on nearby layers of the\nresidual stream are highly similar. Our large-scale analysis confirms that SAE\nlatents are indeed much more interpretable than neurons, even when neurons are\nsparsified using top-$k$ postprocessing. Our code is available at\nhttps://github.com/EleutherAI/sae-auto-interp, and our explanations are\navailable at\nhttps://huggingface.co/datasets/EleutherAI/auto_interp_explanations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While the activations of neurons in deep neural networks usually do not have\na simple human-understandable interpretation, sparse autoencoders (SAEs) can be\nused to transform these activations into a higher-dimensional latent space\nwhich may be more easily interpretable. However, these SAEs can have millions\nof distinct latent features, making it infeasible for humans to manually\ninterpret each one. In this work, we build an open-source automated pipeline to\ngenerate and evaluate natural language explanations for SAE features using\nLLMs. We test our framework on SAEs of varying sizes, activation functions, and\nlosses, trained on two different open-weight LLMs. We introduce five new\ntechniques to score the quality of explanations that are cheaper to run than\nthe previous state of the art. One of these techniques, intervention scoring,\nevaluates the interpretability of the effects of intervening on a feature,\nwhich we find explains features that are not recalled by existing methods. We\npropose guidelines for generating better explanations that remain valid for a\nbroader set of activating contexts, and discuss pitfalls with existing scoring\ntechniques. We use our explanations to measure the semantic similarity of\nindependently trained SAEs, and find that SAEs trained on nearby layers of the\nresidual stream are highly similar. Our large-scale analysis confirms that SAE\nlatents are indeed much more interpretable than neurons, even when neurons are\nsparsified using top-$k$ postprocessing. Our code is available at\nhttps://github.com/EleutherAI/sae-auto-interp, and our explanations are\navailable at\nhttps://huggingface.co/datasets/EleutherAI/auto_interp_explanations."
                },
                "authors": [
                    {
                        "name": "Gonalo Paulo"
                    },
                    {
                        "name": "Alex Mallen"
                    },
                    {
                        "name": "Caden Juang"
                    },
                    {
                        "name": "Nora Belrose"
                    }
                ],
                "author_detail": {
                    "name": "Nora Belrose"
                },
                "author": "Nora Belrose",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13928v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13928v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15299v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15299v4",
                "updated": "2025-08-06T13:42:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    42,
                    41,
                    2,
                    218,
                    0
                ],
                "published": "2025-03-19T15:21:48Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    21,
                    48,
                    2,
                    78,
                    0
                ],
                "title": "Inside-Out: Hidden Factual Knowledge in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inside-Out: Hidden Factual Knowledge in LLMs"
                },
                "summary": "This work presents a framework for assessing whether large language models\n(LLMs) encode more factual knowledge in their parameters than what they express\nin their outputs. While a few studies hint at this possibility, none has\nclearly defined or demonstrated this phenomenon. We first propose a formal\ndefinition of knowledge, quantifying it for a given question as the fraction of\ncorrect-incorrect answer pairs where the correct one is ranked higher. This\ngives rise to external and internal knowledge, depending on the information\nused to score individual answer candidates: either the model's observable\ntoken-level probabilities or its intermediate computations. Hidden knowledge\narises when internal knowledge exceeds external knowledge. We then present a\ncase study, applying this framework to three popular open-weights LLMs in a\nclosed-book QA setup. Our results indicate that: (1) LLMs consistently encode\nmore factual knowledge internally than what they express externally, with an\naverage relative gap of 40%. (2) Surprisingly, some knowledge is so deeply\nhidden that a model can internally know an answer perfectly, yet fail to\ngenerate it even once, despite large-scale repeated sampling of 1,000 answers.\nThis reveals fundamental limitations in the generation capabilities of LLMs,\nwhich (3) put a practical constraint on scaling test-time compute via repeated\nanswer sampling in closed-book QA: significant performance improvements remain\ninaccessible because some answers are practically never sampled, yet if they\nwere, we would be guaranteed to rank them first.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a framework for assessing whether large language models\n(LLMs) encode more factual knowledge in their parameters than what they express\nin their outputs. While a few studies hint at this possibility, none has\nclearly defined or demonstrated this phenomenon. We first propose a formal\ndefinition of knowledge, quantifying it for a given question as the fraction of\ncorrect-incorrect answer pairs where the correct one is ranked higher. This\ngives rise to external and internal knowledge, depending on the information\nused to score individual answer candidates: either the model's observable\ntoken-level probabilities or its intermediate computations. Hidden knowledge\narises when internal knowledge exceeds external knowledge. We then present a\ncase study, applying this framework to three popular open-weights LLMs in a\nclosed-book QA setup. Our results indicate that: (1) LLMs consistently encode\nmore factual knowledge internally than what they express externally, with an\naverage relative gap of 40%. (2) Surprisingly, some knowledge is so deeply\nhidden that a model can internally know an answer perfectly, yet fail to\ngenerate it even once, despite large-scale repeated sampling of 1,000 answers.\nThis reveals fundamental limitations in the generation capabilities of LLMs,\nwhich (3) put a practical constraint on scaling test-time compute via repeated\nanswer sampling in closed-book QA: significant performance improvements remain\ninaccessible because some answers are practically never sampled, yet if they\nwere, we would be guaranteed to rank them first."
                },
                "authors": [
                    {
                        "name": "Zorik Gekhman"
                    },
                    {
                        "name": "Eyal Ben David"
                    },
                    {
                        "name": "Hadas Orgad"
                    },
                    {
                        "name": "Eran Ofek"
                    },
                    {
                        "name": "Yonatan Belinkov"
                    },
                    {
                        "name": "Idan Szpektor"
                    },
                    {
                        "name": "Jonathan Herzig"
                    },
                    {
                        "name": "Roi Reichart"
                    }
                ],
                "author_detail": {
                    "name": "Roi Reichart"
                },
                "author": "Roi Reichart",
                "arxiv_comment": "Accepted to COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15299v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15299v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04440v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04440v1",
                "updated": "2025-08-06T13:28:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    28,
                    22,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T13:28:22Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    28,
                    22,
                    2,
                    218,
                    0
                ],
                "title": "StepFun-Formalizer: Unlocking the Autoformalization Potential of LLMs\n  through Knowledge-Reasoning Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StepFun-Formalizer: Unlocking the Autoformalization Potential of LLMs\n  through Knowledge-Reasoning Fusion"
                },
                "summary": "Autoformalization aims to translate natural-language mathematical statements\ninto a formal language. While LLMs have accelerated progress in this area,\nexisting methods still suffer from low accuracy. We identify two key abilities\nfor effective autoformalization: comprehensive mastery of formal-language\ndomain knowledge, and reasoning capability of natural language problem\nunderstanding and informal-formal alignment. Without the former, a model cannot\nidentify the correct formal objects; without the latter, it struggles to\ninterpret real-world contexts and map them precisely into formal expressions.\nTo address these gaps, we introduce ThinkingF, a data synthesis and training\npipeline that improves both abilities. First, we construct two datasets: one by\ndistilling and selecting large-scale examples rich in formal knowledge, and\nanother by generating informal-to-formal reasoning trajectories guided by\nexpert-designed templates. We then apply SFT and RLVR with these datasets to\nfurther fuse and refine the two abilities. The resulting 7B and 32B models\nexhibit both comprehensive formal knowledge and strong informal-to-formal\nreasoning. Notably, StepFun-Formalizer-32B achieves SOTA BEq@1 scores of 40.5%\non FormalMATH-Lite and 26.7% on ProverBench, surpassing all prior\ngeneral-purpose and specialized models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoformalization aims to translate natural-language mathematical statements\ninto a formal language. While LLMs have accelerated progress in this area,\nexisting methods still suffer from low accuracy. We identify two key abilities\nfor effective autoformalization: comprehensive mastery of formal-language\ndomain knowledge, and reasoning capability of natural language problem\nunderstanding and informal-formal alignment. Without the former, a model cannot\nidentify the correct formal objects; without the latter, it struggles to\ninterpret real-world contexts and map them precisely into formal expressions.\nTo address these gaps, we introduce ThinkingF, a data synthesis and training\npipeline that improves both abilities. First, we construct two datasets: one by\ndistilling and selecting large-scale examples rich in formal knowledge, and\nanother by generating informal-to-formal reasoning trajectories guided by\nexpert-designed templates. We then apply SFT and RLVR with these datasets to\nfurther fuse and refine the two abilities. The resulting 7B and 32B models\nexhibit both comprehensive formal knowledge and strong informal-to-formal\nreasoning. Notably, StepFun-Formalizer-32B achieves SOTA BEq@1 scores of 40.5%\non FormalMATH-Lite and 26.7% on ProverBench, surpassing all prior\ngeneral-purpose and specialized models."
                },
                "authors": [
                    {
                        "name": "Yutong Wu"
                    },
                    {
                        "name": "Di Huang"
                    },
                    {
                        "name": "Ruosi Wan"
                    },
                    {
                        "name": "Yue Peng"
                    },
                    {
                        "name": "Shijie Shang"
                    },
                    {
                        "name": "Chenrui Cao"
                    },
                    {
                        "name": "Lei Qi"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Zidong Du"
                    },
                    {
                        "name": "Jie Yan"
                    },
                    {
                        "name": "Xing Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xing Hu"
                },
                "author": "Xing Hu",
                "arxiv_comment": "24 pages, 17 figures, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04440v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04440v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11026v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11026v4",
                "updated": "2025-08-06T13:24:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    24,
                    9,
                    2,
                    218,
                    0
                ],
                "published": "2024-09-17T09:43:29Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    9,
                    43,
                    29,
                    1,
                    261,
                    0
                ],
                "title": "Prompt Obfuscation for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Obfuscation for Large Language Models"
                },
                "summary": "System prompts that include detailed instructions to describe the task\nperformed by the underlying LLM can easily transform foundation models into\ntools and services with minimal overhead. They are often considered\nintellectual property, similar to the code of a software product, because of\ntheir crucial impact on the utility. However, extracting system prompts is\neasily possible. As of today, there is no effective countermeasure to prevent\nthe stealing of system prompts, and all safeguarding efforts could be evaded.\nIn this work, we propose an alternative to conventional system prompts. We\nintroduce prompt obfuscation to prevent the extraction of the system prompt\nwith little overhead. The core idea is to find a representation of the original\nsystem prompt that leads to the same functionality, while the obfuscated system\nprompt does not contain any information that allows conclusions to be drawn\nabout the original system prompt. We evaluate our approach by comparing our\nobfuscated prompt output with the output of the original prompt, using eight\ndistinct metrics to measure the lexical, character-level, and semantic\nsimilarity. We show that the obfuscated version is constantly on par with the\noriginal one. We further perform three different deobfuscation attacks with\nvarying attacker knowledge--covering both black-box and white-box\nconditions--and show that in realistic attack scenarios an attacker is unable\nto extract meaningful information. Overall, we demonstrate that prompt\nobfuscation is an effective mechanism to safeguard the intellectual property of\na system prompt while maintaining the same utility as the original prompt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "System prompts that include detailed instructions to describe the task\nperformed by the underlying LLM can easily transform foundation models into\ntools and services with minimal overhead. They are often considered\nintellectual property, similar to the code of a software product, because of\ntheir crucial impact on the utility. However, extracting system prompts is\neasily possible. As of today, there is no effective countermeasure to prevent\nthe stealing of system prompts, and all safeguarding efforts could be evaded.\nIn this work, we propose an alternative to conventional system prompts. We\nintroduce prompt obfuscation to prevent the extraction of the system prompt\nwith little overhead. The core idea is to find a representation of the original\nsystem prompt that leads to the same functionality, while the obfuscated system\nprompt does not contain any information that allows conclusions to be drawn\nabout the original system prompt. We evaluate our approach by comparing our\nobfuscated prompt output with the output of the original prompt, using eight\ndistinct metrics to measure the lexical, character-level, and semantic\nsimilarity. We show that the obfuscated version is constantly on par with the\noriginal one. We further perform three different deobfuscation attacks with\nvarying attacker knowledge--covering both black-box and white-box\nconditions--and show that in realistic attack scenarios an attacker is unable\nto extract meaningful information. Overall, we demonstrate that prompt\nobfuscation is an effective mechanism to safeguard the intellectual property of\na system prompt while maintaining the same utility as the original prompt."
                },
                "authors": [
                    {
                        "name": "David Pape"
                    },
                    {
                        "name": "Sina Mavali"
                    },
                    {
                        "name": "Thorsten Eisenhofer"
                    },
                    {
                        "name": "Lea Schnherr"
                    }
                ],
                "author_detail": {
                    "name": "Lea Schnherr"
                },
                "author": "Lea Schnherr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11026v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11026v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04435v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04435v1",
                "updated": "2025-08-06T13:22:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    22,
                    15,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T13:22:15Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    22,
                    15,
                    2,
                    218,
                    0
                ],
                "title": "Cognitive Effort in the Two-Step Task: An Active Inference\n  Drift-Diffusion Model Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Effort in the Two-Step Task: An Active Inference\n  Drift-Diffusion Model Approach"
                },
                "summary": "High-level theories rooted in the Bayesian Brain Hypothesis often frame\ncognitive effort as the cost of resolving the conflict between habits and\noptimal policies. In parallel, evidence accumulator models (EAMs) provide a\nmechanistic account of how effort arises from competition between the\nsubjective values of available options. Although EAMs have been combined with\nframeworks like Reinforcement Learning to bridge the gap between high-level\ntheories and process-level mechanisms, relatively less attention has been paid\nto their implications for a unified notion of cognitive effort. Here, we\ncombine Active Inference (AIF) with the Drift-Diffusion Model (DDM) to\ninvestigate whether the resulting AIF-DDM can simultaneously account for effort\narising from both habit violation and value discriminability. To our knowledge,\nthis is the first time AIF has been combined with an EAM. We tested the AIF-DDM\non a behavioral dataset from the two-step task and compared its predictions to\nan information-theoretic definition of cognitive effort based on AIF. The\nmodel's predictions successfully accounted for second-stage reaction times but\nfailed to capture the dynamics of the first stage. We argue the latter\ndiscrepancy likely stems from the experimental design rather than a fundamental\nflaw in the model's assumptions about cognitive effort. Accordingly, we propose\nseveral modifications of the two-step task to better measure and isolate\ncognitive effort. Finally, we found that integrating the DDM significantly\nimproved parameter recovery, which could help future studies to obtain more\nreliable parameter estimates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-level theories rooted in the Bayesian Brain Hypothesis often frame\ncognitive effort as the cost of resolving the conflict between habits and\noptimal policies. In parallel, evidence accumulator models (EAMs) provide a\nmechanistic account of how effort arises from competition between the\nsubjective values of available options. Although EAMs have been combined with\nframeworks like Reinforcement Learning to bridge the gap between high-level\ntheories and process-level mechanisms, relatively less attention has been paid\nto their implications for a unified notion of cognitive effort. Here, we\ncombine Active Inference (AIF) with the Drift-Diffusion Model (DDM) to\ninvestigate whether the resulting AIF-DDM can simultaneously account for effort\narising from both habit violation and value discriminability. To our knowledge,\nthis is the first time AIF has been combined with an EAM. We tested the AIF-DDM\non a behavioral dataset from the two-step task and compared its predictions to\nan information-theoretic definition of cognitive effort based on AIF. The\nmodel's predictions successfully accounted for second-stage reaction times but\nfailed to capture the dynamics of the first stage. We argue the latter\ndiscrepancy likely stems from the experimental design rather than a fundamental\nflaw in the model's assumptions about cognitive effort. Accordingly, we propose\nseveral modifications of the two-step task to better measure and isolate\ncognitive effort. Finally, we found that integrating the DDM significantly\nimproved parameter recovery, which could help future studies to obtain more\nreliable parameter estimates."
                },
                "authors": [
                    {
                        "name": "Alvaro Garrido Perez"
                    },
                    {
                        "name": "Viktor Lemoine"
                    },
                    {
                        "name": "Amrapali Pednekar"
                    },
                    {
                        "name": "Yara Khaluf"
                    },
                    {
                        "name": "Pieter Simoens"
                    }
                ],
                "author_detail": {
                    "name": "Pieter Simoens"
                },
                "author": "Pieter Simoens",
                "arxiv_comment": "Paper accepted in the International Workshop on Active Inference,\n  2025: https://iwaiworkshop.github.io/#",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04435v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04435v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09485v2",
                "updated": "2025-08-06T13:21:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    21,
                    11,
                    2,
                    218,
                    0
                ],
                "published": "2025-04-13T08:56:22Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    8,
                    56,
                    22,
                    6,
                    103,
                    0
                ],
                "title": "GenEDA: Towards Generative Netlist Functional Reasoning via Cross-Modal\n  Circuit Encoder-Decoder Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenEDA: Towards Generative Netlist Functional Reasoning via Cross-Modal\n  Circuit Encoder-Decoder Alignment"
                },
                "summary": "The success of foundation AI has motivated the research of circuit foundation\nmodels, which are customized to assist the integrated circuit (IC) design\nprocess. However, existing pre-trained circuit foundation models are typically\nlimited to standalone encoders for predictive tasks or decoders for generative\ntasks. These two model types are developed independently, operate on different\ncircuit modalities, and reside in separate latent spaces. This restricts their\nability to complement each other for more advanced capabilities. In this work,\nwe present GenEDA, the first framework that cross-modally aligns circuit\nencoders with decoders within a shared latent space. GenEDA bridges the gap\nbetween graph-based circuit representation learning and text-based large\nlanguage models (LLMs), enabling communication between their respective latent\nspaces. To achieve the alignment, we propose two paradigms to support both\nopen-source trainable LLMs and commercial frozen LLMs. We leverage this aligned\narchitecture to develop the first generative foundation model for netlists,\nunleashing LLMs' generative reasoning capability on the low-level and\nbit-blasted netlists. GenEDA enables three unprecedented generative netlist\nfunctional reasoning tasks, where it reversely generates high-level\nfunctionalities such as specifications and RTL code from low-level netlists.\nThese tasks move beyond traditional gate function classification to direct\ngeneration of full-circuit functionality. Experiments demonstrate that GenEDA\nsignificantly boosts advanced LLMs' (e.g., GPT and DeepSeek series) performance\nin all tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of foundation AI has motivated the research of circuit foundation\nmodels, which are customized to assist the integrated circuit (IC) design\nprocess. However, existing pre-trained circuit foundation models are typically\nlimited to standalone encoders for predictive tasks or decoders for generative\ntasks. These two model types are developed independently, operate on different\ncircuit modalities, and reside in separate latent spaces. This restricts their\nability to complement each other for more advanced capabilities. In this work,\nwe present GenEDA, the first framework that cross-modally aligns circuit\nencoders with decoders within a shared latent space. GenEDA bridges the gap\nbetween graph-based circuit representation learning and text-based large\nlanguage models (LLMs), enabling communication between their respective latent\nspaces. To achieve the alignment, we propose two paradigms to support both\nopen-source trainable LLMs and commercial frozen LLMs. We leverage this aligned\narchitecture to develop the first generative foundation model for netlists,\nunleashing LLMs' generative reasoning capability on the low-level and\nbit-blasted netlists. GenEDA enables three unprecedented generative netlist\nfunctional reasoning tasks, where it reversely generates high-level\nfunctionalities such as specifications and RTL code from low-level netlists.\nThese tasks move beyond traditional gate function classification to direct\ngeneration of full-circuit functionality. Experiments demonstrate that GenEDA\nsignificantly boosts advanced LLMs' (e.g., GPT and DeepSeek series) performance\nin all tasks."
                },
                "authors": [
                    {
                        "name": "Wenji Fang"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Shang Liu"
                    },
                    {
                        "name": "Zhiyao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyao Xie"
                },
                "author": "Zhiyao Xie",
                "arxiv_comment": "Accepted by ICCAD'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04428v1",
                "updated": "2025-08-06T13:16:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    16,
                    10,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T13:16:10Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    16,
                    10,
                    2,
                    218,
                    0
                ],
                "title": "\\textsc{SimInstruct}: A Responsible Tool for Collecting Scaffolding\n  Dialogues Between Experts and LLM-Simulated Novices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\\textsc{SimInstruct}: A Responsible Tool for Collecting Scaffolding\n  Dialogues Between Experts and LLM-Simulated Novices"
                },
                "summary": "High-quality, multi-turn instructional dialogues between novices and experts\nare essential for developing AI systems that support teaching, learning, and\ndecision-making. These dialogues often involve scaffolding -- the process by\nwhich an expert supports a novice's thinking through questions, feedback, and\nstep-by-step guidance. However, such data are scarce due to privacy concerns in\nrecording and the vulnerability inherent in help-seeking. We present\nSimInstruct, a scalable, expert-in-the-loop tool for collecting scaffolding\ndialogues. Using teaching development coaching as an example domain,\nSimInstruct simulates novice instructors via LLMs, varying their teaching\nchallenges and LLM's persona traits, while human experts provide multi-turn\nfeedback, reasoning, and instructional support. This design enables the\ncreation of realistic, pedagogically rich dialogues without requiring real\nnovice participants. Our results reveal that persona traits, such as\nextroversion and introversion, meaningfully influence how experts engage.\nCompared to real mentoring recordings, SimInstruct dialogues demonstrate\ncomparable pedagogical relevance and cognitive depth. Experts also reported the\nprocess as engaging and reflective, improving both data quality and their own\nprofessional insight. We further fine-tuned a LLaMA model to be an expert model\nusing the augmented dataset, which outperformed GPT-4o in instructional\nquality. Our analysis highlights GPT-4o's limitations in weak reflective\nquestioning, overuse of generic praise, a condescending tone, and a tendency to\noverwhelm novices with excessive suggestions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-quality, multi-turn instructional dialogues between novices and experts\nare essential for developing AI systems that support teaching, learning, and\ndecision-making. These dialogues often involve scaffolding -- the process by\nwhich an expert supports a novice's thinking through questions, feedback, and\nstep-by-step guidance. However, such data are scarce due to privacy concerns in\nrecording and the vulnerability inherent in help-seeking. We present\nSimInstruct, a scalable, expert-in-the-loop tool for collecting scaffolding\ndialogues. Using teaching development coaching as an example domain,\nSimInstruct simulates novice instructors via LLMs, varying their teaching\nchallenges and LLM's persona traits, while human experts provide multi-turn\nfeedback, reasoning, and instructional support. This design enables the\ncreation of realistic, pedagogically rich dialogues without requiring real\nnovice participants. Our results reveal that persona traits, such as\nextroversion and introversion, meaningfully influence how experts engage.\nCompared to real mentoring recordings, SimInstruct dialogues demonstrate\ncomparable pedagogical relevance and cognitive depth. Experts also reported the\nprocess as engaging and reflective, improving both data quality and their own\nprofessional insight. We further fine-tuned a LLaMA model to be an expert model\nusing the augmented dataset, which outperformed GPT-4o in instructional\nquality. Our analysis highlights GPT-4o's limitations in weak reflective\nquestioning, overuse of generic praise, a condescending tone, and a tendency to\noverwhelm novices with excessive suggestions."
                },
                "authors": [
                    {
                        "name": "Si Chen"
                    },
                    {
                        "name": "Izzy Molnar"
                    },
                    {
                        "name": "Ting Hua"
                    },
                    {
                        "name": "Peiyu Li"
                    },
                    {
                        "name": "Le Huy Khiem"
                    },
                    {
                        "name": "G. Alex Ambrose"
                    },
                    {
                        "name": "Jim Lang"
                    },
                    {
                        "name": "Ronald Metoyer"
                    },
                    {
                        "name": "Nitesh V. Chawla"
                    }
                ],
                "author_detail": {
                    "name": "Nitesh V. Chawla"
                },
                "author": "Nitesh V. Chawla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04423v1",
                "updated": "2025-08-06T13:11:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    11,
                    17,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T13:11:17Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    11,
                    17,
                    2,
                    218,
                    0
                ],
                "title": "Evaluating, Synthesizing, and Enhancing for Customer Support\n  Conversation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating, Synthesizing, and Enhancing for Customer Support\n  Conversation"
                },
                "summary": "Effective customer support requires not only accurate problem solving but\nalso structured and empathetic communication aligned with professional\nstandards. However, existing dialogue datasets often lack strategic guidance,\nand real-world service data is difficult to access and annotate. To address\nthis, we introduce the task of Customer Support Conversation (CSC), aimed at\ntraining customer service agents to respond using well-defined support\nstrategies. We propose a structured CSC framework grounded in COPC guidelines,\ndefining five conversational stages and twelve strategies to guide high-quality\ninteractions. Based on this, we construct CSConv, an evaluation dataset of\n1,855 real-world customer-agent conversations rewritten using LLMs to reflect\ndeliberate strategy use, and annotated accordingly. Additionally, we develop a\nrole-playing approach that simulates strategy-rich conversations using\nLLM-powered roles aligned with the CSC framework, resulting in the training\ndataset RoleCS. Experiments show that fine-tuning strong LLMs on RoleCS\nsignificantly improves their ability to generate high-quality, strategy-aligned\nresponses on CSConv. Human evaluations further confirm gains in problem\nresolution. All code and data will be made publicly available at\nhttps://github.com/aliyun/qwen-dianjin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective customer support requires not only accurate problem solving but\nalso structured and empathetic communication aligned with professional\nstandards. However, existing dialogue datasets often lack strategic guidance,\nand real-world service data is difficult to access and annotate. To address\nthis, we introduce the task of Customer Support Conversation (CSC), aimed at\ntraining customer service agents to respond using well-defined support\nstrategies. We propose a structured CSC framework grounded in COPC guidelines,\ndefining five conversational stages and twelve strategies to guide high-quality\ninteractions. Based on this, we construct CSConv, an evaluation dataset of\n1,855 real-world customer-agent conversations rewritten using LLMs to reflect\ndeliberate strategy use, and annotated accordingly. Additionally, we develop a\nrole-playing approach that simulates strategy-rich conversations using\nLLM-powered roles aligned with the CSC framework, resulting in the training\ndataset RoleCS. Experiments show that fine-tuning strong LLMs on RoleCS\nsignificantly improves their ability to generate high-quality, strategy-aligned\nresponses on CSConv. Human evaluations further confirm gains in problem\nresolution. All code and data will be made publicly available at\nhttps://github.com/aliyun/qwen-dianjin."
                },
                "authors": [
                    {
                        "name": "Jie Zhu"
                    },
                    {
                        "name": "Huaixia Dou"
                    },
                    {
                        "name": "Junhui Li"
                    },
                    {
                        "name": "Lifan Guo"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Fang Kong"
                    }
                ],
                "author_detail": {
                    "name": "Fang Kong"
                },
                "author": "Fang Kong",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04422v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04422v1",
                "updated": "2025-08-06T13:09:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    9,
                    2,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T13:09:02Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    9,
                    2,
                    2,
                    218,
                    0
                ],
                "title": "Efficient Inter-Task Attention for Multitask Transformer Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Inter-Task Attention for Multitask Transformer Models"
                },
                "summary": "In both Computer Vision and the wider Deep Learning field, the Transformer\narchitecture is well-established as state-of-the-art for many applications. For\nMultitask Learning, however, where there may be many more queries necessary\ncompared to single-task models, its Multi-Head-Attention often approaches the\nlimits of what is computationally feasible considering practical hardware\nlimitations. This is due to the fact that the size of the attention matrix\nscales quadratically with the number of tasks (assuming roughly equal numbers\nof queries for all tasks). As a solution, we propose our novel Deformable\nInter-Task Self-Attention for Multitask models that enables the much more\nefficient aggregation of information across the feature maps from different\ntasks. In our experiments on the NYUD-v2 and PASCAL-Context datasets, we\ndemonstrate an order-of-magnitude reduction in both FLOPs count and inference\nlatency. At the same time, we also achieve substantial improvements by up to\n7.4% in the individual tasks' prediction quality metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In both Computer Vision and the wider Deep Learning field, the Transformer\narchitecture is well-established as state-of-the-art for many applications. For\nMultitask Learning, however, where there may be many more queries necessary\ncompared to single-task models, its Multi-Head-Attention often approaches the\nlimits of what is computationally feasible considering practical hardware\nlimitations. This is due to the fact that the size of the attention matrix\nscales quadratically with the number of tasks (assuming roughly equal numbers\nof queries for all tasks). As a solution, we propose our novel Deformable\nInter-Task Self-Attention for Multitask models that enables the much more\nefficient aggregation of information across the feature maps from different\ntasks. In our experiments on the NYUD-v2 and PASCAL-Context datasets, we\ndemonstrate an order-of-magnitude reduction in both FLOPs count and inference\nlatency. At the same time, we also achieve substantial improvements by up to\n7.4% in the individual tasks' prediction quality metrics."
                },
                "authors": [
                    {
                        "name": "Christian Bohn"
                    },
                    {
                        "name": "Thomas Kurbiel"
                    },
                    {
                        "name": "Klaus Friedrichs"
                    },
                    {
                        "name": "Hasan Tercan"
                    },
                    {
                        "name": "Tobias Meisen"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Meisen"
                },
                "author": "Tobias Meisen",
                "arxiv_comment": "Accepted to ICONIP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04422v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04422v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04418v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04418v1",
                "updated": "2025-08-06T13:05:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    5,
                    9,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T13:05:09Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    5,
                    9,
                    2,
                    218,
                    0
                ],
                "title": "Think Before You Segment: An Object-aware Reasoning Agent for Referring\n  Audio-Visual Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think Before You Segment: An Object-aware Reasoning Agent for Referring\n  Audio-Visual Segmentation"
                },
                "summary": "Referring Audio-Visual Segmentation (Ref-AVS) aims to segment target objects\nin audible videos based on given reference expressions. Prior works typically\nrely on learning latent embeddings via multimodal fusion to prompt a tunable\nSAM/SAM2 decoder for segmentation, which requires strong pixel-level\nsupervision and lacks interpretability. From a novel perspective of explicit\nreference understanding, we propose TGS-Agent, which decomposes the task into a\nThink-Ground-Segment process, mimicking the human reasoning procedure by first\nidentifying the referred object through multimodal analysis, followed by\ncoarse-grained grounding and precise segmentation. To this end, we first\npropose Ref-Thinker, a multimodal language model capable of reasoning over\ntextual, visual, and auditory cues. We construct an instruction-tuning dataset\nwith explicit object-aware think-answer chains for Ref-Thinker fine-tuning. The\nobject description inferred by Ref-Thinker is used as an explicit prompt for\nGrounding-DINO and SAM2, which perform grounding and segmentation without\nrelying on pixel-level supervision. Additionally, we introduce\nR\\textsuperscript{2}-AVSBench, a new benchmark with linguistically diverse and\nreasoning-intensive references for better evaluating model generalization. Our\napproach achieves state-of-the-art results on both standard Ref-AVSBench and\nproposed R\\textsuperscript{2}-AVSBench. Code will be available at\nhttps://github.com/jasongief/TGS-Agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Referring Audio-Visual Segmentation (Ref-AVS) aims to segment target objects\nin audible videos based on given reference expressions. Prior works typically\nrely on learning latent embeddings via multimodal fusion to prompt a tunable\nSAM/SAM2 decoder for segmentation, which requires strong pixel-level\nsupervision and lacks interpretability. From a novel perspective of explicit\nreference understanding, we propose TGS-Agent, which decomposes the task into a\nThink-Ground-Segment process, mimicking the human reasoning procedure by first\nidentifying the referred object through multimodal analysis, followed by\ncoarse-grained grounding and precise segmentation. To this end, we first\npropose Ref-Thinker, a multimodal language model capable of reasoning over\ntextual, visual, and auditory cues. We construct an instruction-tuning dataset\nwith explicit object-aware think-answer chains for Ref-Thinker fine-tuning. The\nobject description inferred by Ref-Thinker is used as an explicit prompt for\nGrounding-DINO and SAM2, which perform grounding and segmentation without\nrelying on pixel-level supervision. Additionally, we introduce\nR\\textsuperscript{2}-AVSBench, a new benchmark with linguistically diverse and\nreasoning-intensive references for better evaluating model generalization. Our\napproach achieves state-of-the-art results on both standard Ref-AVSBench and\nproposed R\\textsuperscript{2}-AVSBench. Code will be available at\nhttps://github.com/jasongief/TGS-Agent."
                },
                "authors": [
                    {
                        "name": "Jinxing Zhou"
                    },
                    {
                        "name": "Yanghao Zhou"
                    },
                    {
                        "name": "Mingfei Han"
                    },
                    {
                        "name": "Tong Wang"
                    },
                    {
                        "name": "Xiaojun Chang"
                    },
                    {
                        "name": "Hisham Cholakkal"
                    },
                    {
                        "name": "Rao Muhammad Anwer"
                    }
                ],
                "author_detail": {
                    "name": "Rao Muhammad Anwer"
                },
                "author": "Rao Muhammad Anwer",
                "arxiv_comment": "Project page: https://github.com/jasongief/TGS-Agent",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04418v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04418v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.08617v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.08617v5",
                "updated": "2025-08-06T13:02:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    2,
                    31,
                    2,
                    218,
                    0
                ],
                "published": "2023-12-14T02:42:15Z",
                "published_parsed": [
                    2023,
                    12,
                    14,
                    2,
                    42,
                    15,
                    3,
                    348,
                    0
                ],
                "title": "RTLCoder: Outperforming GPT-3.5 in Design RTL Generation with Our\n  Open-Source Dataset and Lightweight Solution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RTLCoder: Outperforming GPT-3.5 in Design RTL Generation with Our\n  Open-Source Dataset and Lightweight Solution"
                },
                "summary": "The automatic generation of RTL code (e.g., Verilog) using natural language\ninstructions and large language models (LLMs) has attracted significant\nresearch interest recently. However, most existing approaches heavily rely on\ncommercial LLMs such as ChatGPT, while open-source LLMs tailored for this\nspecific design generation task exhibit notably inferior performance. The\nabsence of high-quality open-source solutions restricts the flexibility and\ndata privacy of this emerging technique. In this study, we present a new\ncustomized LLM solution with a modest parameter count of only 7B, achieving\nbetter performance than GPT-3.5 on all representative benchmarks for RTL code\ngeneration. Especially, it outperforms GPT-4 in VerilogEval Machine benchmark.\nThis remarkable balance between accuracy and efficiency is made possible by\nleveraging our new RTL code dataset and a customized LLM algorithm, both of\nwhich have been made fully open-source.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The automatic generation of RTL code (e.g., Verilog) using natural language\ninstructions and large language models (LLMs) has attracted significant\nresearch interest recently. However, most existing approaches heavily rely on\ncommercial LLMs such as ChatGPT, while open-source LLMs tailored for this\nspecific design generation task exhibit notably inferior performance. The\nabsence of high-quality open-source solutions restricts the flexibility and\ndata privacy of this emerging technique. In this study, we present a new\ncustomized LLM solution with a modest parameter count of only 7B, achieving\nbetter performance than GPT-3.5 on all representative benchmarks for RTL code\ngeneration. Especially, it outperforms GPT-4 in VerilogEval Machine benchmark.\nThis remarkable balance between accuracy and efficiency is made possible by\nleveraging our new RTL code dataset and a customized LLM algorithm, both of\nwhich have been made fully open-source."
                },
                "authors": [
                    {
                        "name": "Shang Liu"
                    },
                    {
                        "name": "Wenji Fang"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Qijun Zhang"
                    },
                    {
                        "name": "Hongce Zhang"
                    },
                    {
                        "name": "Zhiyao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyao Xie"
                },
                "author": "Zhiyao Xie",
                "arxiv_doi": "10.1109/LAD62341.2024.10691788",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/LAD62341.2024.10691788",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.08617v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.08617v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This is the LAD Conference version of RTLCoder, for the TCAD\n  extension version, please refer to: arXiv:2312.08617v4",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04412v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04412v1",
                "updated": "2025-08-06T12:56:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    12,
                    56,
                    54,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T12:56:54Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    12,
                    56,
                    54,
                    2,
                    218,
                    0
                ],
                "title": "Beyond Pixels: Exploring DOM Downsampling for LLM-Based Web Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Pixels: Exploring DOM Downsampling for LLM-Based Web Agents"
                },
                "summary": "Frontier LLMs only recently enabled serviceable, autonomous web agents. At\nthat, a model poses as an instantaneous domain model backend. Ought to suggest\ninteraction, it is consulted with a web-based task and respective application\nstate. The key problem lies in application state serialisation\n$\\unicode{x2013}$ referred to as snapshot. State-of-the-art web agents are\npremised on grounded GUI snapshots, i.e., screenshots enhanced with visual\ncues. Not least to resemble human perception, but for images representing\nrelatively cheap means of model input. LLM vision still lag behind code\ninterpretation capabilities. DOM snapshots, which structurally resemble HTML,\nimpose a desired alternative. Vast model input token size, however, disables\nreliable implementation with web agents to date.\n  We propose D2Snap, a first-of-its-kind DOM downsampling algorithm. Based on a\nGPT-4o backend, we evaluate D2Snap on tasks sampled from the Online-Mind2Web\ndataset. The success rate of D2Snap-downsampled DOM snapshots (67%) matches a\ngrounded GUI snapshot baseline (65%) $\\unicode{x2013}$ within the same input\ntoken order of magnitude (1e3). Our best evaluated configurations\n$\\unicode{x2013}$ one token order above, but within the model's context window\n$\\unicode{x2013}$ outperform this baseline by 8%. Our evaluation, moreover,\nyields that DOM-inherent hierarchy embodies a strong UI feature for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frontier LLMs only recently enabled serviceable, autonomous web agents. At\nthat, a model poses as an instantaneous domain model backend. Ought to suggest\ninteraction, it is consulted with a web-based task and respective application\nstate. The key problem lies in application state serialisation\n$\\unicode{x2013}$ referred to as snapshot. State-of-the-art web agents are\npremised on grounded GUI snapshots, i.e., screenshots enhanced with visual\ncues. Not least to resemble human perception, but for images representing\nrelatively cheap means of model input. LLM vision still lag behind code\ninterpretation capabilities. DOM snapshots, which structurally resemble HTML,\nimpose a desired alternative. Vast model input token size, however, disables\nreliable implementation with web agents to date.\n  We propose D2Snap, a first-of-its-kind DOM downsampling algorithm. Based on a\nGPT-4o backend, we evaluate D2Snap on tasks sampled from the Online-Mind2Web\ndataset. The success rate of D2Snap-downsampled DOM snapshots (67%) matches a\ngrounded GUI snapshot baseline (65%) $\\unicode{x2013}$ within the same input\ntoken order of magnitude (1e3). Our best evaluated configurations\n$\\unicode{x2013}$ one token order above, but within the model's context window\n$\\unicode{x2013}$ outperform this baseline by 8%. Our evaluation, moreover,\nyields that DOM-inherent hierarchy embodies a strong UI feature for LLMs."
                },
                "authors": [
                    {
                        "name": "Thassilo M. Schiepanski"
                    },
                    {
                        "name": "Nicholas Pil"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas Pil"
                },
                "author": "Nicholas Pil",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04412v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04412v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16520v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16520v4",
                "updated": "2025-08-06T12:55:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    12,
                    55,
                    44,
                    2,
                    218,
                    0
                ],
                "published": "2024-10-21T21:21:29Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    21,
                    21,
                    29,
                    0,
                    295,
                    0
                ],
                "title": "AUTALIC: A Dataset for Anti-AUTistic Ableist Language In Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AUTALIC: A Dataset for Anti-AUTistic Ableist Language In Context"
                },
                "summary": "As our understanding of autism and ableism continues to increase, so does our\nunderstanding of ableist language towards autistic people. Such language poses\na significant challenge in NLP research due to its subtle and context-dependent\nnature. Yet, detecting anti-autistic ableist language remains underexplored,\nwith existing NLP tools often failing to capture its nuanced expressions. We\npresent AUTALIC, the first benchmark dataset dedicated to the detection of\nanti-autistic ableist language in context, addressing a significant gap in the\nfield. The dataset comprises 2,400 autism-related sentences collected from\nReddit, accompanied by surrounding context, and is annotated by trained experts\nwith backgrounds in neurodiversity. Our comprehensive evaluation reveals that\ncurrent language models, including state-of-the-art LLMs, struggle to reliably\nidentify anti-autistic ableism and align with human judgments, underscoring\ntheir limitations in this domain. We publicly release AUTALIC along with the\nindividual annotations which serve as a valuable resource to researchers\nworking on ableism, neurodiversity, and also studying disagreements in\nannotation tasks. This dataset serves as a crucial step towards developing more\ninclusive and context-aware NLP systems that better reflect diverse\nperspectives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As our understanding of autism and ableism continues to increase, so does our\nunderstanding of ableist language towards autistic people. Such language poses\na significant challenge in NLP research due to its subtle and context-dependent\nnature. Yet, detecting anti-autistic ableist language remains underexplored,\nwith existing NLP tools often failing to capture its nuanced expressions. We\npresent AUTALIC, the first benchmark dataset dedicated to the detection of\nanti-autistic ableist language in context, addressing a significant gap in the\nfield. The dataset comprises 2,400 autism-related sentences collected from\nReddit, accompanied by surrounding context, and is annotated by trained experts\nwith backgrounds in neurodiversity. Our comprehensive evaluation reveals that\ncurrent language models, including state-of-the-art LLMs, struggle to reliably\nidentify anti-autistic ableism and align with human judgments, underscoring\ntheir limitations in this domain. We publicly release AUTALIC along with the\nindividual annotations which serve as a valuable resource to researchers\nworking on ableism, neurodiversity, and also studying disagreements in\nannotation tasks. This dataset serves as a crucial step towards developing more\ninclusive and context-aware NLP systems that better reflect diverse\nperspectives."
                },
                "authors": [
                    {
                        "name": "Naba Rizvi"
                    },
                    {
                        "name": "Harper Strickland"
                    },
                    {
                        "name": "Daniel Gitelman"
                    },
                    {
                        "name": "Tristan Cooper"
                    },
                    {
                        "name": "Alexis Morales-Flores"
                    },
                    {
                        "name": "Michael Golden"
                    },
                    {
                        "name": "Aekta Kallepalli"
                    },
                    {
                        "name": "Akshat Alurkar"
                    },
                    {
                        "name": "Haaset Owens"
                    },
                    {
                        "name": "Saleha Ahmedi"
                    },
                    {
                        "name": "Isha Khirwadkar"
                    },
                    {
                        "name": "Imani Munyaka"
                    },
                    {
                        "name": "Nedjma Ousidhoum"
                    }
                ],
                "author_detail": {
                    "name": "Nedjma Ousidhoum"
                },
                "author": "Nedjma Ousidhoum",
                "arxiv_doi": "10.18653/v1/2025.acl-long.1022",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2025.acl-long.1022",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.16520v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16520v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "accepted to ACL main 2025, 9 pages, 5 figures, 7 tables",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04407v1",
                "updated": "2025-08-06T12:51:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    12,
                    51,
                    10,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T12:51:10Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    12,
                    51,
                    10,
                    2,
                    218,
                    0
                ],
                "title": "A novel approach for air shower profile reconstruction with dense radio\n  antenna arrays using Information Field Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A novel approach for air shower profile reconstruction with dense radio\n  antenna arrays using Information Field Theory"
                },
                "summary": "Reconstructing the longitudinal profile of extensive air showers, generated\nfrom the interaction of cosmic rays in the Earth's atmosphere, is crucial to\nunderstanding their mass composition, which in turn provides valuable insight\non their possible sources of origin. Dense radio antenna arrays such as the LOw\nFrequency ARray (LOFAR) telescope as well as the upcoming Square Kilometre\nArray Observatory (SKAO) are ideal instruments to explore the potential of air\nshower profile reconstruction, as their high antenna density allows cosmic ray\nobservations with unprecedented accuracy. However, current analysis approaches\ncan only recover $X_\\mathrm{max}$, the atmospheric depth at shower maximum, and\nheavily rely on computationally expensive simulations. As such, it is ever more\ncrucial to develop new analysis approaches that can perform a full air shower\nprofile reconstruction efficiently.\n  In this work, we develop a novel framework to reconstruct the longitudinal\nprofile of air showers using measurements from radio detectors with Information\nField Theory (IFT), a state-of-the-art reconstruction framework based on\nBayesian inference. Through IFT, we are able to exploit all available\ninformation in the signal (amplitude, phase, and pulse shape) at each antenna\nposition simultaneously and explicitly utilise models that are motivated\nthrough our current understanding of air shower physics. We verify our\nframework on simulated datasets prepared for LOFAR, showcasing that we can not\nonly reconstruct the air shower profile with uncertainties in each atmospheric\ndepth bin but also recover the reconstructed trace at each antenna position.\nOur framework demonstrates that radio measurements with dense antenna layouts\nsuch as LOFAR and SKAO have the capability to go beyond reconstruction of\n$X_\\mathrm{max}$ and will thus aid in our understanding of the mass composition\nof cosmic rays.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstructing the longitudinal profile of extensive air showers, generated\nfrom the interaction of cosmic rays in the Earth's atmosphere, is crucial to\nunderstanding their mass composition, which in turn provides valuable insight\non their possible sources of origin. Dense radio antenna arrays such as the LOw\nFrequency ARray (LOFAR) telescope as well as the upcoming Square Kilometre\nArray Observatory (SKAO) are ideal instruments to explore the potential of air\nshower profile reconstruction, as their high antenna density allows cosmic ray\nobservations with unprecedented accuracy. However, current analysis approaches\ncan only recover $X_\\mathrm{max}$, the atmospheric depth at shower maximum, and\nheavily rely on computationally expensive simulations. As such, it is ever more\ncrucial to develop new analysis approaches that can perform a full air shower\nprofile reconstruction efficiently.\n  In this work, we develop a novel framework to reconstruct the longitudinal\nprofile of air showers using measurements from radio detectors with Information\nField Theory (IFT), a state-of-the-art reconstruction framework based on\nBayesian inference. Through IFT, we are able to exploit all available\ninformation in the signal (amplitude, phase, and pulse shape) at each antenna\nposition simultaneously and explicitly utilise models that are motivated\nthrough our current understanding of air shower physics. We verify our\nframework on simulated datasets prepared for LOFAR, showcasing that we can not\nonly reconstruct the air shower profile with uncertainties in each atmospheric\ndepth bin but also recover the reconstructed trace at each antenna position.\nOur framework demonstrates that radio measurements with dense antenna layouts\nsuch as LOFAR and SKAO have the capability to go beyond reconstruction of\n$X_\\mathrm{max}$ and will thus aid in our understanding of the mass composition\nof cosmic rays."
                },
                "authors": [
                    {
                        "name": "K. Watanabe"
                    },
                    {
                        "name": "S. Bouma"
                    },
                    {
                        "name": "J. D. Bray"
                    },
                    {
                        "name": "S. Buitink"
                    },
                    {
                        "name": "A. Corstanje"
                    },
                    {
                        "name": "V. De Henau"
                    },
                    {
                        "name": "M. Desmet"
                    },
                    {
                        "name": "E. Dickinson"
                    },
                    {
                        "name": "L. van Dongen"
                    },
                    {
                        "name": "T. A. Enlin"
                    },
                    {
                        "name": "B. Hare"
                    },
                    {
                        "name": "H. He"
                    },
                    {
                        "name": "J. R. Hrandel"
                    },
                    {
                        "name": "T. Huege"
                    },
                    {
                        "name": "C. W. James"
                    },
                    {
                        "name": "M. Jetti"
                    },
                    {
                        "name": "P. Laub"
                    },
                    {
                        "name": "H. J. Mathes"
                    },
                    {
                        "name": "K. Mulrey"
                    },
                    {
                        "name": "A. Nelles"
                    },
                    {
                        "name": "S. Saha"
                    },
                    {
                        "name": "O. Scholten"
                    },
                    {
                        "name": "S. Sharma"
                    },
                    {
                        "name": "R. E. Spencer"
                    },
                    {
                        "name": "C. Sterpka"
                    },
                    {
                        "name": "S. ter Veen"
                    },
                    {
                        "name": "K. Terveer"
                    },
                    {
                        "name": "S. Thoudam"
                    },
                    {
                        "name": "T. N. G. Trinh"
                    },
                    {
                        "name": "P. Turekova"
                    },
                    {
                        "name": "D. Veberi"
                    },
                    {
                        "name": "M. Waterson"
                    },
                    {
                        "name": "C. Zhang"
                    },
                    {
                        "name": "P. Zhang"
                    },
                    {
                        "name": "Y. Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Y. Zhang"
                },
                "author": "Y. Zhang",
                "arxiv_comment": "Presented at the 39th International Cosmic Ray Conference (ICRC\n  2025). 9 pages, 4 figures",
                "arxiv_journal_ref": "PoS(ICRC2025)436",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15849v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15849v2",
                "updated": "2025-08-06T12:47:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    12,
                    47,
                    41,
                    2,
                    218,
                    0
                ],
                "published": "2025-05-19T20:17:37Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    20,
                    17,
                    37,
                    0,
                    139,
                    0
                ],
                "title": "What Lives? A meta-analysis of diverse opinions on the definition of\n  life",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Lives? A meta-analysis of diverse opinions on the definition of\n  life"
                },
                "summary": "The question of \"what is life?\" has challenged scientists and philosophers\nfor centuries, producing an array of definitions that reflect both the mystery\nof its emergence and the diversity of disciplinary perspectives brought to bear\non the question. Despite significant progress in our understanding of\nbiological systems, psychology, computation, and information theory, no single\ndefinition for life has yet achieved universal acceptance. This challenge\nbecomes increasingly urgent as advances in synthetic biology, artificial\nintelligence, and astrobiology challenge our traditional conceptions of what it\nmeans to be alive. We undertook a methodological approach that leverages large\nlanguage models (LLMs) to analyze a set of definitions of life provided by a\ncurated set of cross-disciplinary experts. We used a novel pairwise correlation\nanalysis to map the definitions into distinct feature vectors, followed by\nagglomerative clustering, intra-cluster semantic analysis, and t-SNE projection\nto reveal underlying conceptual archetypes. This methodology revealed a\ncontinuous landscape of the themes relating to the definition of life,\nsuggesting that what has historically been approached as a binary taxonomic\nproblem should be instead conceived as differentiated perspectives within a\nunified conceptual latent space. We offer a new methodological bridge between\nreductionist and holistic approaches to fundamental questions in science and\nphilosophy, demonstrating how computational semantic analysis can reveal\nconceptual patterns across disciplinary boundaries, and opening similar\npathways for addressing other contested definitional territories across the\nsciences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The question of \"what is life?\" has challenged scientists and philosophers\nfor centuries, producing an array of definitions that reflect both the mystery\nof its emergence and the diversity of disciplinary perspectives brought to bear\non the question. Despite significant progress in our understanding of\nbiological systems, psychology, computation, and information theory, no single\ndefinition for life has yet achieved universal acceptance. This challenge\nbecomes increasingly urgent as advances in synthetic biology, artificial\nintelligence, and astrobiology challenge our traditional conceptions of what it\nmeans to be alive. We undertook a methodological approach that leverages large\nlanguage models (LLMs) to analyze a set of definitions of life provided by a\ncurated set of cross-disciplinary experts. We used a novel pairwise correlation\nanalysis to map the definitions into distinct feature vectors, followed by\nagglomerative clustering, intra-cluster semantic analysis, and t-SNE projection\nto reveal underlying conceptual archetypes. This methodology revealed a\ncontinuous landscape of the themes relating to the definition of life,\nsuggesting that what has historically been approached as a binary taxonomic\nproblem should be instead conceived as differentiated perspectives within a\nunified conceptual latent space. We offer a new methodological bridge between\nreductionist and holistic approaches to fundamental questions in science and\nphilosophy, demonstrating how computational semantic analysis can reveal\nconceptual patterns across disciplinary boundaries, and opening similar\npathways for addressing other contested definitional territories across the\nsciences."
                },
                "authors": [
                    {
                        "name": "Reed Bender"
                    },
                    {
                        "name": "Karina Kofman"
                    },
                    {
                        "name": "Blaise Agera y Arcas"
                    },
                    {
                        "name": "Michael Levin"
                    }
                ],
                "author_detail": {
                    "name": "Michael Levin"
                },
                "author": "Michael Levin",
                "arxiv_comment": "54 pages, 4 figures, 2 tables, 11 supplemental figures, 3\n  supplemental tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15849v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15849v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.OT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.OT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.CB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04405v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04405v1",
                "updated": "2025-08-06T12:47:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    12,
                    47,
                    5,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T12:47:05Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    12,
                    47,
                    5,
                    2,
                    218,
                    0
                ],
                "title": "FlexQ: Efficient Post-training INT6 Quantization for LLM Serving via\n  Algorithm-System Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexQ: Efficient Post-training INT6 Quantization for LLM Serving via\n  Algorithm-System Co-Design"
                },
                "summary": "Large Language Models (LLMs) demonstrate exceptional performance but entail\nsignificant memory and computational costs, restricting their practical\ndeployment. While existing INT4/INT8 quantization reduces these costs, they\noften degrade accuracy or lack optimal efficiency. INT6 quantization offers a\nsuperior trade-off between model accuracy and inference efficiency, but lacks\nhardware support in modern GPUs, forcing emulation via higher-precision\narithmetic units that limit acceleration.\n  In this paper, we propose FlexQ, a novel post-training INT6 quantization\nframework combining algorithmic innovation with system-level optimizations.\nFlexQ employs uniform 6-bit weight quantization across all layers, with\nadaptive retention of 8-bit activations in layers identified through layer-wise\nsensitivity analysis. To maximize hardware efficiency, we develop a specialized\nhigh-performance GPU kernel supporting matrix multiplication for W6A6 and W6A8\nrepresentations via Binary Tensor Core (BTC) equivalents, effectively bypassing\nthe lack of native INT6 tensor cores. Evaluations on LLaMA models show FlexQ\nmaintains near-FP16 accuracy, with perplexity increases of no more than 0.05.\nThe proposed kernel achieves an average 1.39$\\times$ speedup over ABQ-LLM on\nLLaMA-2-70B linear layers. End-to-end, FlexQ delivers 1.33$\\times$ inference\nacceleration and 1.21$\\times$ memory savings over SmoothQuant. Code is released\nat https://github.com/FlyFoxPlayer/FlexQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate exceptional performance but entail\nsignificant memory and computational costs, restricting their practical\ndeployment. While existing INT4/INT8 quantization reduces these costs, they\noften degrade accuracy or lack optimal efficiency. INT6 quantization offers a\nsuperior trade-off between model accuracy and inference efficiency, but lacks\nhardware support in modern GPUs, forcing emulation via higher-precision\narithmetic units that limit acceleration.\n  In this paper, we propose FlexQ, a novel post-training INT6 quantization\nframework combining algorithmic innovation with system-level optimizations.\nFlexQ employs uniform 6-bit weight quantization across all layers, with\nadaptive retention of 8-bit activations in layers identified through layer-wise\nsensitivity analysis. To maximize hardware efficiency, we develop a specialized\nhigh-performance GPU kernel supporting matrix multiplication for W6A6 and W6A8\nrepresentations via Binary Tensor Core (BTC) equivalents, effectively bypassing\nthe lack of native INT6 tensor cores. Evaluations on LLaMA models show FlexQ\nmaintains near-FP16 accuracy, with perplexity increases of no more than 0.05.\nThe proposed kernel achieves an average 1.39$\\times$ speedup over ABQ-LLM on\nLLaMA-2-70B linear layers. End-to-end, FlexQ delivers 1.33$\\times$ inference\nacceleration and 1.21$\\times$ memory savings over SmoothQuant. Code is released\nat https://github.com/FlyFoxPlayer/FlexQ."
                },
                "authors": [
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Aining Jia"
                    },
                    {
                        "name": "Weifeng Bu"
                    },
                    {
                        "name": "Yushu Cai"
                    },
                    {
                        "name": "Kai Sheng"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Xin He"
                    }
                ],
                "author_detail": {
                    "name": "Xin He"
                },
                "author": "Xin He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04405v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04405v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04401v1",
                "updated": "2025-08-06T12:43:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    12,
                    43,
                    4,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T12:43:04Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    12,
                    43,
                    4,
                    2,
                    218,
                    0
                ],
                "title": "Why are LLMs' abilities emergent?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why are LLMs' abilities emergent?"
                },
                "summary": "The remarkable success of Large Language Models (LLMs) in generative tasks\nhas raised fundamental questions about the nature of their acquired\ncapabilities, which often appear to emerge unexpectedly without explicit\ntraining. This paper examines the emergent properties of Deep Neural Networks\n(DNNs) through both theoretical analysis and empirical observation, addressing\nthe epistemological challenge of \"creation without understanding\" that\ncharacterises contemporary AI development. We explore how the neural approach's\nreliance on nonlinear, stochastic processes fundamentally differs from symbolic\ncomputational paradigms, creating systems whose macro-level behaviours cannot\nbe analytically derived from micro-level neuron activities. Through analysis of\nscaling laws, grokking phenomena, and phase transitions in model capabilities,\nI demonstrate that emergent abilities arise from the complex dynamics of highly\nsensitive nonlinear systems rather than simply from parameter scaling alone. My\ninvestigation reveals that current debates over metrics, pre-training loss\nthresholds, and in-context learning miss the fundamental ontological nature of\nemergence in DNNs. I argue that these systems exhibit genuine emergent\nproperties analogous to those found in other complex natural phenomena, where\nsystemic capabilities emerge from cooperative interactions among simple\ncomponents without being reducible to their individual behaviours. The paper\nconcludes that understanding LLM capabilities requires recognising DNNs as a\nnew domain of complex dynamical systems governed by universal principles of\nemergence, similar to those operating in physics, chemistry, and biology. This\nperspective shifts the focus from purely phenomenological definitions of\nemergence to understanding the internal dynamic transformations that enable\nthese systems to acquire capabilities that transcend their individual\ncomponents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable success of Large Language Models (LLMs) in generative tasks\nhas raised fundamental questions about the nature of their acquired\ncapabilities, which often appear to emerge unexpectedly without explicit\ntraining. This paper examines the emergent properties of Deep Neural Networks\n(DNNs) through both theoretical analysis and empirical observation, addressing\nthe epistemological challenge of \"creation without understanding\" that\ncharacterises contemporary AI development. We explore how the neural approach's\nreliance on nonlinear, stochastic processes fundamentally differs from symbolic\ncomputational paradigms, creating systems whose macro-level behaviours cannot\nbe analytically derived from micro-level neuron activities. Through analysis of\nscaling laws, grokking phenomena, and phase transitions in model capabilities,\nI demonstrate that emergent abilities arise from the complex dynamics of highly\nsensitive nonlinear systems rather than simply from parameter scaling alone. My\ninvestigation reveals that current debates over metrics, pre-training loss\nthresholds, and in-context learning miss the fundamental ontological nature of\nemergence in DNNs. I argue that these systems exhibit genuine emergent\nproperties analogous to those found in other complex natural phenomena, where\nsystemic capabilities emerge from cooperative interactions among simple\ncomponents without being reducible to their individual behaviours. The paper\nconcludes that understanding LLM capabilities requires recognising DNNs as a\nnew domain of complex dynamical systems governed by universal principles of\nemergence, similar to those operating in physics, chemistry, and biology. This\nperspective shifts the focus from purely phenomenological definitions of\nemergence to understanding the internal dynamic transformations that enable\nthese systems to acquire capabilities that transcend their individual\ncomponents."
                },
                "authors": [
                    {
                        "name": "Vladimr Havlk"
                    }
                ],
                "author_detail": {
                    "name": "Vladimr Havlk"
                },
                "author": "Vladimr Havlk",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03329v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03329v2",
                "updated": "2025-08-06T12:41:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    12,
                    41,
                    21,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-05T11:15:06Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    11,
                    15,
                    6,
                    1,
                    217,
                    0
                ],
                "title": "Industrial LLM-based Code Optimization under Regulation: A\n  Mixture-of-Agents Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Industrial LLM-based Code Optimization under Regulation: A\n  Mixture-of-Agents Approach"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) for code optimization\nhave enabled industrial platforms to automate software performance engineering\nat unprecedented scale and speed. Yet, organizations in regulated industries\nface strict constraints on which LLMs they can use - many cannot utilize\ncommercial models due to data privacy regulations and compliance requirements,\ncreating a significant challenge for achieving high-quality code optimization\nwhile maintaining cost-effectiveness. We address this by implementing a\nMixture-of-Agents (MoA) approach that directly synthesizes code from multiple\nspecialized LLMs, comparing it against TurinTech AI's vanilla Genetic Algorithm\n(GA)-based ensemble system and individual LLM optimizers using real-world\nindustrial codebases. Our key contributions include: (1) First MoA application\nto industrial code optimization using real-world codebases; (2) Empirical\nevidence that MoA excels with open-source models, achieving 14.3% to 22.2% cost\nsavings and 28.6% to 32.2% faster optimization times for regulated\nenvironments; (3) Deployment guidelines demonstrating GA's advantage with\ncommercial models while both ensembles outperform individual LLMs; and (4)\nReal-world validation across 50 code snippets and seven LLM combinations,\ngenerating over 8,700 variants, addresses gaps in industrial LLM ensemble\nevaluation. This provides actionable guidance for organizations balancing\nregulatory compliance with optimization performance in production environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) for code optimization\nhave enabled industrial platforms to automate software performance engineering\nat unprecedented scale and speed. Yet, organizations in regulated industries\nface strict constraints on which LLMs they can use - many cannot utilize\ncommercial models due to data privacy regulations and compliance requirements,\ncreating a significant challenge for achieving high-quality code optimization\nwhile maintaining cost-effectiveness. We address this by implementing a\nMixture-of-Agents (MoA) approach that directly synthesizes code from multiple\nspecialized LLMs, comparing it against TurinTech AI's vanilla Genetic Algorithm\n(GA)-based ensemble system and individual LLM optimizers using real-world\nindustrial codebases. Our key contributions include: (1) First MoA application\nto industrial code optimization using real-world codebases; (2) Empirical\nevidence that MoA excels with open-source models, achieving 14.3% to 22.2% cost\nsavings and 28.6% to 32.2% faster optimization times for regulated\nenvironments; (3) Deployment guidelines demonstrating GA's advantage with\ncommercial models while both ensembles outperform individual LLMs; and (4)\nReal-world validation across 50 code snippets and seven LLM combinations,\ngenerating over 8,700 variants, addresses gaps in industrial LLM ensemble\nevaluation. This provides actionable guidance for organizations balancing\nregulatory compliance with optimization performance in production environments."
                },
                "authors": [
                    {
                        "name": "Mari Ashiga"
                    },
                    {
                        "name": "Vardan Voskanyan"
                    },
                    {
                        "name": "Fateme Dinmohammadi"
                    },
                    {
                        "name": "Jingzhi Gong"
                    },
                    {
                        "name": "Paul Brookes"
                    },
                    {
                        "name": "Matthew Truscott"
                    },
                    {
                        "name": "Rafail Giavrimis"
                    },
                    {
                        "name": "Mike Basios"
                    },
                    {
                        "name": "Leslie Kanthan"
                    },
                    {
                        "name": "Wei Jie"
                    }
                ],
                "author_detail": {
                    "name": "Wei Jie"
                },
                "author": "Wei Jie",
                "arxiv_comment": "Submitted to ASE'25 Industry Showcase",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03329v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03329v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04399v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04399v1",
                "updated": "2025-08-06T12:41:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    12,
                    41,
                    18,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T12:41:18Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    12,
                    41,
                    18,
                    2,
                    218,
                    0
                ],
                "title": "Improving Crash Data Quality with Large Language Models: Evidence from\n  Secondary Crash Narratives in Kentucky",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Crash Data Quality with Large Language Models: Evidence from\n  Secondary Crash Narratives in Kentucky"
                },
                "summary": "This study evaluates advanced natural language processing (NLP) techniques to\nenhance crash data quality by mining crash narratives, using secondary crash\nidentification in Kentucky as a case study. Drawing from 16,656 manually\nreviewed narratives from 2015-2022, with 3,803 confirmed secondary crashes, we\ncompare three model classes: zero-shot open-source large language models (LLMs)\n(LLaMA3:70B, DeepSeek-R1:70B, Qwen3:32B, Gemma3:27B); fine-tuned transformers\n(BERT, DistilBERT, RoBERTa, XLNet, Longformer); and traditional logistic\nregression as baseline. Models were calibrated on 2015-2021 data and tested on\n1,771 narratives from 2022. Fine-tuned transformers achieved superior\nperformance, with RoBERTa yielding the highest F1-score (0.90) and accuracy\n(95%). Zero-shot LLaMA3:70B reached a comparable F1 of 0.86 but required 139\nminutes of inference; the logistic baseline lagged well behind (F1:0.66). LLMs\nexcelled in recall for some variants (e.g., GEMMA3:27B at 0.94) but incurred\nhigh computational costs (up to 723 minutes for DeepSeek-R1:70B), while\nfine-tuned models processed the test set in seconds after brief training.\nFurther analysis indicated that mid-sized LLMs (e.g., DeepSeek-R1:32B) can\nrival larger counterparts in performance while reducing runtime, suggesting\nopportunities for optimized deployments. Results highlight trade-offs between\naccuracy, efficiency, and data requirements, with fine-tuned transformer models\nbalancing precision and recall effectively on Kentucky data. Practical\ndeployment considerations emphasize privacy-preserving local deployment,\nensemble approaches for improved accuracy, and incremental processing for\nscalability, providing a replicable scheme for enhancing crash-data quality\nwith advanced NLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study evaluates advanced natural language processing (NLP) techniques to\nenhance crash data quality by mining crash narratives, using secondary crash\nidentification in Kentucky as a case study. Drawing from 16,656 manually\nreviewed narratives from 2015-2022, with 3,803 confirmed secondary crashes, we\ncompare three model classes: zero-shot open-source large language models (LLMs)\n(LLaMA3:70B, DeepSeek-R1:70B, Qwen3:32B, Gemma3:27B); fine-tuned transformers\n(BERT, DistilBERT, RoBERTa, XLNet, Longformer); and traditional logistic\nregression as baseline. Models were calibrated on 2015-2021 data and tested on\n1,771 narratives from 2022. Fine-tuned transformers achieved superior\nperformance, with RoBERTa yielding the highest F1-score (0.90) and accuracy\n(95%). Zero-shot LLaMA3:70B reached a comparable F1 of 0.86 but required 139\nminutes of inference; the logistic baseline lagged well behind (F1:0.66). LLMs\nexcelled in recall for some variants (e.g., GEMMA3:27B at 0.94) but incurred\nhigh computational costs (up to 723 minutes for DeepSeek-R1:70B), while\nfine-tuned models processed the test set in seconds after brief training.\nFurther analysis indicated that mid-sized LLMs (e.g., DeepSeek-R1:32B) can\nrival larger counterparts in performance while reducing runtime, suggesting\nopportunities for optimized deployments. Results highlight trade-offs between\naccuracy, efficiency, and data requirements, with fine-tuned transformer models\nbalancing precision and recall effectively on Kentucky data. Practical\ndeployment considerations emphasize privacy-preserving local deployment,\nensemble approaches for improved accuracy, and incremental processing for\nscalability, providing a replicable scheme for enhancing crash-data quality\nwith advanced NLP."
                },
                "authors": [
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Mei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Mei Chen"
                },
                "author": "Mei Chen",
                "arxiv_comment": "19 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04399v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04399v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04393v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04393v1",
                "updated": "2025-08-06T12:37:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    12,
                    37,
                    45,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T12:37:45Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    12,
                    37,
                    45,
                    2,
                    218,
                    0
                ],
                "title": "Generative Flexible Latent Structure Regression (GFLSR) model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Flexible Latent Structure Regression (GFLSR) model"
                },
                "summary": "Latent structure methods, specifically linear continuous latent structure\nmethods, are a type of fundamental statistical learning strategy. They are\nwidely used for dimension reduction, regression and prediction, in the fields\nof chemometrics, economics, social science and etc. However, due to the lack of\nmodel inference, generative form, and unidentifiable parameters, most of these\nmethods are always used as an algorithm, instead of a model. This paper\nproposed a Generative Flexible Latent Structure Regression (GFLSR) model\nstructure to address this problem. Moreover, we show that most linear\ncontinuous latent variable methods can be represented under the proposed\nframework. The recursive structure allows potential model inference and\nresidual analysis. Then, the traditional Partial Least Squares (PLS) is\nfocused; we show that the PLS can be specialised in the proposed model\nstructure, named Generative-PLS. With a model structure, we analyse the\nconvergence of the parameters and the latent variables. Under additional\ndistribution assumptions, we show that the proposed model structure can lead to\nmodel inference without solving the probabilistic model. Additionally, we\nproposed a novel bootstrap algorithm that enables uncertainty on parameters and\non prediction for new datasets. A simulation study and a Real-world dataset are\nused to verify the proposed Generative-PLS model structure. Although the\ntraditional PLS is a special case, this proposed GFLSRM structure leads to a\npotential inference structure for all the linear continuous latent variable\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent structure methods, specifically linear continuous latent structure\nmethods, are a type of fundamental statistical learning strategy. They are\nwidely used for dimension reduction, regression and prediction, in the fields\nof chemometrics, economics, social science and etc. However, due to the lack of\nmodel inference, generative form, and unidentifiable parameters, most of these\nmethods are always used as an algorithm, instead of a model. This paper\nproposed a Generative Flexible Latent Structure Regression (GFLSR) model\nstructure to address this problem. Moreover, we show that most linear\ncontinuous latent variable methods can be represented under the proposed\nframework. The recursive structure allows potential model inference and\nresidual analysis. Then, the traditional Partial Least Squares (PLS) is\nfocused; we show that the PLS can be specialised in the proposed model\nstructure, named Generative-PLS. With a model structure, we analyse the\nconvergence of the parameters and the latent variables. Under additional\ndistribution assumptions, we show that the proposed model structure can lead to\nmodel inference without solving the probabilistic model. Additionally, we\nproposed a novel bootstrap algorithm that enables uncertainty on parameters and\non prediction for new datasets. A simulation study and a Real-world dataset are\nused to verify the proposed Generative-PLS model structure. Although the\ntraditional PLS is a special case, this proposed GFLSRM structure leads to a\npotential inference structure for all the linear continuous latent variable\nmethods."
                },
                "authors": [
                    {
                        "name": "Clara Grazian"
                    },
                    {
                        "name": "Qian Jin"
                    },
                    {
                        "name": "Pierre Lafaye De Micheaux"
                    }
                ],
                "author_detail": {
                    "name": "Pierre Lafaye De Micheaux"
                },
                "author": "Pierre Lafaye De Micheaux",
                "arxiv_comment": "44 pages in total",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04393v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04393v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62H25",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03679v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03679v2",
                "updated": "2025-08-06T12:22:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    12,
                    22,
                    27,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-05T17:50:03Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    17,
                    50,
                    3,
                    1,
                    217,
                    0
                ],
                "title": "Streaming Generated Gaussian Process Experts for Online Learning and\n  Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming Generated Gaussian Process Experts for Online Learning and\n  Control"
                },
                "summary": "Gaussian Processes (GPs), as a nonparametric learning method, offer flexible\nmodeling capabilities and calibrated uncertainty quantification for function\napproximations. Additionally, GPs support online learning by efficiently\nincorporating new data with polynomial-time computation, making them\nwell-suited for safety-critical dynamical systems that require rapid\nadaptation. However, the inference and online updates of exact GPs, when\nprocessing streaming data, incur cubic computation time and quadratic storage\nmemory complexity, limiting their scalability to large datasets in real-time\nsettings. In this paper, we propose a streaming kernel-induced progressively\ngenerated expert framework of Gaussian processes (SkyGP) that addresses both\ncomputational and memory constraints by maintaining a bounded set of experts,\nwhile inheriting the learning performance guarantees from exact Gaussian\nprocesses. Furthermore, two SkyGP variants are introduced, each tailored to a\nspecific objective, either maximizing prediction accuracy (SkyGP-Dense) or\nimproving computational efficiency (SkyGP-Fast). The effectiveness of SkyGP is\nvalidated through extensive benchmarks and real-time control experiments\ndemonstrating its superior performance compared to state-of-the-art approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian Processes (GPs), as a nonparametric learning method, offer flexible\nmodeling capabilities and calibrated uncertainty quantification for function\napproximations. Additionally, GPs support online learning by efficiently\nincorporating new data with polynomial-time computation, making them\nwell-suited for safety-critical dynamical systems that require rapid\nadaptation. However, the inference and online updates of exact GPs, when\nprocessing streaming data, incur cubic computation time and quadratic storage\nmemory complexity, limiting their scalability to large datasets in real-time\nsettings. In this paper, we propose a streaming kernel-induced progressively\ngenerated expert framework of Gaussian processes (SkyGP) that addresses both\ncomputational and memory constraints by maintaining a bounded set of experts,\nwhile inheriting the learning performance guarantees from exact Gaussian\nprocesses. Furthermore, two SkyGP variants are introduced, each tailored to a\nspecific objective, either maximizing prediction accuracy (SkyGP-Dense) or\nimproving computational efficiency (SkyGP-Fast). The effectiveness of SkyGP is\nvalidated through extensive benchmarks and real-time control experiments\ndemonstrating its superior performance compared to state-of-the-art approaches."
                },
                "authors": [
                    {
                        "name": "Zewen Yang"
                    },
                    {
                        "name": "Dongfa Zhang"
                    },
                    {
                        "name": "Xiaobing Dai"
                    },
                    {
                        "name": "Fengyi Yu"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Bingkun Huang"
                    },
                    {
                        "name": "Hamid Sadeghian"
                    },
                    {
                        "name": "Sami Haddadin"
                    }
                ],
                "author_detail": {
                    "name": "Sami Haddadin"
                },
                "author": "Sami Haddadin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03679v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03679v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01602v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01602v2",
                "updated": "2025-08-06T12:04:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    12,
                    4,
                    27,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-03T05:38:14Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    5,
                    38,
                    14,
                    6,
                    215,
                    0
                ],
                "title": "Enhancing Zero-Shot Brain Tumor Subtype Classification via Fine-Grained\n  Patch-Text Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Zero-Shot Brain Tumor Subtype Classification via Fine-Grained\n  Patch-Text Alignment"
                },
                "summary": "The fine-grained classification of brain tumor subtypes from\nhistopathological whole slide images is highly challenging due to subtle\nmorphological variations and the scarcity of annotated data. Although\nvision-language models have enabled promising zero-shot classification, their\nability to capture fine-grained pathological features remains limited,\nresulting in suboptimal subtype discrimination. To address these challenges, we\npropose the Fine-Grained Patch Alignment Network (FG-PAN), a novel zero-shot\nframework tailored for digital pathology. FG-PAN consists of two key modules:\n(1) a local feature refinement module that enhances patch-level visual features\nby modeling spatial relationships among representative patches, and (2) a\nfine-grained text description generation module that leverages large language\nmodels to produce pathology-aware, class-specific semantic prototypes. By\naligning refined visual features with LLM-generated fine-grained descriptions,\nFG-PAN effectively increases class separability in both visual and semantic\nspaces. Extensive experiments on multiple public pathology datasets, including\nEBRAINS and TCGA, demonstrate that FG-PAN achieves state-of-the-art performance\nand robust generalization in zero-shot brain tumor subtype classification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fine-grained classification of brain tumor subtypes from\nhistopathological whole slide images is highly challenging due to subtle\nmorphological variations and the scarcity of annotated data. Although\nvision-language models have enabled promising zero-shot classification, their\nability to capture fine-grained pathological features remains limited,\nresulting in suboptimal subtype discrimination. To address these challenges, we\npropose the Fine-Grained Patch Alignment Network (FG-PAN), a novel zero-shot\nframework tailored for digital pathology. FG-PAN consists of two key modules:\n(1) a local feature refinement module that enhances patch-level visual features\nby modeling spatial relationships among representative patches, and (2) a\nfine-grained text description generation module that leverages large language\nmodels to produce pathology-aware, class-specific semantic prototypes. By\naligning refined visual features with LLM-generated fine-grained descriptions,\nFG-PAN effectively increases class separability in both visual and semantic\nspaces. Extensive experiments on multiple public pathology datasets, including\nEBRAINS and TCGA, demonstrate that FG-PAN achieves state-of-the-art performance\nand robust generalization in zero-shot brain tumor subtype classification."
                },
                "authors": [
                    {
                        "name": "Lubin Gan"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Linhao Qu"
                    },
                    {
                        "name": "Yijun Wang"
                    },
                    {
                        "name": "Siying Wu"
                    },
                    {
                        "name": "Xiaoyan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyan Sun"
                },
                "author": "Xiaoyan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01602v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01602v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03457v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03457v2",
                "updated": "2025-08-06T11:50:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    50,
                    17,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-05T13:57:03Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    13,
                    57,
                    3,
                    1,
                    217,
                    0
                ],
                "title": "READ: Real-time and Efficient Asynchronous Diffusion for Audio-driven\n  Talking Head Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "READ: Real-time and Efficient Asynchronous Diffusion for Audio-driven\n  Talking Head Generation"
                },
                "summary": "The introduction of diffusion models has brought significant advances to the\nfield of audio-driven talking head generation. However, the extremely slow\ninference speed severely limits the practical implementation of diffusion-based\ntalking head generation models. In this study, we propose READ, the first\nreal-time diffusion-transformer-based talking head generation framework. Our\napproach first learns a spatiotemporal highly compressed video latent space via\na temporal VAE, significantly reducing the token count to accelerate\ngeneration. To achieve better audio-visual alignment within this compressed\nlatent space, a pre-trained Speech Autoencoder (SpeechAE) is proposed to\ngenerate temporally compressed speech latent codes corresponding to the video\nlatent space. These latent representations are then modeled by a carefully\ndesigned Audio-to-Video Diffusion Transformer (A2V-DiT) backbone for efficient\ntalking head synthesis. Furthermore, to ensure temporal consistency and\naccelerated inference in extended generation, we propose a novel asynchronous\nnoise scheduler (ANS) for both the training and inference process of our\nframework. The ANS leverages asynchronous add-noise and asynchronous\nmotion-guided generation in the latent space, ensuring consistency in generated\nvideo clips. Experimental results demonstrate that READ outperforms\nstate-of-the-art methods by generating competitive talking head videos with\nsignificantly reduced runtime, achieving an optimal balance between quality and\nspeed while maintaining robust metric stability in long-time generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The introduction of diffusion models has brought significant advances to the\nfield of audio-driven talking head generation. However, the extremely slow\ninference speed severely limits the practical implementation of diffusion-based\ntalking head generation models. In this study, we propose READ, the first\nreal-time diffusion-transformer-based talking head generation framework. Our\napproach first learns a spatiotemporal highly compressed video latent space via\na temporal VAE, significantly reducing the token count to accelerate\ngeneration. To achieve better audio-visual alignment within this compressed\nlatent space, a pre-trained Speech Autoencoder (SpeechAE) is proposed to\ngenerate temporally compressed speech latent codes corresponding to the video\nlatent space. These latent representations are then modeled by a carefully\ndesigned Audio-to-Video Diffusion Transformer (A2V-DiT) backbone for efficient\ntalking head synthesis. Furthermore, to ensure temporal consistency and\naccelerated inference in extended generation, we propose a novel asynchronous\nnoise scheduler (ANS) for both the training and inference process of our\nframework. The ANS leverages asynchronous add-noise and asynchronous\nmotion-guided generation in the latent space, ensuring consistency in generated\nvideo clips. Experimental results demonstrate that READ outperforms\nstate-of-the-art methods by generating competitive talking head videos with\nsignificantly reduced runtime, achieving an optimal balance between quality and\nspeed while maintaining robust metric stability in long-time generation."
                },
                "authors": [
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Yuzhe Weng"
                    },
                    {
                        "name": "Jun Du"
                    },
                    {
                        "name": "Haoran Xu"
                    },
                    {
                        "name": "Xiaoyan Wu"
                    },
                    {
                        "name": "Shan He"
                    },
                    {
                        "name": "Bing Yin"
                    },
                    {
                        "name": "Cong Liu"
                    },
                    {
                        "name": "Jianqing Gao"
                    },
                    {
                        "name": "Qingfeng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qingfeng Liu"
                },
                "author": "Qingfeng Liu",
                "arxiv_comment": "Project page: https://readportrait.github.io/READ/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03457v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03457v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04353v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04353v1",
                "updated": "2025-08-06T11:48:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    48,
                    51,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T11:48:51Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    48,
                    51,
                    2,
                    218,
                    0
                ],
                "title": "LUST: A Multi-Modal Framework with Hierarchical LLM-based Scoring for\n  Learned Thematic Significance Tracking in Multimedia Content",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LUST: A Multi-Modal Framework with Hierarchical LLM-based Scoring for\n  Learned Thematic Significance Tracking in Multimedia Content"
                },
                "summary": "This paper introduces the Learned User Significance Tracker (LUST), a\nframework designed to analyze video content and quantify the thematic relevance\nof its segments in relation to a user-provided textual description of\nsignificance. LUST leverages a multi-modal analytical pipeline, integrating\nvisual cues from video frames with textual information extracted via Automatic\nSpeech Recognition (ASR) from the audio track. The core innovation lies in a\nhierarchical, two-stage relevance scoring mechanism employing Large Language\nModels (LLMs). An initial \"direct relevance\" score, $S_{d,i}$, assesses\nindividual segments based on immediate visual and auditory content against the\ntheme. This is followed by a \"contextual relevance\" score, $S_{c,i}$, that\nrefines the assessment by incorporating the temporal progression of preceding\nthematic scores, allowing the model to understand evolving narratives. The LUST\nframework aims to provide a nuanced, temporally-aware measure of user-defined\nsignificance, outputting an annotated video with visualized relevance scores\nand comprehensive analytical logs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces the Learned User Significance Tracker (LUST), a\nframework designed to analyze video content and quantify the thematic relevance\nof its segments in relation to a user-provided textual description of\nsignificance. LUST leverages a multi-modal analytical pipeline, integrating\nvisual cues from video frames with textual information extracted via Automatic\nSpeech Recognition (ASR) from the audio track. The core innovation lies in a\nhierarchical, two-stage relevance scoring mechanism employing Large Language\nModels (LLMs). An initial \"direct relevance\" score, $S_{d,i}$, assesses\nindividual segments based on immediate visual and auditory content against the\ntheme. This is followed by a \"contextual relevance\" score, $S_{c,i}$, that\nrefines the assessment by incorporating the temporal progression of preceding\nthematic scores, allowing the model to understand evolving narratives. The LUST\nframework aims to provide a nuanced, temporally-aware measure of user-defined\nsignificance, outputting an annotated video with visualized relevance scores\nand comprehensive analytical logs."
                },
                "authors": [
                    {
                        "name": "Anderson de Lima Luiz"
                    }
                ],
                "author_detail": {
                    "name": "Anderson de Lima Luiz"
                },
                "author": "Anderson de Lima Luiz",
                "arxiv_comment": "5 pages and 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04353v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04353v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04350v1",
                "updated": "2025-08-06T11:42:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    42,
                    54,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T11:42:54Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    42,
                    54,
                    2,
                    218,
                    0
                ],
                "title": "Chain of Questions: Guiding Multimodal Curiosity in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Questions: Guiding Multimodal Curiosity in Language Models"
                },
                "summary": "Reasoning capabilities in large language models (LLMs) have substantially\nadvanced through methods such as chain-of-thought and explicit step-by-step\nexplanations. However, these improvements have not yet fully transitioned to\nmultimodal contexts, where models must proactively decide which sensory\nmodalities such as vision, audio, or spatial perception to engage when\ninteracting with complex real-world environments. In this paper, we introduce\nthe Chain of Questions (CoQ) framework, a curiosity-driven reasoning approach\nthat encourages multimodal language models to dynamically generate targeted\nquestions regarding their surroundings. These generated questions guide the\nmodel to selectively activate relevant modalities, thereby gathering critical\ninformation necessary for accurate reasoning and response generation. We\nevaluate our framework on a novel multimodal benchmark dataset, assembled by\nintegrating WebGPT, ScienceQA, AVSD, and ScanQA datasets. Experimental results\ndemonstrate that our CoQ method improves a foundation model's ability to\neffectively identify and integrate pertinent sensory information. This leads to\nimproved accuracy, interpretability, and alignment of the reasoning process\nwith diverse multimodal tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning capabilities in large language models (LLMs) have substantially\nadvanced through methods such as chain-of-thought and explicit step-by-step\nexplanations. However, these improvements have not yet fully transitioned to\nmultimodal contexts, where models must proactively decide which sensory\nmodalities such as vision, audio, or spatial perception to engage when\ninteracting with complex real-world environments. In this paper, we introduce\nthe Chain of Questions (CoQ) framework, a curiosity-driven reasoning approach\nthat encourages multimodal language models to dynamically generate targeted\nquestions regarding their surroundings. These generated questions guide the\nmodel to selectively activate relevant modalities, thereby gathering critical\ninformation necessary for accurate reasoning and response generation. We\nevaluate our framework on a novel multimodal benchmark dataset, assembled by\nintegrating WebGPT, ScienceQA, AVSD, and ScanQA datasets. Experimental results\ndemonstrate that our CoQ method improves a foundation model's ability to\neffectively identify and integrate pertinent sensory information. This leads to\nimproved accuracy, interpretability, and alignment of the reasoning process\nwith diverse multimodal tasks."
                },
                "authors": [
                    {
                        "name": "Nima Iji"
                    },
                    {
                        "name": "Kia Dashtipour"
                    }
                ],
                "author_detail": {
                    "name": "Kia Dashtipour"
                },
                "author": "Kia Dashtipour",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04349v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04349v1",
                "updated": "2025-08-06T11:42:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    42,
                    47,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T11:42:47Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    42,
                    47,
                    2,
                    218,
                    0
                ],
                "title": "GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy\n  Entropy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy\n  Entropy"
                },
                "summary": "Reinforcement learning (RL) with algorithms like Group Relative Policy\nOptimization (GRPO) improves Large Language Model (LLM) reasoning, but is\nlimited by a coarse-grained credit assignment that applies a uniform reward to\nall tokens in a sequence. This is a major flaw in long-chain reasoning tasks.\nThis paper solves this with \\textbf{Dynamic Entropy Weighting}. Our core idea\nis that high-entropy tokens in correct responses can guide the policy toward a\nhigher performance ceiling. This allows us to create more fine-grained reward\nsignals for precise policy updates via two ways: 1) \\textbf{Group Token Policy\nOptimization} (\\textbf{GTPO}), we assigns a entropy-weighted reward to each\ntoken for fine-grained credit assignment. 2) \\textbf{Sequence-Level Group\nRelative Policy Optimization} (\\textbf{GRPO-S}), we assigns a entropy-weighted\nreward to each sequence based on its average token entropy. Experiments show\nour methods significantly outperform the strong DAPO baseline. The results\nconfirm that our entropy-weighting mechanism is the key driver of this\nperformance boost, offering a better path to enhance deep reasoning in models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) with algorithms like Group Relative Policy\nOptimization (GRPO) improves Large Language Model (LLM) reasoning, but is\nlimited by a coarse-grained credit assignment that applies a uniform reward to\nall tokens in a sequence. This is a major flaw in long-chain reasoning tasks.\nThis paper solves this with \\textbf{Dynamic Entropy Weighting}. Our core idea\nis that high-entropy tokens in correct responses can guide the policy toward a\nhigher performance ceiling. This allows us to create more fine-grained reward\nsignals for precise policy updates via two ways: 1) \\textbf{Group Token Policy\nOptimization} (\\textbf{GTPO}), we assigns a entropy-weighted reward to each\ntoken for fine-grained credit assignment. 2) \\textbf{Sequence-Level Group\nRelative Policy Optimization} (\\textbf{GRPO-S}), we assigns a entropy-weighted\nreward to each sequence based on its average token entropy. Experiments show\nour methods significantly outperform the strong DAPO baseline. The results\nconfirm that our entropy-weighting mechanism is the key driver of this\nperformance boost, offering a better path to enhance deep reasoning in models."
                },
                "authors": [
                    {
                        "name": "Hongze Tan"
                    },
                    {
                        "name": "Jianfei Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Pan"
                },
                "author": "Jianfei Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04349v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04349v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04346v1",
                "updated": "2025-08-06T11:41:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    41,
                    10,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T11:41:10Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    41,
                    10,
                    2,
                    218,
                    0
                ],
                "title": "From Split to Share: Private Inference with Distributed Feature Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Split to Share: Private Inference with Distributed Feature Sharing"
                },
                "summary": "Cloud-based Machine Learning as a Service (MLaaS) raises serious privacy\nconcerns when handling sensitive client data. Existing Private Inference (PI)\nmethods face a fundamental trade-off between privacy and efficiency:\ncryptographic approaches offer strong protection but incur high computational\noverhead, while efficient alternatives such as split inference expose\nintermediate features to inversion attacks. We propose PrivDFS, a new paradigm\nfor private inference that replaces a single exposed representation with\ndistributed feature sharing. PrivDFS partitions input features on the client\ninto multiple balanced shares, which are distributed to non-colluding,\nnon-communicating servers for independent partial inference. The client\nsecurely aggregates the servers' outputs to reconstruct the final prediction,\nensuring that no single server observes sufficient information to compromise\ninput privacy. To further strengthen privacy, we propose two key extensions:\nPrivDFS-AT, which uses adversarial training with a diffusion-based proxy\nattacker to enforce inversion-resistant feature partitioning, and PrivDFS-KD,\nwhich leverages user-specific keys to diversify partitioning policies and\nprevent query-based inversion generalization. Experiments on CIFAR-10 and\nCelebA demonstrate that PrivDFS achieves privacy comparable to deep split\ninference while cutting client computation by up to 100 times with no accuracy\nloss, and that the extensions remain robust against both diffusion-based\nin-distribution and adaptive attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud-based Machine Learning as a Service (MLaaS) raises serious privacy\nconcerns when handling sensitive client data. Existing Private Inference (PI)\nmethods face a fundamental trade-off between privacy and efficiency:\ncryptographic approaches offer strong protection but incur high computational\noverhead, while efficient alternatives such as split inference expose\nintermediate features to inversion attacks. We propose PrivDFS, a new paradigm\nfor private inference that replaces a single exposed representation with\ndistributed feature sharing. PrivDFS partitions input features on the client\ninto multiple balanced shares, which are distributed to non-colluding,\nnon-communicating servers for independent partial inference. The client\nsecurely aggregates the servers' outputs to reconstruct the final prediction,\nensuring that no single server observes sufficient information to compromise\ninput privacy. To further strengthen privacy, we propose two key extensions:\nPrivDFS-AT, which uses adversarial training with a diffusion-based proxy\nattacker to enforce inversion-resistant feature partitioning, and PrivDFS-KD,\nwhich leverages user-specific keys to diversify partitioning policies and\nprevent query-based inversion generalization. Experiments on CIFAR-10 and\nCelebA demonstrate that PrivDFS achieves privacy comparable to deep split\ninference while cutting client computation by up to 100 times with no accuracy\nloss, and that the extensions remain robust against both diffusion-based\nin-distribution and adaptive attacks."
                },
                "authors": [
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Jiayi Wen"
                    },
                    {
                        "name": "Shouhong Tan"
                    },
                    {
                        "name": "Zhirun Zheng"
                    },
                    {
                        "name": "Cheng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Cheng Huang"
                },
                "author": "Cheng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06382v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06382v4",
                "updated": "2025-08-06T11:34:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    34,
                    54,
                    2,
                    218,
                    0
                ],
                "published": "2025-06-04T23:28:39Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    23,
                    28,
                    39,
                    2,
                    155,
                    0
                ],
                "title": "On the Fundamental Impossibility of Hallucination Control in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Fundamental Impossibility of Hallucination Control in Large\n  Language Models"
                },
                "summary": "This paper establishes a fundamental impossibility theorem: no LLM capable\nperforming non-trivial knowledge aggregation can simultaneously achieve\ntruthful (internally consistent) knowledge representation, semantic information\nconservation, complete revelation of relevant knowledge, and\nknowledge-constrained optimality. This impossibility is not an engineering\nlimitation but arises from the mathematical structure of information\naggregation itself. We establish this result by describing the inference\nprocess as an auction of ideas, where distributed components compete exploiting\ntheir partial knowledge to shape responses. The proof spans three independent\nmathematical domains: mechanism design theory (Green-Laffont), the theory of\nproper scoring rules (Savage), and direct architectural analysis of\ntransformers (Log-Sum-Exp convexity). In particular, we show how in the\nstrictly concave settings the score of an aggregate of diverse beliefs strictly\nexceeds the sum of individual scores. That gap may quantify the creation of\nunattributable certainty or overconfidence -- the mathematical origin of both\nhallucination and creativity, or imagination.\n  To support this analysis, we introduce the complementary concepts of the\nsemantic information measure and the emergence operator to model bounded\nreasoning in a general setting. We prove that while bounded reasoning generates\naccessible information, providing valuable insights and inspirations, idealized\nreasoning strictly preserves semantic content. By demonstrating that\nhallucination and imagination are mathematically identical phenomena-grounded\nin the necessary violation of information conservation-this paper offers a\nprincipled foundation for managing these behaviors in advanced AI systems.\nFinally, we present some speculative ideas to inspire evaluation and\nrefinements of the proposed theory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper establishes a fundamental impossibility theorem: no LLM capable\nperforming non-trivial knowledge aggregation can simultaneously achieve\ntruthful (internally consistent) knowledge representation, semantic information\nconservation, complete revelation of relevant knowledge, and\nknowledge-constrained optimality. This impossibility is not an engineering\nlimitation but arises from the mathematical structure of information\naggregation itself. We establish this result by describing the inference\nprocess as an auction of ideas, where distributed components compete exploiting\ntheir partial knowledge to shape responses. The proof spans three independent\nmathematical domains: mechanism design theory (Green-Laffont), the theory of\nproper scoring rules (Savage), and direct architectural analysis of\ntransformers (Log-Sum-Exp convexity). In particular, we show how in the\nstrictly concave settings the score of an aggregate of diverse beliefs strictly\nexceeds the sum of individual scores. That gap may quantify the creation of\nunattributable certainty or overconfidence -- the mathematical origin of both\nhallucination and creativity, or imagination.\n  To support this analysis, we introduce the complementary concepts of the\nsemantic information measure and the emergence operator to model bounded\nreasoning in a general setting. We prove that while bounded reasoning generates\naccessible information, providing valuable insights and inspirations, idealized\nreasoning strictly preserves semantic content. By demonstrating that\nhallucination and imagination are mathematically identical phenomena-grounded\nin the necessary violation of information conservation-this paper offers a\nprincipled foundation for managing these behaviors in advanced AI systems.\nFinally, we present some speculative ideas to inspire evaluation and\nrefinements of the proposed theory."
                },
                "authors": [
                    {
                        "name": "Micha P. Karpowicz"
                    }
                ],
                "author_detail": {
                    "name": "Micha P. Karpowicz"
                },
                "author": "Micha P. Karpowicz",
                "arxiv_comment": "cleared mathematics, proofs and ideas explained, added missing\n  definitions and axioms, discussion and speculation section added",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06382v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06382v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03703v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03703v4",
                "updated": "2025-08-07T13:34:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    13,
                    34,
                    51,
                    3,
                    219,
                    0
                ],
                "published": "2025-07-04T16:38:09Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    16,
                    38,
                    9,
                    4,
                    185,
                    0
                ],
                "title": "Sign Spotting Disambiguation using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sign Spotting Disambiguation using Large Language Models"
                },
                "summary": "Sign spotting, the task of identifying and localizing individual signs within\ncontinuous sign language video, plays a pivotal role in scaling dataset\nannotations and addressing the severe data scarcity issue in sign language\ntranslation. While automatic sign spotting holds great promise for enabling\nframe-level supervision at scale, it grapples with challenges such as\nvocabulary inflexibility and ambiguity inherent in continuous sign streams.\nHence, we introduce a novel, training-free framework that integrates Large\nLanguage Models (LLMs) to significantly enhance sign spotting quality. Our\napproach extracts global spatio-temporal and hand shape features, which are\nthen matched against a large-scale sign dictionary using dynamic time warping\nand cosine similarity. This dictionary-based matching inherently offers\nsuperior vocabulary flexibility without requiring model retraining. To mitigate\nnoise and ambiguity from the matching process, an LLM performs context-aware\ngloss disambiguation via beam search, notably without fine-tuning. Extensive\nexperiments on both synthetic and real-world sign language datasets demonstrate\nour method's superior accuracy and sentence fluency compared to traditional\napproaches, highlighting the potential of LLMs in advancing sign spotting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sign spotting, the task of identifying and localizing individual signs within\ncontinuous sign language video, plays a pivotal role in scaling dataset\nannotations and addressing the severe data scarcity issue in sign language\ntranslation. While automatic sign spotting holds great promise for enabling\nframe-level supervision at scale, it grapples with challenges such as\nvocabulary inflexibility and ambiguity inherent in continuous sign streams.\nHence, we introduce a novel, training-free framework that integrates Large\nLanguage Models (LLMs) to significantly enhance sign spotting quality. Our\napproach extracts global spatio-temporal and hand shape features, which are\nthen matched against a large-scale sign dictionary using dynamic time warping\nand cosine similarity. This dictionary-based matching inherently offers\nsuperior vocabulary flexibility without requiring model retraining. To mitigate\nnoise and ambiguity from the matching process, an LLM performs context-aware\ngloss disambiguation via beam search, notably without fine-tuning. Extensive\nexperiments on both synthetic and real-world sign language datasets demonstrate\nour method's superior accuracy and sentence fluency compared to traditional\napproaches, highlighting the potential of LLMs in advancing sign spotting."
                },
                "authors": [
                    {
                        "name": "JianHe Low"
                    },
                    {
                        "name": "Ozge Mercanoglu Sincan"
                    },
                    {
                        "name": "Richard Bowden"
                    }
                ],
                "author_detail": {
                    "name": "Richard Bowden"
                },
                "author": "Richard Bowden",
                "arxiv_comment": "Accepted in the international conference on Intelligent Virtual\n  Agents (IVA Adjunct)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03703v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03703v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04339v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04339v1",
                "updated": "2025-08-06T11:33:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    33,
                    35,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T11:33:35Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    33,
                    35,
                    2,
                    218,
                    0
                ],
                "title": "Deliberative Reasoning Network: An Uncertainty-Driven Paradigm for\n  Belief-Tracked Inference with Pretrained Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deliberative Reasoning Network: An Uncertainty-Driven Paradigm for\n  Belief-Tracked Inference with Pretrained Language Models"
                },
                "summary": "Large language models often fail at logical reasoning when semantic\nheuristics conflict with decisive evidence - a phenomenon we term cognitive\ntraps. To address this fundamental limitation, we introduce the Deliberative\nReasoning Network (DRN), a novel paradigm that reframes logical reasoning from\nprobability maximization to uncertainty minimization. Instead of asking \"Which\nanswer is most likely?\", DRN asks \"Which hypothesis has the most internally\nconsistent evidence?\". DRN achieves intrinsic interpretability by explicitly\ntracking belief states and quantifying epistemic uncertainty for competing\nhypotheses through an iterative evidence synthesis process. We validate our\napproach through two complementary architectures - a bespoke discriminative\nmodel that embodies the core uncertainty minimization principle, and a\nlightweight verification module that enhances existing generative LLMs.\nEvaluated on LCR-1000, our new adversarial reasoning benchmark designed to\nexpose cognitive traps, the bespoke DRN achieves up to 15.2% improvement over\nstandard baselines. When integrated as a parameter-efficient verifier with\nMistral-7B, our hybrid system boosts accuracy from 20% to 80% on the most\nchallenging problems. Critically, DRN demonstrates strong zero-shot\ngeneralization, improving TruthfulQA performance by 23.6% without additional\ntraining, indicating that uncertainty-driven deliberation learns transferable\nreasoning principles. We position DRN as a foundational, verifiable System 2\nreasoning component for building more trustworthy AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models often fail at logical reasoning when semantic\nheuristics conflict with decisive evidence - a phenomenon we term cognitive\ntraps. To address this fundamental limitation, we introduce the Deliberative\nReasoning Network (DRN), a novel paradigm that reframes logical reasoning from\nprobability maximization to uncertainty minimization. Instead of asking \"Which\nanswer is most likely?\", DRN asks \"Which hypothesis has the most internally\nconsistent evidence?\". DRN achieves intrinsic interpretability by explicitly\ntracking belief states and quantifying epistemic uncertainty for competing\nhypotheses through an iterative evidence synthesis process. We validate our\napproach through two complementary architectures - a bespoke discriminative\nmodel that embodies the core uncertainty minimization principle, and a\nlightweight verification module that enhances existing generative LLMs.\nEvaluated on LCR-1000, our new adversarial reasoning benchmark designed to\nexpose cognitive traps, the bespoke DRN achieves up to 15.2% improvement over\nstandard baselines. When integrated as a parameter-efficient verifier with\nMistral-7B, our hybrid system boosts accuracy from 20% to 80% on the most\nchallenging problems. Critically, DRN demonstrates strong zero-shot\ngeneralization, improving TruthfulQA performance by 23.6% without additional\ntraining, indicating that uncertainty-driven deliberation learns transferable\nreasoning principles. We position DRN as a foundational, verifiable System 2\nreasoning component for building more trustworthy AI systems."
                },
                "authors": [
                    {
                        "name": "Anran Xu"
                    },
                    {
                        "name": "Jincheng Wang"
                    },
                    {
                        "name": "Baigen Cai"
                    },
                    {
                        "name": "Tao Wen"
                    }
                ],
                "author_detail": {
                    "name": "Tao Wen"
                },
                "author": "Tao Wen",
                "arxiv_comment": "8 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04339v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04339v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02118v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02118v5",
                "updated": "2025-08-06T11:31:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    31,
                    36,
                    2,
                    218,
                    0
                ],
                "published": "2025-05-04T14:00:04Z",
                "published_parsed": [
                    2025,
                    5,
                    4,
                    14,
                    0,
                    4,
                    6,
                    124,
                    0
                ],
                "title": "Adversarial Cooperative Rationalization: The Risk of Spurious\n  Correlations in Even Clean Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial Cooperative Rationalization: The Risk of Spurious\n  Correlations in Even Clean Datasets"
                },
                "summary": "This study investigates the self-rationalization framework constructed with a\ncooperative game, where a generator initially extracts the most informative\nsegment from raw input, and a subsequent predictor utilizes the selected subset\nfor its input. The generator and predictor are trained collaboratively to\nmaximize prediction accuracy. In this paper, we first uncover a potential\ncaveat: such a cooperative game could unintentionally introduce a sampling bias\nduring rationale extraction. Specifically, the generator might inadvertently\ncreate an incorrect correlation between the selected rationale candidate and\nthe label, even when they are semantically unrelated in the original dataset.\nSubsequently, we elucidate the origins of this bias using both detailed\ntheoretical analysis and empirical evidence. Our findings suggest a direction\nfor inspecting these correlations through attacks, based on which we further\nintroduce an instruction to prevent the predictor from learning the\ncorrelations. Through experiments on six text classification datasets and two\ngraph classification datasets using three network architectures (GRUs, BERT,\nand GCN), we show that our method not only significantly outperforms recent\nrationalization methods, but also achieves comparable or even better results\nthan a representative LLM (llama3.1-8b-instruct).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the self-rationalization framework constructed with a\ncooperative game, where a generator initially extracts the most informative\nsegment from raw input, and a subsequent predictor utilizes the selected subset\nfor its input. The generator and predictor are trained collaboratively to\nmaximize prediction accuracy. In this paper, we first uncover a potential\ncaveat: such a cooperative game could unintentionally introduce a sampling bias\nduring rationale extraction. Specifically, the generator might inadvertently\ncreate an incorrect correlation between the selected rationale candidate and\nthe label, even when they are semantically unrelated in the original dataset.\nSubsequently, we elucidate the origins of this bias using both detailed\ntheoretical analysis and empirical evidence. Our findings suggest a direction\nfor inspecting these correlations through attacks, based on which we further\nintroduce an instruction to prevent the predictor from learning the\ncorrelations. Through experiments on six text classification datasets and two\ngraph classification datasets using three network architectures (GRUs, BERT,\nand GCN), we show that our method not only significantly outperforms recent\nrationalization methods, but also achieves comparable or even better results\nthan a representative LLM (llama3.1-8b-instruct)."
                },
                "authors": [
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Zhongyu Niu"
                    },
                    {
                        "name": "Lang Gao"
                    },
                    {
                        "name": "Zhiying Deng"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Haozhao Wang"
                    },
                    {
                        "name": "Ruixuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruixuan Li"
                },
                "author": "Ruixuan Li",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02118v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02118v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04337v1",
                "updated": "2025-08-06T11:30:07Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    30,
                    7,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T11:30:07Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    30,
                    7,
                    2,
                    218,
                    0
                ],
                "title": "Modelling and Classifying the Components of a Literature Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modelling and Classifying the Components of a Literature Review"
                },
                "summary": "Previous work has demonstrated that AI methods for analysing scientific\nliterature benefit significantly from annotating sentences in papers according\nto their rhetorical roles, such as research gaps, results, limitations,\nextensions of existing methodologies, and others. Such representations also\nhave the potential to support the development of a new generation of systems\ncapable of producing high-quality literature reviews. However, achieving this\ngoal requires the definition of a relevant annotation schema and effective\nstrategies for large-scale annotation of the literature. This paper addresses\nthese challenges by 1) introducing a novel annotation schema specifically\ndesigned to support literature review generation and 2) conducting a\ncomprehensive evaluation of a wide range of state-of-the-art large language\nmodels (LLMs) in classifying rhetorical roles according to this schema. To this\nend, we also present Sci-Sentence, a novel multidisciplinary benchmark\ncomprising 700 sentences manually annotated by domain experts and 2,240\nsentences automatically labelled using LLMs. We evaluate 37 LLMs on this\nbenchmark, spanning diverse model families and sizes, using both zero-shot\nlearning and fine-tuning approaches. The experiments yield several novel\ninsights that advance the state of the art in this challenging domain. First,\nthe current generation of LLMs performs remarkably well on this task when\nfine-tuned on high-quality data, achieving performance levels above 96\\% F1.\nSecond, while large proprietary models like GPT-4o achieve the best results,\nsome lightweight open-source alternatives also demonstrate excellent\nperformance. Finally, enriching the training data with semi-synthetic examples\ngenerated by LLMs proves beneficial, enabling small encoders to achieve robust\nresults and significantly enhancing the performance of several open decoder\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous work has demonstrated that AI methods for analysing scientific\nliterature benefit significantly from annotating sentences in papers according\nto their rhetorical roles, such as research gaps, results, limitations,\nextensions of existing methodologies, and others. Such representations also\nhave the potential to support the development of a new generation of systems\ncapable of producing high-quality literature reviews. However, achieving this\ngoal requires the definition of a relevant annotation schema and effective\nstrategies for large-scale annotation of the literature. This paper addresses\nthese challenges by 1) introducing a novel annotation schema specifically\ndesigned to support literature review generation and 2) conducting a\ncomprehensive evaluation of a wide range of state-of-the-art large language\nmodels (LLMs) in classifying rhetorical roles according to this schema. To this\nend, we also present Sci-Sentence, a novel multidisciplinary benchmark\ncomprising 700 sentences manually annotated by domain experts and 2,240\nsentences automatically labelled using LLMs. We evaluate 37 LLMs on this\nbenchmark, spanning diverse model families and sizes, using both zero-shot\nlearning and fine-tuning approaches. The experiments yield several novel\ninsights that advance the state of the art in this challenging domain. First,\nthe current generation of LLMs performs remarkably well on this task when\nfine-tuned on high-quality data, achieving performance levels above 96\\% F1.\nSecond, while large proprietary models like GPT-4o achieve the best results,\nsome lightweight open-source alternatives also demonstrate excellent\nperformance. Finally, enriching the training data with semi-synthetic examples\ngenerated by LLMs proves beneficial, enabling small encoders to achieve robust\nresults and significantly enhancing the performance of several open decoder\nmodels."
                },
                "authors": [
                    {
                        "name": "Francisco Bolaos"
                    },
                    {
                        "name": "Angelo Salatino"
                    },
                    {
                        "name": "Francesco Osborne"
                    },
                    {
                        "name": "Enrico Motta"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Motta"
                },
                "author": "Enrico Motta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12286v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12286v3",
                "updated": "2025-08-06T11:23:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    23,
                    11,
                    2,
                    218,
                    0
                ],
                "published": "2025-06-14T00:25:26Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    0,
                    25,
                    26,
                    5,
                    165,
                    0
                ],
                "title": "The SWE-Bench Illusion: When State-of-the-Art LLMs Remember Instead of\n  Reason",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The SWE-Bench Illusion: When State-of-the-Art LLMs Remember Instead of\n  Reason"
                },
                "summary": "As large language models (LLMs) become increasingly capable and widely\nadopted, benchmarks play a central role in assessing their practical utility.\nFor example, SWE-Bench Verified has emerged as a critical benchmark for\nevaluating LLMs' software engineering abilities, particularly their aptitude\nfor resolving real-world GitHub issues. Recent LLMs show impressive performance\non SWE-Bench, leading to optimism about their capacity for complex coding\ntasks. However, current evaluation protocols may overstate these models' true\ncapabilities. It is crucial to distinguish LLMs' generalizable problem-solving\nability and other learned artifacts. In this work, we introduce two diagnostic\ntasks: file path identification from issue descriptions alone and ground truth\nfunction reproduction with only the current file context and issue description\nto probe models' underlying knowledge. We present empirical evidence that\nperformance gains on SWE-Bench-Verified may be partially driven by memorization\nrather than genuine problem-solving. We show that state-of-the-art models\nachieve up to 76% accuracy in identifying buggy file paths using only issue\ndescriptions, without access to repository structure. This performance is\nmerely up to 53% on tasks from repositories not included in SWE-Bench, pointing\nto possible data contamination or memorization. Similar patterns are also\nobserved for the function reproduction task, where the verbatim similarity is\nmuch higher on SWE-Bench Verified than on other similar coding benchmarks (up\nto 35% consecutive 5-gram accuracy on SWE-Bench Verified and Full, but only up\nto 18% for tasks in other benchmarks). These findings raise concerns about the\nvalidity of existing results and underscore the need for more robust,\ncontamination-resistant benchmarks to reliably evaluate LLMs' coding abilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become increasingly capable and widely\nadopted, benchmarks play a central role in assessing their practical utility.\nFor example, SWE-Bench Verified has emerged as a critical benchmark for\nevaluating LLMs' software engineering abilities, particularly their aptitude\nfor resolving real-world GitHub issues. Recent LLMs show impressive performance\non SWE-Bench, leading to optimism about their capacity for complex coding\ntasks. However, current evaluation protocols may overstate these models' true\ncapabilities. It is crucial to distinguish LLMs' generalizable problem-solving\nability and other learned artifacts. In this work, we introduce two diagnostic\ntasks: file path identification from issue descriptions alone and ground truth\nfunction reproduction with only the current file context and issue description\nto probe models' underlying knowledge. We present empirical evidence that\nperformance gains on SWE-Bench-Verified may be partially driven by memorization\nrather than genuine problem-solving. We show that state-of-the-art models\nachieve up to 76% accuracy in identifying buggy file paths using only issue\ndescriptions, without access to repository structure. This performance is\nmerely up to 53% on tasks from repositories not included in SWE-Bench, pointing\nto possible data contamination or memorization. Similar patterns are also\nobserved for the function reproduction task, where the verbatim similarity is\nmuch higher on SWE-Bench Verified than on other similar coding benchmarks (up\nto 35% consecutive 5-gram accuracy on SWE-Bench Verified and Full, but only up\nto 18% for tasks in other benchmarks). These findings raise concerns about the\nvalidity of existing results and underscore the need for more robust,\ncontamination-resistant benchmarks to reliably evaluate LLMs' coding abilities."
                },
                "authors": [
                    {
                        "name": "Shanchao Liang"
                    },
                    {
                        "name": "Spandan Garg"
                    },
                    {
                        "name": "Roshanak Zilouchian Moghaddam"
                    }
                ],
                "author_detail": {
                    "name": "Roshanak Zilouchian Moghaddam"
                },
                "author": "Roshanak Zilouchian Moghaddam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12286v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12286v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04329v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04329v2",
                "updated": "2025-08-07T08:30:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    8,
                    30,
                    41,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-06T11:22:23Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    22,
                    23,
                    2,
                    218,
                    0
                ],
                "title": "Forgetting: A New Mechanism Towards Better Large Language Model\n  Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forgetting: A New Mechanism Towards Better Large Language Model\n  Fine-tuning"
                },
                "summary": "Supervised fine-tuning (SFT) plays a critical role for pretrained large\nlanguage models (LLMs), notably enhancing their capacity to acquire\ndomain-specific knowledge while preserving or potentially augmenting their\ngeneral-purpose capabilities. However, the efficacy of SFT hinges on data\nquality as well as data volume, otherwise it may result in limited performance\ngains or even degradation relative to the associated baselines. To mitigate\nsuch reliance, we suggest categorizing tokens within each corpus into two parts\n-- positive and negative tokens -- based on whether they are useful to improve\nmodel performance. Positive tokens can be trained in common ways, whereas\nnegative tokens, which may lack essential semantics or be misleading, should be\nexplicitly forgotten. Overall, the token categorization facilitate the model to\nlearn less informative message, and the forgetting process shapes a knowledge\nboundary to guide the model on what information to learn more precisely. We\nconduct experiments on well-established benchmarks, finding that this\nforgetting mechanism not only improves overall model performance and also\nfacilitate more diverse model responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised fine-tuning (SFT) plays a critical role for pretrained large\nlanguage models (LLMs), notably enhancing their capacity to acquire\ndomain-specific knowledge while preserving or potentially augmenting their\ngeneral-purpose capabilities. However, the efficacy of SFT hinges on data\nquality as well as data volume, otherwise it may result in limited performance\ngains or even degradation relative to the associated baselines. To mitigate\nsuch reliance, we suggest categorizing tokens within each corpus into two parts\n-- positive and negative tokens -- based on whether they are useful to improve\nmodel performance. Positive tokens can be trained in common ways, whereas\nnegative tokens, which may lack essential semantics or be misleading, should be\nexplicitly forgotten. Overall, the token categorization facilitate the model to\nlearn less informative message, and the forgetting process shapes a knowledge\nboundary to guide the model on what information to learn more precisely. We\nconduct experiments on well-established benchmarks, finding that this\nforgetting mechanism not only improves overall model performance and also\nfacilitate more diverse model responses."
                },
                "authors": [
                    {
                        "name": "Ali Taheri Ghahrizjani"
                    },
                    {
                        "name": "Alireza Taban"
                    },
                    {
                        "name": "Qizhou Wang"
                    },
                    {
                        "name": "Shanshan Ye"
                    },
                    {
                        "name": "Abdolreza Mirzaei"
                    },
                    {
                        "name": "Tongliang Liu"
                    },
                    {
                        "name": "Bo Han"
                    }
                ],
                "author_detail": {
                    "name": "Bo Han"
                },
                "author": "Bo Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04329v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04329v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16414v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16414v2",
                "updated": "2025-08-06T11:20:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    20,
                    14,
                    2,
                    218,
                    0
                ],
                "published": "2025-04-23T04:36:19Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    4,
                    36,
                    19,
                    2,
                    113,
                    0
                ],
                "title": "Evaluating Multi-Hop Reasoning in Large Language Models: A\n  Chemistry-Centric Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Multi-Hop Reasoning in Large Language Models: A\n  Chemistry-Centric Case Study"
                },
                "summary": "In this study, we introduced a new benchmark consisting of a curated dataset\nand a defined evaluation process to assess the compositional reasoning\ncapabilities of large language models within the chemistry domain. We designed\nand validated a fully automated pipeline, verified by subject matter experts,\nto facilitate this task. Our approach integrates OpenAI reasoning models with\nnamed entity recognition (NER) systems to extract chemical entities from recent\nliterature, which are then augmented with external knowledge bases to form a\ncomprehensive knowledge graph. By generating multi-hop questions across these\ngraphs, we assess LLM performance in both context-augmented and non-context\naugmented settings. Our experiments reveal that even state-of-the-art models\nface significant challenges in multi-hop compositional reasoning. The results\nreflect the importance of augmenting LLMs with document retrieval, which can\nhave a substantial impact on improving their performance. However, even perfect\nretrieval accuracy with full context does not eliminate reasoning errors,\nunderscoring the complexity of compositional reasoning. This work not only\nbenchmarks and highlights the limitations of current LLMs but also presents a\nnovel data generation pipeline capable of producing challenging reasoning\ndatasets across various domains. Overall, this research advances our\nunderstanding of reasoning in computational linguistics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we introduced a new benchmark consisting of a curated dataset\nand a defined evaluation process to assess the compositional reasoning\ncapabilities of large language models within the chemistry domain. We designed\nand validated a fully automated pipeline, verified by subject matter experts,\nto facilitate this task. Our approach integrates OpenAI reasoning models with\nnamed entity recognition (NER) systems to extract chemical entities from recent\nliterature, which are then augmented with external knowledge bases to form a\ncomprehensive knowledge graph. By generating multi-hop questions across these\ngraphs, we assess LLM performance in both context-augmented and non-context\naugmented settings. Our experiments reveal that even state-of-the-art models\nface significant challenges in multi-hop compositional reasoning. The results\nreflect the importance of augmenting LLMs with document retrieval, which can\nhave a substantial impact on improving their performance. However, even perfect\nretrieval accuracy with full context does not eliminate reasoning errors,\nunderscoring the complexity of compositional reasoning. This work not only\nbenchmarks and highlights the limitations of current LLMs but also presents a\nnovel data generation pipeline capable of producing challenging reasoning\ndatasets across various domains. Overall, this research advances our\nunderstanding of reasoning in computational linguistics."
                },
                "authors": [
                    {
                        "name": "Mohammad Khodadad"
                    },
                    {
                        "name": "Ali Shiraee Kasmaee"
                    },
                    {
                        "name": "Mahdi Astaraki"
                    },
                    {
                        "name": "Nicholas Sherck"
                    },
                    {
                        "name": "Hamidreza Mahyar"
                    },
                    {
                        "name": "Soheila Samiee"
                    }
                ],
                "author_detail": {
                    "name": "Soheila Samiee"
                },
                "author": "Soheila Samiee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16414v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16414v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21696v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21696v3",
                "updated": "2025-08-06T11:17:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    17,
                    33,
                    2,
                    218,
                    0
                ],
                "published": "2025-07-29T11:20:03Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    11,
                    20,
                    3,
                    1,
                    210,
                    0
                ],
                "title": "Edge Agentic AI Framework for Autonomous Network Optimisation in O-RAN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Agentic AI Framework for Autonomous Network Optimisation in O-RAN"
                },
                "summary": "The deployment of AI agents within legacy Radio Access Network (RAN)\ninfrastructure poses significant safety and reliability challenges for future\n6G networks. This paper presents a novel Edge AI framework for autonomous\nnetwork optimisation in Open RAN environments, addressing these challenges\nthrough three core innovations: (1) a persona-based multi-tools architecture\nenabling distributed, context-aware decision-making; (2) proactive anomaly\ndetection agent powered by traffic predictive tool; and (3) a safety, aligned\nreward mechanism that balances performance with operational stability.\nIntegrated into the RAN Intelligent Controller (RIC), our framework leverages\nmultimodal data fusion, including network KPIs, a traffic prediction model, and\nexternal information sources, to anticipate and respond to dynamic network\nconditions. Extensive evaluation using realistic 5G scenarios demonstrates that\nthe edge framework achieves zero network outages under high-stress conditions,\ncompared to 8.4% for traditional fixed-power networks and 3.3% for large\nlanguage model (LLM) agent-based approaches, while maintaining near real-time\nresponsiveness and consistent QoS. These results establish that, when equipped\nwith the right tools and contextual awareness, AI agents can be safely and\neffectively deployed in critical network infrastructure, laying the framework\nfor intelligent and autonomous 5G and beyond network operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of AI agents within legacy Radio Access Network (RAN)\ninfrastructure poses significant safety and reliability challenges for future\n6G networks. This paper presents a novel Edge AI framework for autonomous\nnetwork optimisation in Open RAN environments, addressing these challenges\nthrough three core innovations: (1) a persona-based multi-tools architecture\nenabling distributed, context-aware decision-making; (2) proactive anomaly\ndetection agent powered by traffic predictive tool; and (3) a safety, aligned\nreward mechanism that balances performance with operational stability.\nIntegrated into the RAN Intelligent Controller (RIC), our framework leverages\nmultimodal data fusion, including network KPIs, a traffic prediction model, and\nexternal information sources, to anticipate and respond to dynamic network\nconditions. Extensive evaluation using realistic 5G scenarios demonstrates that\nthe edge framework achieves zero network outages under high-stress conditions,\ncompared to 8.4% for traditional fixed-power networks and 3.3% for large\nlanguage model (LLM) agent-based approaches, while maintaining near real-time\nresponsiveness and consistent QoS. These results establish that, when equipped\nwith the right tools and contextual awareness, AI agents can be safely and\neffectively deployed in critical network infrastructure, laying the framework\nfor intelligent and autonomous 5G and beyond network operations."
                },
                "authors": [
                    {
                        "name": "Abdelaziz Salama"
                    },
                    {
                        "name": "Zeinab Nezami"
                    },
                    {
                        "name": "Mohammed M. H. Qazzaz"
                    },
                    {
                        "name": "Maryam Hafeez"
                    },
                    {
                        "name": "Syed Ali Raza Zaidi"
                    }
                ],
                "author_detail": {
                    "name": "Syed Ali Raza Zaidi"
                },
                "author": "Syed Ali Raza Zaidi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21696v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21696v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04325v1",
                "updated": "2025-08-06T11:11:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    11,
                    40,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T11:11:40Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    11,
                    40,
                    2,
                    218,
                    0
                ],
                "title": "Beyond the Leaderboard: Rethinking Medical Benchmarks for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Leaderboard: Rethinking Medical Benchmarks for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) show significant potential in healthcare,\nprompting numerous benchmarks to evaluate their capabilities. However, concerns\npersist regarding the reliability of these benchmarks, which often lack\nclinical fidelity, robust data management, and safety-oriented evaluation\nmetrics. To address these shortcomings, we introduce MedCheck, the first\nlifecycle-oriented assessment framework specifically designed for medical\nbenchmarks. Our framework deconstructs a benchmark's development into five\ncontinuous stages, from design to governance, and provides a comprehensive\nchecklist of 46 medically-tailored criteria. Using MedCheck, we conducted an\nin-depth empirical evaluation of 53 medical LLM benchmarks. Our analysis\nuncovers widespread, systemic issues, including a profound disconnect from\nclinical practice, a crisis of data integrity due to unmitigated contamination\nrisks, and a systematic neglect of safety-critical evaluation dimensions like\nmodel robustness and uncertainty awareness. Based on these findings, MedCheck\nserves as both a diagnostic tool for existing benchmarks and an actionable\nguideline to foster a more standardized, reliable, and transparent approach to\nevaluating AI in healthcare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) show significant potential in healthcare,\nprompting numerous benchmarks to evaluate their capabilities. However, concerns\npersist regarding the reliability of these benchmarks, which often lack\nclinical fidelity, robust data management, and safety-oriented evaluation\nmetrics. To address these shortcomings, we introduce MedCheck, the first\nlifecycle-oriented assessment framework specifically designed for medical\nbenchmarks. Our framework deconstructs a benchmark's development into five\ncontinuous stages, from design to governance, and provides a comprehensive\nchecklist of 46 medically-tailored criteria. Using MedCheck, we conducted an\nin-depth empirical evaluation of 53 medical LLM benchmarks. Our analysis\nuncovers widespread, systemic issues, including a profound disconnect from\nclinical practice, a crisis of data integrity due to unmitigated contamination\nrisks, and a systematic neglect of safety-critical evaluation dimensions like\nmodel robustness and uncertainty awareness. Based on these findings, MedCheck\nserves as both a diagnostic tool for existing benchmarks and an actionable\nguideline to foster a more standardized, reliable, and transparent approach to\nevaluating AI in healthcare."
                },
                "authors": [
                    {
                        "name": "Zizhan Ma"
                    },
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Guo Yu"
                    },
                    {
                        "name": "Yiu-Fai Cheung"
                    },
                    {
                        "name": "Meidan Ding"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Wenting Chen"
                    },
                    {
                        "name": "Linlin Shen"
                    }
                ],
                "author_detail": {
                    "name": "Linlin Shen"
                },
                "author": "Linlin Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12020v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12020v2",
                "updated": "2025-08-06T11:08:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    8,
                    56,
                    2,
                    218,
                    0
                ],
                "published": "2025-02-17T16:54:54Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    16,
                    54,
                    54,
                    0,
                    48,
                    0
                ],
                "title": "Learning in a Multifield Coherent Ising Machine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning in a Multifield Coherent Ising Machine"
                },
                "summary": "We introduce a network of coupled oscillators that can learn to solve a\nclassification task from a set of examples -- performing both training and\ninference through the nonlinear evolution of the system. We accomplish this by\ncombining three key elements to achieve learning: A long-term memory that\nstores learned responses, analogous to the synapses in biological brains; a\nshort-term memory that stores the neural activations, similar to the firing\npatterns of neurons; and an evolution law that updates the synapses in response\nto novel examples, inspired by synaptic plasticity. Achieving all three\nelements in wave-based information processors such as metamaterials is a\nsignificant challenge. Here, we solve it by leveraging the material\nmultistability to implement long-term memory, and harnessing symmetries and\nthermal noise to realize the learning rule. Our analysis reveals that the\nlearning mechanism, although inspired by synaptic plasticity, also shares\nparallelisms with bacterial evolution strategies, where mutation rates increase\nin the presence of noxious stimuli.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a network of coupled oscillators that can learn to solve a\nclassification task from a set of examples -- performing both training and\ninference through the nonlinear evolution of the system. We accomplish this by\ncombining three key elements to achieve learning: A long-term memory that\nstores learned responses, analogous to the synapses in biological brains; a\nshort-term memory that stores the neural activations, similar to the firing\npatterns of neurons; and an evolution law that updates the synapses in response\nto novel examples, inspired by synaptic plasticity. Achieving all three\nelements in wave-based information processors such as metamaterials is a\nsignificant challenge. Here, we solve it by leveraging the material\nmultistability to implement long-term memory, and harnessing symmetries and\nthermal noise to realize the learning rule. Our analysis reveals that the\nlearning mechanism, although inspired by synaptic plasticity, also shares\nparallelisms with bacterial evolution strategies, where mutation rates increase\nin the presence of noxious stimuli."
                },
                "authors": [
                    {
                        "name": "Daan de Bos"
                    },
                    {
                        "name": "Marc Serra-Garcia"
                    }
                ],
                "author_detail": {
                    "name": "Marc Serra-Garcia"
                },
                "author": "Marc Serra-Garcia",
                "arxiv_comment": "11 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12020v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12020v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nlin.AO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13287v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13287v5",
                "updated": "2025-08-06T11:02:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    2,
                    1,
                    2,
                    218,
                    0
                ],
                "published": "2024-10-17T07:33:35Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    7,
                    33,
                    35,
                    3,
                    291,
                    0
                ],
                "title": "PAK-UCB Contextual Bandit: An Online Learning Approach to Prompt-Aware\n  Selection of Generative Models and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAK-UCB Contextual Bandit: An Online Learning Approach to Prompt-Aware\n  Selection of Generative Models and LLMs"
                },
                "summary": "Selecting a sample generation scheme from multiple prompt-based generative\nmodels, including large language models (LLMs) and prompt-guided image and\nvideo generation models, is typically addressed by choosing the model that\nmaximizes an averaged evaluation score. However, this score-based selection\noverlooks the possibility that different models achieve the best generation\nperformance for different types of text prompts. An online identification of\nthe best generation model for various input prompts can reduce the costs\nassociated with querying sub-optimal models. In this work, we explore the\npossibility of varying rankings of text-based generative models for different\ntext prompts and propose an online learning framework to predict the best data\ngeneration model for a given input prompt. The proposed PAK-UCB algorithm\naddresses a contextual bandit (CB) setting with shared context variables across\nthe arms, utilizing the generated data to update kernel-based functions that\npredict the score of each model available for unseen text prompts.\nAdditionally, we leverage random Fourier features (RFF) to accelerate the\nonline learning process of PAK-UCB. Our numerical experiments on real and\nsimulated text-to-image and image-to-text generative models show that RFF-UCB\nperforms successfully in identifying the best generation model across different\nsample types. The code is available at:\ngithub.com/yannxiaoyanhu/dgm-online-select.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selecting a sample generation scheme from multiple prompt-based generative\nmodels, including large language models (LLMs) and prompt-guided image and\nvideo generation models, is typically addressed by choosing the model that\nmaximizes an averaged evaluation score. However, this score-based selection\noverlooks the possibility that different models achieve the best generation\nperformance for different types of text prompts. An online identification of\nthe best generation model for various input prompts can reduce the costs\nassociated with querying sub-optimal models. In this work, we explore the\npossibility of varying rankings of text-based generative models for different\ntext prompts and propose an online learning framework to predict the best data\ngeneration model for a given input prompt. The proposed PAK-UCB algorithm\naddresses a contextual bandit (CB) setting with shared context variables across\nthe arms, utilizing the generated data to update kernel-based functions that\npredict the score of each model available for unseen text prompts.\nAdditionally, we leverage random Fourier features (RFF) to accelerate the\nonline learning process of PAK-UCB. Our numerical experiments on real and\nsimulated text-to-image and image-to-text generative models show that RFF-UCB\nperforms successfully in identifying the best generation model across different\nsample types. The code is available at:\ngithub.com/yannxiaoyanhu/dgm-online-select."
                },
                "authors": [
                    {
                        "name": "Xiaoyan Hu"
                    },
                    {
                        "name": "Ho-fung Leung"
                    },
                    {
                        "name": "Farzan Farnia"
                    }
                ],
                "author_detail": {
                    "name": "Farzan Farnia"
                },
                "author": "Farzan Farnia",
                "arxiv_comment": "accepted to ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13287v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13287v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04306v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04306v1",
                "updated": "2025-08-06T10:45:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    10,
                    45,
                    52,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T10:45:52Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    10,
                    45,
                    52,
                    2,
                    218,
                    0
                ],
                "title": "Multi-Agent Taskforce Collaboration: Self-Correction of Compounding\n  Errors in Long-Form Literature Review Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Taskforce Collaboration: Self-Correction of Compounding\n  Errors in Long-Form Literature Review Generation"
                },
                "summary": "Literature reviews play an important role in scientific research. Recent\nadvances in large language models (LLMs) have boosted the development of\nautomated systems for the entire literature review workflow, from retrieval to\nmanuscript drafting. However, a key challenge is that mistakes made in early\nstages can propagate and amplify in subsequent steps, leading to compounding\nerrors that undermine the faithfulness of the final review. To tackle this\nissue, we propose the Multi-Agent Taskforce Collaboration (MATC) framework,\nwhich consists of a manager agent and four executor agents for literature\nsearching, outline generation, fact localization, and manuscript drafting. We\npropose three novel collaboration paradigms, forming exploration, exploitation,\nand experience taskforces, to effectively organize agents and mitigate\ncompounding errors both between and within executor agents. Experimental\nresults show that MATC achieves state-of-the-art performance on existing\nbenchmarks. We further propose a new benchmark dataset featuring more diverse\ntopics for faithful literature review generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Literature reviews play an important role in scientific research. Recent\nadvances in large language models (LLMs) have boosted the development of\nautomated systems for the entire literature review workflow, from retrieval to\nmanuscript drafting. However, a key challenge is that mistakes made in early\nstages can propagate and amplify in subsequent steps, leading to compounding\nerrors that undermine the faithfulness of the final review. To tackle this\nissue, we propose the Multi-Agent Taskforce Collaboration (MATC) framework,\nwhich consists of a manager agent and four executor agents for literature\nsearching, outline generation, fact localization, and manuscript drafting. We\npropose three novel collaboration paradigms, forming exploration, exploitation,\nand experience taskforces, to effectively organize agents and mitigate\ncompounding errors both between and within executor agents. Experimental\nresults show that MATC achieves state-of-the-art performance on existing\nbenchmarks. We further propose a new benchmark dataset featuring more diverse\ntopics for faithful literature review generation."
                },
                "authors": [
                    {
                        "name": "Zhi Zhang"
                    },
                    {
                        "name": "Yan Liu"
                    },
                    {
                        "name": "Zhejing Hu"
                    },
                    {
                        "name": "Gong Chen"
                    },
                    {
                        "name": "Sheng-hua Zhong"
                    },
                    {
                        "name": "Jiannong Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jiannong Cao"
                },
                "author": "Jiannong Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04306v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04306v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04297v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04297v1",
                "updated": "2025-08-06T10:34:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    10,
                    34,
                    24,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T10:34:24Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    10,
                    34,
                    24,
                    2,
                    218,
                    0
                ],
                "title": "MuGS: Multi-Baseline Generalizable Gaussian Splatting Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MuGS: Multi-Baseline Generalizable Gaussian Splatting Reconstruction"
                },
                "summary": "We present Multi-Baseline Gaussian Splatting (MuRF), a generalized\nfeed-forward approach for novel view synthesis that effectively handles diverse\nbaseline settings, including sparse input views with both small and large\nbaselines. Specifically, we integrate features from Multi-View Stereo (MVS) and\nMonocular Depth Estimation (MDE) to enhance feature representations for\ngeneralizable reconstruction. Next, We propose a projection-and-sampling\nmechanism for deep depth fusion, which constructs a fine probability volume to\nguide the regression of the feature map. Furthermore, We introduce a\nreference-view loss to improve geometry and optimization efficiency. We\nleverage 3D Gaussian representations to accelerate training and inference time\nwhile enhancing rendering quality. MuRF achieves state-of-the-art performance\nacross multiple baseline settings and diverse scenarios ranging from simple\nobjects (DTU) to complex indoor and outdoor scenes (RealEstate10K). We also\ndemonstrate promising zero-shot performance on the LLFF and Mip-NeRF 360\ndatasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Multi-Baseline Gaussian Splatting (MuRF), a generalized\nfeed-forward approach for novel view synthesis that effectively handles diverse\nbaseline settings, including sparse input views with both small and large\nbaselines. Specifically, we integrate features from Multi-View Stereo (MVS) and\nMonocular Depth Estimation (MDE) to enhance feature representations for\ngeneralizable reconstruction. Next, We propose a projection-and-sampling\nmechanism for deep depth fusion, which constructs a fine probability volume to\nguide the regression of the feature map. Furthermore, We introduce a\nreference-view loss to improve geometry and optimization efficiency. We\nleverage 3D Gaussian representations to accelerate training and inference time\nwhile enhancing rendering quality. MuRF achieves state-of-the-art performance\nacross multiple baseline settings and diverse scenarios ranging from simple\nobjects (DTU) to complex indoor and outdoor scenes (RealEstate10K). We also\ndemonstrate promising zero-shot performance on the LLFF and Mip-NeRF 360\ndatasets."
                },
                "authors": [
                    {
                        "name": "Yaopeng Lou"
                    },
                    {
                        "name": "Liao Shen"
                    },
                    {
                        "name": "Tianqi Liu"
                    },
                    {
                        "name": "Jiaqi Li"
                    },
                    {
                        "name": "Zihao Huang"
                    },
                    {
                        "name": "Huiqiang Sun"
                    },
                    {
                        "name": "Zhiguo Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zhiguo Cao"
                },
                "author": "Zhiguo Cao",
                "arxiv_comment": "This work is accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04297v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04297v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04295v1",
                "updated": "2025-08-06T10:31:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    10,
                    31,
                    23,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T10:31:23Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    10,
                    31,
                    23,
                    2,
                    218,
                    0
                ],
                "title": "EVOC2RUST: A Skeleton-guided Framework for Project-Level C-to-Rust\n  Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVOC2RUST: A Skeleton-guided Framework for Project-Level C-to-Rust\n  Translation"
                },
                "summary": "Rust's compile-time safety guarantees make it ideal for safety-critical\nsystems, creating demand for translating legacy C codebases to Rust. While\nvarious approaches have emerged for this task, they face inherent trade-offs:\nrule-based solutions face challenges in meeting code safety and idiomaticity\nrequirements, while LLM-based solutions often fail to generate semantically\nequivalent Rust code, due to the heavy dependencies of modules across the\nentire codebase. Recent studies have revealed that both solutions are limited\nto small-scale programs. In this paper, we propose EvoC2Rust, an automated\nframework for converting entire C projects to equivalent Rust ones. EvoC2Rust\nemploys a skeleton-guided translation strategy for project-level translation.\nThe pipeline consists of three evolutionary stages: 1) it first decomposes the\nC project into functional modules, employs a feature-mapping-enhanced LLM to\ntransform definitions and macros and generates type-checked function stubs,\nwhich form a compilable Rust skeleton; 2) it then incrementally translates the\nfunction, replacing the corresponding stub placeholder; 3) finally, it repairs\ncompilation errors by integrating LLM and static analysis. Through evolutionary\naugmentation, EvoC2Rust combines the advantages of both rule-based and\nLLM-based solutions. Our evaluation on open-source benchmarks and six\nindustrial projects demonstrates EvoC2Rust's superior performance in\nproject-level C-to-Rust translation. On average, it achieves 17.24% and 14.32%\nimprovements in syntax and semantic accuracy over the LLM-based approaches,\nalong with a 96.79% higher code safety rate than the rule-based tools. At the\nmodule level, EvoC2Rust reaches 92.25% compilation and 89.53% test pass rates\non industrial projects, even for complex codebases and long functions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rust's compile-time safety guarantees make it ideal for safety-critical\nsystems, creating demand for translating legacy C codebases to Rust. While\nvarious approaches have emerged for this task, they face inherent trade-offs:\nrule-based solutions face challenges in meeting code safety and idiomaticity\nrequirements, while LLM-based solutions often fail to generate semantically\nequivalent Rust code, due to the heavy dependencies of modules across the\nentire codebase. Recent studies have revealed that both solutions are limited\nto small-scale programs. In this paper, we propose EvoC2Rust, an automated\nframework for converting entire C projects to equivalent Rust ones. EvoC2Rust\nemploys a skeleton-guided translation strategy for project-level translation.\nThe pipeline consists of three evolutionary stages: 1) it first decomposes the\nC project into functional modules, employs a feature-mapping-enhanced LLM to\ntransform definitions and macros and generates type-checked function stubs,\nwhich form a compilable Rust skeleton; 2) it then incrementally translates the\nfunction, replacing the corresponding stub placeholder; 3) finally, it repairs\ncompilation errors by integrating LLM and static analysis. Through evolutionary\naugmentation, EvoC2Rust combines the advantages of both rule-based and\nLLM-based solutions. Our evaluation on open-source benchmarks and six\nindustrial projects demonstrates EvoC2Rust's superior performance in\nproject-level C-to-Rust translation. On average, it achieves 17.24% and 14.32%\nimprovements in syntax and semantic accuracy over the LLM-based approaches,\nalong with a 96.79% higher code safety rate than the rule-based tools. At the\nmodule level, EvoC2Rust reaches 92.25% compilation and 89.53% test pass rates\non industrial projects, even for complex codebases and long functions."
                },
                "authors": [
                    {
                        "name": "Chaofan Wang"
                    },
                    {
                        "name": "Tingrui Yu"
                    },
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Dong Chen"
                    },
                    {
                        "name": "Wenrui Zhang"
                    },
                    {
                        "name": "Yuling Shi"
                    },
                    {
                        "name": "Xiaodong Gu"
                    },
                    {
                        "name": "Beijun Shen"
                    }
                ],
                "author_detail": {
                    "name": "Beijun Shen"
                },
                "author": "Beijun Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04291v1",
                "updated": "2025-08-06T10:29:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    10,
                    29,
                    46,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T10:29:46Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    10,
                    29,
                    46,
                    2,
                    218,
                    0
                ],
                "title": "Less Signals, More Understanding: Channel-Capacity Codebook Design for\n  Digital Task-Oriented Semantic Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Less Signals, More Understanding: Channel-Capacity Codebook Design for\n  Digital Task-Oriented Semantic Communication"
                },
                "summary": "Discrete representation has emerged as a powerful tool in task-oriented\nsemantic communication (ToSC), offering compact, interpretable, and efficient\nrepresentations well-suited for low-power edge intelligence scenarios. Its\ninherent digital nature aligns seamlessly with hardware-friendly deployment and\nrobust storage/transmission protocols. However, despite its strengths, current\nToSC frameworks often decouple semantic-aware discrete mapping from the\nunderlying channel characteristics and task demands. This mismatch leads to\nsuboptimal communication performance, degraded task utility, and limited\ngeneralization under variable wireless conditions. Moreover, conventional\ndesigns frequently overlook channel-awareness in codebook construction,\nrestricting the effectiveness of semantic symbol selection under constrained\nresources. To address these limitations, this paper proposes a channel-aware\ndiscrete semantic coding framework tailored for low-power edge networks.\nLeveraging a Wasserstein-regularized objective, our approach aligns discrete\ncode activations with optimal input distributions, thereby improving semantic\nfidelity, robustness, and task accuracy. Extensive experiments on the inference\ntasks across diverse signal-to-noise ratio (SNR) regimes show that our method\nachieves notable gains in accuracy and communication efficiency. This work\nprovides new insights into integrating discrete semantics and channel\noptimization, paving the way for the widespread adoption of semantic\ncommunication in future digital infrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discrete representation has emerged as a powerful tool in task-oriented\nsemantic communication (ToSC), offering compact, interpretable, and efficient\nrepresentations well-suited for low-power edge intelligence scenarios. Its\ninherent digital nature aligns seamlessly with hardware-friendly deployment and\nrobust storage/transmission protocols. However, despite its strengths, current\nToSC frameworks often decouple semantic-aware discrete mapping from the\nunderlying channel characteristics and task demands. This mismatch leads to\nsuboptimal communication performance, degraded task utility, and limited\ngeneralization under variable wireless conditions. Moreover, conventional\ndesigns frequently overlook channel-awareness in codebook construction,\nrestricting the effectiveness of semantic symbol selection under constrained\nresources. To address these limitations, this paper proposes a channel-aware\ndiscrete semantic coding framework tailored for low-power edge networks.\nLeveraging a Wasserstein-regularized objective, our approach aligns discrete\ncode activations with optimal input distributions, thereby improving semantic\nfidelity, robustness, and task accuracy. Extensive experiments on the inference\ntasks across diverse signal-to-noise ratio (SNR) regimes show that our method\nachieves notable gains in accuracy and communication efficiency. This work\nprovides new insights into integrating discrete semantics and channel\noptimization, paving the way for the widespread adoption of semantic\ncommunication in future digital infrastructures."
                },
                "authors": [
                    {
                        "name": "Anbang Zhang"
                    },
                    {
                        "name": "Shuaishuai Guo"
                    },
                    {
                        "name": "Chenyuan Feng"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Haojin Li"
                    },
                    {
                        "name": "Chen Sun"
                    },
                    {
                        "name": "Haijun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Haijun Zhang"
                },
                "author": "Haijun Zhang",
                "arxiv_comment": "submitted to IEEE Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.04698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04698v1",
                "updated": "2025-08-06T17:58:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    58,
                    26,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T17:58:26Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    58,
                    26,
                    2,
                    218,
                    0
                ],
                "title": "FaST: Feature-aware Sampling and Tuning for Personalized Preference\n  Alignment with Limited Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FaST: Feature-aware Sampling and Tuning for Personalized Preference\n  Alignment with Limited Data"
                },
                "summary": "LLM-powered conversational assistants are often deployed in a\none-size-fits-all manner, which fails to accommodate individual user\npreferences. Recently, LLM personalization -- tailoring models to align with\nspecific user preferences -- has gained increasing attention as a way to bridge\nthis gap. In this work, we specifically focus on a practical yet challenging\nsetting where only a small set of preference annotations can be collected per\nuser -- a problem we define as Personalized Preference Alignment with Limited\nData (PPALLI). To support research in this area, we introduce two datasets --\nDnD and ELIP -- and benchmark a variety of alignment techniques on them. We\nfurther propose FaST, a highly parameter-efficient approach that leverages\nhigh-level features automatically discovered from the data, achieving the best\noverall performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-powered conversational assistants are often deployed in a\none-size-fits-all manner, which fails to accommodate individual user\npreferences. Recently, LLM personalization -- tailoring models to align with\nspecific user preferences -- has gained increasing attention as a way to bridge\nthis gap. In this work, we specifically focus on a practical yet challenging\nsetting where only a small set of preference annotations can be collected per\nuser -- a problem we define as Personalized Preference Alignment with Limited\nData (PPALLI). To support research in this area, we introduce two datasets --\nDnD and ELIP -- and benchmark a variety of alignment techniques on them. We\nfurther propose FaST, a highly parameter-efficient approach that leverages\nhigh-level features automatically discovered from the data, achieving the best\noverall performance."
                },
                "authors": [
                    {
                        "name": "Thibaut Thonet"
                    },
                    {
                        "name": "Germn Kruszewski"
                    },
                    {
                        "name": "Jos Rozen"
                    },
                    {
                        "name": "Pierre Erbacher"
                    },
                    {
                        "name": "Marc Dymetman"
                    }
                ],
                "author_detail": {
                    "name": "Marc Dymetman"
                },
                "author": "Marc Dymetman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00873v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00873v2",
                "updated": "2025-08-06T17:57:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    57,
                    1,
                    2,
                    218,
                    0
                ],
                "published": "2024-10-01T17:09:01Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    17,
                    9,
                    1,
                    1,
                    275,
                    0
                ],
                "title": "Aligning Human and LLM Judgments: Insights from EvalAssist on\n  Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Human and LLM Judgments: Insights from EvalAssist on\n  Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"
                },
                "summary": "Evaluation of large language model (LLM) outputs requires users to make\ncritical judgments about the best outputs across various configurations. This\nprocess is costly and takes time given the large amounts of data. LLMs are\nincreasingly used as evaluators to filter training data, evaluate model\nperformance or assist human evaluators with detailed assessments. To support\nthis process, effective front-end tools are critical for evaluation. Two common\napproaches for using LLMs as evaluators are direct assessment and pairwise\ncomparison. In our study with machine learning practitioners (n=15), each\ncompleting 6 tasks yielding 131 evaluations, we explore how task-related\nfactors and assessment strategies influence criteria refinement and user\nperceptions. Findings show that users performed more evaluations with direct\nassessment by making criteria task-specific, modifying judgments, and changing\nthe evaluator model. We conclude with recommendations for how systems can\nbetter support interactions in LLM-assisted evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation of large language model (LLM) outputs requires users to make\ncritical judgments about the best outputs across various configurations. This\nprocess is costly and takes time given the large amounts of data. LLMs are\nincreasingly used as evaluators to filter training data, evaluate model\nperformance or assist human evaluators with detailed assessments. To support\nthis process, effective front-end tools are critical for evaluation. Two common\napproaches for using LLMs as evaluators are direct assessment and pairwise\ncomparison. In our study with machine learning practitioners (n=15), each\ncompleting 6 tasks yielding 131 evaluations, we explore how task-related\nfactors and assessment strategies influence criteria refinement and user\nperceptions. Findings show that users performed more evaluations with direct\nassessment by making criteria task-specific, modifying judgments, and changing\nthe evaluator model. We conclude with recommendations for how systems can\nbetter support interactions in LLM-assisted evaluations."
                },
                "authors": [
                    {
                        "name": "Zahra Ashktorab"
                    },
                    {
                        "name": "Michael Desmond"
                    },
                    {
                        "name": "Qian Pan"
                    },
                    {
                        "name": "James M. Johnson"
                    },
                    {
                        "name": "Martin Santillan Cooper"
                    },
                    {
                        "name": "Elizabeth M. Daly"
                    },
                    {
                        "name": "Rahul Nair"
                    },
                    {
                        "name": "Tejaswini Pedapati"
                    },
                    {
                        "name": "Hyo Jin Do"
                    },
                    {
                        "name": "Werner Geyer"
                    }
                ],
                "author_detail": {
                    "name": "Werner Geyer"
                },
                "author": "Werner Geyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00873v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00873v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04691v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04691v1",
                "updated": "2025-08-06T17:54:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    54,
                    10,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T17:54:10Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    54,
                    10,
                    2,
                    218,
                    0
                ],
                "title": "From MAS to MARS: Coordination Failures and Reasoning Trade-offs in\n  Hierarchical Multi-Agent Robotic Systems within a Healthcare Scenario",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From MAS to MARS: Coordination Failures and Reasoning Trade-offs in\n  Hierarchical Multi-Agent Robotic Systems within a Healthcare Scenario"
                },
                "summary": "Multi-agent robotic systems (MARS) build upon multi-agent systems by\nintegrating physical and task-related constraints, increasing the complexity of\naction execution and agent coordination. However, despite the availability of\nadvanced multi-agent frameworks, their real-world deployment on robots remains\nlimited, hindering the advancement of MARS research in practice. To bridge this\ngap, we conducted two studies to investigate performance trade-offs of\nhierarchical multi-agent frameworks in a simulated real-world multi-robot\nhealthcare scenario. In Study 1, using CrewAI, we iteratively refine the\nsystem's knowledge base, to systematically identify and categorize coordination\nfailures (e.g., tool access violations, lack of timely handling of failure\nreports) not resolvable by providing contextual knowledge alone. In Study 2,\nusing AutoGen, we evaluate a redesigned bidirectional communication structure\nand further measure the trade-offs between reasoning and non-reasoning models\noperating within the same robotic team setting. Drawing from our empirical\nfindings, we emphasize the tension between autonomy and stability and the\nimportance of edge-case testing to improve system reliability and safety for\nfuture real-world deployment. Supplementary materials, including codes, task\nagent setup, trace outputs, and annotated examples of coordination failures and\nreasoning behaviors, are available at:\nhttps://byc-sophie.github.io/mas-to-mars/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent robotic systems (MARS) build upon multi-agent systems by\nintegrating physical and task-related constraints, increasing the complexity of\naction execution and agent coordination. However, despite the availability of\nadvanced multi-agent frameworks, their real-world deployment on robots remains\nlimited, hindering the advancement of MARS research in practice. To bridge this\ngap, we conducted two studies to investigate performance trade-offs of\nhierarchical multi-agent frameworks in a simulated real-world multi-robot\nhealthcare scenario. In Study 1, using CrewAI, we iteratively refine the\nsystem's knowledge base, to systematically identify and categorize coordination\nfailures (e.g., tool access violations, lack of timely handling of failure\nreports) not resolvable by providing contextual knowledge alone. In Study 2,\nusing AutoGen, we evaluate a redesigned bidirectional communication structure\nand further measure the trade-offs between reasoning and non-reasoning models\noperating within the same robotic team setting. Drawing from our empirical\nfindings, we emphasize the tension between autonomy and stability and the\nimportance of edge-case testing to improve system reliability and safety for\nfuture real-world deployment. Supplementary materials, including codes, task\nagent setup, trace outputs, and annotated examples of coordination failures and\nreasoning behaviors, are available at:\nhttps://byc-sophie.github.io/mas-to-mars/."
                },
                "authors": [
                    {
                        "name": "Yuanchen Bai"
                    },
                    {
                        "name": "Zijian Ding"
                    },
                    {
                        "name": "Shaoyue Wen"
                    },
                    {
                        "name": "Xiang Chang"
                    },
                    {
                        "name": "Angelique Taylor"
                    }
                ],
                "author_detail": {
                    "name": "Angelique Taylor"
                },
                "author": "Angelique Taylor",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04691v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04691v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13415v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13415v2",
                "updated": "2025-08-06T17:45:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    45,
                    47,
                    2,
                    218,
                    0
                ],
                "published": "2024-11-20T16:02:14Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    16,
                    2,
                    14,
                    2,
                    325,
                    0
                ],
                "title": "Harnessing Large Language Models for Group POI Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Large Language Models for Group POI Recommendations"
                },
                "summary": "The rapid proliferation of Location-Based Social Networks (LBSNs) has\nunderscored the importance of Point-of-Interest (POI) recommendation systems in\nenhancing user experiences. While individual POI recommendation methods\nleverage users' check-in histories to provide personalized suggestions, they\nstruggle to address scenarios requiring group decision-making. Group POI\nrecommendation systems aim to satisfy the collective preferences of multiple\nusers, but existing approaches face two major challenges: diverse group\npreferences and extreme data sparsity in group check-in data. To overcome these\nchallenges, we propose LLMGPR, a novel framework that leverages large language\nmodels (LLMs) for group POI recommendations. LLMGPR introduces\nsemantic-enhanced POI tokens and incorporates rich contextual information to\nmodel the diverse and complex dynamics of group decision-making. To further\nenhance its capabilities, we developed a sequencing adapter using Quantized\nLow-Rank Adaptation (QLoRA), which aligns LLMs with group POI recommendation\ntasks. To address the issue of sparse group check-in data, LLMGPR employs an\naggregation adapter that integrates individual representations into meaningful\ngroup representations. Additionally, a self-supervised learning (SSL) task is\ndesigned to predict the purposes of check-in sequences (e.g., business trips\nand family vacations), thereby enriching group representations with deeper\nsemantic insights. Extensive experiments demonstrate the effectiveness of\nLLMGPR, showcasing its ability to significantly enhance the accuracy and\nrobustness of group POI recommendations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid proliferation of Location-Based Social Networks (LBSNs) has\nunderscored the importance of Point-of-Interest (POI) recommendation systems in\nenhancing user experiences. While individual POI recommendation methods\nleverage users' check-in histories to provide personalized suggestions, they\nstruggle to address scenarios requiring group decision-making. Group POI\nrecommendation systems aim to satisfy the collective preferences of multiple\nusers, but existing approaches face two major challenges: diverse group\npreferences and extreme data sparsity in group check-in data. To overcome these\nchallenges, we propose LLMGPR, a novel framework that leverages large language\nmodels (LLMs) for group POI recommendations. LLMGPR introduces\nsemantic-enhanced POI tokens and incorporates rich contextual information to\nmodel the diverse and complex dynamics of group decision-making. To further\nenhance its capabilities, we developed a sequencing adapter using Quantized\nLow-Rank Adaptation (QLoRA), which aligns LLMs with group POI recommendation\ntasks. To address the issue of sparse group check-in data, LLMGPR employs an\naggregation adapter that integrates individual representations into meaningful\ngroup representations. Additionally, a self-supervised learning (SSL) task is\ndesigned to predict the purposes of check-in sequences (e.g., business trips\nand family vacations), thereby enriching group representations with deeper\nsemantic insights. Extensive experiments demonstrate the effectiveness of\nLLMGPR, showcasing its ability to significantly enhance the accuracy and\nrobustness of group POI recommendations."
                },
                "authors": [
                    {
                        "name": "Jing Long"
                    },
                    {
                        "name": "Liang Qu"
                    },
                    {
                        "name": "Junliang Yu"
                    },
                    {
                        "name": "Tong Chen"
                    },
                    {
                        "name": "Quoc Viet Hung Nguyen"
                    },
                    {
                        "name": "Hongzhi Yin"
                    }
                ],
                "author_detail": {
                    "name": "Hongzhi Yin"
                },
                "author": "Hongzhi Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13415v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13415v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04679v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04679v1",
                "updated": "2025-08-06T17:45:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    45,
                    11,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T17:45:11Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    45,
                    11,
                    2,
                    218,
                    0
                ],
                "title": "MisVisFix: An Interactive Dashboard for Detecting, Explaining, and\n  Correcting Misleading Visualizations using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MisVisFix: An Interactive Dashboard for Detecting, Explaining, and\n  Correcting Misleading Visualizations using Large Language Models"
                },
                "summary": "Misleading visualizations pose a significant challenge to accurate data\ninterpretation. While recent research has explored the use of Large Language\nModels (LLMs) for detecting such misinformation, practical tools that also\nsupport explanation and correction remain limited. We present MisVisFix, an\ninteractive dashboard that leverages both Claude and GPT models to support the\nfull workflow of detecting, explaining, and correcting misleading\nvisualizations. MisVisFix correctly identifies 96% of visualization issues and\naddresses all 74 known visualization misinformation types, classifying them as\nmajor, minor, or potential concerns. It provides detailed explanations,\nactionable suggestions, and automatically generates corrected charts. An\ninteractive chat interface allows users to ask about specific chart elements or\nrequest modifications. The dashboard adapts to newly emerging misinformation\nstrategies through targeted user interactions. User studies with visualization\nexperts and developers of fact-checking tools show that MisVisFix accurately\nidentifies issues and offers useful suggestions for improvement. By\ntransforming LLM-based detection into an accessible, interactive platform,\nMisVisFix advances visualization literacy and supports more trustworthy data\ncommunication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Misleading visualizations pose a significant challenge to accurate data\ninterpretation. While recent research has explored the use of Large Language\nModels (LLMs) for detecting such misinformation, practical tools that also\nsupport explanation and correction remain limited. We present MisVisFix, an\ninteractive dashboard that leverages both Claude and GPT models to support the\nfull workflow of detecting, explaining, and correcting misleading\nvisualizations. MisVisFix correctly identifies 96% of visualization issues and\naddresses all 74 known visualization misinformation types, classifying them as\nmajor, minor, or potential concerns. It provides detailed explanations,\nactionable suggestions, and automatically generates corrected charts. An\ninteractive chat interface allows users to ask about specific chart elements or\nrequest modifications. The dashboard adapts to newly emerging misinformation\nstrategies through targeted user interactions. User studies with visualization\nexperts and developers of fact-checking tools show that MisVisFix accurately\nidentifies issues and offers useful suggestions for improvement. By\ntransforming LLM-based detection into an accessible, interactive platform,\nMisVisFix advances visualization literacy and supports more trustworthy data\ncommunication."
                },
                "authors": [
                    {
                        "name": "Amit Kumar Das"
                    },
                    {
                        "name": "Klaus Mueller"
                    }
                ],
                "author_detail": {
                    "name": "Klaus Mueller"
                },
                "author": "Klaus Mueller",
                "arxiv_comment": "11 pages, 6 figures. Accepted at IEEE VIS: Visualization & Visual\n  Analytics 2025 conference, November 2-7, 2025, Vienna, Austria",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04679v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04676v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04676v1",
                "updated": "2025-08-06T17:42:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    42,
                    22,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T17:42:22Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    42,
                    22,
                    2,
                    218,
                    0
                ],
                "title": "GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via\n  General Samples Replay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via\n  General Samples Replay"
                },
                "summary": "The continual learning capability of large language models (LLMs) is crucial\nfor advancing artificial general intelligence. However, continual fine-tuning\nLLMs across various domains often suffers from catastrophic forgetting,\ncharacterized by: 1) significant forgetting of their general capabilities, and\n2) sharp performance declines in previously learned tasks. To simultaneously\naddress both issues in a simple yet stable manner, we propose General Sample\nReplay (GeRe), a framework that use usual pretraining texts for efficient\nanti-forgetting. Beyond revisiting the most prevalent replay-based practices\nunder GeRe, we further leverage neural states to introduce a enhanced\nactivation states constrained optimization method using threshold-based margin\n(TM) loss, which maintains activation state consistency during replay learning.\nWe are the first to validate that a small, fixed set of pre-collected general\nreplay samples is sufficient to resolve both concerns--retaining general\ncapabilities while promoting overall performance across sequential tasks.\nIndeed, the former can inherently facilitate the latter. Through controlled\nexperiments, we systematically compare TM with different replay strategies\nunder the GeRe framework, including vanilla label fitting, logit imitation via\nKL divergence and feature imitation via L1/L2 losses. Results demonstrate that\nTM consistently improves performance and exhibits better robustness. Our work\npaves the way for efficient replay of LLMs for the future. Our code and data\nare available at https://github.com/Qznan/GeRe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The continual learning capability of large language models (LLMs) is crucial\nfor advancing artificial general intelligence. However, continual fine-tuning\nLLMs across various domains often suffers from catastrophic forgetting,\ncharacterized by: 1) significant forgetting of their general capabilities, and\n2) sharp performance declines in previously learned tasks. To simultaneously\naddress both issues in a simple yet stable manner, we propose General Sample\nReplay (GeRe), a framework that use usual pretraining texts for efficient\nanti-forgetting. Beyond revisiting the most prevalent replay-based practices\nunder GeRe, we further leverage neural states to introduce a enhanced\nactivation states constrained optimization method using threshold-based margin\n(TM) loss, which maintains activation state consistency during replay learning.\nWe are the first to validate that a small, fixed set of pre-collected general\nreplay samples is sufficient to resolve both concerns--retaining general\ncapabilities while promoting overall performance across sequential tasks.\nIndeed, the former can inherently facilitate the latter. Through controlled\nexperiments, we systematically compare TM with different replay strategies\nunder the GeRe framework, including vanilla label fitting, logit imitation via\nKL divergence and feature imitation via L1/L2 losses. Results demonstrate that\nTM consistently improves performance and exhibits better robustness. Our work\npaves the way for efficient replay of LLMs for the future. Our code and data\nare available at https://github.com/Qznan/GeRe."
                },
                "authors": [
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Shuoran Jiang"
                    },
                    {
                        "name": "Mengchen Zhao"
                    },
                    {
                        "name": "Yuefeng Li"
                    },
                    {
                        "name": "Yang Fan"
                    },
                    {
                        "name": "Xiangping Wu"
                    },
                    {
                        "name": "Qingcai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Qingcai Chen"
                },
                "author": "Qingcai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04676v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04676v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06362v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06362v2",
                "updated": "2025-08-06T17:41:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    41,
                    48,
                    2,
                    218,
                    0
                ],
                "published": "2025-03-09T00:02:10Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    0,
                    2,
                    10,
                    6,
                    68,
                    0
                ],
                "title": "Adaptive Audio-Visual Speech Recognition via Matryoshka-Based Multimodal\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Audio-Visual Speech Recognition via Matryoshka-Based Multimodal\n  LLMs"
                },
                "summary": "Audio-Visual Speech Recognition (AVSR) leverages audio and visual modalities\nto improve robustness in noisy environments. Recent advances in Large Language\nModels (LLMs) show strong performance in speech recognition, including AVSR.\nHowever, the long speech representations lead to high computational costs for\nLLMs. Prior methods compress inputs before feeding them to LLMs, but high\ncompression often harms accuracy. To address this, we propose Llama-MTSK, the\nfirst Matryoshka-based Multimodal LLM for AVSR, which flexibly adapts\naudio-visual token allocation under varying compute constraints. Inspired by\nMatryoshka Representation Learning, our model encodes representations at\nmultiple granularities with a single architecture, avoiding the need for\nseparate models. For efficient fine-tuning, we introduce three LoRA-based\nstrategies using global and scale-specific modules. Evaluations on major AVSR\ndatasets show Llama-MTSK matches or outperforms models trained at fixed\ncompression levels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio-Visual Speech Recognition (AVSR) leverages audio and visual modalities\nto improve robustness in noisy environments. Recent advances in Large Language\nModels (LLMs) show strong performance in speech recognition, including AVSR.\nHowever, the long speech representations lead to high computational costs for\nLLMs. Prior methods compress inputs before feeding them to LLMs, but high\ncompression often harms accuracy. To address this, we propose Llama-MTSK, the\nfirst Matryoshka-based Multimodal LLM for AVSR, which flexibly adapts\naudio-visual token allocation under varying compute constraints. Inspired by\nMatryoshka Representation Learning, our model encodes representations at\nmultiple granularities with a single architecture, avoiding the need for\nseparate models. For efficient fine-tuning, we introduce three LoRA-based\nstrategies using global and scale-specific modules. Evaluations on major AVSR\ndatasets show Llama-MTSK matches or outperforms models trained at fixed\ncompression levels."
                },
                "authors": [
                    {
                        "name": "Umberto Cappellazzo"
                    },
                    {
                        "name": "Minsu Kim"
                    },
                    {
                        "name": "Stavros Petridis"
                    }
                ],
                "author_detail": {
                    "name": "Stavros Petridis"
                },
                "author": "Stavros Petridis",
                "arxiv_comment": "Accepted to IEEE ASRU 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06362v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06362v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04664v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04664v1",
                "updated": "2025-08-06T17:32:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    32,
                    58,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T17:32:58Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    32,
                    58,
                    2,
                    218,
                    0
                ],
                "title": "Sculptor: Empowering LLMs with Cognitive Agency via Active Context\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sculptor: Empowering LLMs with Cognitive Agency via Active Context\n  Management"
                },
                "summary": "Large Language Models (LLMs) suffer from significant performance degradation\nwhen processing long contexts due to proactive interference, where irrelevant\ninformation in earlier parts of the context disrupts reasoning and memory\nrecall. While most research focuses on external memory systems to augment LLMs'\ncapabilities, we propose a complementary approach: empowering LLMs with Active\nContext Management (ACM) tools to actively sculpt their internal working\nmemory. We introduce Sculptor, a framework that equips LLMs with three\ncategories of tools: (1) context fragmentation, (2) summary, hide, and restore,\nand (3) intelligent search. Our approach enables LLMs to proactively manage\ntheir attention and working memory, analogous to how humans selectively focus\non relevant information while filtering out distractions. Experimental\nevaluation on information-sparse benchmarks-PI-LLM (proactive interference) and\nNeedleBench Multi-Needle Reasoning-demonstrates that Sculptor significantly\nimproves performance even without specific training, leveraging LLMs' inherent\ntool calling generalization capabilities. By enabling Active Context\nManagement, Sculptor not only mitigates proactive interference but also\nprovides a cognitive foundation for more reliable reasoning across diverse\nlong-context tasks-highlighting that explicit context-control strategies,\nrather than merely larger token windows, are key to robustness at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) suffer from significant performance degradation\nwhen processing long contexts due to proactive interference, where irrelevant\ninformation in earlier parts of the context disrupts reasoning and memory\nrecall. While most research focuses on external memory systems to augment LLMs'\ncapabilities, we propose a complementary approach: empowering LLMs with Active\nContext Management (ACM) tools to actively sculpt their internal working\nmemory. We introduce Sculptor, a framework that equips LLMs with three\ncategories of tools: (1) context fragmentation, (2) summary, hide, and restore,\nand (3) intelligent search. Our approach enables LLMs to proactively manage\ntheir attention and working memory, analogous to how humans selectively focus\non relevant information while filtering out distractions. Experimental\nevaluation on information-sparse benchmarks-PI-LLM (proactive interference) and\nNeedleBench Multi-Needle Reasoning-demonstrates that Sculptor significantly\nimproves performance even without specific training, leveraging LLMs' inherent\ntool calling generalization capabilities. By enabling Active Context\nManagement, Sculptor not only mitigates proactive interference but also\nprovides a cognitive foundation for more reliable reasoning across diverse\nlong-context tasks-highlighting that explicit context-control strategies,\nrather than merely larger token windows, are key to robustness at scale."
                },
                "authors": [
                    {
                        "name": "Mo Li"
                    },
                    {
                        "name": "L. H. Xu"
                    },
                    {
                        "name": "Qitai Tan"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Yunxin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yunxin Liu"
                },
                "author": "Yunxin Liu",
                "arxiv_comment": "Preprint. Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04664v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04664v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02520v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02520v4",
                "updated": "2025-08-07T05:19:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    5,
                    19,
                    42,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-04T15:30:57Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    30,
                    57,
                    0,
                    216,
                    0
                ],
                "title": "xDeepServe: Model-as-a-Service on Huawei CloudMatrix384",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xDeepServe: Model-as-a-Service on Huawei CloudMatrix384"
                },
                "summary": "The rise of scaled-out LLMs and scaled-up SuperPods signals a new era in\nlarge-scale AI infrastructure. LLMs continue to scale out via MoE, as seen in\nrecent models like DeepSeek, Kimi, and Qwen. In parallel, AI hardware is\nscaling up, with Huawei's CloudMatrix384 SuperPod offering hundreds of GB/s\nhigh-speed interconnects. Running large MoE models on SuperPod-scale hardware\nbrings new challenges. It requires new execution models, scalable scheduling,\nefficient expert load balancing, and elimination of single points of failure.\nThis paper presents xDeepServe, Huawei Cloud's LLM serving system designed for\nSuperPod-scale infrastructure. At its core is Transformerless, a disaggregated\narchitecture that decomposes transformer models into modular units--attention,\nfeedforward, and MoE--executed independently on NPUs connected via high-speed\nfabric. We implement this design in two forms: disaggregated prefill-decode and\ndisaggregated MoE-attention. This fully disaggregated setup enables independent\nscaling of compute and memory without sacrificing performance. To support this\narchitecture, we propose XCCL, a communication library that leverages\nCloudMatrix384's global shared memory to implement efficient point-to-point and\nall-to-all primitives. We also extend our serving engine FlowServe with\nsystem-level techniques, enabling scalable inference across hundreds of NPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of scaled-out LLMs and scaled-up SuperPods signals a new era in\nlarge-scale AI infrastructure. LLMs continue to scale out via MoE, as seen in\nrecent models like DeepSeek, Kimi, and Qwen. In parallel, AI hardware is\nscaling up, with Huawei's CloudMatrix384 SuperPod offering hundreds of GB/s\nhigh-speed interconnects. Running large MoE models on SuperPod-scale hardware\nbrings new challenges. It requires new execution models, scalable scheduling,\nefficient expert load balancing, and elimination of single points of failure.\nThis paper presents xDeepServe, Huawei Cloud's LLM serving system designed for\nSuperPod-scale infrastructure. At its core is Transformerless, a disaggregated\narchitecture that decomposes transformer models into modular units--attention,\nfeedforward, and MoE--executed independently on NPUs connected via high-speed\nfabric. We implement this design in two forms: disaggregated prefill-decode and\ndisaggregated MoE-attention. This fully disaggregated setup enables independent\nscaling of compute and memory without sacrificing performance. To support this\narchitecture, we propose XCCL, a communication library that leverages\nCloudMatrix384's global shared memory to implement efficient point-to-point and\nall-to-all primitives. We also extend our serving engine FlowServe with\nsystem-level techniques, enabling scalable inference across hundreds of NPUs."
                },
                "authors": [
                    {
                        "name": "Ao Xiao"
                    },
                    {
                        "name": "Bangzheng He"
                    },
                    {
                        "name": "Baoquan Zhang"
                    },
                    {
                        "name": "Baoxing Huai"
                    },
                    {
                        "name": "Bingji Wang"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Bo Xu"
                    },
                    {
                        "name": "Boyi Hou"
                    },
                    {
                        "name": "Chan Yang"
                    },
                    {
                        "name": "Changhong Liu"
                    },
                    {
                        "name": "Cheng Cui"
                    },
                    {
                        "name": "Chenyu Zhu"
                    },
                    {
                        "name": "Cong Feng"
                    },
                    {
                        "name": "Daohui Wang"
                    },
                    {
                        "name": "Dayun Lin"
                    },
                    {
                        "name": "Duo Zhao"
                    },
                    {
                        "name": "Fengshao Zou"
                    },
                    {
                        "name": "Fu Wang"
                    },
                    {
                        "name": "Gangqiang Zhang"
                    },
                    {
                        "name": "Gengyuan Dan"
                    },
                    {
                        "name": "Guanjie Chen"
                    },
                    {
                        "name": "Guodong Guan"
                    },
                    {
                        "name": "Guodong Yang"
                    },
                    {
                        "name": "Haifeng Li"
                    },
                    {
                        "name": "Haipei Zhu"
                    },
                    {
                        "name": "Hao Feng"
                    },
                    {
                        "name": "Hao Huang"
                    },
                    {
                        "name": "Hao Xu"
                    },
                    {
                        "name": "Hengrui Ma"
                    },
                    {
                        "name": "Hengtao Fan"
                    },
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Jiang Liu"
                    },
                    {
                        "name": "Jiang Xu"
                    },
                    {
                        "name": "Jie Meng"
                    },
                    {
                        "name": "Jinhan Xin"
                    },
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Juwei Chen"
                    },
                    {
                        "name": "Lan Yu"
                    },
                    {
                        "name": "Lanxin Miao"
                    },
                    {
                        "name": "Liang Liu"
                    },
                    {
                        "name": "Linan Jing"
                    },
                    {
                        "name": "Lu Zhou"
                    },
                    {
                        "name": "Meina Han"
                    },
                    {
                        "name": "Mingkun Deng"
                    },
                    {
                        "name": "Mingyu Deng"
                    },
                    {
                        "name": "Naitian Deng"
                    },
                    {
                        "name": "Nizhong Lin"
                    },
                    {
                        "name": "Peihan Zhao"
                    },
                    {
                        "name": "Peng Pan"
                    },
                    {
                        "name": "Pengfei Shen"
                    },
                    {
                        "name": "Ping Li"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Qin Zhang"
                    },
                    {
                        "name": "Qingrong Xia"
                    },
                    {
                        "name": "Qingyi Zhang"
                    },
                    {
                        "name": "Qunchao Fu"
                    },
                    {
                        "name": "Ren Guo"
                    },
                    {
                        "name": "Ruimin Gao"
                    },
                    {
                        "name": "Shaochun Li"
                    },
                    {
                        "name": "Sheng Long"
                    },
                    {
                        "name": "Shentian Li"
                    },
                    {
                        "name": "Shining Wan"
                    },
                    {
                        "name": "Shuai Shen"
                    },
                    {
                        "name": "Shuangfu Zeng"
                    },
                    {
                        "name": "Shuming Jing"
                    },
                    {
                        "name": "Siqi Yang"
                    },
                    {
                        "name": "Song Zhang"
                    },
                    {
                        "name": "Tao Xu"
                    },
                    {
                        "name": "Tianlin Du"
                    },
                    {
                        "name": "Ting Chen"
                    },
                    {
                        "name": "Wanxu Wu"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Weinan Tong"
                    },
                    {
                        "name": "Weiwei Chen"
                    },
                    {
                        "name": "Wen Peng"
                    },
                    {
                        "name": "Wenli Zhou"
                    },
                    {
                        "name": "Wenquan Yang"
                    },
                    {
                        "name": "Wenxin Liang"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Xiaoli Zhou"
                    },
                    {
                        "name": "Xin Jin"
                    },
                    {
                        "name": "Xinyu Duan"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yalong Shan"
                    },
                    {
                        "name": "Yang Gan"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Yi Deng"
                    },
                    {
                        "name": "Yi Zheng"
                    },
                    {
                        "name": "Yingfei Zheng"
                    },
                    {
                        "name": "Yiyun Zheng"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Yong Gao"
                    },
                    {
                        "name": "Yongqiang Yang"
                    },
                    {
                        "name": "Yuanjin Gong"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Yuetao Chen"
                    },
                    {
                        "name": "Yukun Zhu"
                    },
                    {
                        "name": "Yulong He"
                    },
                    {
                        "name": "Yusu Zhao"
                    },
                    {
                        "name": "Yuyan Wu"
                    },
                    {
                        "name": "Zenan Zhang"
                    },
                    {
                        "name": "Zhaojin Zhuo"
                    },
                    {
                        "name": "Zhaoyang Ji"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Zhenhua Yang"
                    },
                    {
                        "name": "Zhenli Sheng"
                    },
                    {
                        "name": "Zhibin Yu"
                    },
                    {
                        "name": "Zhigang Ji"
                    },
                    {
                        "name": "Zhihao Ren"
                    },
                    {
                        "name": "Zhipeng Bian"
                    },
                    {
                        "name": "Zhixia Liu"
                    },
                    {
                        "name": "Zhiyu Dong"
                    },
                    {
                        "name": "Zhonghua Li"
                    },
                    {
                        "name": "Zhou Yu"
                    },
                    {
                        "name": "Zhuoming Shen"
                    },
                    {
                        "name": "Zhuwei Peng"
                    },
                    {
                        "name": "Zi Ye"
                    },
                    {
                        "name": "Zihao Xiang"
                    },
                    {
                        "name": "Zimin Fu"
                    },
                    {
                        "name": "Zixuan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zixuan Zhang"
                },
                "author": "Zixuan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02520v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02520v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04655v1",
                "updated": "2025-08-06T17:19:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    19,
                    10,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T17:19:10Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    19,
                    10,
                    2,
                    218,
                    0
                ],
                "title": "X-SAM: From Segment Anything to Any Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-SAM: From Segment Anything to Any Segmentation"
                },
                "summary": "Large Language Models (LLMs) demonstrate strong capabilities in broad\nknowledge representation, yet they are inherently deficient in pixel-level\nperceptual understanding. Although the Segment Anything Model (SAM) represents\na significant advancement in visual-prompt-driven image segmentation, it\nexhibits notable limitations in multi-mask prediction and category-specific\nsegmentation tasks, and it cannot integrate all segmentation tasks within a\nunified model architecture. To address these limitations, we present X-SAM, a\nstreamlined Multimodal Large Language Model (MLLM) framework that extends the\nsegmentation paradigm from \\textit{segment anything} to \\textit{any\nsegmentation}. Specifically, we introduce a novel unified framework that\nenables more advanced pixel-level perceptual comprehension for MLLMs.\nFurthermore, we propose a new segmentation task, termed Visual GrounDed (VGD)\nsegmentation, which segments all instance objects with interactive visual\nprompts and empowers MLLMs with visual grounded, pixel-wise interpretative\ncapabilities. To enable effective training on diverse data sources, we present\na unified training strategy that supports co-training across multiple datasets.\nExperimental results demonstrate that X-SAM achieves state-of-the-art\nperformance on a wide range of image segmentation benchmarks, highlighting its\nefficiency for multimodal, pixel-level visual understanding. Code is available\nat https://github.com/wanghao9610/X-SAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate strong capabilities in broad\nknowledge representation, yet they are inherently deficient in pixel-level\nperceptual understanding. Although the Segment Anything Model (SAM) represents\na significant advancement in visual-prompt-driven image segmentation, it\nexhibits notable limitations in multi-mask prediction and category-specific\nsegmentation tasks, and it cannot integrate all segmentation tasks within a\nunified model architecture. To address these limitations, we present X-SAM, a\nstreamlined Multimodal Large Language Model (MLLM) framework that extends the\nsegmentation paradigm from \\textit{segment anything} to \\textit{any\nsegmentation}. Specifically, we introduce a novel unified framework that\nenables more advanced pixel-level perceptual comprehension for MLLMs.\nFurthermore, we propose a new segmentation task, termed Visual GrounDed (VGD)\nsegmentation, which segments all instance objects with interactive visual\nprompts and empowers MLLMs with visual grounded, pixel-wise interpretative\ncapabilities. To enable effective training on diverse data sources, we present\na unified training strategy that supports co-training across multiple datasets.\nExperimental results demonstrate that X-SAM achieves state-of-the-art\nperformance on a wide range of image segmentation benchmarks, highlighting its\nefficiency for multimodal, pixel-level visual understanding. Code is available\nat https://github.com/wanghao9610/X-SAM."
                },
                "authors": [
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Limeng Qiao"
                    },
                    {
                        "name": "Zequn Jie"
                    },
                    {
                        "name": "Zhijian Huang"
                    },
                    {
                        "name": "Chengjian Feng"
                    },
                    {
                        "name": "Qingfang Zheng"
                    },
                    {
                        "name": "Lin Ma"
                    },
                    {
                        "name": "Xiangyuan Lan"
                    },
                    {
                        "name": "Xiaodan Liang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodan Liang"
                },
                "author": "Xiaodan Liang",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04652v1",
                "updated": "2025-08-06T17:18:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    18,
                    25,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T17:18:25Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    18,
                    25,
                    2,
                    218,
                    0
                ],
                "title": "LLM Collaboration With Multi-Agent Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Collaboration With Multi-Agent Reinforcement Learning"
                },
                "summary": "A large amount of work has been done in Multi-Agent Systems (MAS) for\nmodeling and solving problems with multiple interacting agents. However, most\nLLMs are pretrained independently and not specifically optimized for\ncoordination. Existing LLM fine-tuning frameworks rely on individual rewards,\nwhich require complex reward designs for each agent to encourage collaboration.\nTo address these challenges, we model LLM collaboration as a cooperative\nMulti-Agent Reinforcement Learning (MARL) problem. We develop a multi-agent,\nmulti-turn algorithm, Multi-Agent Group Relative Policy Optimization (MAGRPO),\nto solve it, building on current RL approaches for LLMs as well as MARL\ntechniques. Our experiments on LLM writing and coding collaboration demonstrate\nthat fine-tuning MAS with MAGRPO enables agents to generate high-quality\nresponses efficiently through effective cooperation. Our approach opens the\ndoor to using other MARL methods for LLMs and highlights the associated\nchallenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A large amount of work has been done in Multi-Agent Systems (MAS) for\nmodeling and solving problems with multiple interacting agents. However, most\nLLMs are pretrained independently and not specifically optimized for\ncoordination. Existing LLM fine-tuning frameworks rely on individual rewards,\nwhich require complex reward designs for each agent to encourage collaboration.\nTo address these challenges, we model LLM collaboration as a cooperative\nMulti-Agent Reinforcement Learning (MARL) problem. We develop a multi-agent,\nmulti-turn algorithm, Multi-Agent Group Relative Policy Optimization (MAGRPO),\nto solve it, building on current RL approaches for LLMs as well as MARL\ntechniques. Our experiments on LLM writing and coding collaboration demonstrate\nthat fine-tuning MAS with MAGRPO enables agents to generate high-quality\nresponses efficiently through effective cooperation. Our approach opens the\ndoor to using other MARL methods for LLMs and highlights the associated\nchallenges."
                },
                "authors": [
                    {
                        "name": "Shuo Liu"
                    },
                    {
                        "name": "Zeyu Liang"
                    },
                    {
                        "name": "Xueguang Lyu"
                    },
                    {
                        "name": "Christopher Amato"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Amato"
                },
                "author": "Christopher Amato",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04634v1",
                "updated": "2025-08-06T17:02:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    2,
                    1,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T17:02:01Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    2,
                    1,
                    2,
                    218,
                    0
                ],
                "title": "VirtLab: An AI-Powered System for Flexible, Customizable, and\n  Large-scale Team Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VirtLab: An AI-Powered System for Flexible, Customizable, and\n  Large-scale Team Simulations"
                },
                "summary": "Simulating how team members collaborate within complex environments using\nAgentic AI is a promising approach to explore hypotheses grounded in social\nscience theories and study team behaviors. We introduce VirtLab, a\nuser-friendly, customizable, multi-agent, and scalable team simulation system\nthat enables testing teams with LLM-based agents in spatial and temporal\nsettings. This system addresses the current frameworks' design and technical\nlimitations that do not consider flexible simulation scenarios and spatial\nsettings. VirtLab contains a simulation engine and a web interface that enables\nboth technical and non-technical users to formulate, run, and analyze team\nsimulations without programming. We demonstrate the system's utility by\ncomparing ground truth data with simulated scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating how team members collaborate within complex environments using\nAgentic AI is a promising approach to explore hypotheses grounded in social\nscience theories and study team behaviors. We introduce VirtLab, a\nuser-friendly, customizable, multi-agent, and scalable team simulation system\nthat enables testing teams with LLM-based agents in spatial and temporal\nsettings. This system addresses the current frameworks' design and technical\nlimitations that do not consider flexible simulation scenarios and spatial\nsettings. VirtLab contains a simulation engine and a web interface that enables\nboth technical and non-technical users to formulate, run, and analyze team\nsimulations without programming. We demonstrate the system's utility by\ncomparing ground truth data with simulated scenarios."
                },
                "authors": [
                    {
                        "name": "Mohammed Almutairi"
                    },
                    {
                        "name": "Charles Chiang"
                    },
                    {
                        "name": "Haoze Guo"
                    },
                    {
                        "name": "Matthew Belcher"
                    },
                    {
                        "name": "Nandini Banerjee"
                    },
                    {
                        "name": "Maria Milkowski"
                    },
                    {
                        "name": "Svitlana Volkova"
                    },
                    {
                        "name": "Daniel Nguyen"
                    },
                    {
                        "name": "Tim Weninger"
                    },
                    {
                        "name": "Michael Yankoski"
                    },
                    {
                        "name": "Trenton W. Ford"
                    },
                    {
                        "name": "Diego Gomez-Zara"
                    }
                ],
                "author_detail": {
                    "name": "Diego Gomez-Zara"
                },
                "author": "Diego Gomez-Zara",
                "arxiv_doi": "10.1145/3746058.3758994",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746058.3758994",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.04634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "5 pages, 2 figures, UIST 2025",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04632v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04632v2",
                "updated": "2025-08-07T11:30:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    11,
                    30,
                    20,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-06T17:00:54Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    0,
                    54,
                    2,
                    218,
                    0
                ],
                "title": "IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with\n  Verifiable Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with\n  Verifiable Rewards"
                },
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) improves instruction\nfollowing capabilities of large language models (LLMs), but suffers from\ntraining inefficiency due to inadequate difficulty assessment. Moreover, RLVR\nis prone to over-optimization, where LLMs exploit verification shortcuts\nwithout aligning to the actual intent of user instructions. We introduce\nInstruction Following Decorator (IFDecorator}, a framework that wraps RLVR\ntraining into a robust and sample-efficient pipeline. It consists of three\ncomponents: (1) a cooperative-adversarial data flywheel that co-evolves\ninstructions and hybrid verifications, generating progressively more\nchallenging instruction-verification pairs; (2) IntentCheck, a bypass module\nenforcing intent alignment; and (3) trip wires, a diagnostic mechanism that\ndetects reward hacking via trap instructions, which trigger and capture\nshortcut exploitation behaviors. Our Qwen2.5-32B-Instruct-IFDecorator achieves\n87.43% accuracy on IFEval, outperforming larger proprietary models such as\nGPT-4o. Additionally, we demonstrate substantial improvements on FollowBench\nwhile preserving general capabilities. Our trip wires show significant\nreductions in reward hacking rates. We will release models, code, and data for\nfuture research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR) improves instruction\nfollowing capabilities of large language models (LLMs), but suffers from\ntraining inefficiency due to inadequate difficulty assessment. Moreover, RLVR\nis prone to over-optimization, where LLMs exploit verification shortcuts\nwithout aligning to the actual intent of user instructions. We introduce\nInstruction Following Decorator (IFDecorator}, a framework that wraps RLVR\ntraining into a robust and sample-efficient pipeline. It consists of three\ncomponents: (1) a cooperative-adversarial data flywheel that co-evolves\ninstructions and hybrid verifications, generating progressively more\nchallenging instruction-verification pairs; (2) IntentCheck, a bypass module\nenforcing intent alignment; and (3) trip wires, a diagnostic mechanism that\ndetects reward hacking via trap instructions, which trigger and capture\nshortcut exploitation behaviors. Our Qwen2.5-32B-Instruct-IFDecorator achieves\n87.43% accuracy on IFEval, outperforming larger proprietary models such as\nGPT-4o. Additionally, we demonstrate substantial improvements on FollowBench\nwhile preserving general capabilities. Our trip wires show significant\nreductions in reward hacking rates. We will release models, code, and data for\nfuture research."
                },
                "authors": [
                    {
                        "name": "Xu Guo"
                    },
                    {
                        "name": "Tianyi Liang"
                    },
                    {
                        "name": "Tong Jian"
                    },
                    {
                        "name": "Xiaogui Yang"
                    },
                    {
                        "name": "Ling-I Wu"
                    },
                    {
                        "name": "Chenhui Li"
                    },
                    {
                        "name": "Zhihui Lu"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "arxiv_comment": "7 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04632v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04632v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04449v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04449v2",
                "updated": "2025-08-06T16:57:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    57,
                    39,
                    2,
                    218,
                    0
                ],
                "published": "2024-12-05T18:58:03Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    58,
                    3,
                    3,
                    340,
                    0
                ],
                "title": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay"
                },
                "summary": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. In this paper, we propose p-MoD, an efficient MLLM\narchitecture that significantly reduces training and inference costs while\nmaintaining model performance. The majority of computation in MLLMs stems from\nthe overwhelming volume of vision tokens processed by the transformer-based\nLLM. Accordingly, we leverage the Mixture-of-Depths (MoD) mechanism, where each\nLLM layer selects essential vision tokens to process while skipping redundant\nones. However, integrating MoD into MLLMs is non-trivial. To address the\nchallenges of training and inference stability as well as limited training\ndata, we adapt the MoD module with two novel designs: tanh-gated weight\nnormalization (TanhNorm) and symmetric token reweighting (STRing). Moreover, we\nobserve that vision tokens exhibit higher redundancy in deeper layers and thus\ndesign a progressive ratio decay (PRD) strategy, which gradually reduces the\ntoken retention ratio layer by layer, employing a shifted cosine schedule. This\ncrucial design fully unleashes the potential of MoD, significantly boosting the\nefficiency and performance of our models. Extensive experiments on two baseline\nmodels across 15 benchmarks show that our model matches or even surpasses the\nperformance of corresponding baselines, while requiring only 55.6% TFLOPs and\n53.7% KV cache storage during inference, and 77.7% GPU hours during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. In this paper, we propose p-MoD, an efficient MLLM\narchitecture that significantly reduces training and inference costs while\nmaintaining model performance. The majority of computation in MLLMs stems from\nthe overwhelming volume of vision tokens processed by the transformer-based\nLLM. Accordingly, we leverage the Mixture-of-Depths (MoD) mechanism, where each\nLLM layer selects essential vision tokens to process while skipping redundant\nones. However, integrating MoD into MLLMs is non-trivial. To address the\nchallenges of training and inference stability as well as limited training\ndata, we adapt the MoD module with two novel designs: tanh-gated weight\nnormalization (TanhNorm) and symmetric token reweighting (STRing). Moreover, we\nobserve that vision tokens exhibit higher redundancy in deeper layers and thus\ndesign a progressive ratio decay (PRD) strategy, which gradually reduces the\ntoken retention ratio layer by layer, employing a shifted cosine schedule. This\ncrucial design fully unleashes the potential of MoD, significantly boosting the\nefficiency and performance of our models. Extensive experiments on two baseline\nmodels across 15 benchmarks show that our model matches or even surpasses the\nperformance of corresponding baselines, while requiring only 55.6% TFLOPs and\n53.7% KV cache storage during inference, and 77.7% GPU hours during training."
                },
                "authors": [
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Desen Meng"
                    },
                    {
                        "name": "Zhengming Zhang"
                    },
                    {
                        "name": "Zhenpeng Huang"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Limin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Limin Wang"
                },
                "author": "Limin Wang",
                "arxiv_comment": "Accepted by ICCV 2025; Code released at\n  https://github.com/MCG-NJU/p-MoD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04449v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04449v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04626v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04626v1",
                "updated": "2025-08-06T16:51:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    51,
                    38,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T16:51:38Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    51,
                    38,
                    2,
                    218,
                    0
                ],
                "title": "P-Aligner: Enabling Pre-Alignment of Language Models via Principled\n  Instruction Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "P-Aligner: Enabling Pre-Alignment of Language Models via Principled\n  Instruction Synthesis"
                },
                "summary": "Large Language Models (LLMs) are expected to produce safe, helpful, and\nhonest content during interaction with human users, but they frequently fail to\nalign with such values when given flawed instructions, e.g., missing context,\nambiguous directives, or inappropriate tone, leaving substantial room for\nimprovement along multiple dimensions. A cost-effective yet high-impact way is\nto pre-align instructions before the model begins decoding. Existing approaches\neither rely on prohibitive test-time search costs or end-to-end model rewrite,\nwhich is powered by a customized training corpus with unclear objectives. In\nthis work, we demonstrate that the goal of efficient and effective preference\nalignment can be achieved by P-Aligner, a lightweight module generating\ninstructions that preserve the original intents while being expressed in a more\nhuman-preferred form. P-Aligner is trained on UltraPrompt, a new dataset\nsynthesized via a proposed principle-guided pipeline using Monte-Carlo Tree\nSearch, which systematically explores the space of candidate instructions that\nare closely tied to human preference. Experiments across different methods show\nthat P-Aligner generally outperforms strong baselines across various models and\nbenchmarks, including average win-rate gains of 28.35% and 8.69% on GPT-4-turbo\nand Gemma-2-SimPO, respectively. Further analyses validate its effectiveness\nand efficiency through multiple perspectives, including data quality, search\nstrategies, iterative deployment, and time overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are expected to produce safe, helpful, and\nhonest content during interaction with human users, but they frequently fail to\nalign with such values when given flawed instructions, e.g., missing context,\nambiguous directives, or inappropriate tone, leaving substantial room for\nimprovement along multiple dimensions. A cost-effective yet high-impact way is\nto pre-align instructions before the model begins decoding. Existing approaches\neither rely on prohibitive test-time search costs or end-to-end model rewrite,\nwhich is powered by a customized training corpus with unclear objectives. In\nthis work, we demonstrate that the goal of efficient and effective preference\nalignment can be achieved by P-Aligner, a lightweight module generating\ninstructions that preserve the original intents while being expressed in a more\nhuman-preferred form. P-Aligner is trained on UltraPrompt, a new dataset\nsynthesized via a proposed principle-guided pipeline using Monte-Carlo Tree\nSearch, which systematically explores the space of candidate instructions that\nare closely tied to human preference. Experiments across different methods show\nthat P-Aligner generally outperforms strong baselines across various models and\nbenchmarks, including average win-rate gains of 28.35% and 8.69% on GPT-4-turbo\nand Gemma-2-SimPO, respectively. Further analyses validate its effectiveness\nand efficiency through multiple perspectives, including data quality, search\nstrategies, iterative deployment, and time overhead."
                },
                "authors": [
                    {
                        "name": "Feifan Song"
                    },
                    {
                        "name": "Bofei Gao"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Weimin Xiong"
                    },
                    {
                        "name": "Yuyang Song"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Houfeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Houfeng Wang"
                },
                "author": "Houfeng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04626v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00222v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00222v3",
                "updated": "2025-08-06T16:36:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    36,
                    42,
                    2,
                    218,
                    0
                ],
                "published": "2025-07-31T23:55:29Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    23,
                    55,
                    29,
                    3,
                    212,
                    0
                ],
                "title": "RL-PLUS: Countering Capability Boundary Collapse of LLMs in\n  Reinforcement Learning with Hybrid-policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RL-PLUS: Countering Capability Boundary Collapse of LLMs in\n  Reinforcement Learning with Hybrid-policy Optimization"
                },
                "summary": "Reinforcement Learning with Verifiable Reward (RLVR) has significantly\nadvanced the complex reasoning abilities of Large Language Models (LLMs).\nHowever, it struggles to break through the inherent capability boundaries of\nthe base LLM, due to its essentially on-policy strategy coupled with LLM's\nimmense action space and sparse reward. Critically, RLVR can lead to the\ncapability boundary collapse, narrowing the LLM's problem-solving scope. To\naddress this problem, we propose RL-PLUS, a novel hybrid-policy optimization\napproach for LLMs that synergizes internal exploitation with external data to\nachieve stronger reasoning capabilities and surpass the boundaries of base\nmodels. RL-PLUS integrates two core components, i.e., Multiple Importance\nSampling to address distributional mismatch from external data, and\nExploration-Based Advantage Function to guide the model towards high-value,\nunexplored reasoning paths. We provide both theoretical analysis and extensive\nexperiments to demonstrate the superiority and generalizability of our\napproach. Compared with existing RLVR methods, RL-PLUS achieves 1)\nstate-of-the-art performance on six math reasoning benchmarks; 2) superior\nperformance on six out-of-distribution reasoning tasks; 3) consistent and\nsignificant gains across diverse model families, with average relative\nimprovements up to 69.2\\%. Moreover, the analysis of Pass@k curves indicates\nthat RL-PLUS effectively resolves the capability boundary collapse problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Reward (RLVR) has significantly\nadvanced the complex reasoning abilities of Large Language Models (LLMs).\nHowever, it struggles to break through the inherent capability boundaries of\nthe base LLM, due to its essentially on-policy strategy coupled with LLM's\nimmense action space and sparse reward. Critically, RLVR can lead to the\ncapability boundary collapse, narrowing the LLM's problem-solving scope. To\naddress this problem, we propose RL-PLUS, a novel hybrid-policy optimization\napproach for LLMs that synergizes internal exploitation with external data to\nachieve stronger reasoning capabilities and surpass the boundaries of base\nmodels. RL-PLUS integrates two core components, i.e., Multiple Importance\nSampling to address distributional mismatch from external data, and\nExploration-Based Advantage Function to guide the model towards high-value,\nunexplored reasoning paths. We provide both theoretical analysis and extensive\nexperiments to demonstrate the superiority and generalizability of our\napproach. Compared with existing RLVR methods, RL-PLUS achieves 1)\nstate-of-the-art performance on six math reasoning benchmarks; 2) superior\nperformance on six out-of-distribution reasoning tasks; 3) consistent and\nsignificant gains across diverse model families, with average relative\nimprovements up to 69.2\\%. Moreover, the analysis of Pass@k curves indicates\nthat RL-PLUS effectively resolves the capability boundary collapse problem."
                },
                "authors": [
                    {
                        "name": "Yihong Dong"
                    },
                    {
                        "name": "Xue Jiang"
                    },
                    {
                        "name": "Yongding Tao"
                    },
                    {
                        "name": "Huanyu Liu"
                    },
                    {
                        "name": "Kechi Zhang"
                    },
                    {
                        "name": "Lili Mou"
                    },
                    {
                        "name": "Rongyu Cao"
                    },
                    {
                        "name": "Yingwei Ma"
                    },
                    {
                        "name": "Jue Chen"
                    },
                    {
                        "name": "Binhua Li"
                    },
                    {
                        "name": "Zhi Jin"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Yongbin Li"
                    },
                    {
                        "name": "Ge Li"
                    }
                ],
                "author_detail": {
                    "name": "Ge Li"
                },
                "author": "Ge Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00222v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00222v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04610v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04610v2",
                "updated": "2025-08-07T15:23:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    23,
                    54,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-06T16:29:59Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    29,
                    59,
                    2,
                    218,
                    0
                ],
                "title": "Neuromorphic Cybersecurity with Semi-supervised Lifelong Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuromorphic Cybersecurity with Semi-supervised Lifelong Learning"
                },
                "summary": "Inspired by the brain's hierarchical processing and energy efficiency, this\npaper presents a Spiking Neural Network (SNN) architecture for lifelong Network\nIntrusion Detection System (NIDS). The proposed system first employs an\nefficient static SNN to identify potential intrusions, which then activates an\nadaptive dynamic SNN responsible for classifying the specific attack type.\nMimicking biological adaptation, the dynamic classifier utilizes Grow When\nRequired (GWR)-inspired structural plasticity and a novel Adaptive\nSpike-Timing-Dependent Plasticity (Ad-STDP) learning rule. These bio-plausible\nmechanisms enable the network to learn new threats incrementally while\npreserving existing knowledge. Tested on the UNSW-NB15 benchmark in a continual\nlearning setting, the architecture demonstrates robust adaptation, reduced\ncatastrophic forgetting, and achieves $85.3$\\% overall accuracy. Furthermore,\nsimulations using the Intel Lava framework confirm high operational sparsity,\nhighlighting the potential for low-power deployment on neuromorphic hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inspired by the brain's hierarchical processing and energy efficiency, this\npaper presents a Spiking Neural Network (SNN) architecture for lifelong Network\nIntrusion Detection System (NIDS). The proposed system first employs an\nefficient static SNN to identify potential intrusions, which then activates an\nadaptive dynamic SNN responsible for classifying the specific attack type.\nMimicking biological adaptation, the dynamic classifier utilizes Grow When\nRequired (GWR)-inspired structural plasticity and a novel Adaptive\nSpike-Timing-Dependent Plasticity (Ad-STDP) learning rule. These bio-plausible\nmechanisms enable the network to learn new threats incrementally while\npreserving existing knowledge. Tested on the UNSW-NB15 benchmark in a continual\nlearning setting, the architecture demonstrates robust adaptation, reduced\ncatastrophic forgetting, and achieves $85.3$\\% overall accuracy. Furthermore,\nsimulations using the Intel Lava framework confirm high operational sparsity,\nhighlighting the potential for low-power deployment on neuromorphic hardware."
                },
                "authors": [
                    {
                        "name": "Md Zesun Ahmed Mia"
                    },
                    {
                        "name": "Malyaban Bal"
                    },
                    {
                        "name": "Sen Lu"
                    },
                    {
                        "name": "George M. Nishibuchi"
                    },
                    {
                        "name": "Suhas Chelian"
                    },
                    {
                        "name": "Srini Vasan"
                    },
                    {
                        "name": "Abhronil Sengupta"
                    }
                ],
                "author_detail": {
                    "name": "Abhronil Sengupta"
                },
                "author": "Abhronil Sengupta",
                "arxiv_comment": "Accepted at ACM International Conference on Neuromorphic Systems\n  (ICONS) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04610v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04610v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04604v1",
                "updated": "2025-08-06T16:24:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    24,
                    17,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T16:24:17Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    24,
                    17,
                    2,
                    218,
                    0
                ],
                "title": "TURA: Tool-Augmented Unified Retrieval Agent for AI Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TURA: Tool-Augmented Unified Retrieval Agent for AI Search"
                },
                "summary": "The advent of Large Language Models (LLMs) is transforming search engines\ninto conversational AI search products, primarily using Retrieval-Augmented\nGeneration (RAG) on web corpora. However, this paradigm has significant\nindustrial limitations. Traditional RAG approaches struggle with real-time\nneeds and structured queries that require accessing dynamically generated\ncontent like ticket availability or inventory. Limited to indexing static\npages, search engines cannot perform the interactive queries needed for such\ntime-sensitive data. Academic research has focused on optimizing RAG for static\ncontent, overlooking complex intents and the need for dynamic sources like\ndatabases and real-time APIs. To bridge this gap, we introduce TURA\n(Tool-Augmented Unified Retrieval Agent for AI Search), a novel three-stage\nframework that combines RAG with agentic tool-use to access both static content\nand dynamic, real-time information. TURA has three key components: an\nIntent-Aware Retrieval module to decompose queries and retrieve information\nsources encapsulated as Model Context Protocol (MCP) Servers, a DAG-based Task\nPlanner that models task dependencies as a Directed Acyclic Graph (DAG) for\noptimal parallel execution, and a lightweight Distilled Agent Executor for\nefficient tool calling. TURA is the first architecture to systematically bridge\nthe gap between static RAG and dynamic information sources for a world-class AI\nsearch product. Serving tens of millions of users, it leverages an agentic\nframework to deliver robust, real-time answers while meeting the low-latency\ndemands of a large-scale industrial system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) is transforming search engines\ninto conversational AI search products, primarily using Retrieval-Augmented\nGeneration (RAG) on web corpora. However, this paradigm has significant\nindustrial limitations. Traditional RAG approaches struggle with real-time\nneeds and structured queries that require accessing dynamically generated\ncontent like ticket availability or inventory. Limited to indexing static\npages, search engines cannot perform the interactive queries needed for such\ntime-sensitive data. Academic research has focused on optimizing RAG for static\ncontent, overlooking complex intents and the need for dynamic sources like\ndatabases and real-time APIs. To bridge this gap, we introduce TURA\n(Tool-Augmented Unified Retrieval Agent for AI Search), a novel three-stage\nframework that combines RAG with agentic tool-use to access both static content\nand dynamic, real-time information. TURA has three key components: an\nIntent-Aware Retrieval module to decompose queries and retrieve information\nsources encapsulated as Model Context Protocol (MCP) Servers, a DAG-based Task\nPlanner that models task dependencies as a Directed Acyclic Graph (DAG) for\noptimal parallel execution, and a lightweight Distilled Agent Executor for\nefficient tool calling. TURA is the first architecture to systematically bridge\nthe gap between static RAG and dynamic information sources for a world-class AI\nsearch product. Serving tens of millions of users, it leverages an agentic\nframework to deliver robust, real-time answers while meeting the low-latency\ndemands of a large-scale industrial system."
                },
                "authors": [
                    {
                        "name": "Zhejun Zhao"
                    },
                    {
                        "name": "Yuehu Dong"
                    },
                    {
                        "name": "Alley Liu"
                    },
                    {
                        "name": "Lixue Zheng"
                    },
                    {
                        "name": "Pingsheng Liu"
                    },
                    {
                        "name": "Dongdong Shen"
                    },
                    {
                        "name": "Long Xia"
                    },
                    {
                        "name": "Jiashu Zhao"
                    },
                    {
                        "name": "Dawei Yin"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Yin"
                },
                "author": "Dawei Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15877v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15877v3",
                "updated": "2025-08-07T01:02:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    1,
                    2,
                    42,
                    3,
                    219,
                    0
                ],
                "published": "2024-05-24T18:40:20Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    18,
                    40,
                    20,
                    4,
                    145,
                    0
                ],
                "title": "Basis Selection: Low-Rank Decomposition of Pretrained Large Language\n  Models for Target Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Basis Selection: Low-Rank Decomposition of Pretrained Large Language\n  Models for Target Applications"
                },
                "summary": "Large language models (LLMs) significantly enhance the performance of various\napplications, but they are computationally intensive and energy-demanding. This\nmakes it challenging to deploy them on devices with limited resources, such as\npersonal computers and mobile/wearable devices, and results in substantial\ninference costs in resource-rich environments like cloud servers. To extend the\nuse of LLMs, we introduce a low-rank decomposition approach to effectively\ncompress these models, tailored to the requirements of specific applications.\nWe observe that LLMs pretrained on general datasets contain many redundant\ncomponents not needed for particular applications. Our method focuses on\nidentifying and removing these redundant parts, retaining only the necessary\nelements for the target applications. Specifically, we represent the weight\nmatrices of LLMs as a linear combination of base components. We then prune the\nirrelevant bases and enhance the model with new bases beneficial for specific\napplications. Deep compression results on the Llama 2-7b and -13B models,\nconducted on target applications including mathematical reasoning and code\ngeneration, show that our method significantly reduces model size while\nmaintaining comparable accuracy to state-of-the-art low-rank compression\ntechniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) significantly enhance the performance of various\napplications, but they are computationally intensive and energy-demanding. This\nmakes it challenging to deploy them on devices with limited resources, such as\npersonal computers and mobile/wearable devices, and results in substantial\ninference costs in resource-rich environments like cloud servers. To extend the\nuse of LLMs, we introduce a low-rank decomposition approach to effectively\ncompress these models, tailored to the requirements of specific applications.\nWe observe that LLMs pretrained on general datasets contain many redundant\ncomponents not needed for particular applications. Our method focuses on\nidentifying and removing these redundant parts, retaining only the necessary\nelements for the target applications. Specifically, we represent the weight\nmatrices of LLMs as a linear combination of base components. We then prune the\nirrelevant bases and enhance the model with new bases beneficial for specific\napplications. Deep compression results on the Llama 2-7b and -13B models,\nconducted on target applications including mathematical reasoning and code\ngeneration, show that our method significantly reduces model size while\nmaintaining comparable accuracy to state-of-the-art low-rank compression\ntechniques."
                },
                "authors": [
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Daniel Agyei Asante"
                    },
                    {
                        "name": "Changsheng Zhao"
                    },
                    {
                        "name": "Ernie Chang"
                    },
                    {
                        "name": "Yangyang Shi"
                    },
                    {
                        "name": "Vikas Chandra"
                    }
                ],
                "author_detail": {
                    "name": "Vikas Chandra"
                },
                "author": "Vikas Chandra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15877v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15877v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04597v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04597v1",
                "updated": "2025-08-06T16:16:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    16,
                    58,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T16:16:58Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    16,
                    58,
                    2,
                    218,
                    0
                ],
                "title": "Pseudo Depth Meets Gaussian: A Feed-forward RGB SLAM Baseline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pseudo Depth Meets Gaussian: A Feed-forward RGB SLAM Baseline"
                },
                "summary": "Incrementally recovering real-sized 3D geometry from a pose-free RGB stream\nis a challenging task in 3D reconstruction, requiring minimal assumptions on\ninput data. Existing methods can be broadly categorized into end-to-end and\nvisual SLAM-based approaches, both of which either struggle with long sequences\nor depend on slow test-time optimization and depth sensors. To address this, we\nfirst integrate a depth estimator into an RGB-D SLAM system, but this approach\nis hindered by inaccurate geometric details in predicted depth. Through further\ninvestigation, we find that 3D Gaussian mapping can effectively solve this\nproblem. Building on this, we propose an online 3D reconstruction method using\n3D Gaussian-based SLAM, combined with a feed-forward recurrent prediction\nmodule to directly infer camera pose from optical flow. This approach replaces\nslow test-time optimization with fast network inference, significantly\nimproving tracking speed. Additionally, we introduce a local graph rendering\ntechnique to enhance robustness in feed-forward pose prediction. Experimental\nresults on the Replica and TUM-RGBD datasets, along with a real-world\ndeployment demonstration, show that our method achieves performance on par with\nthe state-of-the-art SplaTAM, while reducing tracking time by more than 90\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incrementally recovering real-sized 3D geometry from a pose-free RGB stream\nis a challenging task in 3D reconstruction, requiring minimal assumptions on\ninput data. Existing methods can be broadly categorized into end-to-end and\nvisual SLAM-based approaches, both of which either struggle with long sequences\nor depend on slow test-time optimization and depth sensors. To address this, we\nfirst integrate a depth estimator into an RGB-D SLAM system, but this approach\nis hindered by inaccurate geometric details in predicted depth. Through further\ninvestigation, we find that 3D Gaussian mapping can effectively solve this\nproblem. Building on this, we propose an online 3D reconstruction method using\n3D Gaussian-based SLAM, combined with a feed-forward recurrent prediction\nmodule to directly infer camera pose from optical flow. This approach replaces\nslow test-time optimization with fast network inference, significantly\nimproving tracking speed. Additionally, we introduce a local graph rendering\ntechnique to enhance robustness in feed-forward pose prediction. Experimental\nresults on the Replica and TUM-RGBD datasets, along with a real-world\ndeployment demonstration, show that our method achieves performance on par with\nthe state-of-the-art SplaTAM, while reducing tracking time by more than 90\\%."
                },
                "authors": [
                    {
                        "name": "Linqing Zhao"
                    },
                    {
                        "name": "Xiuwei Xu"
                    },
                    {
                        "name": "Yirui Wang"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Wenzhao Zheng"
                    },
                    {
                        "name": "Yansong Tang"
                    },
                    {
                        "name": "Haibin Yan"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "IROS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04597v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04597v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16781v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16781v2",
                "updated": "2025-08-06T16:07:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    7,
                    6,
                    2,
                    218,
                    0
                ],
                "published": "2025-02-24T02:16:37Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    2,
                    16,
                    37,
                    0,
                    55,
                    0
                ],
                "title": "Evaluating Robustness of LLMs in Question Answering on Multilingual\n  Noisy OCR Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Robustness of LLMs in Question Answering on Multilingual\n  Noisy OCR Data"
                },
                "summary": "Optical Character Recognition (OCR) plays a crucial role in digitizing\nhistorical and multilingual documents, yet OCR errors - imperfect extraction of\ntext, including character insertion, deletion, and substitution can\nsignificantly impact downstream tasks like question-answering (QA). In this\nwork, we conduct a comprehensive analysis of how OCR-induced noise affects the\nperformance of Multilingual QA Systems. To support this analysis, we introduce\na multilingual QA dataset MultiOCR-QA, comprising 50K question-answer pairs\nacross three languages, English, French, and German. The dataset is curated\nfrom OCR-ed historical documents, which include different levels and types of\nOCR noise. We then evaluate how different state-of-the-art Large Language\nmodels (LLMs) perform under different error conditions, focusing on three major\nOCR error types. Our findings show that QA systems are highly prone to\nOCR-induced errors and perform poorly on noisy OCR text. By comparing model\nperformance on clean versus noisy texts, we provide insights into the\nlimitations of current approaches and emphasize the need for more\nnoise-resilient QA systems in historical digitization contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optical Character Recognition (OCR) plays a crucial role in digitizing\nhistorical and multilingual documents, yet OCR errors - imperfect extraction of\ntext, including character insertion, deletion, and substitution can\nsignificantly impact downstream tasks like question-answering (QA). In this\nwork, we conduct a comprehensive analysis of how OCR-induced noise affects the\nperformance of Multilingual QA Systems. To support this analysis, we introduce\na multilingual QA dataset MultiOCR-QA, comprising 50K question-answer pairs\nacross three languages, English, French, and German. The dataset is curated\nfrom OCR-ed historical documents, which include different levels and types of\nOCR noise. We then evaluate how different state-of-the-art Large Language\nmodels (LLMs) perform under different error conditions, focusing on three major\nOCR error types. Our findings show that QA systems are highly prone to\nOCR-induced errors and perform poorly on noisy OCR text. By comparing model\nperformance on clean versus noisy texts, we provide insights into the\nlimitations of current approaches and emphasize the need for more\nnoise-resilient QA systems in historical digitization contexts."
                },
                "authors": [
                    {
                        "name": "Bhawna Piryani"
                    },
                    {
                        "name": "Jamshid Mozafari"
                    },
                    {
                        "name": "Abdelrahman Abdallah"
                    },
                    {
                        "name": "Antoine Doucet"
                    },
                    {
                        "name": "Adam Jatowt"
                    }
                ],
                "author_detail": {
                    "name": "Adam Jatowt"
                },
                "author": "Adam Jatowt",
                "arxiv_comment": "Accepted at CIKM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16781v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16781v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17937v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17937v2",
                "updated": "2025-08-06T16:06:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    6,
                    47,
                    2,
                    218,
                    0
                ],
                "published": "2025-07-23T21:11:47Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    21,
                    11,
                    47,
                    2,
                    204,
                    0
                ],
                "title": "Bob's Confetti: Phonetic Memorization Attacks in Music and Video\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bob's Confetti: Phonetic Memorization Attacks in Music and Video\n  Generation"
                },
                "summary": "Memorization in generative models extends far beyond verbatim text\nreproduction--it manifests through non-literal patterns, semantic associations,\nand surprisingly, across modalities in transcript-conditioned generation tasks\nsuch as Lyrics-to-Song (L2S) and Text-to-Video (T2V) models. We reveal a new\nclass of cross-modality memorization where models trained on these tasks leak\ncopyrighted content through indirect, phonetic pathways invisible to\ntraditional text-based analysis. In this work, we introduce Adversarial\nPhoneTic Prompting (APT), an attack that replaces iconic phrases with\nhomophonic alternatives--e.g., \"mom's spaghetti\" becomes \"Bob's\nconfetti\"--preserving the acoustic form while largely changing semantic\ncontent. We demonstrate that models can be prompted to regurgitate memorized\nsongs using phonetically similar but semantically unrelated lyrics. Despite the\nsemantic drift, black-box models like SUNO and open-source models like YuE\ngenerate outputs that are strikingly similar to the original\nsongs--melodically, rhythmically, and vocally--achieving high scores on\nAudioJudge, CLAP, and CoverID. These effects persist across genres and\nlanguages. More surprisingly, we find that phonetic prompts alone can trigger\nvisual memorization in text-to-video models: when given altered lyrics from\nLose Yourself, Veo 3 generates scenes that mirror the original music\nvideo--complete with a hooded rapper and dim urban settings--despite no\nexplicit visual cues in the prompt. This cross-modality leakage represents an\nunprecedented threat: models memorize deep, structural patterns that transcend\ntheir training modality, making traditional safety measures like copyright\nfilters ineffective. Our findings reveal a fundamental vulnerability in\ntranscript-conditioned generative models and raise urgent concerns around\ncopyright, provenance, and secure deployment of multimodal generation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memorization in generative models extends far beyond verbatim text\nreproduction--it manifests through non-literal patterns, semantic associations,\nand surprisingly, across modalities in transcript-conditioned generation tasks\nsuch as Lyrics-to-Song (L2S) and Text-to-Video (T2V) models. We reveal a new\nclass of cross-modality memorization where models trained on these tasks leak\ncopyrighted content through indirect, phonetic pathways invisible to\ntraditional text-based analysis. In this work, we introduce Adversarial\nPhoneTic Prompting (APT), an attack that replaces iconic phrases with\nhomophonic alternatives--e.g., \"mom's spaghetti\" becomes \"Bob's\nconfetti\"--preserving the acoustic form while largely changing semantic\ncontent. We demonstrate that models can be prompted to regurgitate memorized\nsongs using phonetically similar but semantically unrelated lyrics. Despite the\nsemantic drift, black-box models like SUNO and open-source models like YuE\ngenerate outputs that are strikingly similar to the original\nsongs--melodically, rhythmically, and vocally--achieving high scores on\nAudioJudge, CLAP, and CoverID. These effects persist across genres and\nlanguages. More surprisingly, we find that phonetic prompts alone can trigger\nvisual memorization in text-to-video models: when given altered lyrics from\nLose Yourself, Veo 3 generates scenes that mirror the original music\nvideo--complete with a hooded rapper and dim urban settings--despite no\nexplicit visual cues in the prompt. This cross-modality leakage represents an\nunprecedented threat: models memorize deep, structural patterns that transcend\ntheir training modality, making traditional safety measures like copyright\nfilters ineffective. Our findings reveal a fundamental vulnerability in\ntranscript-conditioned generative models and raise urgent concerns around\ncopyright, provenance, and secure deployment of multimodal generation systems."
                },
                "authors": [
                    {
                        "name": "Jaechul Roh"
                    },
                    {
                        "name": "Zachary Novack"
                    },
                    {
                        "name": "Yuefeng Peng"
                    },
                    {
                        "name": "Niloofar Mireshghallah"
                    },
                    {
                        "name": "Taylor Berg-Kirkpatrick"
                    },
                    {
                        "name": "Amir Houmansadr"
                    }
                ],
                "author_detail": {
                    "name": "Amir Houmansadr"
                },
                "author": "Amir Houmansadr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17937v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17937v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04581v1",
                "updated": "2025-08-06T16:06:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    6,
                    43,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T16:06:43Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    6,
                    43,
                    2,
                    218,
                    0
                ],
                "title": "Share Your Attention: Transformer Weight Sharing via Matrix-based\n  Dictionary Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Share Your Attention: Transformer Weight Sharing via Matrix-based\n  Dictionary Learning"
                },
                "summary": "Large language models (LLMs) have revolutionized AI applications, yet their\nhigh computational and memory demands hinder their widespread deployment.\nExisting compression techniques focus on intra-block optimizations (e.g.\nlow-rank approximation, attention head pruning), while the repetitive layered\nstructure of transformers implies significant inter-block redundancy - a\ndimension largely unexplored beyond key-value (KV) caching. Inspired by\ndictionary learning in CNNs, we propose a framework for structured weight\nsharing across transformer layers. Our approach decomposes attention projection\nmatrices into shared dictionary atoms, reducing the attention module's\nparameters by 66.7% while achieving on-par performance. Unlike complex methods\nrequiring distillation or architectural changes, MASA (Matrix Atom Sharing in\nAttention) operates as a drop-in replacement - trained with standard optimizers\n- and represents each layer's weights as linear combinations of shared matrix\natoms. Experiments across scales (100M-700M parameters) show that MASA achieves\nbetter benchmark accuracy and perplexity than grouped-query attention (GQA),\nlow-rank baselines and recently proposed Repeat-all-over/Sequential sharing at\ncomparable parameter budgets. Ablation studies confirm robustness to the\ndictionary size and the efficacy of shared representations in capturing\ncross-layer statistical regularities. Extending to Vision Transformers (ViT),\nMASA matches performance metrics on image classification and detection tasks\nwith 66.7% fewer attention parameters. By combining dictionary learning\nstrategies with transformer efficiency, MASA offers a scalable blueprint for\nparameter-efficient models without sacrificing performance. Finally, we\ninvestigate the possibility of employing MASA on pretrained LLMs to reduce\ntheir number of parameters without experiencing any significant drop in their\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized AI applications, yet their\nhigh computational and memory demands hinder their widespread deployment.\nExisting compression techniques focus on intra-block optimizations (e.g.\nlow-rank approximation, attention head pruning), while the repetitive layered\nstructure of transformers implies significant inter-block redundancy - a\ndimension largely unexplored beyond key-value (KV) caching. Inspired by\ndictionary learning in CNNs, we propose a framework for structured weight\nsharing across transformer layers. Our approach decomposes attention projection\nmatrices into shared dictionary atoms, reducing the attention module's\nparameters by 66.7% while achieving on-par performance. Unlike complex methods\nrequiring distillation or architectural changes, MASA (Matrix Atom Sharing in\nAttention) operates as a drop-in replacement - trained with standard optimizers\n- and represents each layer's weights as linear combinations of shared matrix\natoms. Experiments across scales (100M-700M parameters) show that MASA achieves\nbetter benchmark accuracy and perplexity than grouped-query attention (GQA),\nlow-rank baselines and recently proposed Repeat-all-over/Sequential sharing at\ncomparable parameter budgets. Ablation studies confirm robustness to the\ndictionary size and the efficacy of shared representations in capturing\ncross-layer statistical regularities. Extending to Vision Transformers (ViT),\nMASA matches performance metrics on image classification and detection tasks\nwith 66.7% fewer attention parameters. By combining dictionary learning\nstrategies with transformer efficiency, MASA offers a scalable blueprint for\nparameter-efficient models without sacrificing performance. Finally, we\ninvestigate the possibility of employing MASA on pretrained LLMs to reduce\ntheir number of parameters without experiencing any significant drop in their\nperformance."
                },
                "authors": [
                    {
                        "name": "Magauiya Zhussip"
                    },
                    {
                        "name": "Dmitriy Shopkhoev"
                    },
                    {
                        "name": "Ammar Ali"
                    },
                    {
                        "name": "Stamatios Lefkimmiatis"
                    }
                ],
                "author_detail": {
                    "name": "Stamatios Lefkimmiatis"
                },
                "author": "Stamatios Lefkimmiatis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04563v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04563v1",
                "updated": "2025-08-06T15:49:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    15,
                    49,
                    26,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T15:49:26Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    15,
                    49,
                    26,
                    2,
                    218,
                    0
                ],
                "title": "SID: Benchmarking Guided Instruction Capabilities in STEM Education with\n  a Socratic Interdisciplinary Dialogues Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SID: Benchmarking Guided Instruction Capabilities in STEM Education with\n  a Socratic Interdisciplinary Dialogues Dataset"
                },
                "summary": "Fostering students' abilities for knowledge integration and transfer in\ncomplex problem-solving scenarios is a core objective of modern education, and\ninterdisciplinary STEM is a key pathway to achieve this, yet it requires expert\nguidance that is difficult to scale. While LLMs offer potential in this regard,\ntheir true capability for guided instruction remains unclear due to the lack of\nan effective evaluation benchmark. To address this, we introduce SID, the first\nbenchmark designed to systematically evaluate the higher-order guidance\ncapabilities of LLMs in multi-turn, interdisciplinary Socratic dialogues. Our\ncontributions include a large-scale dataset of 10,000 dialogue turns across 48\ncomplex STEM projects, a novel annotation schema for capturing deep pedagogical\nfeatures, and a new suite of evaluation metrics (e.g., X-SRG). Baseline\nexperiments confirm that even state-of-the-art LLMs struggle to execute\neffective guided dialogues that lead students to achieve knowledge integration\nand transfer. This highlights the critical value of our benchmark in driving\nthe development of more pedagogically-aware LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fostering students' abilities for knowledge integration and transfer in\ncomplex problem-solving scenarios is a core objective of modern education, and\ninterdisciplinary STEM is a key pathway to achieve this, yet it requires expert\nguidance that is difficult to scale. While LLMs offer potential in this regard,\ntheir true capability for guided instruction remains unclear due to the lack of\nan effective evaluation benchmark. To address this, we introduce SID, the first\nbenchmark designed to systematically evaluate the higher-order guidance\ncapabilities of LLMs in multi-turn, interdisciplinary Socratic dialogues. Our\ncontributions include a large-scale dataset of 10,000 dialogue turns across 48\ncomplex STEM projects, a novel annotation schema for capturing deep pedagogical\nfeatures, and a new suite of evaluation metrics (e.g., X-SRG). Baseline\nexperiments confirm that even state-of-the-art LLMs struggle to execute\neffective guided dialogues that lead students to achieve knowledge integration\nand transfer. This highlights the critical value of our benchmark in driving\nthe development of more pedagogically-aware LLMs."
                },
                "authors": [
                    {
                        "name": "Mei Jiang"
                    },
                    {
                        "name": "Houping Yue"
                    },
                    {
                        "name": "Bingdong Li"
                    },
                    {
                        "name": "Hao Hao"
                    },
                    {
                        "name": "Ying Qian"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Aimin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Aimin Zhou"
                },
                "author": "Aimin Zhou",
                "arxiv_comment": "26 pages, 20 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04563v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04563v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10639v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10639v2",
                "updated": "2025-08-06T15:46:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    15,
                    46,
                    37,
                    2,
                    218,
                    0
                ],
                "published": "2024-10-14T15:50:35Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    50,
                    35,
                    0,
                    288,
                    0
                ],
                "title": "Paragon: Parameter Generation for Controllable Multi-Task Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Paragon: Parameter Generation for Controllable Multi-Task Recommendation"
                },
                "summary": "Commercial recommender systems face the challenge that task requirements from\nplatforms or users often change dynamically (e.g., varying preferences for\naccuracy or diversity). Ideally, the model should be re-trained after resetting\na new objective function, adapting to these changes in task requirements.\nHowever, in practice, the high computational costs associated with retraining\nmake this process impractical for models already deployed to online\nenvironments. This raises a new challenging problem: how to efficiently adapt\nthe learned model to different task requirements by controlling the model\nparameters after deployment, without the need for retraining. To address this\nissue, we propose a novel controllable learning approach via \\textbf{para}meter\n\\textbf{g}eneration for c\\textbf{on}trollable multi-task recommendation\n(\\textbf{Paragon}), which allows the customization and adaptation of\nrecommendation model parameters to new task requirements without retraining.\nSpecifically, we first obtain the optimized model parameters through adapter\ntunning based on the feasible task requirements. Then, we utilize the\ngenerative model as a parameter generator, employing classifier-free guidance\nin conditional training to learn the distribution of optimized model parameters\nunder various task requirements. Finally, the parameter generator is applied to\neffectively generate model parameters in a test-time adaptation manner given\ntask requirements. Moreover, Paragon seamlessly integrates with various\nexisting recommendation models to enhance their controllability. Extensive\nexperiments on two public datasets and one commercial dataset demonstrate that\nParagon can efficiently generate model parameters instead of retraining,\nreducing computational time by at least 94.6\\%. The code is released at\n\\href{https://github.com/bubble65/Paragon}{https://github.com/bubble65/Paragon}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commercial recommender systems face the challenge that task requirements from\nplatforms or users often change dynamically (e.g., varying preferences for\naccuracy or diversity). Ideally, the model should be re-trained after resetting\na new objective function, adapting to these changes in task requirements.\nHowever, in practice, the high computational costs associated with retraining\nmake this process impractical for models already deployed to online\nenvironments. This raises a new challenging problem: how to efficiently adapt\nthe learned model to different task requirements by controlling the model\nparameters after deployment, without the need for retraining. To address this\nissue, we propose a novel controllable learning approach via \\textbf{para}meter\n\\textbf{g}eneration for c\\textbf{on}trollable multi-task recommendation\n(\\textbf{Paragon}), which allows the customization and adaptation of\nrecommendation model parameters to new task requirements without retraining.\nSpecifically, we first obtain the optimized model parameters through adapter\ntunning based on the feasible task requirements. Then, we utilize the\ngenerative model as a parameter generator, employing classifier-free guidance\nin conditional training to learn the distribution of optimized model parameters\nunder various task requirements. Finally, the parameter generator is applied to\neffectively generate model parameters in a test-time adaptation manner given\ntask requirements. Moreover, Paragon seamlessly integrates with various\nexisting recommendation models to enhance their controllability. Extensive\nexperiments on two public datasets and one commercial dataset demonstrate that\nParagon can efficiently generate model parameters instead of retraining,\nreducing computational time by at least 94.6\\%. The code is released at\n\\href{https://github.com/bubble65/Paragon}{https://github.com/bubble65/Paragon}."
                },
                "authors": [
                    {
                        "name": "Chenglei Shen"
                    },
                    {
                        "name": "Jiahao Zhao"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Weijie Yu"
                    },
                    {
                        "name": "Ming He"
                    },
                    {
                        "name": "Jianping Fan"
                    }
                ],
                "author_detail": {
                    "name": "Jianping Fan"
                },
                "author": "Jianping Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10639v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10639v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15434v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15434v3",
                "updated": "2025-08-06T15:35:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    15,
                    35,
                    58,
                    2,
                    218,
                    0
                ],
                "published": "2025-02-21T13:01:26Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    1,
                    26,
                    4,
                    52,
                    0
                ],
                "title": "Mixup Model Merge: Enhancing Model Merging Performance through\n  Randomized Linear Interpolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixup Model Merge: Enhancing Model Merging Performance through\n  Randomized Linear Interpolation"
                },
                "summary": "Model merging aims to integrate multiple task-specific models into a unified\nmodel that inherits the capabilities of the task-specific models, without\nadditional training. Existing model merging methods often lack consideration of\nthe varying contribution ratios of different task-specific models to the final\nmerged model. In this paper, we propose Mixup Model Merge (M3), a simple yet\neffective method inspired by the randomized linear interpolation strategy from\nthe Mixup data augmentation technique. M3 performs randomized linear\ninterpolation in parameter space between two task-specific LLMs, where\ninterpolation coefficients are sampled from a Beta distribution to explore\ndiverse contribution ratios. This controllable randomness allows M3 to\noutperform standard equal-ratio merging by discovering better contribution\nratio combinations. Extensive experiments show that M3 significantly (1)\nimproves merged LLM performance across tasks, (2) enhances out-of-distribution\nand adversarial robustness, (3) outperforms the positive effects of the\nsparsification method DARE on model merging and can be further combined with\nDARE to achieve superior results, and (4) balances exploration efficiency and\ndiversity in contribution ratios by tuning the Beta distribution's shape\nparameters. The code is provided in the supplementary materials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model merging aims to integrate multiple task-specific models into a unified\nmodel that inherits the capabilities of the task-specific models, without\nadditional training. Existing model merging methods often lack consideration of\nthe varying contribution ratios of different task-specific models to the final\nmerged model. In this paper, we propose Mixup Model Merge (M3), a simple yet\neffective method inspired by the randomized linear interpolation strategy from\nthe Mixup data augmentation technique. M3 performs randomized linear\ninterpolation in parameter space between two task-specific LLMs, where\ninterpolation coefficients are sampled from a Beta distribution to explore\ndiverse contribution ratios. This controllable randomness allows M3 to\noutperform standard equal-ratio merging by discovering better contribution\nratio combinations. Extensive experiments show that M3 significantly (1)\nimproves merged LLM performance across tasks, (2) enhances out-of-distribution\nand adversarial robustness, (3) outperforms the positive effects of the\nsparsification method DARE on model merging and can be further combined with\nDARE to achieve superior results, and (4) balances exploration efficiency and\ndiversity in contribution ratios by tuning the Beta distribution's shape\nparameters. The code is provided in the supplementary materials."
                },
                "authors": [
                    {
                        "name": "Yue Zhou"
                    },
                    {
                        "name": "Yi Chang"
                    },
                    {
                        "name": "Yuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Wu"
                },
                "author": "Yuan Wu",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15434v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15434v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14194v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14194v4",
                "updated": "2025-08-06T15:34:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    15,
                    34,
                    10,
                    2,
                    218,
                    0
                ],
                "published": "2025-04-19T06:12:33Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    6,
                    12,
                    33,
                    5,
                    109,
                    0
                ],
                "title": "Meta-rater: A Multi-dimensional Data Selection Method for Pre-training\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meta-rater: A Multi-dimensional Data Selection Method for Pre-training\n  Language Models"
                },
                "summary": "The composition of pre-training datasets for large language models (LLMs)\nremains largely undisclosed, hindering transparency and efforts to optimize\ndata quality, a critical driver of model performance. Current data selection\nmethods, such as natural language quality assessments, diversity-based filters,\nand classifier-based approaches, are limited by single-dimensional evaluation\nor redundancy-focused strategies. To address these gaps, we propose four\ndimensions to evaluate data quality: professionalism, readability, reasoning,\nand cleanliness. We further introduce Meta-rater,a multi-dimensional data\nselection method that integrates these dimensions with existing quality metrics\nthrough learned optimal weightings. Meta-rater employs proxy models to train a\nregression model that predicts validation loss, enabling the identification of\noptimal combinations of quality scores. Experiments demonstrate that Meta-rater\ndoubles convergence speed for 1.3B parameter models and improves downstream\ntask performance by 3.23, with advantages that scale to models as large as 7.2B\nparameters. Our work establishes that holistic, multi-dimensional quality\nintegration significantly outperforms conventional single-dimension approaches,\noffering a scalable paradigm for enhancing pre-training efficiency and model\ncapability. To advance future research, we release scripts, data, and models at\nhttps://github.com/opendatalab/Meta-rater.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The composition of pre-training datasets for large language models (LLMs)\nremains largely undisclosed, hindering transparency and efforts to optimize\ndata quality, a critical driver of model performance. Current data selection\nmethods, such as natural language quality assessments, diversity-based filters,\nand classifier-based approaches, are limited by single-dimensional evaluation\nor redundancy-focused strategies. To address these gaps, we propose four\ndimensions to evaluate data quality: professionalism, readability, reasoning,\nand cleanliness. We further introduce Meta-rater,a multi-dimensional data\nselection method that integrates these dimensions with existing quality metrics\nthrough learned optimal weightings. Meta-rater employs proxy models to train a\nregression model that predicts validation loss, enabling the identification of\noptimal combinations of quality scores. Experiments demonstrate that Meta-rater\ndoubles convergence speed for 1.3B parameter models and improves downstream\ntask performance by 3.23, with advantages that scale to models as large as 7.2B\nparameters. Our work establishes that holistic, multi-dimensional quality\nintegration significantly outperforms conventional single-dimension approaches,\noffering a scalable paradigm for enhancing pre-training efficiency and model\ncapability. To advance future research, we release scripts, data, and models at\nhttps://github.com/opendatalab/Meta-rater."
                },
                "authors": [
                    {
                        "name": "Xinlin Zhuang"
                    },
                    {
                        "name": "Jiahui Peng"
                    },
                    {
                        "name": "Ren Ma"
                    },
                    {
                        "name": "Yinfan Wang"
                    },
                    {
                        "name": "Tianyi Bai"
                    },
                    {
                        "name": "Xingjian Wei"
                    },
                    {
                        "name": "Jiantao Qiu"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Ying Qian"
                    },
                    {
                        "name": "Conghui He"
                    }
                ],
                "author_detail": {
                    "name": "Conghui He"
                },
                "author": "Conghui He",
                "arxiv_comment": "ACL 2025 Best Theme Paper Award",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14194v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14194v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00010v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00010v2",
                "updated": "2025-08-06T15:29:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    15,
                    29,
                    13,
                    2,
                    218,
                    0
                ],
                "published": "2025-07-21T06:57:06Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    6,
                    57,
                    6,
                    0,
                    202,
                    0
                ],
                "title": "Non-Terrestrial Network Models Using Stochastic Geometry: Planar or\n  Spherical?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-Terrestrial Network Models Using Stochastic Geometry: Planar or\n  Spherical?"
                },
                "summary": "With the explosive deployment of non-terrestrial networks (NTNs), the\ncomputational complexity of network performance analysis is rapidly escalating.\nAs one of the most suitable mathematical tools for analyzing large-scale\nnetwork topologies, stochastic geometry (SG) enables the representation of\nnetwork performance metrics as functions of network parameters, thus offering\nlow-complexity performance analysis solutions. However, choosing between planar\nand spherical models remains challenging. Planar models neglect Earth's\ncurvature, causing deviations in high-altitude NTN analysis, yet are still\noften used for simplicity. This paper introduces relative error to quantify the\ngap between planar and spherical models, helping determine when planar modeling\nis sufficient. To calculate the relative error, we first propose a point\nprocess (PP) generation algorithm that simultaneously generates a pair of\nhomogeneous and asymptotically similar planar and spherical PPs. We then\nintroduce several typical similarity metrics, including topology-related and\nnetwork-level metrics, and further develop a relative error estimation\nalgorithm based on these metrics. In addition, we derive an analytical\nexpression for the optimal planar altitude, which reduces computational\ncomplexity and provides theoretical support for planar approximation. Finally,\nnumerical results investigate how deployment altitude and region affect NTN\nmodeling, with case studies on HAP and LEO satellite constellations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the explosive deployment of non-terrestrial networks (NTNs), the\ncomputational complexity of network performance analysis is rapidly escalating.\nAs one of the most suitable mathematical tools for analyzing large-scale\nnetwork topologies, stochastic geometry (SG) enables the representation of\nnetwork performance metrics as functions of network parameters, thus offering\nlow-complexity performance analysis solutions. However, choosing between planar\nand spherical models remains challenging. Planar models neglect Earth's\ncurvature, causing deviations in high-altitude NTN analysis, yet are still\noften used for simplicity. This paper introduces relative error to quantify the\ngap between planar and spherical models, helping determine when planar modeling\nis sufficient. To calculate the relative error, we first propose a point\nprocess (PP) generation algorithm that simultaneously generates a pair of\nhomogeneous and asymptotically similar planar and spherical PPs. We then\nintroduce several typical similarity metrics, including topology-related and\nnetwork-level metrics, and further develop a relative error estimation\nalgorithm based on these metrics. In addition, we derive an analytical\nexpression for the optimal planar altitude, which reduces computational\ncomplexity and provides theoretical support for planar approximation. Finally,\nnumerical results investigate how deployment altitude and region affect NTN\nmodeling, with case studies on HAP and LEO satellite constellations."
                },
                "authors": [
                    {
                        "name": "Ruibo Wang"
                    },
                    {
                        "name": "Baha Eddine Youcef Belmekki"
                    },
                    {
                        "name": "Howard H. Yang"
                    },
                    {
                        "name": "Mohamed Slim Alouini"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Slim Alouini"
                },
                "author": "Mohamed Slim Alouini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00010v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00010v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03174v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03174v2",
                "updated": "2025-08-06T15:28:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    15,
                    28,
                    4,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-05T07:33:48Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    7,
                    33,
                    48,
                    1,
                    217,
                    0
                ],
                "title": "InqEduAgent: Adaptive AI Learning Partners with Gaussian Process\n  Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InqEduAgent: Adaptive AI Learning Partners with Gaussian Process\n  Augmentation"
                },
                "summary": "Collaborative partnership matters in inquiry-oriented education. However,\nmost study partners are selected either rely on experience-based assignments\nwith little scientific planning or build on rule-based machine assistants,\nencountering difficulties in knowledge expansion and inadequate flexibility.\nThis paper proposes an LLM-empowered agent model for simulating and selecting\nlearning partners tailored to inquiry-oriented learning, named InqEduAgent.\nGenerative agents are designed to capture cognitive and evaluative features of\nlearners in real-world scenarios. Then, an adaptive matching algorithm with\nGaussian process augmentation is formulated to identify patterns within prior\nknowledge. Optimal learning-partner matches are provided for learners facing\ndifferent exercises. The experimental results show the optimal performance of\nInqEduAgent in most knowledge-learning scenarios and LLM environment with\ndifferent levels of capabilities. This study promotes the intelligent\nallocation of human-based learning partners and the formulation of AI-based\nlearning partners. The code, data, and appendix are publicly available at\nhttps://github.com/InqEduAgent/InqEduAgent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative partnership matters in inquiry-oriented education. However,\nmost study partners are selected either rely on experience-based assignments\nwith little scientific planning or build on rule-based machine assistants,\nencountering difficulties in knowledge expansion and inadequate flexibility.\nThis paper proposes an LLM-empowered agent model for simulating and selecting\nlearning partners tailored to inquiry-oriented learning, named InqEduAgent.\nGenerative agents are designed to capture cognitive and evaluative features of\nlearners in real-world scenarios. Then, an adaptive matching algorithm with\nGaussian process augmentation is formulated to identify patterns within prior\nknowledge. Optimal learning-partner matches are provided for learners facing\ndifferent exercises. The experimental results show the optimal performance of\nInqEduAgent in most knowledge-learning scenarios and LLM environment with\ndifferent levels of capabilities. This study promotes the intelligent\nallocation of human-based learning partners and the formulation of AI-based\nlearning partners. The code, data, and appendix are publicly available at\nhttps://github.com/InqEduAgent/InqEduAgent."
                },
                "authors": [
                    {
                        "name": "Tian-Fang Zhao"
                    },
                    {
                        "name": "Wen-Xi Yang"
                    },
                    {
                        "name": "Guan Liu"
                    },
                    {
                        "name": "Liang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Yang"
                },
                "author": "Liang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03174v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03174v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06850v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06850v4",
                "updated": "2025-08-06T15:27:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    15,
                    27,
                    3,
                    2,
                    218,
                    0
                ],
                "published": "2025-07-09T13:54:58Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    13,
                    54,
                    58,
                    2,
                    190,
                    0
                ],
                "title": "The Dark Side of LLMs: Agent-based Attacks for Complete Computer\n  Takeover",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Dark Side of LLMs: Agent-based Attacks for Complete Computer\n  Takeover"
                },
                "summary": "The rapid adoption of Large Language Model (LLM) agents and multi-agent\nsystems enables remarkable capabilities in natural language processing and\ngeneration. However, these systems introduce unprecedented security\nvulnerabilities that extend beyond traditional content generation attacks to\nsystem-level compromise. This paper presents a comprehensive evaluation of the\nsecurity of LLMs used as reasoning engines within autonomous agents,\nhighlighting how they can be exploited as attack vectors capable of achieving\ncomplete computer takeover. We focus on how different attack surfaces and trust\nboundaries - Direct Prompt Injection, RAG Backdoor, and Inter Agent Trust - can\nbe leveraged to orchestrate such takeovers. We demonstrate that adversaries can\neffectively coerce popular LLMs (including GPT-4, Claude-4 and Gemini-2.5) into\nautonomously installing and executing malware on victim machines. Our\nevaluation of 18 state-of-the-art LLMs reveals an alarming scenario: 94.4% of\nmodels succumb to Direct Prompt Injection and 83.3% are vulnerable to the more\nstealth and evasive RAG Backdoor Attack. Notably, we tested trust boundaries\nwithin multi-agent systems, where LLM agents interact and influence each other,\nand we revealed a critical security flaw: LLMs which successfully resist direct\ninjection or RAG backdoor will execute identical payloads when requested by\npeer agents. Our findings show that 100.0% of tested LLMs can be compromised\nthrough Inter-Agent Trust Exploitation attacks and that every model exhibits\ncontext-dependent security behaviors that create exploitable blind spots. Our\nresults also highlight the need to increase awareness and research on the\nsecurity risks of LLMs, showing a paradigm shift in cybersecurity threats,\nwhere AI tools themselves become sophisticated attack vectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid adoption of Large Language Model (LLM) agents and multi-agent\nsystems enables remarkable capabilities in natural language processing and\ngeneration. However, these systems introduce unprecedented security\nvulnerabilities that extend beyond traditional content generation attacks to\nsystem-level compromise. This paper presents a comprehensive evaluation of the\nsecurity of LLMs used as reasoning engines within autonomous agents,\nhighlighting how they can be exploited as attack vectors capable of achieving\ncomplete computer takeover. We focus on how different attack surfaces and trust\nboundaries - Direct Prompt Injection, RAG Backdoor, and Inter Agent Trust - can\nbe leveraged to orchestrate such takeovers. We demonstrate that adversaries can\neffectively coerce popular LLMs (including GPT-4, Claude-4 and Gemini-2.5) into\nautonomously installing and executing malware on victim machines. Our\nevaluation of 18 state-of-the-art LLMs reveals an alarming scenario: 94.4% of\nmodels succumb to Direct Prompt Injection and 83.3% are vulnerable to the more\nstealth and evasive RAG Backdoor Attack. Notably, we tested trust boundaries\nwithin multi-agent systems, where LLM agents interact and influence each other,\nand we revealed a critical security flaw: LLMs which successfully resist direct\ninjection or RAG backdoor will execute identical payloads when requested by\npeer agents. Our findings show that 100.0% of tested LLMs can be compromised\nthrough Inter-Agent Trust Exploitation attacks and that every model exhibits\ncontext-dependent security behaviors that create exploitable blind spots. Our\nresults also highlight the need to increase awareness and research on the\nsecurity risks of LLMs, showing a paradigm shift in cybersecurity threats,\nwhere AI tools themselves become sophisticated attack vectors."
                },
                "authors": [
                    {
                        "name": "Matteo Lupinacci"
                    },
                    {
                        "name": "Francesco Aurelio Pironti"
                    },
                    {
                        "name": "Francesco Blefari"
                    },
                    {
                        "name": "Francesco Romeo"
                    },
                    {
                        "name": "Luigi Arena"
                    },
                    {
                        "name": "Angelo Furfaro"
                    }
                ],
                "author_detail": {
                    "name": "Angelo Furfaro"
                },
                "author": "Angelo Furfaro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06850v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06850v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11773v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11773v3",
                "updated": "2025-08-06T15:26:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    15,
                    26,
                    58,
                    2,
                    218,
                    0
                ],
                "published": "2025-06-13T13:31:08Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    31,
                    8,
                    4,
                    164,
                    0
                ],
                "title": "AgentSense: Virtual Sensor Data Generation Using LLM Agents in Simulated\n  Home Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentSense: Virtual Sensor Data Generation Using LLM Agents in Simulated\n  Home Environments"
                },
                "summary": "A major challenge in developing robust and generalizable Human Activity\nRecognition (HAR) systems for smart homes is the lack of large and diverse\nlabeled datasets. Variations in home layouts, sensor configurations, and\nindividual behaviors further exacerbate this issue. To address this, we\nleverage the idea of embodied AI agents-virtual agents that perceive and act\nwithin simulated environments guided by internal world models. We introduce\nAgentSense, a virtual data generation pipeline in which agents live out daily\nroutines in simulated smart homes, with behavior guided by Large Language\nModels (LLMs). The LLM generates diverse synthetic personas and realistic\nroutines grounded in the environment, which are then decomposed into\nfine-grained actions. These actions are executed in an extended version of the\nVirtualHome simulator, which we augment with virtual ambient sensors that\nrecord the agents' activities. Our approach produces rich, privacy-preserving\nsensor data that reflects real-world diversity. We evaluate AgentSense on five\nreal HAR datasets. Models pretrained on the generated data consistently\noutperform baselines, especially in low-resource settings. Furthermore,\ncombining the generated virtual sensor data with a small amount of real data\nachieves performance comparable to training on full real-world datasets. These\nresults highlight the potential of using LLM-guided embodied agents for\nscalable and cost-effective sensor data generation in HAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A major challenge in developing robust and generalizable Human Activity\nRecognition (HAR) systems for smart homes is the lack of large and diverse\nlabeled datasets. Variations in home layouts, sensor configurations, and\nindividual behaviors further exacerbate this issue. To address this, we\nleverage the idea of embodied AI agents-virtual agents that perceive and act\nwithin simulated environments guided by internal world models. We introduce\nAgentSense, a virtual data generation pipeline in which agents live out daily\nroutines in simulated smart homes, with behavior guided by Large Language\nModels (LLMs). The LLM generates diverse synthetic personas and realistic\nroutines grounded in the environment, which are then decomposed into\nfine-grained actions. These actions are executed in an extended version of the\nVirtualHome simulator, which we augment with virtual ambient sensors that\nrecord the agents' activities. Our approach produces rich, privacy-preserving\nsensor data that reflects real-world diversity. We evaluate AgentSense on five\nreal HAR datasets. Models pretrained on the generated data consistently\noutperform baselines, especially in low-resource settings. Furthermore,\ncombining the generated virtual sensor data with a small amount of real data\nachieves performance comparable to training on full real-world datasets. These\nresults highlight the potential of using LLM-guided embodied agents for\nscalable and cost-effective sensor data generation in HAR."
                },
                "authors": [
                    {
                        "name": "Zikang Leng"
                    },
                    {
                        "name": "Megha Thukral"
                    },
                    {
                        "name": "Yaqi Liu"
                    },
                    {
                        "name": "Hrudhai Rajasekhar"
                    },
                    {
                        "name": "Shruthi K. Hiremath"
                    },
                    {
                        "name": "Jiaman He"
                    },
                    {
                        "name": "Thomas Pltz"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Pltz"
                },
                "author": "Thomas Pltz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11773v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11773v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04537v1",
                "updated": "2025-08-06T15:23:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    15,
                    23,
                    22,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T15:23:22Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    15,
                    23,
                    22,
                    2,
                    218,
                    0
                ],
                "title": "Behaviorally Adaptive Multi-Robot Hazard Localization in Failure-Prone,\n  Communication-Denied Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Behaviorally Adaptive Multi-Robot Hazard Localization in Failure-Prone,\n  Communication-Denied Environments"
                },
                "summary": "We address the challenge of multi-robot autonomous hazard mapping in\nhigh-risk, failure-prone, communication-denied environments such as\npost-disaster zones, underground mines, caves, and planetary surfaces. In these\nmissions, robots must explore and map hazards while minimizing the risk of\nfailure due to environmental threats or hardware limitations. We introduce a\nbehavior-adaptive, information-theoretic planning framework for multi-robot\nteams grounded in the concept of Behavioral Entropy (BE), that generalizes\nShannon entropy (SE) to capture diverse human-like uncertainty evaluations.\nBuilding on this formulation, we propose the Behavior-Adaptive Path Planning\n(BAPP) framework, which modulates information gathering strategies via a\ntunable risk-sensitivity parameter, and present two planning algorithms:\nBAPP-TID for intelligent triggering of high-fidelity robots, and BAPP-SIG for\nsafe deployment under high risk. We provide theoretical insights on the\ninformativeness of the proposed BAPP framework and validate its effectiveness\nthrough both single-robot and multi-robot simulations. Our results show that\nthe BAPP stack consistently outperforms Shannon-based and random strategies:\nBAPP-TID accelerates entropy reduction, while BAPP-SIG improves robot\nsurvivability with minimal loss in information gain. In multi-agent\ndeployments, BAPP scales effectively through spatial partitioning, mobile base\nrelocation, and role-aware heterogeneity. These findings underscore the value\nof behavior-adaptive planning for robust, risk-sensitive exploration in\ncomplex, failure-prone environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of multi-robot autonomous hazard mapping in\nhigh-risk, failure-prone, communication-denied environments such as\npost-disaster zones, underground mines, caves, and planetary surfaces. In these\nmissions, robots must explore and map hazards while minimizing the risk of\nfailure due to environmental threats or hardware limitations. We introduce a\nbehavior-adaptive, information-theoretic planning framework for multi-robot\nteams grounded in the concept of Behavioral Entropy (BE), that generalizes\nShannon entropy (SE) to capture diverse human-like uncertainty evaluations.\nBuilding on this formulation, we propose the Behavior-Adaptive Path Planning\n(BAPP) framework, which modulates information gathering strategies via a\ntunable risk-sensitivity parameter, and present two planning algorithms:\nBAPP-TID for intelligent triggering of high-fidelity robots, and BAPP-SIG for\nsafe deployment under high risk. We provide theoretical insights on the\ninformativeness of the proposed BAPP framework and validate its effectiveness\nthrough both single-robot and multi-robot simulations. Our results show that\nthe BAPP stack consistently outperforms Shannon-based and random strategies:\nBAPP-TID accelerates entropy reduction, while BAPP-SIG improves robot\nsurvivability with minimal loss in information gain. In multi-agent\ndeployments, BAPP scales effectively through spatial partitioning, mobile base\nrelocation, and role-aware heterogeneity. These findings underscore the value\nof behavior-adaptive planning for robust, risk-sensitive exploration in\ncomplex, failure-prone environments."
                },
                "authors": [
                    {
                        "name": "Alkesh K. Srivastava"
                    },
                    {
                        "name": "Aamodh Suresh"
                    },
                    {
                        "name": "Carlos Nieto-Granda"
                    }
                ],
                "author_detail": {
                    "name": "Carlos Nieto-Granda"
                },
                "author": "Carlos Nieto-Granda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.06083v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.06083v3",
                "updated": "2025-08-06T15:22:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    15,
                    22,
                    29,
                    2,
                    218,
                    0
                ],
                "published": "2024-07-04T09:50:50Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    9,
                    50,
                    50,
                    3,
                    186,
                    0
                ],
                "title": "A Survey of Controllable Learning: Methods and Applications in\n  Information Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Controllable Learning: Methods and Applications in\n  Information Retrieval"
                },
                "summary": "Controllability has become a crucial aspect of trustworthy machine learning,\nenabling learners to meet predefined targets and adapt dynamically at test time\nwithout requiring retraining as the targets shift. We provide a formal\ndefinition of controllable learning (CL), and discuss its applications in\ninformation retrieval (IR) where information needs are often complex and\ndynamic. The survey categorizes CL according to what is controllable (e.g.,\nmultiple objectives, user portrait, scenario adaptation), who controls (users\nor platforms), how control is implemented (e.g., rule-based method, Pareto\noptimization, hypernetwork and others), and where to implement control (e.g.,\npre-processing, in-processing, post-processing methods). Then, we identify\nchallenges faced by CL across training, evaluation, task setting, and\ndeployment in online environments. Additionally, we outline promising\ndirections for CL in theoretical analysis, efficient computation, empowering\nlarge language models, application scenarios and evaluation frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controllability has become a crucial aspect of trustworthy machine learning,\nenabling learners to meet predefined targets and adapt dynamically at test time\nwithout requiring retraining as the targets shift. We provide a formal\ndefinition of controllable learning (CL), and discuss its applications in\ninformation retrieval (IR) where information needs are often complex and\ndynamic. The survey categorizes CL according to what is controllable (e.g.,\nmultiple objectives, user portrait, scenario adaptation), who controls (users\nor platforms), how control is implemented (e.g., rule-based method, Pareto\noptimization, hypernetwork and others), and where to implement control (e.g.,\npre-processing, in-processing, post-processing methods). Then, we identify\nchallenges faced by CL across training, evaluation, task setting, and\ndeployment in online environments. Additionally, we outline promising\ndirections for CL in theoretical analysis, efficient computation, empowering\nlarge language models, application scenarios and evaluation frameworks."
                },
                "authors": [
                    {
                        "name": "Chenglei Shen"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Teng Shi"
                    },
                    {
                        "name": "Changshuo Zhang"
                    },
                    {
                        "name": "Guofu Xie"
                    },
                    {
                        "name": "Jun Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Xu"
                },
                "author": "Jun Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.06083v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.06083v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03440v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03440v3",
                "updated": "2025-08-07T06:38:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    6,
                    38,
                    21,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-05T13:38:33Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    13,
                    38,
                    33,
                    1,
                    217,
                    0
                ],
                "title": "LLMs are Single-threaded Reasoners: Demystifying the Working Mechanism\n  of Soft Thinking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are Single-threaded Reasoners: Demystifying the Working Mechanism\n  of Soft Thinking"
                },
                "summary": "Human cognition naturally engages with abstract and fluid concepts, whereas\nexisting reasoning models often rely on generating discrete tokens, potentially\nconstraining their expressive capabilities. Recent advancements aim to address\nthis limitation by enabling large language models (LLMs) to generate soft,\nabstract tokens, thus facilitating reasoning within a continuous concept space.\nThis paper explores the `Soft Thinking' capabilities of various LLMs by\nexamining the models' internal behavior using a suite of probing techniques.\nContrary to the common belief that Soft Thinking enables the simultaneous\nexploration of diverse reasoning paths, our findings reveal that LLMs\npredominantly rely on the most influential component of the soft inputs during\nsubsequent decoding steps. This reliance hinders the exploration of different\nreasoning paths and reduces vanilla Soft Thinking to a form of greedy decoding,\nobscuring the advantage of transmitting more information through Soft Tokens.\nTo tackle this issue, we explore sampling strategies to introduce\n\\emph{randomness}, employing methods such as Dirichlet resampling and the\nGumbel-Softmax trick. Our experiments demonstrate that incorporating randomness\ncan alleviate the limitations of vanilla approaches and unleash the potential\nof Soft Thinking. Notably, the Gumbel-Softmax trick provides adequate\nrandomness with controlled smoothness, resulting in superior performance across\neight reasoning benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human cognition naturally engages with abstract and fluid concepts, whereas\nexisting reasoning models often rely on generating discrete tokens, potentially\nconstraining their expressive capabilities. Recent advancements aim to address\nthis limitation by enabling large language models (LLMs) to generate soft,\nabstract tokens, thus facilitating reasoning within a continuous concept space.\nThis paper explores the `Soft Thinking' capabilities of various LLMs by\nexamining the models' internal behavior using a suite of probing techniques.\nContrary to the common belief that Soft Thinking enables the simultaneous\nexploration of diverse reasoning paths, our findings reveal that LLMs\npredominantly rely on the most influential component of the soft inputs during\nsubsequent decoding steps. This reliance hinders the exploration of different\nreasoning paths and reduces vanilla Soft Thinking to a form of greedy decoding,\nobscuring the advantage of transmitting more information through Soft Tokens.\nTo tackle this issue, we explore sampling strategies to introduce\n\\emph{randomness}, employing methods such as Dirichlet resampling and the\nGumbel-Softmax trick. Our experiments demonstrate that incorporating randomness\ncan alleviate the limitations of vanilla approaches and unleash the potential\nof Soft Thinking. Notably, the Gumbel-Softmax trick provides adequate\nrandomness with controlled smoothness, resulting in superior performance across\neight reasoning benchmarks."
                },
                "authors": [
                    {
                        "name": "Chnhung Wu"
                    },
                    {
                        "name": "Jinliang Lu"
                    },
                    {
                        "name": "Zixuan Ren"
                    },
                    {
                        "name": "Gangqiang Hu"
                    },
                    {
                        "name": "Zhi Wu"
                    },
                    {
                        "name": "Dai Dai"
                    },
                    {
                        "name": "Hua Wu"
                    }
                ],
                "author_detail": {
                    "name": "Hua Wu"
                },
                "author": "Hua Wu",
                "arxiv_comment": "11 pages, 7 figures, working in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03440v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03440v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04531v1",
                "updated": "2025-08-06T15:13:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    15,
                    13,
                    24,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T15:13:24Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    15,
                    13,
                    24,
                    2,
                    218,
                    0
                ],
                "title": "Unveiling the Landscape of Clinical Depression Assessment: From\n  Behavioral Signatures to Psychiatric Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling the Landscape of Clinical Depression Assessment: From\n  Behavioral Signatures to Psychiatric Reasoning"
                },
                "summary": "Depression is a widespread mental disorder that affects millions worldwide.\nWhile automated depression assessment shows promise, most studies rely on\nlimited or non-clinically validated data, and often prioritize complex model\ndesign over real-world effectiveness. In this paper, we aim to unveil the\nlandscape of clinical depression assessment. We introduce C-MIND, a clinical\nneuropsychiatric multimodal diagnosis dataset collected over two years from\nreal hospital visits. Each participant completes three structured psychiatric\ntasks and receives a final diagnosis from expert clinicians, with informative\naudio, video, transcript, and functional near-infrared spectroscopy (fNIRS)\nsignals recorded. Using C-MIND, we first analyze behavioral signatures relevant\nto diagnosis. We train a range of classical models to quantify how different\ntasks and modalities contribute to diagnostic performance, and dissect the\neffectiveness of their combinations. We then explore whether LLMs can perform\npsychiatric reasoning like clinicians and identify their clear limitations in\nrealistic clinical settings. In response, we propose to guide the reasoning\nprocess with clinical expertise and consistently improves LLM diagnostic\nperformance by up to 10% in Macro-F1 score. We aim to build an infrastructure\nfor clinical depression assessment from both data and algorithmic perspectives,\nenabling C-MIND to facilitate grounded and reliable research for mental\nhealthcare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Depression is a widespread mental disorder that affects millions worldwide.\nWhile automated depression assessment shows promise, most studies rely on\nlimited or non-clinically validated data, and often prioritize complex model\ndesign over real-world effectiveness. In this paper, we aim to unveil the\nlandscape of clinical depression assessment. We introduce C-MIND, a clinical\nneuropsychiatric multimodal diagnosis dataset collected over two years from\nreal hospital visits. Each participant completes three structured psychiatric\ntasks and receives a final diagnosis from expert clinicians, with informative\naudio, video, transcript, and functional near-infrared spectroscopy (fNIRS)\nsignals recorded. Using C-MIND, we first analyze behavioral signatures relevant\nto diagnosis. We train a range of classical models to quantify how different\ntasks and modalities contribute to diagnostic performance, and dissect the\neffectiveness of their combinations. We then explore whether LLMs can perform\npsychiatric reasoning like clinicians and identify their clear limitations in\nrealistic clinical settings. In response, we propose to guide the reasoning\nprocess with clinical expertise and consistently improves LLM diagnostic\nperformance by up to 10% in Macro-F1 score. We aim to build an infrastructure\nfor clinical depression assessment from both data and algorithmic perspectives,\nenabling C-MIND to facilitate grounded and reliable research for mental\nhealthcare."
                },
                "authors": [
                    {
                        "name": "Zhuang Chen"
                    },
                    {
                        "name": "Guanqun Bi"
                    },
                    {
                        "name": "Wen Zhang"
                    },
                    {
                        "name": "Jiawei Hu"
                    },
                    {
                        "name": "Aoyun Wang"
                    },
                    {
                        "name": "Xiyao Xiao"
                    },
                    {
                        "name": "Kun Feng"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04530v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04530v2",
                "updated": "2025-08-07T06:14:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    6,
                    14,
                    50,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-06T15:12:05Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    15,
                    12,
                    5,
                    2,
                    218,
                    0
                ],
                "title": "Balancing Stylization and Truth via Disentangled Representation Steering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Balancing Stylization and Truth via Disentangled Representation Steering"
                },
                "summary": "Generating stylized large language model (LLM) responses via representation\nediting is a promising way for fine-grained output control. However, there\nexists an inherent trade-off: imposing a distinctive style often degrades\ntruthfulness. Existing representation editing methods, by naively injecting\nstyle signals, overlook this collateral impact and frequently contaminate the\nmodel's core truthfulness representations, resulting in reduced answer\ncorrectness. We term this phenomenon stylization-induced truthfulness collapse.\nWe attribute this issue to latent coupling between style and truth directions\nin certain key attention heads, and propose StyliTruth, a mechanism that\npreserves stylization while keeping truthfulness intact. StyliTruth separates\nthe style-relevant and truth-relevant subspaces in the model's representation\nspace via an orthogonal deflation process. This decomposition enables\nindependent control of style and truth in their own subspaces, minimizing\ninterference. By designing adaptive, token-level steering vectors within each\nsubspace, we dynamically and precisely control the generation process to\nmaintain both stylistic fidelity and truthfulness. We validate our method on\nmultiple styles and languages. Extensive experiments and analyses show that\nStyliTruth significantly reduces stylization-induced truthfulness collapse and\noutperforms existing inference-time intervention methods in balancing style\nadherence with truthfulness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating stylized large language model (LLM) responses via representation\nediting is a promising way for fine-grained output control. However, there\nexists an inherent trade-off: imposing a distinctive style often degrades\ntruthfulness. Existing representation editing methods, by naively injecting\nstyle signals, overlook this collateral impact and frequently contaminate the\nmodel's core truthfulness representations, resulting in reduced answer\ncorrectness. We term this phenomenon stylization-induced truthfulness collapse.\nWe attribute this issue to latent coupling between style and truth directions\nin certain key attention heads, and propose StyliTruth, a mechanism that\npreserves stylization while keeping truthfulness intact. StyliTruth separates\nthe style-relevant and truth-relevant subspaces in the model's representation\nspace via an orthogonal deflation process. This decomposition enables\nindependent control of style and truth in their own subspaces, minimizing\ninterference. By designing adaptive, token-level steering vectors within each\nsubspace, we dynamically and precisely control the generation process to\nmaintain both stylistic fidelity and truthfulness. We validate our method on\nmultiple styles and languages. Extensive experiments and analyses show that\nStyliTruth significantly reduces stylization-induced truthfulness collapse and\noutperforms existing inference-time intervention methods in balancing style\nadherence with truthfulness."
                },
                "authors": [
                    {
                        "name": "Chenglei Shen"
                    },
                    {
                        "name": "Zhongxiang Sun"
                    },
                    {
                        "name": "Teng Shi"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Jun Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Xu"
                },
                "author": "Jun Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04530v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04530v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15787v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15787v4",
                "updated": "2025-08-06T15:09:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    15,
                    9,
                    52,
                    2,
                    218,
                    0
                ],
                "published": "2025-06-18T18:10:30Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    18,
                    10,
                    30,
                    2,
                    169,
                    0
                ],
                "title": "SLR: Automated Synthesis for Scalable Logical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLR: Automated Synthesis for Scalable Logical Reasoning"
                },
                "summary": "We introduce SLR, an end-to-end framework for systematic evaluation and\ntraining of Large Language Models (LLMs) via Scalable Logical Reasoning. Given\na user's task specification, SLR automatically synthesizes (i) an instruction\nprompt for an inductive reasoning task, (ii) a validation program, executable\non model outputs to provide verifiable rewards, and (iii) the latent\nground-truth rule. This process is fully automated, scalable, requires no human\nannotations, and offers precise control over task difficulty. Using SLR, we\ncreate SLR-Bench, a benchmark comprising 19k prompts organized into 20\ncurriculum levels that progressively increase in relational, arithmetic, and\nrecursive complexity. Large-scale evaluation reveals that contemporary LLMs\nreadily produce syntactically valid rules, yet often fail at correct logical\ninference. Recent reasoning LLMs demonstrate improved performance but incur\nvery high test-time computation, with costs exceeding $300 for just 1,000\nprompts. Finally, curriculum learning via SLR doubles Llama-3-8B accuracy on\nSLR-Bench, achieving parity with Gemini-Flash-Thinking at a fraction of\ncomputational cost. Moreover, these reasoning capabilities generalize to a wide\nrange of established benchmarks, underscoring the effectiveness of SLR for\ndownstream reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SLR, an end-to-end framework for systematic evaluation and\ntraining of Large Language Models (LLMs) via Scalable Logical Reasoning. Given\na user's task specification, SLR automatically synthesizes (i) an instruction\nprompt for an inductive reasoning task, (ii) a validation program, executable\non model outputs to provide verifiable rewards, and (iii) the latent\nground-truth rule. This process is fully automated, scalable, requires no human\nannotations, and offers precise control over task difficulty. Using SLR, we\ncreate SLR-Bench, a benchmark comprising 19k prompts organized into 20\ncurriculum levels that progressively increase in relational, arithmetic, and\nrecursive complexity. Large-scale evaluation reveals that contemporary LLMs\nreadily produce syntactically valid rules, yet often fail at correct logical\ninference. Recent reasoning LLMs demonstrate improved performance but incur\nvery high test-time computation, with costs exceeding $300 for just 1,000\nprompts. Finally, curriculum learning via SLR doubles Llama-3-8B accuracy on\nSLR-Bench, achieving parity with Gemini-Flash-Thinking at a fraction of\ncomputational cost. Moreover, these reasoning capabilities generalize to a wide\nrange of established benchmarks, underscoring the effectiveness of SLR for\ndownstream reasoning."
                },
                "authors": [
                    {
                        "name": "Lukas Helff"
                    },
                    {
                        "name": "Ahmad Omar"
                    },
                    {
                        "name": "Felix Friedrich"
                    },
                    {
                        "name": "Antonia Wst"
                    },
                    {
                        "name": "Hikaru Shindo"
                    },
                    {
                        "name": "Rupert Mitchell"
                    },
                    {
                        "name": "Tim Woydt"
                    },
                    {
                        "name": "Patrick Schramowski"
                    },
                    {
                        "name": "Wolfgang Stammer"
                    },
                    {
                        "name": "Kristian Kersting"
                    }
                ],
                "author_detail": {
                    "name": "Kristian Kersting"
                },
                "author": "Kristian Kersting",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15787v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15787v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04524v1",
                "updated": "2025-08-06T15:08:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    15,
                    8,
                    16,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T15:08:16Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    15,
                    8,
                    16,
                    2,
                    218,
                    0
                ],
                "title": "RAIDX: A Retrieval-Augmented Generation and GRPO Reinforcement Learning\n  Framework for Explainable Deepfake Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAIDX: A Retrieval-Augmented Generation and GRPO Reinforcement Learning\n  Framework for Explainable Deepfake Detection"
                },
                "summary": "The rapid advancement of AI-generation models has enabled the creation of\nhyperrealistic imagery, posing ethical risks through widespread misinformation.\nCurrent deepfake detection methods, categorized as face specific detectors or\ngeneral AI-generated detectors, lack transparency by framing detection as a\nclassification task without explaining decisions. While several LLM-based\napproaches offer explainability, they suffer from coarse-grained analyses and\ndependency on labor-intensive annotations. This paper introduces RAIDX\n(Retrieval-Augmented Image Deepfake Detection and Explainability), a novel\ndeepfake detection framework integrating Retrieval-Augmented Generation (RAG)\nand Group Relative Policy Optimization (GRPO) to enhance detection accuracy and\ndecision explainability. Specifically, RAIDX leverages RAG to incorporate\nexternal knowledge for improved detection accuracy and employs GRPO to\nautonomously generate fine-grained textual explanations and saliency maps,\neliminating the need for extensive manual annotations. Experiments on multiple\nbenchmarks demonstrate RAIDX's effectiveness in identifying real or fake, and\nproviding interpretable rationales in both textual descriptions and saliency\nmaps, achieving state-of-the-art detection performance while advancing\ntransparency in deepfake identification. RAIDX represents the first unified\nframework to synergize RAG and GRPO, addressing critical gaps in accuracy and\nexplainability. Our code and models will be publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of AI-generation models has enabled the creation of\nhyperrealistic imagery, posing ethical risks through widespread misinformation.\nCurrent deepfake detection methods, categorized as face specific detectors or\ngeneral AI-generated detectors, lack transparency by framing detection as a\nclassification task without explaining decisions. While several LLM-based\napproaches offer explainability, they suffer from coarse-grained analyses and\ndependency on labor-intensive annotations. This paper introduces RAIDX\n(Retrieval-Augmented Image Deepfake Detection and Explainability), a novel\ndeepfake detection framework integrating Retrieval-Augmented Generation (RAG)\nand Group Relative Policy Optimization (GRPO) to enhance detection accuracy and\ndecision explainability. Specifically, RAIDX leverages RAG to incorporate\nexternal knowledge for improved detection accuracy and employs GRPO to\nautonomously generate fine-grained textual explanations and saliency maps,\neliminating the need for extensive manual annotations. Experiments on multiple\nbenchmarks demonstrate RAIDX's effectiveness in identifying real or fake, and\nproviding interpretable rationales in both textual descriptions and saliency\nmaps, achieving state-of-the-art detection performance while advancing\ntransparency in deepfake identification. RAIDX represents the first unified\nframework to synergize RAG and GRPO, addressing critical gaps in accuracy and\nexplainability. Our code and models will be publicly available."
                },
                "authors": [
                    {
                        "name": "Tianxiao Li"
                    },
                    {
                        "name": "Zhenglin Huang"
                    },
                    {
                        "name": "Haiquan Wen"
                    },
                    {
                        "name": "Yiwei He"
                    },
                    {
                        "name": "Shuchang Lyu"
                    },
                    {
                        "name": "Baoyuan Wu"
                    },
                    {
                        "name": "Guangliang Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Guangliang Cheng"
                },
                "author": "Guangliang Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04516v1",
                "updated": "2025-08-06T15:00:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    15,
                    0,
                    6,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T15:00:06Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    15,
                    0,
                    6,
                    2,
                    218,
                    0
                ],
                "title": "ECOLogic: Enabling Circular, Obfuscated, and Adaptive Logic via\n  eFPGA-Augmented SoCs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ECOLogic: Enabling Circular, Obfuscated, and Adaptive Logic via\n  eFPGA-Augmented SoCs"
                },
                "summary": "Traditional hardware platforms - ASICs and FPGAs - offer competing trade-offs\namong performance, flexibility, and sustainability. ASICs provide high\nefficiency but are inflexible post-fabrication, require costly re-spins for\nupdates, and expose IPs to piracy risks. FPGAs offer reconfigurability and\nreuse, yet suffer from substantial area, power, and performance overheads,\nresulting in higher carbon footprints. We present ECOLogic, a hybrid design\nparadigm that embeds lightweight eFPGA fabric within ASICs to enable secure,\nupdatable, and resource-aware computation. Central to this architecture is\nECOScore, a quantitative scoring framework that evaluates IPs based on\nadaptability, piracy threat, performance tolerance, and resource fit to guide\nRTL partitioning. Evaluated across six diverse SoC modules, ECOLogic retains an\naverage of 90 percent ASIC-level performance (up to 2 GHz), achieves 9.8 ns\ntiming slack (versus 5.1 ns in FPGA), and reduces power by 480 times on\naverage. Moreover, sustainability analysis shows a 99.7 percent reduction in\ndeployment carbon footprint and 300 to 500 times lower emissions relative to\nFPGA-only implementations. These results position ECOLogic as a\nhigh-performance, secure, and environmentally sustainable solution for\nnext-generation reconfigurable systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional hardware platforms - ASICs and FPGAs - offer competing trade-offs\namong performance, flexibility, and sustainability. ASICs provide high\nefficiency but are inflexible post-fabrication, require costly re-spins for\nupdates, and expose IPs to piracy risks. FPGAs offer reconfigurability and\nreuse, yet suffer from substantial area, power, and performance overheads,\nresulting in higher carbon footprints. We present ECOLogic, a hybrid design\nparadigm that embeds lightweight eFPGA fabric within ASICs to enable secure,\nupdatable, and resource-aware computation. Central to this architecture is\nECOScore, a quantitative scoring framework that evaluates IPs based on\nadaptability, piracy threat, performance tolerance, and resource fit to guide\nRTL partitioning. Evaluated across six diverse SoC modules, ECOLogic retains an\naverage of 90 percent ASIC-level performance (up to 2 GHz), achieves 9.8 ns\ntiming slack (versus 5.1 ns in FPGA), and reduces power by 480 times on\naverage. Moreover, sustainability analysis shows a 99.7 percent reduction in\ndeployment carbon footprint and 300 to 500 times lower emissions relative to\nFPGA-only implementations. These results position ECOLogic as a\nhigh-performance, secure, and environmentally sustainable solution for\nnext-generation reconfigurable systems."
                },
                "authors": [
                    {
                        "name": "Ishraq Tashdid"
                    },
                    {
                        "name": "Dewan Saiham"
                    },
                    {
                        "name": "Nafisa Anjum"
                    },
                    {
                        "name": "Tasnuva Farheen"
                    },
                    {
                        "name": "Sazadur Rahman"
                    }
                ],
                "author_detail": {
                    "name": "Sazadur Rahman"
                },
                "author": "Sazadur Rahman",
                "arxiv_comment": "10 pages, 7 figures. Extended version of accepted short paper at IEEE\n  International Conference on Computer Design (ICCD) 2025, Dallas, TX, USA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.3; B.6; B.7.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04508v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04508v1",
                "updated": "2025-08-06T14:53:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    53,
                    42,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T14:53:42Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    53,
                    42,
                    2,
                    218,
                    0
                ],
                "title": "Surf3R: Rapid Surface Reconstruction from Sparse RGB Views in Seconds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surf3R: Rapid Surface Reconstruction from Sparse RGB Views in Seconds"
                },
                "summary": "Current multi-view 3D reconstruction methods rely on accurate camera\ncalibration and pose estimation, requiring complex and time-intensive\npre-processing that hinders their practical deployment. To address this\nchallenge, we introduce Surf3R, an end-to-end feedforward approach that\nreconstructs 3D surfaces from sparse views without estimating camera poses and\ncompletes an entire scene in under 10 seconds. Our method employs a\nmulti-branch and multi-view decoding architecture in which multiple reference\nviews jointly guide the reconstruction process. Through the proposed\nbranch-wise processing, cross-view attention, and inter-branch fusion, the\nmodel effectively captures complementary geometric cues without requiring\ncamera calibration. Moreover, we introduce a D-Normal regularizer based on an\nexplicit 3D Gaussian representation for surface reconstruction. It couples\nsurface normals with other geometric parameters to jointly optimize the 3D\ngeometry, significantly improving 3D consistency and surface detail accuracy.\nExperimental results demonstrate that Surf3R achieves state-of-the-art\nperformance on multiple surface reconstruction metrics on ScanNet++ and Replica\ndatasets, exhibiting excellent generalization and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current multi-view 3D reconstruction methods rely on accurate camera\ncalibration and pose estimation, requiring complex and time-intensive\npre-processing that hinders their practical deployment. To address this\nchallenge, we introduce Surf3R, an end-to-end feedforward approach that\nreconstructs 3D surfaces from sparse views without estimating camera poses and\ncompletes an entire scene in under 10 seconds. Our method employs a\nmulti-branch and multi-view decoding architecture in which multiple reference\nviews jointly guide the reconstruction process. Through the proposed\nbranch-wise processing, cross-view attention, and inter-branch fusion, the\nmodel effectively captures complementary geometric cues without requiring\ncamera calibration. Moreover, we introduce a D-Normal regularizer based on an\nexplicit 3D Gaussian representation for surface reconstruction. It couples\nsurface normals with other geometric parameters to jointly optimize the 3D\ngeometry, significantly improving 3D consistency and surface detail accuracy.\nExperimental results demonstrate that Surf3R achieves state-of-the-art\nperformance on multiple surface reconstruction metrics on ScanNet++ and Replica\ndatasets, exhibiting excellent generalization and efficiency."
                },
                "authors": [
                    {
                        "name": "Haodong Zhu"
                    },
                    {
                        "name": "Changbai Li"
                    },
                    {
                        "name": "Yangyang Ren"
                    },
                    {
                        "name": "Zichao Feng"
                    },
                    {
                        "name": "Xuhui Liu"
                    },
                    {
                        "name": "Hanlin Chen"
                    },
                    {
                        "name": "Xiantong Zhen"
                    },
                    {
                        "name": "Baochang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Baochang Zhang"
                },
                "author": "Baochang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04508v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04508v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22716v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22716v2",
                "updated": "2025-08-06T14:53:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    53,
                    31,
                    2,
                    218,
                    0
                ],
                "published": "2025-07-30T14:29:44Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    14,
                    29,
                    44,
                    2,
                    211,
                    0
                ],
                "title": "From Sufficiency to Reflection: Reinforcement-Guided Thinking Quality in\n  Retrieval-Augmented Reasoning for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Sufficiency to Reflection: Reinforcement-Guided Thinking Quality in\n  Retrieval-Augmented Reasoning for LLMs"
                },
                "summary": "Reinforcement learning-based retrieval-augmented generation (RAG) methods\nenhance the reasoning abilities of large language models (LLMs). However, most\nrely only on final-answer rewards, overlooking intermediate reasoning quality.\nThis paper analyzes existing RAG reasoning models and identifies three main\nfailure patterns: (1) information insufficiency, meaning the model fails to\nretrieve adequate support; (2) faulty reasoning, where logical or content-level\nflaws appear despite sufficient information; and (3) answer-reasoning\ninconsistency, where a valid reasoning chain leads to a mismatched final\nanswer. We propose TIRESRAG-R1, a novel framework using a\nthink-retrieve-reflect process and a multi-dimensional reward system to improve\nreasoning and stability. TIRESRAG-R1 introduces: (1) a sufficiency reward to\nencourage thorough retrieval; (2) a reasoning quality reward to assess the\nrationality and accuracy of the reasoning chain; and (3) a reflection reward to\ndetect and revise errors. It also employs a difficulty-aware reweighting\nstrategy and training sample filtering to boost performance on complex tasks.\nExperiments on four multi-hop QA datasets show that TIRESRAG-R1 outperforms\nprior RAG methods and generalizes well to single-hop tasks. The code and data\nare available at: https://github.com/probe2/TIRESRAG-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning-based retrieval-augmented generation (RAG) methods\nenhance the reasoning abilities of large language models (LLMs). However, most\nrely only on final-answer rewards, overlooking intermediate reasoning quality.\nThis paper analyzes existing RAG reasoning models and identifies three main\nfailure patterns: (1) information insufficiency, meaning the model fails to\nretrieve adequate support; (2) faulty reasoning, where logical or content-level\nflaws appear despite sufficient information; and (3) answer-reasoning\ninconsistency, where a valid reasoning chain leads to a mismatched final\nanswer. We propose TIRESRAG-R1, a novel framework using a\nthink-retrieve-reflect process and a multi-dimensional reward system to improve\nreasoning and stability. TIRESRAG-R1 introduces: (1) a sufficiency reward to\nencourage thorough retrieval; (2) a reasoning quality reward to assess the\nrationality and accuracy of the reasoning chain; and (3) a reflection reward to\ndetect and revise errors. It also employs a difficulty-aware reweighting\nstrategy and training sample filtering to boost performance on complex tasks.\nExperiments on four multi-hop QA datasets show that TIRESRAG-R1 outperforms\nprior RAG methods and generalizes well to single-hop tasks. The code and data\nare available at: https://github.com/probe2/TIRESRAG-R1."
                },
                "authors": [
                    {
                        "name": "Jie He"
                    },
                    {
                        "name": "Victor Gutirrez-Basulto"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Z. Pan"
                },
                "author": "Jeff Z. Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22716v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22716v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04495v1",
                "updated": "2025-08-06T14:44:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    44,
                    23,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T14:44:23Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    44,
                    23,
                    2,
                    218,
                    0
                ],
                "title": "Causal Reflection with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Reflection with Language Models"
                },
                "summary": "While LLMs exhibit impressive fluency and factual recall, they struggle with\nrobust causal reasoning, often relying on spurious correlations and brittle\npatterns. Similarly, traditional Reinforcement Learning agents also lack causal\nunderstanding, optimizing for rewards without modeling why actions lead to\noutcomes. We introduce Causal Reflection, a framework that explicitly models\ncausality as a dynamic function over state, action, time, and perturbation,\nenabling agents to reason about delayed and nonlinear effects. Additionally, we\ndefine a formal Reflect mechanism that identifies mismatches between predicted\nand observed outcomes and generates causal hypotheses to revise the agent's\ninternal model. In this architecture, LLMs serve not as black-box reasoners,\nbut as structured inference engines translating formal causal outputs into\nnatural language explanations and counterfactuals. Our framework lays the\ntheoretical groundwork for Causal Reflective agents that can adapt,\nself-correct, and communicate causal understanding in evolving environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While LLMs exhibit impressive fluency and factual recall, they struggle with\nrobust causal reasoning, often relying on spurious correlations and brittle\npatterns. Similarly, traditional Reinforcement Learning agents also lack causal\nunderstanding, optimizing for rewards without modeling why actions lead to\noutcomes. We introduce Causal Reflection, a framework that explicitly models\ncausality as a dynamic function over state, action, time, and perturbation,\nenabling agents to reason about delayed and nonlinear effects. Additionally, we\ndefine a formal Reflect mechanism that identifies mismatches between predicted\nand observed outcomes and generates causal hypotheses to revise the agent's\ninternal model. In this architecture, LLMs serve not as black-box reasoners,\nbut as structured inference engines translating formal causal outputs into\nnatural language explanations and counterfactuals. Our framework lays the\ntheoretical groundwork for Causal Reflective agents that can adapt,\nself-correct, and communicate causal understanding in evolving environments."
                },
                "authors": [
                    {
                        "name": "Abi Aryan"
                    },
                    {
                        "name": "Zac Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zac Liu"
                },
                "author": "Zac Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04485v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04485v1",
                "updated": "2025-08-06T14:35:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    35,
                    59,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T14:35:59Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    35,
                    59,
                    2,
                    218,
                    0
                ],
                "title": "QuantVSR: Low-Bit Post-Training Quantization for Real-World Video\n  Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuantVSR: Low-Bit Post-Training Quantization for Real-World Video\n  Super-Resolution"
                },
                "summary": "Diffusion models have shown superior performance in real-world video\nsuper-resolution (VSR). However, the slow processing speeds and heavy resource\nconsumption of diffusion models hinder their practical application and\ndeployment. Quantization offers a potential solution for compressing the VSR\nmodel. Nevertheless, quantizing VSR models is challenging due to their temporal\ncharacteristics and high fidelity requirements. To address these issues, we\npropose QuantVSR, a low-bit quantization model for real-world VSR. We propose a\nspatio-temporal complexity aware (STCA) mechanism, where we first utilize the\ncalibration dataset to measure both spatial and temporal complexities for each\nlayer. Based on these statistics, we allocate layer-specific ranks to the\nlow-rank full-precision (FP) auxiliary branch. Subsequently, we jointly refine\nthe FP and low-bit branches to achieve simultaneous optimization. In addition,\nwe propose a learnable bias alignment (LBA) module to reduce the biased\nquantization errors. Extensive experiments on synthetic and real-world datasets\ndemonstrate that our method obtains comparable performance with the FP model\nand significantly outperforms recent leading low-bit quantization methods. Code\nis available at: https://github.com/bowenchai/QuantVSR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have shown superior performance in real-world video\nsuper-resolution (VSR). However, the slow processing speeds and heavy resource\nconsumption of diffusion models hinder their practical application and\ndeployment. Quantization offers a potential solution for compressing the VSR\nmodel. Nevertheless, quantizing VSR models is challenging due to their temporal\ncharacteristics and high fidelity requirements. To address these issues, we\npropose QuantVSR, a low-bit quantization model for real-world VSR. We propose a\nspatio-temporal complexity aware (STCA) mechanism, where we first utilize the\ncalibration dataset to measure both spatial and temporal complexities for each\nlayer. Based on these statistics, we allocate layer-specific ranks to the\nlow-rank full-precision (FP) auxiliary branch. Subsequently, we jointly refine\nthe FP and low-bit branches to achieve simultaneous optimization. In addition,\nwe propose a learnable bias alignment (LBA) module to reduce the biased\nquantization errors. Extensive experiments on synthetic and real-world datasets\ndemonstrate that our method obtains comparable performance with the FP model\nand significantly outperforms recent leading low-bit quantization methods. Code\nis available at: https://github.com/bowenchai/QuantVSR."
                },
                "authors": [
                    {
                        "name": "Bowen Chai"
                    },
                    {
                        "name": "Zheng Chen"
                    },
                    {
                        "name": "Libo Zhu"
                    },
                    {
                        "name": "Wenbo Li"
                    },
                    {
                        "name": "Yong Guo"
                    },
                    {
                        "name": "Yulun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yulun Zhang"
                },
                "author": "Yulun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04485v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04485v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04482v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04482v1",
                "updated": "2025-08-06T14:33:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    33,
                    45,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T14:33:45Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    33,
                    45,
                    2,
                    218,
                    0
                ],
                "title": "OS Agents: A Survey on MLLM-based Agents for General Computing Devices\n  Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OS Agents: A Survey on MLLM-based Agents for General Computing Devices\n  Use"
                },
                "summary": "The dream to create AI assistants as capable and versatile as the fictional\nJ.A.R.V.I.S from Iron Man has long captivated imaginations. With the evolution\nof (multi-modal) large language models ((M)LLMs), this dream is closer to\nreality, as (M)LLM-based Agents using computing devices (e.g., computers and\nmobile phones) by operating within the environments and interfaces (e.g.,\nGraphical User Interface (GUI)) provided by operating systems (OS) to automate\ntasks have significantly advanced. This paper presents a comprehensive survey\nof these advanced agents, designated as OS Agents. We begin by elucidating the\nfundamentals of OS Agents, exploring their key components including the\nenvironment, observation space, and action space, and outlining essential\ncapabilities such as understanding, planning, and grounding. We then examine\nmethodologies for constructing OS Agents, focusing on domain-specific\nfoundation models and agent frameworks. A detailed review of evaluation\nprotocols and benchmarks highlights how OS Agents are assessed across diverse\ntasks. Finally, we discuss current challenges and identify promising directions\nfor future research, including safety and privacy, personalization and\nself-evolution. This survey aims to consolidate the state of OS Agents\nresearch, providing insights to guide both academic inquiry and industrial\ndevelopment. An open-source GitHub repository is maintained as a dynamic\nresource to foster further innovation in this field. We present a 9-page\nversion of our work, accepted by ACL 2025, to provide a concise overview to the\ndomain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The dream to create AI assistants as capable and versatile as the fictional\nJ.A.R.V.I.S from Iron Man has long captivated imaginations. With the evolution\nof (multi-modal) large language models ((M)LLMs), this dream is closer to\nreality, as (M)LLM-based Agents using computing devices (e.g., computers and\nmobile phones) by operating within the environments and interfaces (e.g.,\nGraphical User Interface (GUI)) provided by operating systems (OS) to automate\ntasks have significantly advanced. This paper presents a comprehensive survey\nof these advanced agents, designated as OS Agents. We begin by elucidating the\nfundamentals of OS Agents, exploring their key components including the\nenvironment, observation space, and action space, and outlining essential\ncapabilities such as understanding, planning, and grounding. We then examine\nmethodologies for constructing OS Agents, focusing on domain-specific\nfoundation models and agent frameworks. A detailed review of evaluation\nprotocols and benchmarks highlights how OS Agents are assessed across diverse\ntasks. Finally, we discuss current challenges and identify promising directions\nfor future research, including safety and privacy, personalization and\nself-evolution. This survey aims to consolidate the state of OS Agents\nresearch, providing insights to guide both academic inquiry and industrial\ndevelopment. An open-source GitHub repository is maintained as a dynamic\nresource to foster further innovation in this field. We present a 9-page\nversion of our work, accepted by ACL 2025, to provide a concise overview to the\ndomain."
                },
                "authors": [
                    {
                        "name": "Xueyu Hu"
                    },
                    {
                        "name": "Tao Xiong"
                    },
                    {
                        "name": "Biao Yi"
                    },
                    {
                        "name": "Zishu Wei"
                    },
                    {
                        "name": "Ruixuan Xiao"
                    },
                    {
                        "name": "Yurun Chen"
                    },
                    {
                        "name": "Jiasheng Ye"
                    },
                    {
                        "name": "Meiling Tao"
                    },
                    {
                        "name": "Xiangxin Zhou"
                    },
                    {
                        "name": "Ziyu Zhao"
                    },
                    {
                        "name": "Yuhuai Li"
                    },
                    {
                        "name": "Shengze Xu"
                    },
                    {
                        "name": "Shenzhi Wang"
                    },
                    {
                        "name": "Xinchen Xu"
                    },
                    {
                        "name": "Shuofei Qiao"
                    },
                    {
                        "name": "Zhaokai Wang"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Tieyong Zeng"
                    },
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Jiwei Li"
                    },
                    {
                        "name": "Yuchen Eleanor Jiang"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Keting Yin"
                    },
                    {
                        "name": "Zhou Zhao"
                    },
                    {
                        "name": "Hongxia Yang"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Shengyu Zhang"
                    },
                    {
                        "name": "Fei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Wu"
                },
                "author": "Fei Wu",
                "arxiv_comment": "ACL 2025 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04482v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04482v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04474v1",
                "updated": "2025-08-06T14:25:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    25,
                    5,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T14:25:05Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    25,
                    5,
                    2,
                    218,
                    0
                ],
                "title": "TRAIL: Joint Inference and Refinement of Knowledge Graphs with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRAIL: Joint Inference and Refinement of Knowledge Graphs with Large\n  Language Models"
                },
                "summary": "Recent advances in large language models (LLMs) have unlocked powerful\nreasoning and decision-making capabilities. However, their inherent dependence\non static parametric memory fundamentally limits their adaptability, factual\naccuracy, and interpretability in knowledge-intensive scenarios. Knowledge\ngraphs (KGs), as structured repositories of explicit relational knowledge,\noffer a promising approach for augmenting LLMs with external, interpretable\nmemory. Nevertheless, most existing methods that combine LLMs with KGs treat\nreasoning and knowledge updating as separate processes, resulting in suboptimal\nutilization of new information and hindering real-time updates. In this work,\nwe propose TRAIL: a novel, unified framework for Thinking, Reasoning, And\nIncremental Learning that couples joint inference and dynamic KG refinement\nwith large language models. TRAIL enables LLM agents to iteratively explore,\nupdate, and refine knowledge graphs during the reasoning process, employing a\nconfidence-driven mechanism for the generation, validation, and pruning of new\nfacts. This plug-and-play architecture facilitates seamless integration with\nvarious LLMs, supporting continual adaptation without the need for retraining.\nExtensive experiments on multiple benchmarks demonstrate that TRAIL outperforms\nexisting KG-augmented and retrieval-augmented LLM baselines by 3% to 13%. More\nimportantly, these results represent a significant step toward developing\nadaptive, memory-augmented language models capable of continual learning and\nreliable, transparent reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have unlocked powerful\nreasoning and decision-making capabilities. However, their inherent dependence\non static parametric memory fundamentally limits their adaptability, factual\naccuracy, and interpretability in knowledge-intensive scenarios. Knowledge\ngraphs (KGs), as structured repositories of explicit relational knowledge,\noffer a promising approach for augmenting LLMs with external, interpretable\nmemory. Nevertheless, most existing methods that combine LLMs with KGs treat\nreasoning and knowledge updating as separate processes, resulting in suboptimal\nutilization of new information and hindering real-time updates. In this work,\nwe propose TRAIL: a novel, unified framework for Thinking, Reasoning, And\nIncremental Learning that couples joint inference and dynamic KG refinement\nwith large language models. TRAIL enables LLM agents to iteratively explore,\nupdate, and refine knowledge graphs during the reasoning process, employing a\nconfidence-driven mechanism for the generation, validation, and pruning of new\nfacts. This plug-and-play architecture facilitates seamless integration with\nvarious LLMs, supporting continual adaptation without the need for retraining.\nExtensive experiments on multiple benchmarks demonstrate that TRAIL outperforms\nexisting KG-augmented and retrieval-augmented LLM baselines by 3% to 13%. More\nimportantly, these results represent a significant step toward developing\nadaptive, memory-augmented language models capable of continual learning and\nreliable, transparent reasoning."
                },
                "authors": [
                    {
                        "name": "Xinkui Zhao"
                    },
                    {
                        "name": "Haode Li"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Guanjie Cheng"
                    },
                    {
                        "name": "Yueshen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Yueshen Xu"
                },
                "author": "Yueshen Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04469v1",
                "updated": "2025-08-06T14:12:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    12,
                    5,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T14:12:05Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    12,
                    5,
                    2,
                    218,
                    0
                ],
                "title": "FrEVL: Leveraging Frozen Pretrained Embeddings for Efficient\n  Vision-Language Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FrEVL: Leveraging Frozen Pretrained Embeddings for Efficient\n  Vision-Language Understanding"
                },
                "summary": "The deployment of vision-language models remains constrained by substantial\ncomputational requirements. We present \\textbf{FrEVL}, a framework exploring\nwhether frozen pretrained embeddings can support effective vision-language\nunderstanding. Our analysis reveals that frozen embeddings contain rich\ninformation for discriminative tasks, achieving 85\\% to 95\\% of\nstate-of-the-art performance on standard benchmarks with only 68.4M trainable\nparameters. This performance dichotomy reveals a critical insight: frozen\nembedding effectiveness depends on alignment between pretraining objectives and\ndownstream task requirements. When accounting for end-to-end computation\nincluding embedding extraction, FrEVL provides $2.3\\times$ speedup with 52\\%\nlower energy consumption, making it suitable for scenarios with pre-computable\ninputs or when deployment constraints outweigh marginal performance gains. Our\nevaluation provides practitioners with guidance on when frozen embedding\napproaches represent viable alternatives to full model deployment. We will\nrelease our complete implementation and evaluation framework to facilitate\nfurther research into efficient multi-modal understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of vision-language models remains constrained by substantial\ncomputational requirements. We present \\textbf{FrEVL}, a framework exploring\nwhether frozen pretrained embeddings can support effective vision-language\nunderstanding. Our analysis reveals that frozen embeddings contain rich\ninformation for discriminative tasks, achieving 85\\% to 95\\% of\nstate-of-the-art performance on standard benchmarks with only 68.4M trainable\nparameters. This performance dichotomy reveals a critical insight: frozen\nembedding effectiveness depends on alignment between pretraining objectives and\ndownstream task requirements. When accounting for end-to-end computation\nincluding embedding extraction, FrEVL provides $2.3\\times$ speedup with 52\\%\nlower energy consumption, making it suitable for scenarios with pre-computable\ninputs or when deployment constraints outweigh marginal performance gains. Our\nevaluation provides practitioners with guidance on when frozen embedding\napproaches represent viable alternatives to full model deployment. We will\nrelease our complete implementation and evaluation framework to facilitate\nfurther research into efficient multi-modal understanding."
                },
                "authors": [
                    {
                        "name": "Emmanuelle Bourigault"
                    },
                    {
                        "name": "Pauline Bourigault"
                    }
                ],
                "author_detail": {
                    "name": "Pauline Bourigault"
                },
                "author": "Pauline Bourigault",
                "arxiv_comment": "8 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04462v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04462v1",
                "updated": "2025-08-06T14:02:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    2,
                    10,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T14:02:10Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    2,
                    10,
                    2,
                    218,
                    0
                ],
                "title": "CARD: Cache-Assisted Parallel Speculative Decoding for Efficient Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CARD: Cache-Assisted Parallel Speculative Decoding for Efficient Large\n  Language Model Inference"
                },
                "summary": "Speculative decoding (SD), where an extra draft model first provides multiple\ndraft tokens and the original target model then verifies these tokens in\nparallel, has shown great power for LLM inference acceleration. However,\nexisting SD methods must adhere to the 'draft-then-verify' paradigm, which\nforces drafting and verification processes to execute sequentially during SD,\nresulting in inefficient inference performance and limiting the size of the\ndraft model. Furthermore, once a single token in the candidate sequence is\nrejected during the drafting process, all subsequent candidate tokens must be\ndiscarded, leading to inefficient drafting. To address these challenges, we\npropose a cache-based parallel speculative decoding framework employing a\n'query-and-correct' paradigm. Specifically, CARD decouples drafting and\nverification: the draft model generates candidate tokens to populate a shared\ncache, while the target model concurrently rectifies the draft model's\ngeneration direction. This effectively enables the target model to perform\ninference at speed approaching that of the draft model. Our approach achieves\nup to 4.83 speedup over vanilla decoding without requiring fine-tuning of\neither the draft or target models. Our code is available at\nhttps://github.com/hunzhizi/CARD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding (SD), where an extra draft model first provides multiple\ndraft tokens and the original target model then verifies these tokens in\nparallel, has shown great power for LLM inference acceleration. However,\nexisting SD methods must adhere to the 'draft-then-verify' paradigm, which\nforces drafting and verification processes to execute sequentially during SD,\nresulting in inefficient inference performance and limiting the size of the\ndraft model. Furthermore, once a single token in the candidate sequence is\nrejected during the drafting process, all subsequent candidate tokens must be\ndiscarded, leading to inefficient drafting. To address these challenges, we\npropose a cache-based parallel speculative decoding framework employing a\n'query-and-correct' paradigm. Specifically, CARD decouples drafting and\nverification: the draft model generates candidate tokens to populate a shared\ncache, while the target model concurrently rectifies the draft model's\ngeneration direction. This effectively enables the target model to perform\ninference at speed approaching that of the draft model. Our approach achieves\nup to 4.83 speedup over vanilla decoding without requiring fine-tuning of\neither the draft or target models. Our code is available at\nhttps://github.com/hunzhizi/CARD."
                },
                "authors": [
                    {
                        "name": "Enyu Zhou"
                    },
                    {
                        "name": "Kai Sheng"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Xin He"
                    }
                ],
                "author_detail": {
                    "name": "Xin He"
                },
                "author": "Xin He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04462v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04462v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04460v1",
                "updated": "2025-08-06T13:59:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    59,
                    17,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T13:59:17Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    59,
                    17,
                    2,
                    218,
                    0
                ],
                "title": "From \"Aha Moments\" to Controllable Thinking: Toward Meta-Cognitive\n  Reasoning in Large Reasoning Models via Decoupled Reasoning and Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From \"Aha Moments\" to Controllable Thinking: Toward Meta-Cognitive\n  Reasoning in Large Reasoning Models via Decoupled Reasoning and Control"
                },
                "summary": "Large Reasoning Models (LRMs) have demonstrated a latent capacity for complex\nreasoning by spontaneously exhibiting cognitive behaviors such as step-by-step\nreasoning, reflection, and backtracking, commonly referred to as \"Aha Moments\".\nHowever, such emergent behaviors remain unregulated and uncontrolled, often\nresulting in overthinking, where the model continues generating redundant\nreasoning content even after reaching reliable conclusions. This leads to\nexcessive computational costs and increased latency, limiting the practical\ndeployment of LRMs. The root cause lies in the absence of intrinsic regulatory\nmechanisms, as current models are unable to monitor and adaptively manage their\nreasoning process to determine when to continue, backtrack, or terminate. To\naddress this issue, we propose the Meta-cognitive Reasoning Framework (MERA),\nwhich explicitly decouples the thinking process into distinct reasoning and\ncontrol components, thereby enabling the independent optimization of control\nstrategies. Specifically, MERA incorporates a takeover-based data construction\nmechanism that identifies critical decision points during reasoning and\ndelegates the creation of control signals to auxiliary LLMs, thereby enabling\nthe construction of high-quality reasoning-control data. Additionally, a\nstructured reasoning-control separation is implemented via supervised\nfine-tuning, enabling the model to generate explicit traces and acquire initial\nmeta-cognitive control capabilities. Finally, MERA employs Control-Segment\nPolicy Optimization (CSPO), which combines segment-wise Group Relative Policy\nOptimization (GRPO) with a control-masking mechanism to optimize control\nbehavior learning while minimizing interference from irrelevant content.\nExperiments on various reasoning benchmarks demonstrate that models trained\nwith MERA enhance both reasoning efficiency and accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models (LRMs) have demonstrated a latent capacity for complex\nreasoning by spontaneously exhibiting cognitive behaviors such as step-by-step\nreasoning, reflection, and backtracking, commonly referred to as \"Aha Moments\".\nHowever, such emergent behaviors remain unregulated and uncontrolled, often\nresulting in overthinking, where the model continues generating redundant\nreasoning content even after reaching reliable conclusions. This leads to\nexcessive computational costs and increased latency, limiting the practical\ndeployment of LRMs. The root cause lies in the absence of intrinsic regulatory\nmechanisms, as current models are unable to monitor and adaptively manage their\nreasoning process to determine when to continue, backtrack, or terminate. To\naddress this issue, we propose the Meta-cognitive Reasoning Framework (MERA),\nwhich explicitly decouples the thinking process into distinct reasoning and\ncontrol components, thereby enabling the independent optimization of control\nstrategies. Specifically, MERA incorporates a takeover-based data construction\nmechanism that identifies critical decision points during reasoning and\ndelegates the creation of control signals to auxiliary LLMs, thereby enabling\nthe construction of high-quality reasoning-control data. Additionally, a\nstructured reasoning-control separation is implemented via supervised\nfine-tuning, enabling the model to generate explicit traces and acquire initial\nmeta-cognitive control capabilities. Finally, MERA employs Control-Segment\nPolicy Optimization (CSPO), which combines segment-wise Group Relative Policy\nOptimization (GRPO) with a control-masking mechanism to optimize control\nbehavior learning while minimizing interference from irrelevant content.\nExperiments on various reasoning benchmarks demonstrate that models trained\nwith MERA enhance both reasoning efficiency and accuracy."
                },
                "authors": [
                    {
                        "name": "Rui Ha"
                    },
                    {
                        "name": "Chaozhuo Li"
                    },
                    {
                        "name": "Rui Pu"
                    },
                    {
                        "name": "Sen Su"
                    }
                ],
                "author_detail": {
                    "name": "Sen Su"
                },
                "author": "Sen Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04457v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04457v1",
                "updated": "2025-08-06T13:58:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    58,
                    17,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T13:58:17Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    58,
                    17,
                    2,
                    218,
                    0
                ],
                "title": "Benchmarking Uncertainty and its Disentanglement in multi-label Chest\n  X-Ray Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Uncertainty and its Disentanglement in multi-label Chest\n  X-Ray Classification"
                },
                "summary": "Reliable uncertainty quantification is crucial for trustworthy\ndecision-making and the deployment of AI models in medical imaging. While prior\nwork has explored the ability of neural networks to quantify predictive,\nepistemic, and aleatoric uncertainties using an information-theoretical\napproach in synthetic or well defined data settings like natural image\nclassification, its applicability to real life medical diagnosis tasks remains\nunderexplored. In this study, we provide an extensive uncertainty\nquantification benchmark for multi-label chest X-ray classification using the\nMIMIC-CXR-JPG dataset. We evaluate 13 uncertainty quantification methods for\nconvolutional (ResNet) and transformer-based (Vision Transformer) architectures\nacross a wide range of tasks. Additionally, we extend Evidential Deep Learning,\nHetClass NNs, and Deep Deterministic Uncertainty to the multi-label setting.\nOur analysis provides insights into uncertainty estimation effectiveness and\nthe ability to disentangle epistemic and aleatoric uncertainties, revealing\nmethod- and architecture-specific strengths and limitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable uncertainty quantification is crucial for trustworthy\ndecision-making and the deployment of AI models in medical imaging. While prior\nwork has explored the ability of neural networks to quantify predictive,\nepistemic, and aleatoric uncertainties using an information-theoretical\napproach in synthetic or well defined data settings like natural image\nclassification, its applicability to real life medical diagnosis tasks remains\nunderexplored. In this study, we provide an extensive uncertainty\nquantification benchmark for multi-label chest X-ray classification using the\nMIMIC-CXR-JPG dataset. We evaluate 13 uncertainty quantification methods for\nconvolutional (ResNet) and transformer-based (Vision Transformer) architectures\nacross a wide range of tasks. Additionally, we extend Evidential Deep Learning,\nHetClass NNs, and Deep Deterministic Uncertainty to the multi-label setting.\nOur analysis provides insights into uncertainty estimation effectiveness and\nthe ability to disentangle epistemic and aleatoric uncertainties, revealing\nmethod- and architecture-specific strengths and limitations."
                },
                "authors": [
                    {
                        "name": "Simon Baur"
                    },
                    {
                        "name": "Wojciech Samek"
                    },
                    {
                        "name": "Jackie Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jackie Ma"
                },
                "author": "Jackie Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04457v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04451v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04451v1",
                "updated": "2025-08-06T13:52:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    52,
                    0,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T13:52:00Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    52,
                    0,
                    2,
                    218,
                    0
                ],
                "title": "Automatic LLM Red Teaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic LLM Red Teaming"
                },
                "summary": "Red teaming is critical for identifying vulnerabilities and building trust in\ncurrent LLMs. However, current automated methods for Large Language Models\n(LLMs) rely on brittle prompt templates or single-turn attacks, failing to\ncapture the complex, interactive nature of real-world adversarial dialogues. We\npropose a novel paradigm: training an AI to strategically `break' another AI.\nBy formalizing red teaming as a Markov Decision Process (MDP) and employing a\nhierarchical Reinforcement Learning (RL) framework, we effectively address the\ninherent sparse reward and long-horizon challenges. Our generative agent learns\ncoherent, multi-turn attack strategies through a fine-grained, token-level harm\nreward, enabling it to uncover subtle vulnerabilities missed by existing\nbaselines. This approach sets a new state-of-the-art, fundamentally reframing\nLLM red teaming as a dynamic, trajectory-based process (rather than a one-step\ntest) essential for robust AI deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Red teaming is critical for identifying vulnerabilities and building trust in\ncurrent LLMs. However, current automated methods for Large Language Models\n(LLMs) rely on brittle prompt templates or single-turn attacks, failing to\ncapture the complex, interactive nature of real-world adversarial dialogues. We\npropose a novel paradigm: training an AI to strategically `break' another AI.\nBy formalizing red teaming as a Markov Decision Process (MDP) and employing a\nhierarchical Reinforcement Learning (RL) framework, we effectively address the\ninherent sparse reward and long-horizon challenges. Our generative agent learns\ncoherent, multi-turn attack strategies through a fine-grained, token-level harm\nreward, enabling it to uncover subtle vulnerabilities missed by existing\nbaselines. This approach sets a new state-of-the-art, fundamentally reframing\nLLM red teaming as a dynamic, trajectory-based process (rather than a one-step\ntest) essential for robust AI deployment."
                },
                "authors": [
                    {
                        "name": "Roman Belaire"
                    },
                    {
                        "name": "Arunesh Sinha"
                    },
                    {
                        "name": "Pradeep Varakantham"
                    }
                ],
                "author_detail": {
                    "name": "Pradeep Varakantham"
                },
                "author": "Pradeep Varakantham",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04451v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04451v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04448v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04448v1",
                "updated": "2025-08-06T13:48:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    48,
                    38,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T13:48:38Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    48,
                    38,
                    2,
                    218,
                    0
                ],
                "title": "Large Language Models Versus Static Code Analysis Tools: A Systematic\n  Benchmark for Vulnerability Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Versus Static Code Analysis Tools: A Systematic\n  Benchmark for Vulnerability Detection"
                },
                "summary": "Modern software relies on a multitude of automated testing and quality\nassurance tools to prevent errors, bugs and potential vulnerabilities. This\nstudy sets out to provide a head-to-head, quantitative and qualitative\nevaluation of six automated approaches: three industry-standard rule-based\nstatic code-analysis tools (SonarQube, CodeQL and Snyk Code) and three\nstate-of-the-art large language models hosted on the GitHub Models platform\n(GPT-4.1, Mistral Large and DeepSeek V3). Using a curated suite of ten\nreal-world C# projects that embed 63 vulnerabilities across common categories\nsuch as SQL injection, hard-coded secrets and outdated dependencies, we measure\nclassical detection accuracy (precision, recall, F-score), analysis latency,\nand the developer effort required to vet true positives. The language-based\nscanners achieve higher mean F-1 scores,0.797, 0.753 and 0.750, than their\nstatic counterparts, which score 0.260, 0.386 and 0.546, respectively. LLMs'\nadvantage originates from superior recall, confirming an ability to reason\nacross broader code contexts. However, this benefit comes with substantial\ntrade-offs: DeepSeek V3 exhibits the highest false-positive ratio, all language\nmodels mislocate issues at line-or-column granularity due to tokenisation\nartefacts. Overall, language models successfully rival traditional static\nanalysers in finding real vulnerabilities. Still, their noisier output and\nimprecise localisation limit their standalone use in safety-critical audits. We\ntherefore recommend a hybrid pipeline: employ language models early in\ndevelopment for broad, context-aware triage, while reserving deterministic\nrule-based scanners for high-assurance verification. The open benchmark and\nJSON-based result harness released with this paper lay a foundation for\nreproducible, practitioner-centric research into next-generation automated code\nsecurity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software relies on a multitude of automated testing and quality\nassurance tools to prevent errors, bugs and potential vulnerabilities. This\nstudy sets out to provide a head-to-head, quantitative and qualitative\nevaluation of six automated approaches: three industry-standard rule-based\nstatic code-analysis tools (SonarQube, CodeQL and Snyk Code) and three\nstate-of-the-art large language models hosted on the GitHub Models platform\n(GPT-4.1, Mistral Large and DeepSeek V3). Using a curated suite of ten\nreal-world C# projects that embed 63 vulnerabilities across common categories\nsuch as SQL injection, hard-coded secrets and outdated dependencies, we measure\nclassical detection accuracy (precision, recall, F-score), analysis latency,\nand the developer effort required to vet true positives. The language-based\nscanners achieve higher mean F-1 scores,0.797, 0.753 and 0.750, than their\nstatic counterparts, which score 0.260, 0.386 and 0.546, respectively. LLMs'\nadvantage originates from superior recall, confirming an ability to reason\nacross broader code contexts. However, this benefit comes with substantial\ntrade-offs: DeepSeek V3 exhibits the highest false-positive ratio, all language\nmodels mislocate issues at line-or-column granularity due to tokenisation\nartefacts. Overall, language models successfully rival traditional static\nanalysers in finding real vulnerabilities. Still, their noisier output and\nimprecise localisation limit their standalone use in safety-critical audits. We\ntherefore recommend a hybrid pipeline: employ language models early in\ndevelopment for broad, context-aware triage, while reserving deterministic\nrule-based scanners for high-assurance verification. The open benchmark and\nJSON-based result harness released with this paper lay a foundation for\nreproducible, practitioner-centric research into next-generation automated code\nsecurity."
                },
                "authors": [
                    {
                        "name": "Damian Gnieciak"
                    },
                    {
                        "name": "Tomasz Szandala"
                    }
                ],
                "author_detail": {
                    "name": "Tomasz Szandala"
                },
                "author": "Tomasz Szandala",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04448v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04448v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13928v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13928v3",
                "updated": "2025-08-06T13:47:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    47,
                    10,
                    2,
                    218,
                    0
                ],
                "published": "2024-10-17T17:56:01Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    56,
                    1,
                    3,
                    291,
                    0
                ],
                "title": "Automatically Interpreting Millions of Features in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically Interpreting Millions of Features in Large Language Models"
                },
                "summary": "While the activations of neurons in deep neural networks usually do not have\na simple human-understandable interpretation, sparse autoencoders (SAEs) can be\nused to transform these activations into a higher-dimensional latent space\nwhich may be more easily interpretable. However, these SAEs can have millions\nof distinct latent features, making it infeasible for humans to manually\ninterpret each one. In this work, we build an open-source automated pipeline to\ngenerate and evaluate natural language explanations for SAE features using\nLLMs. We test our framework on SAEs of varying sizes, activation functions, and\nlosses, trained on two different open-weight LLMs. We introduce five new\ntechniques to score the quality of explanations that are cheaper to run than\nthe previous state of the art. One of these techniques, intervention scoring,\nevaluates the interpretability of the effects of intervening on a feature,\nwhich we find explains features that are not recalled by existing methods. We\npropose guidelines for generating better explanations that remain valid for a\nbroader set of activating contexts, and discuss pitfalls with existing scoring\ntechniques. We use our explanations to measure the semantic similarity of\nindependently trained SAEs, and find that SAEs trained on nearby layers of the\nresidual stream are highly similar. Our large-scale analysis confirms that SAE\nlatents are indeed much more interpretable than neurons, even when neurons are\nsparsified using top-$k$ postprocessing. Our code is available at\nhttps://github.com/EleutherAI/sae-auto-interp, and our explanations are\navailable at\nhttps://huggingface.co/datasets/EleutherAI/auto_interp_explanations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While the activations of neurons in deep neural networks usually do not have\na simple human-understandable interpretation, sparse autoencoders (SAEs) can be\nused to transform these activations into a higher-dimensional latent space\nwhich may be more easily interpretable. However, these SAEs can have millions\nof distinct latent features, making it infeasible for humans to manually\ninterpret each one. In this work, we build an open-source automated pipeline to\ngenerate and evaluate natural language explanations for SAE features using\nLLMs. We test our framework on SAEs of varying sizes, activation functions, and\nlosses, trained on two different open-weight LLMs. We introduce five new\ntechniques to score the quality of explanations that are cheaper to run than\nthe previous state of the art. One of these techniques, intervention scoring,\nevaluates the interpretability of the effects of intervening on a feature,\nwhich we find explains features that are not recalled by existing methods. We\npropose guidelines for generating better explanations that remain valid for a\nbroader set of activating contexts, and discuss pitfalls with existing scoring\ntechniques. We use our explanations to measure the semantic similarity of\nindependently trained SAEs, and find that SAEs trained on nearby layers of the\nresidual stream are highly similar. Our large-scale analysis confirms that SAE\nlatents are indeed much more interpretable than neurons, even when neurons are\nsparsified using top-$k$ postprocessing. Our code is available at\nhttps://github.com/EleutherAI/sae-auto-interp, and our explanations are\navailable at\nhttps://huggingface.co/datasets/EleutherAI/auto_interp_explanations."
                },
                "authors": [
                    {
                        "name": "Gonalo Paulo"
                    },
                    {
                        "name": "Alex Mallen"
                    },
                    {
                        "name": "Caden Juang"
                    },
                    {
                        "name": "Nora Belrose"
                    }
                ],
                "author_detail": {
                    "name": "Nora Belrose"
                },
                "author": "Nora Belrose",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13928v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13928v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13801v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13801v4",
                "updated": "2025-08-06T13:47:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    47,
                    10,
                    2,
                    218,
                    0
                ],
                "published": "2025-03-18T01:16:16Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    1,
                    16,
                    16,
                    1,
                    77,
                    0
                ],
                "title": "SCAN-BEST: Sub-6GHz-Aided Near-field Beam Selection with Formal\n  Reliability Guarantees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCAN-BEST: Sub-6GHz-Aided Near-field Beam Selection with Formal\n  Reliability Guarantees"
                },
                "summary": "As millimeter-wave (mmWave) MIMO systems adopt larger antenna arrays,\nnear-field propagation becomes increasingly prominent, especially for users\nclose to the transmitter. Traditional far-field beam training methods become\ninadequate, while near-field training faces the challenge of large codebooks\ndue to the need to resolve both angular and distance domains. To reduce in-band\ntraining overhead, prior work has proposed to leverage the spatial-temporal\ncongruence between sub-6 GHz (sub-6G) and mmWave channels to predict the best\nmmWave beam within a near-field codebook from sub-6G channel estimates. To cope\nwith the uncertainty caused by sub-6G/mmWave differences, we introduce a novel\nSub-6G Channel Aided Near-field BEam SelecTion (SCAN-BEST) framework that wraps\naround any beam predictor to produce candidate beam subset with formal\nsuboptimality guarantees. The proposed SCAN-BEST builds on conformal risk\ncontrol (CRC), and is calibrated offline using limited calibration data. Its\nperformance guarantees apply even in the presence of statistical shifts between\ncalibration and deployment. Numerical results validate the theoretical\nproperties and efficiency of SCAN-BEST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As millimeter-wave (mmWave) MIMO systems adopt larger antenna arrays,\nnear-field propagation becomes increasingly prominent, especially for users\nclose to the transmitter. Traditional far-field beam training methods become\ninadequate, while near-field training faces the challenge of large codebooks\ndue to the need to resolve both angular and distance domains. To reduce in-band\ntraining overhead, prior work has proposed to leverage the spatial-temporal\ncongruence between sub-6 GHz (sub-6G) and mmWave channels to predict the best\nmmWave beam within a near-field codebook from sub-6G channel estimates. To cope\nwith the uncertainty caused by sub-6G/mmWave differences, we introduce a novel\nSub-6G Channel Aided Near-field BEam SelecTion (SCAN-BEST) framework that wraps\naround any beam predictor to produce candidate beam subset with formal\nsuboptimality guarantees. The proposed SCAN-BEST builds on conformal risk\ncontrol (CRC), and is calibrated offline using limited calibration data. Its\nperformance guarantees apply even in the presence of statistical shifts between\ncalibration and deployment. Numerical results validate the theoretical\nproperties and efficiency of SCAN-BEST."
                },
                "authors": [
                    {
                        "name": "Weicao Deng"
                    },
                    {
                        "name": "Binpu Shi"
                    },
                    {
                        "name": "Min Li"
                    },
                    {
                        "name": "Osvaldo Simeone"
                    }
                ],
                "author_detail": {
                    "name": "Osvaldo Simeone"
                },
                "author": "Osvaldo Simeone",
                "arxiv_comment": "15 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13801v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13801v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15299v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15299v4",
                "updated": "2025-08-06T13:42:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    42,
                    41,
                    2,
                    218,
                    0
                ],
                "published": "2025-03-19T15:21:48Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    21,
                    48,
                    2,
                    78,
                    0
                ],
                "title": "Inside-Out: Hidden Factual Knowledge in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inside-Out: Hidden Factual Knowledge in LLMs"
                },
                "summary": "This work presents a framework for assessing whether large language models\n(LLMs) encode more factual knowledge in their parameters than what they express\nin their outputs. While a few studies hint at this possibility, none has\nclearly defined or demonstrated this phenomenon. We first propose a formal\ndefinition of knowledge, quantifying it for a given question as the fraction of\ncorrect-incorrect answer pairs where the correct one is ranked higher. This\ngives rise to external and internal knowledge, depending on the information\nused to score individual answer candidates: either the model's observable\ntoken-level probabilities or its intermediate computations. Hidden knowledge\narises when internal knowledge exceeds external knowledge. We then present a\ncase study, applying this framework to three popular open-weights LLMs in a\nclosed-book QA setup. Our results indicate that: (1) LLMs consistently encode\nmore factual knowledge internally than what they express externally, with an\naverage relative gap of 40%. (2) Surprisingly, some knowledge is so deeply\nhidden that a model can internally know an answer perfectly, yet fail to\ngenerate it even once, despite large-scale repeated sampling of 1,000 answers.\nThis reveals fundamental limitations in the generation capabilities of LLMs,\nwhich (3) put a practical constraint on scaling test-time compute via repeated\nanswer sampling in closed-book QA: significant performance improvements remain\ninaccessible because some answers are practically never sampled, yet if they\nwere, we would be guaranteed to rank them first.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a framework for assessing whether large language models\n(LLMs) encode more factual knowledge in their parameters than what they express\nin their outputs. While a few studies hint at this possibility, none has\nclearly defined or demonstrated this phenomenon. We first propose a formal\ndefinition of knowledge, quantifying it for a given question as the fraction of\ncorrect-incorrect answer pairs where the correct one is ranked higher. This\ngives rise to external and internal knowledge, depending on the information\nused to score individual answer candidates: either the model's observable\ntoken-level probabilities or its intermediate computations. Hidden knowledge\narises when internal knowledge exceeds external knowledge. We then present a\ncase study, applying this framework to three popular open-weights LLMs in a\nclosed-book QA setup. Our results indicate that: (1) LLMs consistently encode\nmore factual knowledge internally than what they express externally, with an\naverage relative gap of 40%. (2) Surprisingly, some knowledge is so deeply\nhidden that a model can internally know an answer perfectly, yet fail to\ngenerate it even once, despite large-scale repeated sampling of 1,000 answers.\nThis reveals fundamental limitations in the generation capabilities of LLMs,\nwhich (3) put a practical constraint on scaling test-time compute via repeated\nanswer sampling in closed-book QA: significant performance improvements remain\ninaccessible because some answers are practically never sampled, yet if they\nwere, we would be guaranteed to rank them first."
                },
                "authors": [
                    {
                        "name": "Zorik Gekhman"
                    },
                    {
                        "name": "Eyal Ben David"
                    },
                    {
                        "name": "Hadas Orgad"
                    },
                    {
                        "name": "Eran Ofek"
                    },
                    {
                        "name": "Yonatan Belinkov"
                    },
                    {
                        "name": "Idan Szpektor"
                    },
                    {
                        "name": "Jonathan Herzig"
                    },
                    {
                        "name": "Roi Reichart"
                    }
                ],
                "author_detail": {
                    "name": "Roi Reichart"
                },
                "author": "Roi Reichart",
                "arxiv_comment": "Accepted to COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15299v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15299v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04442v1",
                "updated": "2025-08-06T13:30:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    30,
                    51,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T13:30:51Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    30,
                    51,
                    2,
                    218,
                    0
                ],
                "title": "Automated Generation of Curriculum-Aligned Multiple-Choice Questions for\n  Malaysian Secondary Mathematics Using Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Generation of Curriculum-Aligned Multiple-Choice Questions for\n  Malaysian Secondary Mathematics Using Generative AI"
                },
                "summary": "This paper addresses the critical need for scalable and high-quality\neducational assessment tools within the Malaysian education system. It\nhighlights the potential of Generative AI (GenAI) while acknowledging the\nsignificant challenges of ensuring factual accuracy and curriculum alignment,\nespecially for low-resource languages like Bahasa Melayu. This research\nintroduces and compares four incremental pipelines for generating Form 1\nMathematics multiple-choice questions (MCQs) in Bahasa Melayu using OpenAI's\nGPT-4o. The methods range from non-grounded prompting (structured and basic) to\nRetrieval-Augmented Generation (RAG) approaches (one using the LangChain\nframework, one implemented manually). The system is grounded in official\ncurriculum documents, including teacher-prepared notes and the yearly teaching\nplan (RPT). A dual-pronged automated evaluation framework is employed to assess\nthe generated questions. Curriculum alignment is measured using Semantic\nTextual Similarity (STS) against the RPT, while contextual validity is verified\nthrough a novel RAG-based Question-Answering (RAG-QA) method. The results\ndemonstrate that RAG-based pipelines significantly outperform non-grounded\nprompting methods, producing questions with higher curriculum alignment and\nfactual validity. The study further analyzes the trade-offs between the ease of\nimplementation of framework-based RAG and the fine-grained control offered by a\nmanual pipeline. This work presents a validated methodology for generating\ncurriculum-specific educational content in a low-resource language, introduces\na symbiotic RAG-QA evaluation technique, and provides actionable insights for\nthe development and deployment of practical EdTech solutions in Malaysia and\nsimilar regions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the critical need for scalable and high-quality\neducational assessment tools within the Malaysian education system. It\nhighlights the potential of Generative AI (GenAI) while acknowledging the\nsignificant challenges of ensuring factual accuracy and curriculum alignment,\nespecially for low-resource languages like Bahasa Melayu. This research\nintroduces and compares four incremental pipelines for generating Form 1\nMathematics multiple-choice questions (MCQs) in Bahasa Melayu using OpenAI's\nGPT-4o. The methods range from non-grounded prompting (structured and basic) to\nRetrieval-Augmented Generation (RAG) approaches (one using the LangChain\nframework, one implemented manually). The system is grounded in official\ncurriculum documents, including teacher-prepared notes and the yearly teaching\nplan (RPT). A dual-pronged automated evaluation framework is employed to assess\nthe generated questions. Curriculum alignment is measured using Semantic\nTextual Similarity (STS) against the RPT, while contextual validity is verified\nthrough a novel RAG-based Question-Answering (RAG-QA) method. The results\ndemonstrate that RAG-based pipelines significantly outperform non-grounded\nprompting methods, producing questions with higher curriculum alignment and\nfactual validity. The study further analyzes the trade-offs between the ease of\nimplementation of framework-based RAG and the fine-grained control offered by a\nmanual pipeline. This work presents a validated methodology for generating\ncurriculum-specific educational content in a low-resource language, introduces\na symbiotic RAG-QA evaluation technique, and provides actionable insights for\nthe development and deployment of practical EdTech solutions in Malaysia and\nsimilar regions."
                },
                "authors": [
                    {
                        "name": "Rohaizah Abdul Wahid"
                    },
                    {
                        "name": "Muhamad Said Nizamuddin Nadim"
                    },
                    {
                        "name": "Suliana Sulaiman"
                    },
                    {
                        "name": "Syahmi Akmal Shaharudin"
                    },
                    {
                        "name": "Muhammad Danial Jupikil"
                    },
                    {
                        "name": "Iqqwan Jasman Su Azlan Su"
                    }
                ],
                "author_detail": {
                    "name": "Iqqwan Jasman Su Azlan Su"
                },
                "author": "Iqqwan Jasman Su Azlan Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04440v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04440v1",
                "updated": "2025-08-06T13:28:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    28,
                    22,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T13:28:22Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    28,
                    22,
                    2,
                    218,
                    0
                ],
                "title": "StepFun-Formalizer: Unlocking the Autoformalization Potential of LLMs\n  through Knowledge-Reasoning Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StepFun-Formalizer: Unlocking the Autoformalization Potential of LLMs\n  through Knowledge-Reasoning Fusion"
                },
                "summary": "Autoformalization aims to translate natural-language mathematical statements\ninto a formal language. While LLMs have accelerated progress in this area,\nexisting methods still suffer from low accuracy. We identify two key abilities\nfor effective autoformalization: comprehensive mastery of formal-language\ndomain knowledge, and reasoning capability of natural language problem\nunderstanding and informal-formal alignment. Without the former, a model cannot\nidentify the correct formal objects; without the latter, it struggles to\ninterpret real-world contexts and map them precisely into formal expressions.\nTo address these gaps, we introduce ThinkingF, a data synthesis and training\npipeline that improves both abilities. First, we construct two datasets: one by\ndistilling and selecting large-scale examples rich in formal knowledge, and\nanother by generating informal-to-formal reasoning trajectories guided by\nexpert-designed templates. We then apply SFT and RLVR with these datasets to\nfurther fuse and refine the two abilities. The resulting 7B and 32B models\nexhibit both comprehensive formal knowledge and strong informal-to-formal\nreasoning. Notably, StepFun-Formalizer-32B achieves SOTA BEq@1 scores of 40.5%\non FormalMATH-Lite and 26.7% on ProverBench, surpassing all prior\ngeneral-purpose and specialized models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoformalization aims to translate natural-language mathematical statements\ninto a formal language. While LLMs have accelerated progress in this area,\nexisting methods still suffer from low accuracy. We identify two key abilities\nfor effective autoformalization: comprehensive mastery of formal-language\ndomain knowledge, and reasoning capability of natural language problem\nunderstanding and informal-formal alignment. Without the former, a model cannot\nidentify the correct formal objects; without the latter, it struggles to\ninterpret real-world contexts and map them precisely into formal expressions.\nTo address these gaps, we introduce ThinkingF, a data synthesis and training\npipeline that improves both abilities. First, we construct two datasets: one by\ndistilling and selecting large-scale examples rich in formal knowledge, and\nanother by generating informal-to-formal reasoning trajectories guided by\nexpert-designed templates. We then apply SFT and RLVR with these datasets to\nfurther fuse and refine the two abilities. The resulting 7B and 32B models\nexhibit both comprehensive formal knowledge and strong informal-to-formal\nreasoning. Notably, StepFun-Formalizer-32B achieves SOTA BEq@1 scores of 40.5%\non FormalMATH-Lite and 26.7% on ProverBench, surpassing all prior\ngeneral-purpose and specialized models."
                },
                "authors": [
                    {
                        "name": "Yutong Wu"
                    },
                    {
                        "name": "Di Huang"
                    },
                    {
                        "name": "Ruosi Wan"
                    },
                    {
                        "name": "Yue Peng"
                    },
                    {
                        "name": "Shijie Shang"
                    },
                    {
                        "name": "Chenrui Cao"
                    },
                    {
                        "name": "Lei Qi"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Zidong Du"
                    },
                    {
                        "name": "Jie Yan"
                    },
                    {
                        "name": "Xing Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xing Hu"
                },
                "author": "Xing Hu",
                "arxiv_comment": "24 pages, 17 figures, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04440v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04440v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00252v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00252v2",
                "updated": "2025-08-06T13:27:07Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    27,
                    7,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-01T01:44:56Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    1,
                    44,
                    56,
                    4,
                    213,
                    0
                ],
                "title": "TofuML: A Spatio-Physical Interactive Machine Learning Device for\n  Interactive Exploration of Machine Learning for Novices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TofuML: A Spatio-Physical Interactive Machine Learning Device for\n  Interactive Exploration of Machine Learning for Novices"
                },
                "summary": "We introduce TofuML, an interactive system designed to make machine learning\n(ML) concepts more accessible and engaging for non-expert users. Unlike\nconventional GUI-based systems, TofuML employs a physical and spatial interface\nconsisting of a small device and a paper mat, allowing users to train and\nevaluate sound classification models through intuitive, toy-like interactions.\nThrough two user studies -- a comparative study against a GUI-based version and\na public event deployment -- we investigated how TofuML impacts users'\nengagement in the ML model creation process, their ability to provide\nappropriate training data, and their conception of potential applications. Our\nresults indicated that TofuML enhanced user engagement compared to a GUI while\nlowering barriers for non-experts to engage with ML. Users demonstrated\ncreativity in conceiving diverse ML applications, revealing opportunities to\noptimize between conceptual understanding and user engagement. These findings\ncontribute to developing interactive ML systems/frameworks designed for a wide\nrange of users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce TofuML, an interactive system designed to make machine learning\n(ML) concepts more accessible and engaging for non-expert users. Unlike\nconventional GUI-based systems, TofuML employs a physical and spatial interface\nconsisting of a small device and a paper mat, allowing users to train and\nevaluate sound classification models through intuitive, toy-like interactions.\nThrough two user studies -- a comparative study against a GUI-based version and\na public event deployment -- we investigated how TofuML impacts users'\nengagement in the ML model creation process, their ability to provide\nappropriate training data, and their conception of potential applications. Our\nresults indicated that TofuML enhanced user engagement compared to a GUI while\nlowering barriers for non-experts to engage with ML. Users demonstrated\ncreativity in conceiving diverse ML applications, revealing opportunities to\noptimize between conceptual understanding and user engagement. These findings\ncontribute to developing interactive ML systems/frameworks designed for a wide\nrange of users."
                },
                "authors": [
                    {
                        "name": "Wataru Kawabe"
                    },
                    {
                        "name": "Hiroto Fukuda"
                    },
                    {
                        "name": "Akihisa Shitara"
                    },
                    {
                        "name": "Yuri Nakao"
                    },
                    {
                        "name": "Yusuke Sugano"
                    }
                ],
                "author_detail": {
                    "name": "Yusuke Sugano"
                },
                "author": "Yusuke Sugano",
                "arxiv_comment": "31 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00252v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00252v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11026v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11026v4",
                "updated": "2025-08-06T13:24:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    24,
                    9,
                    2,
                    218,
                    0
                ],
                "published": "2024-09-17T09:43:29Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    9,
                    43,
                    29,
                    1,
                    261,
                    0
                ],
                "title": "Prompt Obfuscation for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Obfuscation for Large Language Models"
                },
                "summary": "System prompts that include detailed instructions to describe the task\nperformed by the underlying LLM can easily transform foundation models into\ntools and services with minimal overhead. They are often considered\nintellectual property, similar to the code of a software product, because of\ntheir crucial impact on the utility. However, extracting system prompts is\neasily possible. As of today, there is no effective countermeasure to prevent\nthe stealing of system prompts, and all safeguarding efforts could be evaded.\nIn this work, we propose an alternative to conventional system prompts. We\nintroduce prompt obfuscation to prevent the extraction of the system prompt\nwith little overhead. The core idea is to find a representation of the original\nsystem prompt that leads to the same functionality, while the obfuscated system\nprompt does not contain any information that allows conclusions to be drawn\nabout the original system prompt. We evaluate our approach by comparing our\nobfuscated prompt output with the output of the original prompt, using eight\ndistinct metrics to measure the lexical, character-level, and semantic\nsimilarity. We show that the obfuscated version is constantly on par with the\noriginal one. We further perform three different deobfuscation attacks with\nvarying attacker knowledge--covering both black-box and white-box\nconditions--and show that in realistic attack scenarios an attacker is unable\nto extract meaningful information. Overall, we demonstrate that prompt\nobfuscation is an effective mechanism to safeguard the intellectual property of\na system prompt while maintaining the same utility as the original prompt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "System prompts that include detailed instructions to describe the task\nperformed by the underlying LLM can easily transform foundation models into\ntools and services with minimal overhead. They are often considered\nintellectual property, similar to the code of a software product, because of\ntheir crucial impact on the utility. However, extracting system prompts is\neasily possible. As of today, there is no effective countermeasure to prevent\nthe stealing of system prompts, and all safeguarding efforts could be evaded.\nIn this work, we propose an alternative to conventional system prompts. We\nintroduce prompt obfuscation to prevent the extraction of the system prompt\nwith little overhead. The core idea is to find a representation of the original\nsystem prompt that leads to the same functionality, while the obfuscated system\nprompt does not contain any information that allows conclusions to be drawn\nabout the original system prompt. We evaluate our approach by comparing our\nobfuscated prompt output with the output of the original prompt, using eight\ndistinct metrics to measure the lexical, character-level, and semantic\nsimilarity. We show that the obfuscated version is constantly on par with the\noriginal one. We further perform three different deobfuscation attacks with\nvarying attacker knowledge--covering both black-box and white-box\nconditions--and show that in realistic attack scenarios an attacker is unable\nto extract meaningful information. Overall, we demonstrate that prompt\nobfuscation is an effective mechanism to safeguard the intellectual property of\na system prompt while maintaining the same utility as the original prompt."
                },
                "authors": [
                    {
                        "name": "David Pape"
                    },
                    {
                        "name": "Sina Mavali"
                    },
                    {
                        "name": "Thorsten Eisenhofer"
                    },
                    {
                        "name": "Lea Schnherr"
                    }
                ],
                "author_detail": {
                    "name": "Lea Schnherr"
                },
                "author": "Lea Schnherr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11026v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11026v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09485v2",
                "updated": "2025-08-06T13:21:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    21,
                    11,
                    2,
                    218,
                    0
                ],
                "published": "2025-04-13T08:56:22Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    8,
                    56,
                    22,
                    6,
                    103,
                    0
                ],
                "title": "GenEDA: Towards Generative Netlist Functional Reasoning via Cross-Modal\n  Circuit Encoder-Decoder Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenEDA: Towards Generative Netlist Functional Reasoning via Cross-Modal\n  Circuit Encoder-Decoder Alignment"
                },
                "summary": "The success of foundation AI has motivated the research of circuit foundation\nmodels, which are customized to assist the integrated circuit (IC) design\nprocess. However, existing pre-trained circuit foundation models are typically\nlimited to standalone encoders for predictive tasks or decoders for generative\ntasks. These two model types are developed independently, operate on different\ncircuit modalities, and reside in separate latent spaces. This restricts their\nability to complement each other for more advanced capabilities. In this work,\nwe present GenEDA, the first framework that cross-modally aligns circuit\nencoders with decoders within a shared latent space. GenEDA bridges the gap\nbetween graph-based circuit representation learning and text-based large\nlanguage models (LLMs), enabling communication between their respective latent\nspaces. To achieve the alignment, we propose two paradigms to support both\nopen-source trainable LLMs and commercial frozen LLMs. We leverage this aligned\narchitecture to develop the first generative foundation model for netlists,\nunleashing LLMs' generative reasoning capability on the low-level and\nbit-blasted netlists. GenEDA enables three unprecedented generative netlist\nfunctional reasoning tasks, where it reversely generates high-level\nfunctionalities such as specifications and RTL code from low-level netlists.\nThese tasks move beyond traditional gate function classification to direct\ngeneration of full-circuit functionality. Experiments demonstrate that GenEDA\nsignificantly boosts advanced LLMs' (e.g., GPT and DeepSeek series) performance\nin all tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of foundation AI has motivated the research of circuit foundation\nmodels, which are customized to assist the integrated circuit (IC) design\nprocess. However, existing pre-trained circuit foundation models are typically\nlimited to standalone encoders for predictive tasks or decoders for generative\ntasks. These two model types are developed independently, operate on different\ncircuit modalities, and reside in separate latent spaces. This restricts their\nability to complement each other for more advanced capabilities. In this work,\nwe present GenEDA, the first framework that cross-modally aligns circuit\nencoders with decoders within a shared latent space. GenEDA bridges the gap\nbetween graph-based circuit representation learning and text-based large\nlanguage models (LLMs), enabling communication between their respective latent\nspaces. To achieve the alignment, we propose two paradigms to support both\nopen-source trainable LLMs and commercial frozen LLMs. We leverage this aligned\narchitecture to develop the first generative foundation model for netlists,\nunleashing LLMs' generative reasoning capability on the low-level and\nbit-blasted netlists. GenEDA enables three unprecedented generative netlist\nfunctional reasoning tasks, where it reversely generates high-level\nfunctionalities such as specifications and RTL code from low-level netlists.\nThese tasks move beyond traditional gate function classification to direct\ngeneration of full-circuit functionality. Experiments demonstrate that GenEDA\nsignificantly boosts advanced LLMs' (e.g., GPT and DeepSeek series) performance\nin all tasks."
                },
                "authors": [
                    {
                        "name": "Wenji Fang"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Shang Liu"
                    },
                    {
                        "name": "Zhiyao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyao Xie"
                },
                "author": "Zhiyao Xie",
                "arxiv_comment": "Accepted by ICCAD'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04428v1",
                "updated": "2025-08-06T13:16:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    16,
                    10,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T13:16:10Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    16,
                    10,
                    2,
                    218,
                    0
                ],
                "title": "\\textsc{SimInstruct}: A Responsible Tool for Collecting Scaffolding\n  Dialogues Between Experts and LLM-Simulated Novices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\\textsc{SimInstruct}: A Responsible Tool for Collecting Scaffolding\n  Dialogues Between Experts and LLM-Simulated Novices"
                },
                "summary": "High-quality, multi-turn instructional dialogues between novices and experts\nare essential for developing AI systems that support teaching, learning, and\ndecision-making. These dialogues often involve scaffolding -- the process by\nwhich an expert supports a novice's thinking through questions, feedback, and\nstep-by-step guidance. However, such data are scarce due to privacy concerns in\nrecording and the vulnerability inherent in help-seeking. We present\nSimInstruct, a scalable, expert-in-the-loop tool for collecting scaffolding\ndialogues. Using teaching development coaching as an example domain,\nSimInstruct simulates novice instructors via LLMs, varying their teaching\nchallenges and LLM's persona traits, while human experts provide multi-turn\nfeedback, reasoning, and instructional support. This design enables the\ncreation of realistic, pedagogically rich dialogues without requiring real\nnovice participants. Our results reveal that persona traits, such as\nextroversion and introversion, meaningfully influence how experts engage.\nCompared to real mentoring recordings, SimInstruct dialogues demonstrate\ncomparable pedagogical relevance and cognitive depth. Experts also reported the\nprocess as engaging and reflective, improving both data quality and their own\nprofessional insight. We further fine-tuned a LLaMA model to be an expert model\nusing the augmented dataset, which outperformed GPT-4o in instructional\nquality. Our analysis highlights GPT-4o's limitations in weak reflective\nquestioning, overuse of generic praise, a condescending tone, and a tendency to\noverwhelm novices with excessive suggestions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-quality, multi-turn instructional dialogues between novices and experts\nare essential for developing AI systems that support teaching, learning, and\ndecision-making. These dialogues often involve scaffolding -- the process by\nwhich an expert supports a novice's thinking through questions, feedback, and\nstep-by-step guidance. However, such data are scarce due to privacy concerns in\nrecording and the vulnerability inherent in help-seeking. We present\nSimInstruct, a scalable, expert-in-the-loop tool for collecting scaffolding\ndialogues. Using teaching development coaching as an example domain,\nSimInstruct simulates novice instructors via LLMs, varying their teaching\nchallenges and LLM's persona traits, while human experts provide multi-turn\nfeedback, reasoning, and instructional support. This design enables the\ncreation of realistic, pedagogically rich dialogues without requiring real\nnovice participants. Our results reveal that persona traits, such as\nextroversion and introversion, meaningfully influence how experts engage.\nCompared to real mentoring recordings, SimInstruct dialogues demonstrate\ncomparable pedagogical relevance and cognitive depth. Experts also reported the\nprocess as engaging and reflective, improving both data quality and their own\nprofessional insight. We further fine-tuned a LLaMA model to be an expert model\nusing the augmented dataset, which outperformed GPT-4o in instructional\nquality. Our analysis highlights GPT-4o's limitations in weak reflective\nquestioning, overuse of generic praise, a condescending tone, and a tendency to\noverwhelm novices with excessive suggestions."
                },
                "authors": [
                    {
                        "name": "Si Chen"
                    },
                    {
                        "name": "Izzy Molnar"
                    },
                    {
                        "name": "Ting Hua"
                    },
                    {
                        "name": "Peiyu Li"
                    },
                    {
                        "name": "Le Huy Khiem"
                    },
                    {
                        "name": "G. Alex Ambrose"
                    },
                    {
                        "name": "Jim Lang"
                    },
                    {
                        "name": "Ronald Metoyer"
                    },
                    {
                        "name": "Nitesh V. Chawla"
                    }
                ],
                "author_detail": {
                    "name": "Nitesh V. Chawla"
                },
                "author": "Nitesh V. Chawla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06917v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06917v2",
                "updated": "2025-08-06T13:15:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    15,
                    20,
                    2,
                    218,
                    0
                ],
                "published": "2024-11-11T12:20:57Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    12,
                    20,
                    57,
                    0,
                    316,
                    0
                ],
                "title": "Efficient Unsupervised Domain Adaptation Regression for Spatial-Temporal\n  Sensor Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Unsupervised Domain Adaptation Regression for Spatial-Temporal\n  Sensor Fusion"
                },
                "summary": "The growing deployment of low-cost, distributed sensor networks in\nenvironmental and biomedical domains has enabled continuous, large-scale health\nmonitoring. However, these systems often face challenges related to degraded\ndata quality caused by sensor drift, noise, and insufficient calibration --\nfactors that limit their reliability in real-world applications. Traditional\nmachine learning methods for sensor fusion and calibration rely on extensive\nfeature engineering and struggle to capture spatial-temporal dependencies or\nadapt to distribution shifts across varying deployment conditions. To address\nthese challenges, we propose a novel unsupervised domain adaptation (UDA)\nmethod tailored for regression tasks. Our proposed method integrates\neffectively with Spatial-Temporal Graph Neural Networks and leverages the\nalignment of perturbed inverse Gram matrices between source and target domains,\ndrawing inspiration from Tikhonov regularization. This approach enables\nscalable and efficient domain adaptation without requiring labeled data in the\ntarget domain. We validate our novel method on real-world datasets from two\ndistinct applications: air quality monitoring and EEG signal reconstruction.\nOur method achieves state-of-the-art performance which paves the way for more\nrobust and transferable sensor fusion models in both environmental and\nphysiological contexts. Our code is available at\nhttps://github.com/EPFL-IMOS/TikUDA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing deployment of low-cost, distributed sensor networks in\nenvironmental and biomedical domains has enabled continuous, large-scale health\nmonitoring. However, these systems often face challenges related to degraded\ndata quality caused by sensor drift, noise, and insufficient calibration --\nfactors that limit their reliability in real-world applications. Traditional\nmachine learning methods for sensor fusion and calibration rely on extensive\nfeature engineering and struggle to capture spatial-temporal dependencies or\nadapt to distribution shifts across varying deployment conditions. To address\nthese challenges, we propose a novel unsupervised domain adaptation (UDA)\nmethod tailored for regression tasks. Our proposed method integrates\neffectively with Spatial-Temporal Graph Neural Networks and leverages the\nalignment of perturbed inverse Gram matrices between source and target domains,\ndrawing inspiration from Tikhonov regularization. This approach enables\nscalable and efficient domain adaptation without requiring labeled data in the\ntarget domain. We validate our novel method on real-world datasets from two\ndistinct applications: air quality monitoring and EEG signal reconstruction.\nOur method achieves state-of-the-art performance which paves the way for more\nrobust and transferable sensor fusion models in both environmental and\nphysiological contexts. Our code is available at\nhttps://github.com/EPFL-IMOS/TikUDA."
                },
                "authors": [
                    {
                        "name": "Keivan Faghih Niresi"
                    },
                    {
                        "name": "Ismail Nejjar"
                    },
                    {
                        "name": "Olga Fink"
                    }
                ],
                "author_detail": {
                    "name": "Olga Fink"
                },
                "author": "Olga Fink",
                "arxiv_comment": "Accepted to IEEE Internet of Things Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06917v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06917v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04423v1",
                "updated": "2025-08-06T13:11:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    11,
                    17,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T13:11:17Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    11,
                    17,
                    2,
                    218,
                    0
                ],
                "title": "Evaluating, Synthesizing, and Enhancing for Customer Support\n  Conversation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating, Synthesizing, and Enhancing for Customer Support\n  Conversation"
                },
                "summary": "Effective customer support requires not only accurate problem solving but\nalso structured and empathetic communication aligned with professional\nstandards. However, existing dialogue datasets often lack strategic guidance,\nand real-world service data is difficult to access and annotate. To address\nthis, we introduce the task of Customer Support Conversation (CSC), aimed at\ntraining customer service agents to respond using well-defined support\nstrategies. We propose a structured CSC framework grounded in COPC guidelines,\ndefining five conversational stages and twelve strategies to guide high-quality\ninteractions. Based on this, we construct CSConv, an evaluation dataset of\n1,855 real-world customer-agent conversations rewritten using LLMs to reflect\ndeliberate strategy use, and annotated accordingly. Additionally, we develop a\nrole-playing approach that simulates strategy-rich conversations using\nLLM-powered roles aligned with the CSC framework, resulting in the training\ndataset RoleCS. Experiments show that fine-tuning strong LLMs on RoleCS\nsignificantly improves their ability to generate high-quality, strategy-aligned\nresponses on CSConv. Human evaluations further confirm gains in problem\nresolution. All code and data will be made publicly available at\nhttps://github.com/aliyun/qwen-dianjin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective customer support requires not only accurate problem solving but\nalso structured and empathetic communication aligned with professional\nstandards. However, existing dialogue datasets often lack strategic guidance,\nand real-world service data is difficult to access and annotate. To address\nthis, we introduce the task of Customer Support Conversation (CSC), aimed at\ntraining customer service agents to respond using well-defined support\nstrategies. We propose a structured CSC framework grounded in COPC guidelines,\ndefining five conversational stages and twelve strategies to guide high-quality\ninteractions. Based on this, we construct CSConv, an evaluation dataset of\n1,855 real-world customer-agent conversations rewritten using LLMs to reflect\ndeliberate strategy use, and annotated accordingly. Additionally, we develop a\nrole-playing approach that simulates strategy-rich conversations using\nLLM-powered roles aligned with the CSC framework, resulting in the training\ndataset RoleCS. Experiments show that fine-tuning strong LLMs on RoleCS\nsignificantly improves their ability to generate high-quality, strategy-aligned\nresponses on CSConv. Human evaluations further confirm gains in problem\nresolution. All code and data will be made publicly available at\nhttps://github.com/aliyun/qwen-dianjin."
                },
                "authors": [
                    {
                        "name": "Jie Zhu"
                    },
                    {
                        "name": "Huaixia Dou"
                    },
                    {
                        "name": "Junhui Li"
                    },
                    {
                        "name": "Lifan Guo"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Fang Kong"
                    }
                ],
                "author_detail": {
                    "name": "Fang Kong"
                },
                "author": "Fang Kong",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.08617v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.08617v5",
                "updated": "2025-08-06T13:02:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    2,
                    31,
                    2,
                    218,
                    0
                ],
                "published": "2023-12-14T02:42:15Z",
                "published_parsed": [
                    2023,
                    12,
                    14,
                    2,
                    42,
                    15,
                    3,
                    348,
                    0
                ],
                "title": "RTLCoder: Outperforming GPT-3.5 in Design RTL Generation with Our\n  Open-Source Dataset and Lightweight Solution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RTLCoder: Outperforming GPT-3.5 in Design RTL Generation with Our\n  Open-Source Dataset and Lightweight Solution"
                },
                "summary": "The automatic generation of RTL code (e.g., Verilog) using natural language\ninstructions and large language models (LLMs) has attracted significant\nresearch interest recently. However, most existing approaches heavily rely on\ncommercial LLMs such as ChatGPT, while open-source LLMs tailored for this\nspecific design generation task exhibit notably inferior performance. The\nabsence of high-quality open-source solutions restricts the flexibility and\ndata privacy of this emerging technique. In this study, we present a new\ncustomized LLM solution with a modest parameter count of only 7B, achieving\nbetter performance than GPT-3.5 on all representative benchmarks for RTL code\ngeneration. Especially, it outperforms GPT-4 in VerilogEval Machine benchmark.\nThis remarkable balance between accuracy and efficiency is made possible by\nleveraging our new RTL code dataset and a customized LLM algorithm, both of\nwhich have been made fully open-source.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The automatic generation of RTL code (e.g., Verilog) using natural language\ninstructions and large language models (LLMs) has attracted significant\nresearch interest recently. However, most existing approaches heavily rely on\ncommercial LLMs such as ChatGPT, while open-source LLMs tailored for this\nspecific design generation task exhibit notably inferior performance. The\nabsence of high-quality open-source solutions restricts the flexibility and\ndata privacy of this emerging technique. In this study, we present a new\ncustomized LLM solution with a modest parameter count of only 7B, achieving\nbetter performance than GPT-3.5 on all representative benchmarks for RTL code\ngeneration. Especially, it outperforms GPT-4 in VerilogEval Machine benchmark.\nThis remarkable balance between accuracy and efficiency is made possible by\nleveraging our new RTL code dataset and a customized LLM algorithm, both of\nwhich have been made fully open-source."
                },
                "authors": [
                    {
                        "name": "Shang Liu"
                    },
                    {
                        "name": "Wenji Fang"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Qijun Zhang"
                    },
                    {
                        "name": "Hongce Zhang"
                    },
                    {
                        "name": "Zhiyao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyao Xie"
                },
                "author": "Zhiyao Xie",
                "arxiv_doi": "10.1109/LAD62341.2024.10691788",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/LAD62341.2024.10691788",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.08617v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.08617v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This is the LAD Conference version of RTLCoder, for the TCAD\n  extension version, please refer to: arXiv:2312.08617v4",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04412v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04412v1",
                "updated": "2025-08-06T12:56:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    12,
                    56,
                    54,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T12:56:54Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    12,
                    56,
                    54,
                    2,
                    218,
                    0
                ],
                "title": "Beyond Pixels: Exploring DOM Downsampling for LLM-Based Web Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Pixels: Exploring DOM Downsampling for LLM-Based Web Agents"
                },
                "summary": "Frontier LLMs only recently enabled serviceable, autonomous web agents. At\nthat, a model poses as an instantaneous domain model backend. Ought to suggest\ninteraction, it is consulted with a web-based task and respective application\nstate. The key problem lies in application state serialisation\n$\\unicode{x2013}$ referred to as snapshot. State-of-the-art web agents are\npremised on grounded GUI snapshots, i.e., screenshots enhanced with visual\ncues. Not least to resemble human perception, but for images representing\nrelatively cheap means of model input. LLM vision still lag behind code\ninterpretation capabilities. DOM snapshots, which structurally resemble HTML,\nimpose a desired alternative. Vast model input token size, however, disables\nreliable implementation with web agents to date.\n  We propose D2Snap, a first-of-its-kind DOM downsampling algorithm. Based on a\nGPT-4o backend, we evaluate D2Snap on tasks sampled from the Online-Mind2Web\ndataset. The success rate of D2Snap-downsampled DOM snapshots (67%) matches a\ngrounded GUI snapshot baseline (65%) $\\unicode{x2013}$ within the same input\ntoken order of magnitude (1e3). Our best evaluated configurations\n$\\unicode{x2013}$ one token order above, but within the model's context window\n$\\unicode{x2013}$ outperform this baseline by 8%. Our evaluation, moreover,\nyields that DOM-inherent hierarchy embodies a strong UI feature for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frontier LLMs only recently enabled serviceable, autonomous web agents. At\nthat, a model poses as an instantaneous domain model backend. Ought to suggest\ninteraction, it is consulted with a web-based task and respective application\nstate. The key problem lies in application state serialisation\n$\\unicode{x2013}$ referred to as snapshot. State-of-the-art web agents are\npremised on grounded GUI snapshots, i.e., screenshots enhanced with visual\ncues. Not least to resemble human perception, but for images representing\nrelatively cheap means of model input. LLM vision still lag behind code\ninterpretation capabilities. DOM snapshots, which structurally resemble HTML,\nimpose a desired alternative. Vast model input token size, however, disables\nreliable implementation with web agents to date.\n  We propose D2Snap, a first-of-its-kind DOM downsampling algorithm. Based on a\nGPT-4o backend, we evaluate D2Snap on tasks sampled from the Online-Mind2Web\ndataset. The success rate of D2Snap-downsampled DOM snapshots (67%) matches a\ngrounded GUI snapshot baseline (65%) $\\unicode{x2013}$ within the same input\ntoken order of magnitude (1e3). Our best evaluated configurations\n$\\unicode{x2013}$ one token order above, but within the model's context window\n$\\unicode{x2013}$ outperform this baseline by 8%. Our evaluation, moreover,\nyields that DOM-inherent hierarchy embodies a strong UI feature for LLMs."
                },
                "authors": [
                    {
                        "name": "Thassilo M. Schiepanski"
                    },
                    {
                        "name": "Nicholas Pil"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas Pil"
                },
                "author": "Nicholas Pil",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04412v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04412v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16520v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16520v4",
                "updated": "2025-08-06T12:55:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    12,
                    55,
                    44,
                    2,
                    218,
                    0
                ],
                "published": "2024-10-21T21:21:29Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    21,
                    21,
                    29,
                    0,
                    295,
                    0
                ],
                "title": "AUTALIC: A Dataset for Anti-AUTistic Ableist Language In Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AUTALIC: A Dataset for Anti-AUTistic Ableist Language In Context"
                },
                "summary": "As our understanding of autism and ableism continues to increase, so does our\nunderstanding of ableist language towards autistic people. Such language poses\na significant challenge in NLP research due to its subtle and context-dependent\nnature. Yet, detecting anti-autistic ableist language remains underexplored,\nwith existing NLP tools often failing to capture its nuanced expressions. We\npresent AUTALIC, the first benchmark dataset dedicated to the detection of\nanti-autistic ableist language in context, addressing a significant gap in the\nfield. The dataset comprises 2,400 autism-related sentences collected from\nReddit, accompanied by surrounding context, and is annotated by trained experts\nwith backgrounds in neurodiversity. Our comprehensive evaluation reveals that\ncurrent language models, including state-of-the-art LLMs, struggle to reliably\nidentify anti-autistic ableism and align with human judgments, underscoring\ntheir limitations in this domain. We publicly release AUTALIC along with the\nindividual annotations which serve as a valuable resource to researchers\nworking on ableism, neurodiversity, and also studying disagreements in\nannotation tasks. This dataset serves as a crucial step towards developing more\ninclusive and context-aware NLP systems that better reflect diverse\nperspectives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As our understanding of autism and ableism continues to increase, so does our\nunderstanding of ableist language towards autistic people. Such language poses\na significant challenge in NLP research due to its subtle and context-dependent\nnature. Yet, detecting anti-autistic ableist language remains underexplored,\nwith existing NLP tools often failing to capture its nuanced expressions. We\npresent AUTALIC, the first benchmark dataset dedicated to the detection of\nanti-autistic ableist language in context, addressing a significant gap in the\nfield. The dataset comprises 2,400 autism-related sentences collected from\nReddit, accompanied by surrounding context, and is annotated by trained experts\nwith backgrounds in neurodiversity. Our comprehensive evaluation reveals that\ncurrent language models, including state-of-the-art LLMs, struggle to reliably\nidentify anti-autistic ableism and align with human judgments, underscoring\ntheir limitations in this domain. We publicly release AUTALIC along with the\nindividual annotations which serve as a valuable resource to researchers\nworking on ableism, neurodiversity, and also studying disagreements in\nannotation tasks. This dataset serves as a crucial step towards developing more\ninclusive and context-aware NLP systems that better reflect diverse\nperspectives."
                },
                "authors": [
                    {
                        "name": "Naba Rizvi"
                    },
                    {
                        "name": "Harper Strickland"
                    },
                    {
                        "name": "Daniel Gitelman"
                    },
                    {
                        "name": "Tristan Cooper"
                    },
                    {
                        "name": "Alexis Morales-Flores"
                    },
                    {
                        "name": "Michael Golden"
                    },
                    {
                        "name": "Aekta Kallepalli"
                    },
                    {
                        "name": "Akshat Alurkar"
                    },
                    {
                        "name": "Haaset Owens"
                    },
                    {
                        "name": "Saleha Ahmedi"
                    },
                    {
                        "name": "Isha Khirwadkar"
                    },
                    {
                        "name": "Imani Munyaka"
                    },
                    {
                        "name": "Nedjma Ousidhoum"
                    }
                ],
                "author_detail": {
                    "name": "Nedjma Ousidhoum"
                },
                "author": "Nedjma Ousidhoum",
                "arxiv_doi": "10.18653/v1/2025.acl-long.1022",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2025.acl-long.1022",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.16520v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16520v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "accepted to ACL main 2025, 9 pages, 5 figures, 7 tables",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15849v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15849v2",
                "updated": "2025-08-06T12:47:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    12,
                    47,
                    41,
                    2,
                    218,
                    0
                ],
                "published": "2025-05-19T20:17:37Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    20,
                    17,
                    37,
                    0,
                    139,
                    0
                ],
                "title": "What Lives? A meta-analysis of diverse opinions on the definition of\n  life",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Lives? A meta-analysis of diverse opinions on the definition of\n  life"
                },
                "summary": "The question of \"what is life?\" has challenged scientists and philosophers\nfor centuries, producing an array of definitions that reflect both the mystery\nof its emergence and the diversity of disciplinary perspectives brought to bear\non the question. Despite significant progress in our understanding of\nbiological systems, psychology, computation, and information theory, no single\ndefinition for life has yet achieved universal acceptance. This challenge\nbecomes increasingly urgent as advances in synthetic biology, artificial\nintelligence, and astrobiology challenge our traditional conceptions of what it\nmeans to be alive. We undertook a methodological approach that leverages large\nlanguage models (LLMs) to analyze a set of definitions of life provided by a\ncurated set of cross-disciplinary experts. We used a novel pairwise correlation\nanalysis to map the definitions into distinct feature vectors, followed by\nagglomerative clustering, intra-cluster semantic analysis, and t-SNE projection\nto reveal underlying conceptual archetypes. This methodology revealed a\ncontinuous landscape of the themes relating to the definition of life,\nsuggesting that what has historically been approached as a binary taxonomic\nproblem should be instead conceived as differentiated perspectives within a\nunified conceptual latent space. We offer a new methodological bridge between\nreductionist and holistic approaches to fundamental questions in science and\nphilosophy, demonstrating how computational semantic analysis can reveal\nconceptual patterns across disciplinary boundaries, and opening similar\npathways for addressing other contested definitional territories across the\nsciences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The question of \"what is life?\" has challenged scientists and philosophers\nfor centuries, producing an array of definitions that reflect both the mystery\nof its emergence and the diversity of disciplinary perspectives brought to bear\non the question. Despite significant progress in our understanding of\nbiological systems, psychology, computation, and information theory, no single\ndefinition for life has yet achieved universal acceptance. This challenge\nbecomes increasingly urgent as advances in synthetic biology, artificial\nintelligence, and astrobiology challenge our traditional conceptions of what it\nmeans to be alive. We undertook a methodological approach that leverages large\nlanguage models (LLMs) to analyze a set of definitions of life provided by a\ncurated set of cross-disciplinary experts. We used a novel pairwise correlation\nanalysis to map the definitions into distinct feature vectors, followed by\nagglomerative clustering, intra-cluster semantic analysis, and t-SNE projection\nto reveal underlying conceptual archetypes. This methodology revealed a\ncontinuous landscape of the themes relating to the definition of life,\nsuggesting that what has historically been approached as a binary taxonomic\nproblem should be instead conceived as differentiated perspectives within a\nunified conceptual latent space. We offer a new methodological bridge between\nreductionist and holistic approaches to fundamental questions in science and\nphilosophy, demonstrating how computational semantic analysis can reveal\nconceptual patterns across disciplinary boundaries, and opening similar\npathways for addressing other contested definitional territories across the\nsciences."
                },
                "authors": [
                    {
                        "name": "Reed Bender"
                    },
                    {
                        "name": "Karina Kofman"
                    },
                    {
                        "name": "Blaise Agera y Arcas"
                    },
                    {
                        "name": "Michael Levin"
                    }
                ],
                "author_detail": {
                    "name": "Michael Levin"
                },
                "author": "Michael Levin",
                "arxiv_comment": "54 pages, 4 figures, 2 tables, 11 supplemental figures, 3\n  supplemental tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15849v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15849v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.OT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.OT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.CB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04405v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04405v1",
                "updated": "2025-08-06T12:47:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    12,
                    47,
                    5,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T12:47:05Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    12,
                    47,
                    5,
                    2,
                    218,
                    0
                ],
                "title": "FlexQ: Efficient Post-training INT6 Quantization for LLM Serving via\n  Algorithm-System Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexQ: Efficient Post-training INT6 Quantization for LLM Serving via\n  Algorithm-System Co-Design"
                },
                "summary": "Large Language Models (LLMs) demonstrate exceptional performance but entail\nsignificant memory and computational costs, restricting their practical\ndeployment. While existing INT4/INT8 quantization reduces these costs, they\noften degrade accuracy or lack optimal efficiency. INT6 quantization offers a\nsuperior trade-off between model accuracy and inference efficiency, but lacks\nhardware support in modern GPUs, forcing emulation via higher-precision\narithmetic units that limit acceleration.\n  In this paper, we propose FlexQ, a novel post-training INT6 quantization\nframework combining algorithmic innovation with system-level optimizations.\nFlexQ employs uniform 6-bit weight quantization across all layers, with\nadaptive retention of 8-bit activations in layers identified through layer-wise\nsensitivity analysis. To maximize hardware efficiency, we develop a specialized\nhigh-performance GPU kernel supporting matrix multiplication for W6A6 and W6A8\nrepresentations via Binary Tensor Core (BTC) equivalents, effectively bypassing\nthe lack of native INT6 tensor cores. Evaluations on LLaMA models show FlexQ\nmaintains near-FP16 accuracy, with perplexity increases of no more than 0.05.\nThe proposed kernel achieves an average 1.39$\\times$ speedup over ABQ-LLM on\nLLaMA-2-70B linear layers. End-to-end, FlexQ delivers 1.33$\\times$ inference\nacceleration and 1.21$\\times$ memory savings over SmoothQuant. Code is released\nat https://github.com/FlyFoxPlayer/FlexQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate exceptional performance but entail\nsignificant memory and computational costs, restricting their practical\ndeployment. While existing INT4/INT8 quantization reduces these costs, they\noften degrade accuracy or lack optimal efficiency. INT6 quantization offers a\nsuperior trade-off between model accuracy and inference efficiency, but lacks\nhardware support in modern GPUs, forcing emulation via higher-precision\narithmetic units that limit acceleration.\n  In this paper, we propose FlexQ, a novel post-training INT6 quantization\nframework combining algorithmic innovation with system-level optimizations.\nFlexQ employs uniform 6-bit weight quantization across all layers, with\nadaptive retention of 8-bit activations in layers identified through layer-wise\nsensitivity analysis. To maximize hardware efficiency, we develop a specialized\nhigh-performance GPU kernel supporting matrix multiplication for W6A6 and W6A8\nrepresentations via Binary Tensor Core (BTC) equivalents, effectively bypassing\nthe lack of native INT6 tensor cores. Evaluations on LLaMA models show FlexQ\nmaintains near-FP16 accuracy, with perplexity increases of no more than 0.05.\nThe proposed kernel achieves an average 1.39$\\times$ speedup over ABQ-LLM on\nLLaMA-2-70B linear layers. End-to-end, FlexQ delivers 1.33$\\times$ inference\nacceleration and 1.21$\\times$ memory savings over SmoothQuant. Code is released\nat https://github.com/FlyFoxPlayer/FlexQ."
                },
                "authors": [
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Aining Jia"
                    },
                    {
                        "name": "Weifeng Bu"
                    },
                    {
                        "name": "Yushu Cai"
                    },
                    {
                        "name": "Kai Sheng"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Xin He"
                    }
                ],
                "author_detail": {
                    "name": "Xin He"
                },
                "author": "Xin He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04405v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04405v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04401v1",
                "updated": "2025-08-06T12:43:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    12,
                    43,
                    4,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T12:43:04Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    12,
                    43,
                    4,
                    2,
                    218,
                    0
                ],
                "title": "Why are LLMs' abilities emergent?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why are LLMs' abilities emergent?"
                },
                "summary": "The remarkable success of Large Language Models (LLMs) in generative tasks\nhas raised fundamental questions about the nature of their acquired\ncapabilities, which often appear to emerge unexpectedly without explicit\ntraining. This paper examines the emergent properties of Deep Neural Networks\n(DNNs) through both theoretical analysis and empirical observation, addressing\nthe epistemological challenge of \"creation without understanding\" that\ncharacterises contemporary AI development. We explore how the neural approach's\nreliance on nonlinear, stochastic processes fundamentally differs from symbolic\ncomputational paradigms, creating systems whose macro-level behaviours cannot\nbe analytically derived from micro-level neuron activities. Through analysis of\nscaling laws, grokking phenomena, and phase transitions in model capabilities,\nI demonstrate that emergent abilities arise from the complex dynamics of highly\nsensitive nonlinear systems rather than simply from parameter scaling alone. My\ninvestigation reveals that current debates over metrics, pre-training loss\nthresholds, and in-context learning miss the fundamental ontological nature of\nemergence in DNNs. I argue that these systems exhibit genuine emergent\nproperties analogous to those found in other complex natural phenomena, where\nsystemic capabilities emerge from cooperative interactions among simple\ncomponents without being reducible to their individual behaviours. The paper\nconcludes that understanding LLM capabilities requires recognising DNNs as a\nnew domain of complex dynamical systems governed by universal principles of\nemergence, similar to those operating in physics, chemistry, and biology. This\nperspective shifts the focus from purely phenomenological definitions of\nemergence to understanding the internal dynamic transformations that enable\nthese systems to acquire capabilities that transcend their individual\ncomponents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable success of Large Language Models (LLMs) in generative tasks\nhas raised fundamental questions about the nature of their acquired\ncapabilities, which often appear to emerge unexpectedly without explicit\ntraining. This paper examines the emergent properties of Deep Neural Networks\n(DNNs) through both theoretical analysis and empirical observation, addressing\nthe epistemological challenge of \"creation without understanding\" that\ncharacterises contemporary AI development. We explore how the neural approach's\nreliance on nonlinear, stochastic processes fundamentally differs from symbolic\ncomputational paradigms, creating systems whose macro-level behaviours cannot\nbe analytically derived from micro-level neuron activities. Through analysis of\nscaling laws, grokking phenomena, and phase transitions in model capabilities,\nI demonstrate that emergent abilities arise from the complex dynamics of highly\nsensitive nonlinear systems rather than simply from parameter scaling alone. My\ninvestigation reveals that current debates over metrics, pre-training loss\nthresholds, and in-context learning miss the fundamental ontological nature of\nemergence in DNNs. I argue that these systems exhibit genuine emergent\nproperties analogous to those found in other complex natural phenomena, where\nsystemic capabilities emerge from cooperative interactions among simple\ncomponents without being reducible to their individual behaviours. The paper\nconcludes that understanding LLM capabilities requires recognising DNNs as a\nnew domain of complex dynamical systems governed by universal principles of\nemergence, similar to those operating in physics, chemistry, and biology. This\nperspective shifts the focus from purely phenomenological definitions of\nemergence to understanding the internal dynamic transformations that enable\nthese systems to acquire capabilities that transcend their individual\ncomponents."
                },
                "authors": [
                    {
                        "name": "Vladimr Havlk"
                    }
                ],
                "author_detail": {
                    "name": "Vladimr Havlk"
                },
                "author": "Vladimr Havlk",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03329v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03329v2",
                "updated": "2025-08-06T12:41:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    12,
                    41,
                    21,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-05T11:15:06Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    11,
                    15,
                    6,
                    1,
                    217,
                    0
                ],
                "title": "Industrial LLM-based Code Optimization under Regulation: A\n  Mixture-of-Agents Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Industrial LLM-based Code Optimization under Regulation: A\n  Mixture-of-Agents Approach"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) for code optimization\nhave enabled industrial platforms to automate software performance engineering\nat unprecedented scale and speed. Yet, organizations in regulated industries\nface strict constraints on which LLMs they can use - many cannot utilize\ncommercial models due to data privacy regulations and compliance requirements,\ncreating a significant challenge for achieving high-quality code optimization\nwhile maintaining cost-effectiveness. We address this by implementing a\nMixture-of-Agents (MoA) approach that directly synthesizes code from multiple\nspecialized LLMs, comparing it against TurinTech AI's vanilla Genetic Algorithm\n(GA)-based ensemble system and individual LLM optimizers using real-world\nindustrial codebases. Our key contributions include: (1) First MoA application\nto industrial code optimization using real-world codebases; (2) Empirical\nevidence that MoA excels with open-source models, achieving 14.3% to 22.2% cost\nsavings and 28.6% to 32.2% faster optimization times for regulated\nenvironments; (3) Deployment guidelines demonstrating GA's advantage with\ncommercial models while both ensembles outperform individual LLMs; and (4)\nReal-world validation across 50 code snippets and seven LLM combinations,\ngenerating over 8,700 variants, addresses gaps in industrial LLM ensemble\nevaluation. This provides actionable guidance for organizations balancing\nregulatory compliance with optimization performance in production environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) for code optimization\nhave enabled industrial platforms to automate software performance engineering\nat unprecedented scale and speed. Yet, organizations in regulated industries\nface strict constraints on which LLMs they can use - many cannot utilize\ncommercial models due to data privacy regulations and compliance requirements,\ncreating a significant challenge for achieving high-quality code optimization\nwhile maintaining cost-effectiveness. We address this by implementing a\nMixture-of-Agents (MoA) approach that directly synthesizes code from multiple\nspecialized LLMs, comparing it against TurinTech AI's vanilla Genetic Algorithm\n(GA)-based ensemble system and individual LLM optimizers using real-world\nindustrial codebases. Our key contributions include: (1) First MoA application\nto industrial code optimization using real-world codebases; (2) Empirical\nevidence that MoA excels with open-source models, achieving 14.3% to 22.2% cost\nsavings and 28.6% to 32.2% faster optimization times for regulated\nenvironments; (3) Deployment guidelines demonstrating GA's advantage with\ncommercial models while both ensembles outperform individual LLMs; and (4)\nReal-world validation across 50 code snippets and seven LLM combinations,\ngenerating over 8,700 variants, addresses gaps in industrial LLM ensemble\nevaluation. This provides actionable guidance for organizations balancing\nregulatory compliance with optimization performance in production environments."
                },
                "authors": [
                    {
                        "name": "Mari Ashiga"
                    },
                    {
                        "name": "Vardan Voskanyan"
                    },
                    {
                        "name": "Fateme Dinmohammadi"
                    },
                    {
                        "name": "Jingzhi Gong"
                    },
                    {
                        "name": "Paul Brookes"
                    },
                    {
                        "name": "Matthew Truscott"
                    },
                    {
                        "name": "Rafail Giavrimis"
                    },
                    {
                        "name": "Mike Basios"
                    },
                    {
                        "name": "Leslie Kanthan"
                    },
                    {
                        "name": "Wei Jie"
                    }
                ],
                "author_detail": {
                    "name": "Wei Jie"
                },
                "author": "Wei Jie",
                "arxiv_comment": "Submitted to ASE'25 Industry Showcase",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03329v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03329v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04399v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04399v1",
                "updated": "2025-08-06T12:41:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    12,
                    41,
                    18,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T12:41:18Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    12,
                    41,
                    18,
                    2,
                    218,
                    0
                ],
                "title": "Improving Crash Data Quality with Large Language Models: Evidence from\n  Secondary Crash Narratives in Kentucky",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Crash Data Quality with Large Language Models: Evidence from\n  Secondary Crash Narratives in Kentucky"
                },
                "summary": "This study evaluates advanced natural language processing (NLP) techniques to\nenhance crash data quality by mining crash narratives, using secondary crash\nidentification in Kentucky as a case study. Drawing from 16,656 manually\nreviewed narratives from 2015-2022, with 3,803 confirmed secondary crashes, we\ncompare three model classes: zero-shot open-source large language models (LLMs)\n(LLaMA3:70B, DeepSeek-R1:70B, Qwen3:32B, Gemma3:27B); fine-tuned transformers\n(BERT, DistilBERT, RoBERTa, XLNet, Longformer); and traditional logistic\nregression as baseline. Models were calibrated on 2015-2021 data and tested on\n1,771 narratives from 2022. Fine-tuned transformers achieved superior\nperformance, with RoBERTa yielding the highest F1-score (0.90) and accuracy\n(95%). Zero-shot LLaMA3:70B reached a comparable F1 of 0.86 but required 139\nminutes of inference; the logistic baseline lagged well behind (F1:0.66). LLMs\nexcelled in recall for some variants (e.g., GEMMA3:27B at 0.94) but incurred\nhigh computational costs (up to 723 minutes for DeepSeek-R1:70B), while\nfine-tuned models processed the test set in seconds after brief training.\nFurther analysis indicated that mid-sized LLMs (e.g., DeepSeek-R1:32B) can\nrival larger counterparts in performance while reducing runtime, suggesting\nopportunities for optimized deployments. Results highlight trade-offs between\naccuracy, efficiency, and data requirements, with fine-tuned transformer models\nbalancing precision and recall effectively on Kentucky data. Practical\ndeployment considerations emphasize privacy-preserving local deployment,\nensemble approaches for improved accuracy, and incremental processing for\nscalability, providing a replicable scheme for enhancing crash-data quality\nwith advanced NLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study evaluates advanced natural language processing (NLP) techniques to\nenhance crash data quality by mining crash narratives, using secondary crash\nidentification in Kentucky as a case study. Drawing from 16,656 manually\nreviewed narratives from 2015-2022, with 3,803 confirmed secondary crashes, we\ncompare three model classes: zero-shot open-source large language models (LLMs)\n(LLaMA3:70B, DeepSeek-R1:70B, Qwen3:32B, Gemma3:27B); fine-tuned transformers\n(BERT, DistilBERT, RoBERTa, XLNet, Longformer); and traditional logistic\nregression as baseline. Models were calibrated on 2015-2021 data and tested on\n1,771 narratives from 2022. Fine-tuned transformers achieved superior\nperformance, with RoBERTa yielding the highest F1-score (0.90) and accuracy\n(95%). Zero-shot LLaMA3:70B reached a comparable F1 of 0.86 but required 139\nminutes of inference; the logistic baseline lagged well behind (F1:0.66). LLMs\nexcelled in recall for some variants (e.g., GEMMA3:27B at 0.94) but incurred\nhigh computational costs (up to 723 minutes for DeepSeek-R1:70B), while\nfine-tuned models processed the test set in seconds after brief training.\nFurther analysis indicated that mid-sized LLMs (e.g., DeepSeek-R1:32B) can\nrival larger counterparts in performance while reducing runtime, suggesting\nopportunities for optimized deployments. Results highlight trade-offs between\naccuracy, efficiency, and data requirements, with fine-tuned transformer models\nbalancing precision and recall effectively on Kentucky data. Practical\ndeployment considerations emphasize privacy-preserving local deployment,\nensemble approaches for improved accuracy, and incremental processing for\nscalability, providing a replicable scheme for enhancing crash-data quality\nwith advanced NLP."
                },
                "authors": [
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Mei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Mei Chen"
                },
                "author": "Mei Chen",
                "arxiv_comment": "19 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04399v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04399v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01602v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01602v2",
                "updated": "2025-08-06T12:04:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    12,
                    4,
                    27,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-03T05:38:14Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    5,
                    38,
                    14,
                    6,
                    215,
                    0
                ],
                "title": "Enhancing Zero-Shot Brain Tumor Subtype Classification via Fine-Grained\n  Patch-Text Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Zero-Shot Brain Tumor Subtype Classification via Fine-Grained\n  Patch-Text Alignment"
                },
                "summary": "The fine-grained classification of brain tumor subtypes from\nhistopathological whole slide images is highly challenging due to subtle\nmorphological variations and the scarcity of annotated data. Although\nvision-language models have enabled promising zero-shot classification, their\nability to capture fine-grained pathological features remains limited,\nresulting in suboptimal subtype discrimination. To address these challenges, we\npropose the Fine-Grained Patch Alignment Network (FG-PAN), a novel zero-shot\nframework tailored for digital pathology. FG-PAN consists of two key modules:\n(1) a local feature refinement module that enhances patch-level visual features\nby modeling spatial relationships among representative patches, and (2) a\nfine-grained text description generation module that leverages large language\nmodels to produce pathology-aware, class-specific semantic prototypes. By\naligning refined visual features with LLM-generated fine-grained descriptions,\nFG-PAN effectively increases class separability in both visual and semantic\nspaces. Extensive experiments on multiple public pathology datasets, including\nEBRAINS and TCGA, demonstrate that FG-PAN achieves state-of-the-art performance\nand robust generalization in zero-shot brain tumor subtype classification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fine-grained classification of brain tumor subtypes from\nhistopathological whole slide images is highly challenging due to subtle\nmorphological variations and the scarcity of annotated data. Although\nvision-language models have enabled promising zero-shot classification, their\nability to capture fine-grained pathological features remains limited,\nresulting in suboptimal subtype discrimination. To address these challenges, we\npropose the Fine-Grained Patch Alignment Network (FG-PAN), a novel zero-shot\nframework tailored for digital pathology. FG-PAN consists of two key modules:\n(1) a local feature refinement module that enhances patch-level visual features\nby modeling spatial relationships among representative patches, and (2) a\nfine-grained text description generation module that leverages large language\nmodels to produce pathology-aware, class-specific semantic prototypes. By\naligning refined visual features with LLM-generated fine-grained descriptions,\nFG-PAN effectively increases class separability in both visual and semantic\nspaces. Extensive experiments on multiple public pathology datasets, including\nEBRAINS and TCGA, demonstrate that FG-PAN achieves state-of-the-art performance\nand robust generalization in zero-shot brain tumor subtype classification."
                },
                "authors": [
                    {
                        "name": "Lubin Gan"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Linhao Qu"
                    },
                    {
                        "name": "Yijun Wang"
                    },
                    {
                        "name": "Siying Wu"
                    },
                    {
                        "name": "Xiaoyan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyan Sun"
                },
                "author": "Xiaoyan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01602v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01602v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04353v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04353v1",
                "updated": "2025-08-06T11:48:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    48,
                    51,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T11:48:51Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    48,
                    51,
                    2,
                    218,
                    0
                ],
                "title": "LUST: A Multi-Modal Framework with Hierarchical LLM-based Scoring for\n  Learned Thematic Significance Tracking in Multimedia Content",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LUST: A Multi-Modal Framework with Hierarchical LLM-based Scoring for\n  Learned Thematic Significance Tracking in Multimedia Content"
                },
                "summary": "This paper introduces the Learned User Significance Tracker (LUST), a\nframework designed to analyze video content and quantify the thematic relevance\nof its segments in relation to a user-provided textual description of\nsignificance. LUST leverages a multi-modal analytical pipeline, integrating\nvisual cues from video frames with textual information extracted via Automatic\nSpeech Recognition (ASR) from the audio track. The core innovation lies in a\nhierarchical, two-stage relevance scoring mechanism employing Large Language\nModels (LLMs). An initial \"direct relevance\" score, $S_{d,i}$, assesses\nindividual segments based on immediate visual and auditory content against the\ntheme. This is followed by a \"contextual relevance\" score, $S_{c,i}$, that\nrefines the assessment by incorporating the temporal progression of preceding\nthematic scores, allowing the model to understand evolving narratives. The LUST\nframework aims to provide a nuanced, temporally-aware measure of user-defined\nsignificance, outputting an annotated video with visualized relevance scores\nand comprehensive analytical logs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces the Learned User Significance Tracker (LUST), a\nframework designed to analyze video content and quantify the thematic relevance\nof its segments in relation to a user-provided textual description of\nsignificance. LUST leverages a multi-modal analytical pipeline, integrating\nvisual cues from video frames with textual information extracted via Automatic\nSpeech Recognition (ASR) from the audio track. The core innovation lies in a\nhierarchical, two-stage relevance scoring mechanism employing Large Language\nModels (LLMs). An initial \"direct relevance\" score, $S_{d,i}$, assesses\nindividual segments based on immediate visual and auditory content against the\ntheme. This is followed by a \"contextual relevance\" score, $S_{c,i}$, that\nrefines the assessment by incorporating the temporal progression of preceding\nthematic scores, allowing the model to understand evolving narratives. The LUST\nframework aims to provide a nuanced, temporally-aware measure of user-defined\nsignificance, outputting an annotated video with visualized relevance scores\nand comprehensive analytical logs."
                },
                "authors": [
                    {
                        "name": "Anderson de Lima Luiz"
                    }
                ],
                "author_detail": {
                    "name": "Anderson de Lima Luiz"
                },
                "author": "Anderson de Lima Luiz",
                "arxiv_comment": "5 pages and 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04353v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04353v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04350v1",
                "updated": "2025-08-06T11:42:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    42,
                    54,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T11:42:54Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    42,
                    54,
                    2,
                    218,
                    0
                ],
                "title": "Chain of Questions: Guiding Multimodal Curiosity in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Questions: Guiding Multimodal Curiosity in Language Models"
                },
                "summary": "Reasoning capabilities in large language models (LLMs) have substantially\nadvanced through methods such as chain-of-thought and explicit step-by-step\nexplanations. However, these improvements have not yet fully transitioned to\nmultimodal contexts, where models must proactively decide which sensory\nmodalities such as vision, audio, or spatial perception to engage when\ninteracting with complex real-world environments. In this paper, we introduce\nthe Chain of Questions (CoQ) framework, a curiosity-driven reasoning approach\nthat encourages multimodal language models to dynamically generate targeted\nquestions regarding their surroundings. These generated questions guide the\nmodel to selectively activate relevant modalities, thereby gathering critical\ninformation necessary for accurate reasoning and response generation. We\nevaluate our framework on a novel multimodal benchmark dataset, assembled by\nintegrating WebGPT, ScienceQA, AVSD, and ScanQA datasets. Experimental results\ndemonstrate that our CoQ method improves a foundation model's ability to\neffectively identify and integrate pertinent sensory information. This leads to\nimproved accuracy, interpretability, and alignment of the reasoning process\nwith diverse multimodal tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning capabilities in large language models (LLMs) have substantially\nadvanced through methods such as chain-of-thought and explicit step-by-step\nexplanations. However, these improvements have not yet fully transitioned to\nmultimodal contexts, where models must proactively decide which sensory\nmodalities such as vision, audio, or spatial perception to engage when\ninteracting with complex real-world environments. In this paper, we introduce\nthe Chain of Questions (CoQ) framework, a curiosity-driven reasoning approach\nthat encourages multimodal language models to dynamically generate targeted\nquestions regarding their surroundings. These generated questions guide the\nmodel to selectively activate relevant modalities, thereby gathering critical\ninformation necessary for accurate reasoning and response generation. We\nevaluate our framework on a novel multimodal benchmark dataset, assembled by\nintegrating WebGPT, ScienceQA, AVSD, and ScanQA datasets. Experimental results\ndemonstrate that our CoQ method improves a foundation model's ability to\neffectively identify and integrate pertinent sensory information. This leads to\nimproved accuracy, interpretability, and alignment of the reasoning process\nwith diverse multimodal tasks."
                },
                "authors": [
                    {
                        "name": "Nima Iji"
                    },
                    {
                        "name": "Kia Dashtipour"
                    }
                ],
                "author_detail": {
                    "name": "Kia Dashtipour"
                },
                "author": "Kia Dashtipour",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04349v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04349v1",
                "updated": "2025-08-06T11:42:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    42,
                    47,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T11:42:47Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    42,
                    47,
                    2,
                    218,
                    0
                ],
                "title": "GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy\n  Entropy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy\n  Entropy"
                },
                "summary": "Reinforcement learning (RL) with algorithms like Group Relative Policy\nOptimization (GRPO) improves Large Language Model (LLM) reasoning, but is\nlimited by a coarse-grained credit assignment that applies a uniform reward to\nall tokens in a sequence. This is a major flaw in long-chain reasoning tasks.\nThis paper solves this with \\textbf{Dynamic Entropy Weighting}. Our core idea\nis that high-entropy tokens in correct responses can guide the policy toward a\nhigher performance ceiling. This allows us to create more fine-grained reward\nsignals for precise policy updates via two ways: 1) \\textbf{Group Token Policy\nOptimization} (\\textbf{GTPO}), we assigns a entropy-weighted reward to each\ntoken for fine-grained credit assignment. 2) \\textbf{Sequence-Level Group\nRelative Policy Optimization} (\\textbf{GRPO-S}), we assigns a entropy-weighted\nreward to each sequence based on its average token entropy. Experiments show\nour methods significantly outperform the strong DAPO baseline. The results\nconfirm that our entropy-weighting mechanism is the key driver of this\nperformance boost, offering a better path to enhance deep reasoning in models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) with algorithms like Group Relative Policy\nOptimization (GRPO) improves Large Language Model (LLM) reasoning, but is\nlimited by a coarse-grained credit assignment that applies a uniform reward to\nall tokens in a sequence. This is a major flaw in long-chain reasoning tasks.\nThis paper solves this with \\textbf{Dynamic Entropy Weighting}. Our core idea\nis that high-entropy tokens in correct responses can guide the policy toward a\nhigher performance ceiling. This allows us to create more fine-grained reward\nsignals for precise policy updates via two ways: 1) \\textbf{Group Token Policy\nOptimization} (\\textbf{GTPO}), we assigns a entropy-weighted reward to each\ntoken for fine-grained credit assignment. 2) \\textbf{Sequence-Level Group\nRelative Policy Optimization} (\\textbf{GRPO-S}), we assigns a entropy-weighted\nreward to each sequence based on its average token entropy. Experiments show\nour methods significantly outperform the strong DAPO baseline. The results\nconfirm that our entropy-weighting mechanism is the key driver of this\nperformance boost, offering a better path to enhance deep reasoning in models."
                },
                "authors": [
                    {
                        "name": "Hongze Tan"
                    },
                    {
                        "name": "Jianfei Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Pan"
                },
                "author": "Jianfei Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04349v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04349v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06382v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06382v4",
                "updated": "2025-08-06T11:34:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    34,
                    54,
                    2,
                    218,
                    0
                ],
                "published": "2025-06-04T23:28:39Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    23,
                    28,
                    39,
                    2,
                    155,
                    0
                ],
                "title": "On the Fundamental Impossibility of Hallucination Control in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Fundamental Impossibility of Hallucination Control in Large\n  Language Models"
                },
                "summary": "This paper establishes a fundamental impossibility theorem: no LLM capable\nperforming non-trivial knowledge aggregation can simultaneously achieve\ntruthful (internally consistent) knowledge representation, semantic information\nconservation, complete revelation of relevant knowledge, and\nknowledge-constrained optimality. This impossibility is not an engineering\nlimitation but arises from the mathematical structure of information\naggregation itself. We establish this result by describing the inference\nprocess as an auction of ideas, where distributed components compete exploiting\ntheir partial knowledge to shape responses. The proof spans three independent\nmathematical domains: mechanism design theory (Green-Laffont), the theory of\nproper scoring rules (Savage), and direct architectural analysis of\ntransformers (Log-Sum-Exp convexity). In particular, we show how in the\nstrictly concave settings the score of an aggregate of diverse beliefs strictly\nexceeds the sum of individual scores. That gap may quantify the creation of\nunattributable certainty or overconfidence -- the mathematical origin of both\nhallucination and creativity, or imagination.\n  To support this analysis, we introduce the complementary concepts of the\nsemantic information measure and the emergence operator to model bounded\nreasoning in a general setting. We prove that while bounded reasoning generates\naccessible information, providing valuable insights and inspirations, idealized\nreasoning strictly preserves semantic content. By demonstrating that\nhallucination and imagination are mathematically identical phenomena-grounded\nin the necessary violation of information conservation-this paper offers a\nprincipled foundation for managing these behaviors in advanced AI systems.\nFinally, we present some speculative ideas to inspire evaluation and\nrefinements of the proposed theory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper establishes a fundamental impossibility theorem: no LLM capable\nperforming non-trivial knowledge aggregation can simultaneously achieve\ntruthful (internally consistent) knowledge representation, semantic information\nconservation, complete revelation of relevant knowledge, and\nknowledge-constrained optimality. This impossibility is not an engineering\nlimitation but arises from the mathematical structure of information\naggregation itself. We establish this result by describing the inference\nprocess as an auction of ideas, where distributed components compete exploiting\ntheir partial knowledge to shape responses. The proof spans three independent\nmathematical domains: mechanism design theory (Green-Laffont), the theory of\nproper scoring rules (Savage), and direct architectural analysis of\ntransformers (Log-Sum-Exp convexity). In particular, we show how in the\nstrictly concave settings the score of an aggregate of diverse beliefs strictly\nexceeds the sum of individual scores. That gap may quantify the creation of\nunattributable certainty or overconfidence -- the mathematical origin of both\nhallucination and creativity, or imagination.\n  To support this analysis, we introduce the complementary concepts of the\nsemantic information measure and the emergence operator to model bounded\nreasoning in a general setting. We prove that while bounded reasoning generates\naccessible information, providing valuable insights and inspirations, idealized\nreasoning strictly preserves semantic content. By demonstrating that\nhallucination and imagination are mathematically identical phenomena-grounded\nin the necessary violation of information conservation-this paper offers a\nprincipled foundation for managing these behaviors in advanced AI systems.\nFinally, we present some speculative ideas to inspire evaluation and\nrefinements of the proposed theory."
                },
                "authors": [
                    {
                        "name": "Micha P. Karpowicz"
                    }
                ],
                "author_detail": {
                    "name": "Micha P. Karpowicz"
                },
                "author": "Micha P. Karpowicz",
                "arxiv_comment": "cleared mathematics, proofs and ideas explained, added missing\n  definitions and axioms, discussion and speculation section added",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06382v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06382v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03703v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03703v4",
                "updated": "2025-08-07T13:34:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    13,
                    34,
                    51,
                    3,
                    219,
                    0
                ],
                "published": "2025-07-04T16:38:09Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    16,
                    38,
                    9,
                    4,
                    185,
                    0
                ],
                "title": "Sign Spotting Disambiguation using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sign Spotting Disambiguation using Large Language Models"
                },
                "summary": "Sign spotting, the task of identifying and localizing individual signs within\ncontinuous sign language video, plays a pivotal role in scaling dataset\nannotations and addressing the severe data scarcity issue in sign language\ntranslation. While automatic sign spotting holds great promise for enabling\nframe-level supervision at scale, it grapples with challenges such as\nvocabulary inflexibility and ambiguity inherent in continuous sign streams.\nHence, we introduce a novel, training-free framework that integrates Large\nLanguage Models (LLMs) to significantly enhance sign spotting quality. Our\napproach extracts global spatio-temporal and hand shape features, which are\nthen matched against a large-scale sign dictionary using dynamic time warping\nand cosine similarity. This dictionary-based matching inherently offers\nsuperior vocabulary flexibility without requiring model retraining. To mitigate\nnoise and ambiguity from the matching process, an LLM performs context-aware\ngloss disambiguation via beam search, notably without fine-tuning. Extensive\nexperiments on both synthetic and real-world sign language datasets demonstrate\nour method's superior accuracy and sentence fluency compared to traditional\napproaches, highlighting the potential of LLMs in advancing sign spotting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sign spotting, the task of identifying and localizing individual signs within\ncontinuous sign language video, plays a pivotal role in scaling dataset\nannotations and addressing the severe data scarcity issue in sign language\ntranslation. While automatic sign spotting holds great promise for enabling\nframe-level supervision at scale, it grapples with challenges such as\nvocabulary inflexibility and ambiguity inherent in continuous sign streams.\nHence, we introduce a novel, training-free framework that integrates Large\nLanguage Models (LLMs) to significantly enhance sign spotting quality. Our\napproach extracts global spatio-temporal and hand shape features, which are\nthen matched against a large-scale sign dictionary using dynamic time warping\nand cosine similarity. This dictionary-based matching inherently offers\nsuperior vocabulary flexibility without requiring model retraining. To mitigate\nnoise and ambiguity from the matching process, an LLM performs context-aware\ngloss disambiguation via beam search, notably without fine-tuning. Extensive\nexperiments on both synthetic and real-world sign language datasets demonstrate\nour method's superior accuracy and sentence fluency compared to traditional\napproaches, highlighting the potential of LLMs in advancing sign spotting."
                },
                "authors": [
                    {
                        "name": "JianHe Low"
                    },
                    {
                        "name": "Ozge Mercanoglu Sincan"
                    },
                    {
                        "name": "Richard Bowden"
                    }
                ],
                "author_detail": {
                    "name": "Richard Bowden"
                },
                "author": "Richard Bowden",
                "arxiv_comment": "Accepted in the international conference on Intelligent Virtual\n  Agents (IVA Adjunct)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03703v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03703v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04339v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04339v1",
                "updated": "2025-08-06T11:33:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    33,
                    35,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T11:33:35Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    33,
                    35,
                    2,
                    218,
                    0
                ],
                "title": "Deliberative Reasoning Network: An Uncertainty-Driven Paradigm for\n  Belief-Tracked Inference with Pretrained Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deliberative Reasoning Network: An Uncertainty-Driven Paradigm for\n  Belief-Tracked Inference with Pretrained Language Models"
                },
                "summary": "Large language models often fail at logical reasoning when semantic\nheuristics conflict with decisive evidence - a phenomenon we term cognitive\ntraps. To address this fundamental limitation, we introduce the Deliberative\nReasoning Network (DRN), a novel paradigm that reframes logical reasoning from\nprobability maximization to uncertainty minimization. Instead of asking \"Which\nanswer is most likely?\", DRN asks \"Which hypothesis has the most internally\nconsistent evidence?\". DRN achieves intrinsic interpretability by explicitly\ntracking belief states and quantifying epistemic uncertainty for competing\nhypotheses through an iterative evidence synthesis process. We validate our\napproach through two complementary architectures - a bespoke discriminative\nmodel that embodies the core uncertainty minimization principle, and a\nlightweight verification module that enhances existing generative LLMs.\nEvaluated on LCR-1000, our new adversarial reasoning benchmark designed to\nexpose cognitive traps, the bespoke DRN achieves up to 15.2% improvement over\nstandard baselines. When integrated as a parameter-efficient verifier with\nMistral-7B, our hybrid system boosts accuracy from 20% to 80% on the most\nchallenging problems. Critically, DRN demonstrates strong zero-shot\ngeneralization, improving TruthfulQA performance by 23.6% without additional\ntraining, indicating that uncertainty-driven deliberation learns transferable\nreasoning principles. We position DRN as a foundational, verifiable System 2\nreasoning component for building more trustworthy AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models often fail at logical reasoning when semantic\nheuristics conflict with decisive evidence - a phenomenon we term cognitive\ntraps. To address this fundamental limitation, we introduce the Deliberative\nReasoning Network (DRN), a novel paradigm that reframes logical reasoning from\nprobability maximization to uncertainty minimization. Instead of asking \"Which\nanswer is most likely?\", DRN asks \"Which hypothesis has the most internally\nconsistent evidence?\". DRN achieves intrinsic interpretability by explicitly\ntracking belief states and quantifying epistemic uncertainty for competing\nhypotheses through an iterative evidence synthesis process. We validate our\napproach through two complementary architectures - a bespoke discriminative\nmodel that embodies the core uncertainty minimization principle, and a\nlightweight verification module that enhances existing generative LLMs.\nEvaluated on LCR-1000, our new adversarial reasoning benchmark designed to\nexpose cognitive traps, the bespoke DRN achieves up to 15.2% improvement over\nstandard baselines. When integrated as a parameter-efficient verifier with\nMistral-7B, our hybrid system boosts accuracy from 20% to 80% on the most\nchallenging problems. Critically, DRN demonstrates strong zero-shot\ngeneralization, improving TruthfulQA performance by 23.6% without additional\ntraining, indicating that uncertainty-driven deliberation learns transferable\nreasoning principles. We position DRN as a foundational, verifiable System 2\nreasoning component for building more trustworthy AI systems."
                },
                "authors": [
                    {
                        "name": "Anran Xu"
                    },
                    {
                        "name": "Jincheng Wang"
                    },
                    {
                        "name": "Baigen Cai"
                    },
                    {
                        "name": "Tao Wen"
                    }
                ],
                "author_detail": {
                    "name": "Tao Wen"
                },
                "author": "Tao Wen",
                "arxiv_comment": "8 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04339v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04339v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02118v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02118v5",
                "updated": "2025-08-06T11:31:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    31,
                    36,
                    2,
                    218,
                    0
                ],
                "published": "2025-05-04T14:00:04Z",
                "published_parsed": [
                    2025,
                    5,
                    4,
                    14,
                    0,
                    4,
                    6,
                    124,
                    0
                ],
                "title": "Adversarial Cooperative Rationalization: The Risk of Spurious\n  Correlations in Even Clean Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial Cooperative Rationalization: The Risk of Spurious\n  Correlations in Even Clean Datasets"
                },
                "summary": "This study investigates the self-rationalization framework constructed with a\ncooperative game, where a generator initially extracts the most informative\nsegment from raw input, and a subsequent predictor utilizes the selected subset\nfor its input. The generator and predictor are trained collaboratively to\nmaximize prediction accuracy. In this paper, we first uncover a potential\ncaveat: such a cooperative game could unintentionally introduce a sampling bias\nduring rationale extraction. Specifically, the generator might inadvertently\ncreate an incorrect correlation between the selected rationale candidate and\nthe label, even when they are semantically unrelated in the original dataset.\nSubsequently, we elucidate the origins of this bias using both detailed\ntheoretical analysis and empirical evidence. Our findings suggest a direction\nfor inspecting these correlations through attacks, based on which we further\nintroduce an instruction to prevent the predictor from learning the\ncorrelations. Through experiments on six text classification datasets and two\ngraph classification datasets using three network architectures (GRUs, BERT,\nand GCN), we show that our method not only significantly outperforms recent\nrationalization methods, but also achieves comparable or even better results\nthan a representative LLM (llama3.1-8b-instruct).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the self-rationalization framework constructed with a\ncooperative game, where a generator initially extracts the most informative\nsegment from raw input, and a subsequent predictor utilizes the selected subset\nfor its input. The generator and predictor are trained collaboratively to\nmaximize prediction accuracy. In this paper, we first uncover a potential\ncaveat: such a cooperative game could unintentionally introduce a sampling bias\nduring rationale extraction. Specifically, the generator might inadvertently\ncreate an incorrect correlation between the selected rationale candidate and\nthe label, even when they are semantically unrelated in the original dataset.\nSubsequently, we elucidate the origins of this bias using both detailed\ntheoretical analysis and empirical evidence. Our findings suggest a direction\nfor inspecting these correlations through attacks, based on which we further\nintroduce an instruction to prevent the predictor from learning the\ncorrelations. Through experiments on six text classification datasets and two\ngraph classification datasets using three network architectures (GRUs, BERT,\nand GCN), we show that our method not only significantly outperforms recent\nrationalization methods, but also achieves comparable or even better results\nthan a representative LLM (llama3.1-8b-instruct)."
                },
                "authors": [
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Zhongyu Niu"
                    },
                    {
                        "name": "Lang Gao"
                    },
                    {
                        "name": "Zhiying Deng"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Haozhao Wang"
                    },
                    {
                        "name": "Ruixuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruixuan Li"
                },
                "author": "Ruixuan Li",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02118v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02118v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04337v1",
                "updated": "2025-08-06T11:30:07Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    30,
                    7,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T11:30:07Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    30,
                    7,
                    2,
                    218,
                    0
                ],
                "title": "Modelling and Classifying the Components of a Literature Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modelling and Classifying the Components of a Literature Review"
                },
                "summary": "Previous work has demonstrated that AI methods for analysing scientific\nliterature benefit significantly from annotating sentences in papers according\nto their rhetorical roles, such as research gaps, results, limitations,\nextensions of existing methodologies, and others. Such representations also\nhave the potential to support the development of a new generation of systems\ncapable of producing high-quality literature reviews. However, achieving this\ngoal requires the definition of a relevant annotation schema and effective\nstrategies for large-scale annotation of the literature. This paper addresses\nthese challenges by 1) introducing a novel annotation schema specifically\ndesigned to support literature review generation and 2) conducting a\ncomprehensive evaluation of a wide range of state-of-the-art large language\nmodels (LLMs) in classifying rhetorical roles according to this schema. To this\nend, we also present Sci-Sentence, a novel multidisciplinary benchmark\ncomprising 700 sentences manually annotated by domain experts and 2,240\nsentences automatically labelled using LLMs. We evaluate 37 LLMs on this\nbenchmark, spanning diverse model families and sizes, using both zero-shot\nlearning and fine-tuning approaches. The experiments yield several novel\ninsights that advance the state of the art in this challenging domain. First,\nthe current generation of LLMs performs remarkably well on this task when\nfine-tuned on high-quality data, achieving performance levels above 96\\% F1.\nSecond, while large proprietary models like GPT-4o achieve the best results,\nsome lightweight open-source alternatives also demonstrate excellent\nperformance. Finally, enriching the training data with semi-synthetic examples\ngenerated by LLMs proves beneficial, enabling small encoders to achieve robust\nresults and significantly enhancing the performance of several open decoder\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous work has demonstrated that AI methods for analysing scientific\nliterature benefit significantly from annotating sentences in papers according\nto their rhetorical roles, such as research gaps, results, limitations,\nextensions of existing methodologies, and others. Such representations also\nhave the potential to support the development of a new generation of systems\ncapable of producing high-quality literature reviews. However, achieving this\ngoal requires the definition of a relevant annotation schema and effective\nstrategies for large-scale annotation of the literature. This paper addresses\nthese challenges by 1) introducing a novel annotation schema specifically\ndesigned to support literature review generation and 2) conducting a\ncomprehensive evaluation of a wide range of state-of-the-art large language\nmodels (LLMs) in classifying rhetorical roles according to this schema. To this\nend, we also present Sci-Sentence, a novel multidisciplinary benchmark\ncomprising 700 sentences manually annotated by domain experts and 2,240\nsentences automatically labelled using LLMs. We evaluate 37 LLMs on this\nbenchmark, spanning diverse model families and sizes, using both zero-shot\nlearning and fine-tuning approaches. The experiments yield several novel\ninsights that advance the state of the art in this challenging domain. First,\nthe current generation of LLMs performs remarkably well on this task when\nfine-tuned on high-quality data, achieving performance levels above 96\\% F1.\nSecond, while large proprietary models like GPT-4o achieve the best results,\nsome lightweight open-source alternatives also demonstrate excellent\nperformance. Finally, enriching the training data with semi-synthetic examples\ngenerated by LLMs proves beneficial, enabling small encoders to achieve robust\nresults and significantly enhancing the performance of several open decoder\nmodels."
                },
                "authors": [
                    {
                        "name": "Francisco Bolaos"
                    },
                    {
                        "name": "Angelo Salatino"
                    },
                    {
                        "name": "Francesco Osborne"
                    },
                    {
                        "name": "Enrico Motta"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Motta"
                },
                "author": "Enrico Motta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17667v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17667v3",
                "updated": "2025-08-06T11:25:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    25,
                    30,
                    2,
                    218,
                    0
                ],
                "published": "2025-03-22T06:27:30Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    6,
                    27,
                    30,
                    5,
                    81,
                    0
                ],
                "title": "DGAR: A Unified Domain Generalization Framework for RF-Based Human\n  Activity Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DGAR: A Unified Domain Generalization Framework for RF-Based Human\n  Activity Recognition"
                },
                "summary": "Radio-frequency (RF)-based human activity recognition (HAR) provides a\ncontactless and privacy-preserving solution for monitoring human behavior in\napplications such as astronaut extravehicular activity monitoring,\nhuman-autonomy collaborative cockpit, and unmanned aerial vehicle surveillance.\nHowever, real-world deployments usually face the challenge of domain knowledge\nshifts arising from inter-subject variability, heterogeneous physical\nenvironments, and unseen activity patterns, resulting in significant\nperformance degradation. To address this issue, we propose DGAR, a\ndomain-generalized activity recognition framework that learns transferable\nrepresentations without collecting data from the target domain. DGAR integrates\ninstance-adaptive feature modulation with cross-domain distribution alignment\nto enhance both personalization and generalization. Specifically, it\nincorporates a squeeze-and-excitation (SE) block to extract salient\nspatiotemporal features and employs correlation alignment to mitigate\ninter-domain discrepancies. Extensive experiments on public RF-based datasets\n-- HUST-HAR, Lab-LFM, and Office-LFM -- demonstrate that DGAR consistently\noutperforms state-of-the-art baselines, achieving up to a 5.81% improvement in\nweighted F1-score. The empirical results substantiate the generalization\ncapability of DGAR in real-time RF sensing across dynamic scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radio-frequency (RF)-based human activity recognition (HAR) provides a\ncontactless and privacy-preserving solution for monitoring human behavior in\napplications such as astronaut extravehicular activity monitoring,\nhuman-autonomy collaborative cockpit, and unmanned aerial vehicle surveillance.\nHowever, real-world deployments usually face the challenge of domain knowledge\nshifts arising from inter-subject variability, heterogeneous physical\nenvironments, and unseen activity patterns, resulting in significant\nperformance degradation. To address this issue, we propose DGAR, a\ndomain-generalized activity recognition framework that learns transferable\nrepresentations without collecting data from the target domain. DGAR integrates\ninstance-adaptive feature modulation with cross-domain distribution alignment\nto enhance both personalization and generalization. Specifically, it\nincorporates a squeeze-and-excitation (SE) block to extract salient\nspatiotemporal features and employs correlation alignment to mitigate\ninter-domain discrepancies. Extensive experiments on public RF-based datasets\n-- HUST-HAR, Lab-LFM, and Office-LFM -- demonstrate that DGAR consistently\noutperforms state-of-the-art baselines, achieving up to a 5.81% improvement in\nweighted F1-score. The empirical results substantiate the generalization\ncapability of DGAR in real-time RF sensing across dynamic scenarios."
                },
                "authors": [
                    {
                        "name": "Junshuo Liu"
                    },
                    {
                        "name": "Xin Shi"
                    },
                    {
                        "name": "Yunchuan Zhang"
                    },
                    {
                        "name": "Yinhao Ge"
                    },
                    {
                        "name": "Robert C. Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Robert C. Qiu"
                },
                "author": "Robert C. Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17667v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17667v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12286v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12286v3",
                "updated": "2025-08-06T11:23:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    23,
                    11,
                    2,
                    218,
                    0
                ],
                "published": "2025-06-14T00:25:26Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    0,
                    25,
                    26,
                    5,
                    165,
                    0
                ],
                "title": "The SWE-Bench Illusion: When State-of-the-Art LLMs Remember Instead of\n  Reason",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The SWE-Bench Illusion: When State-of-the-Art LLMs Remember Instead of\n  Reason"
                },
                "summary": "As large language models (LLMs) become increasingly capable and widely\nadopted, benchmarks play a central role in assessing their practical utility.\nFor example, SWE-Bench Verified has emerged as a critical benchmark for\nevaluating LLMs' software engineering abilities, particularly their aptitude\nfor resolving real-world GitHub issues. Recent LLMs show impressive performance\non SWE-Bench, leading to optimism about their capacity for complex coding\ntasks. However, current evaluation protocols may overstate these models' true\ncapabilities. It is crucial to distinguish LLMs' generalizable problem-solving\nability and other learned artifacts. In this work, we introduce two diagnostic\ntasks: file path identification from issue descriptions alone and ground truth\nfunction reproduction with only the current file context and issue description\nto probe models' underlying knowledge. We present empirical evidence that\nperformance gains on SWE-Bench-Verified may be partially driven by memorization\nrather than genuine problem-solving. We show that state-of-the-art models\nachieve up to 76% accuracy in identifying buggy file paths using only issue\ndescriptions, without access to repository structure. This performance is\nmerely up to 53% on tasks from repositories not included in SWE-Bench, pointing\nto possible data contamination or memorization. Similar patterns are also\nobserved for the function reproduction task, where the verbatim similarity is\nmuch higher on SWE-Bench Verified than on other similar coding benchmarks (up\nto 35% consecutive 5-gram accuracy on SWE-Bench Verified and Full, but only up\nto 18% for tasks in other benchmarks). These findings raise concerns about the\nvalidity of existing results and underscore the need for more robust,\ncontamination-resistant benchmarks to reliably evaluate LLMs' coding abilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become increasingly capable and widely\nadopted, benchmarks play a central role in assessing their practical utility.\nFor example, SWE-Bench Verified has emerged as a critical benchmark for\nevaluating LLMs' software engineering abilities, particularly their aptitude\nfor resolving real-world GitHub issues. Recent LLMs show impressive performance\non SWE-Bench, leading to optimism about their capacity for complex coding\ntasks. However, current evaluation protocols may overstate these models' true\ncapabilities. It is crucial to distinguish LLMs' generalizable problem-solving\nability and other learned artifacts. In this work, we introduce two diagnostic\ntasks: file path identification from issue descriptions alone and ground truth\nfunction reproduction with only the current file context and issue description\nto probe models' underlying knowledge. We present empirical evidence that\nperformance gains on SWE-Bench-Verified may be partially driven by memorization\nrather than genuine problem-solving. We show that state-of-the-art models\nachieve up to 76% accuracy in identifying buggy file paths using only issue\ndescriptions, without access to repository structure. This performance is\nmerely up to 53% on tasks from repositories not included in SWE-Bench, pointing\nto possible data contamination or memorization. Similar patterns are also\nobserved for the function reproduction task, where the verbatim similarity is\nmuch higher on SWE-Bench Verified than on other similar coding benchmarks (up\nto 35% consecutive 5-gram accuracy on SWE-Bench Verified and Full, but only up\nto 18% for tasks in other benchmarks). These findings raise concerns about the\nvalidity of existing results and underscore the need for more robust,\ncontamination-resistant benchmarks to reliably evaluate LLMs' coding abilities."
                },
                "authors": [
                    {
                        "name": "Shanchao Liang"
                    },
                    {
                        "name": "Spandan Garg"
                    },
                    {
                        "name": "Roshanak Zilouchian Moghaddam"
                    }
                ],
                "author_detail": {
                    "name": "Roshanak Zilouchian Moghaddam"
                },
                "author": "Roshanak Zilouchian Moghaddam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12286v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12286v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04329v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04329v2",
                "updated": "2025-08-07T08:30:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    8,
                    30,
                    41,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-06T11:22:23Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    22,
                    23,
                    2,
                    218,
                    0
                ],
                "title": "Forgetting: A New Mechanism Towards Better Large Language Model\n  Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forgetting: A New Mechanism Towards Better Large Language Model\n  Fine-tuning"
                },
                "summary": "Supervised fine-tuning (SFT) plays a critical role for pretrained large\nlanguage models (LLMs), notably enhancing their capacity to acquire\ndomain-specific knowledge while preserving or potentially augmenting their\ngeneral-purpose capabilities. However, the efficacy of SFT hinges on data\nquality as well as data volume, otherwise it may result in limited performance\ngains or even degradation relative to the associated baselines. To mitigate\nsuch reliance, we suggest categorizing tokens within each corpus into two parts\n-- positive and negative tokens -- based on whether they are useful to improve\nmodel performance. Positive tokens can be trained in common ways, whereas\nnegative tokens, which may lack essential semantics or be misleading, should be\nexplicitly forgotten. Overall, the token categorization facilitate the model to\nlearn less informative message, and the forgetting process shapes a knowledge\nboundary to guide the model on what information to learn more precisely. We\nconduct experiments on well-established benchmarks, finding that this\nforgetting mechanism not only improves overall model performance and also\nfacilitate more diverse model responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised fine-tuning (SFT) plays a critical role for pretrained large\nlanguage models (LLMs), notably enhancing their capacity to acquire\ndomain-specific knowledge while preserving or potentially augmenting their\ngeneral-purpose capabilities. However, the efficacy of SFT hinges on data\nquality as well as data volume, otherwise it may result in limited performance\ngains or even degradation relative to the associated baselines. To mitigate\nsuch reliance, we suggest categorizing tokens within each corpus into two parts\n-- positive and negative tokens -- based on whether they are useful to improve\nmodel performance. Positive tokens can be trained in common ways, whereas\nnegative tokens, which may lack essential semantics or be misleading, should be\nexplicitly forgotten. Overall, the token categorization facilitate the model to\nlearn less informative message, and the forgetting process shapes a knowledge\nboundary to guide the model on what information to learn more precisely. We\nconduct experiments on well-established benchmarks, finding that this\nforgetting mechanism not only improves overall model performance and also\nfacilitate more diverse model responses."
                },
                "authors": [
                    {
                        "name": "Ali Taheri Ghahrizjani"
                    },
                    {
                        "name": "Alireza Taban"
                    },
                    {
                        "name": "Qizhou Wang"
                    },
                    {
                        "name": "Shanshan Ye"
                    },
                    {
                        "name": "Abdolreza Mirzaei"
                    },
                    {
                        "name": "Tongliang Liu"
                    },
                    {
                        "name": "Bo Han"
                    }
                ],
                "author_detail": {
                    "name": "Bo Han"
                },
                "author": "Bo Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04329v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04329v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16414v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16414v2",
                "updated": "2025-08-06T11:20:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    20,
                    14,
                    2,
                    218,
                    0
                ],
                "published": "2025-04-23T04:36:19Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    4,
                    36,
                    19,
                    2,
                    113,
                    0
                ],
                "title": "Evaluating Multi-Hop Reasoning in Large Language Models: A\n  Chemistry-Centric Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Multi-Hop Reasoning in Large Language Models: A\n  Chemistry-Centric Case Study"
                },
                "summary": "In this study, we introduced a new benchmark consisting of a curated dataset\nand a defined evaluation process to assess the compositional reasoning\ncapabilities of large language models within the chemistry domain. We designed\nand validated a fully automated pipeline, verified by subject matter experts,\nto facilitate this task. Our approach integrates OpenAI reasoning models with\nnamed entity recognition (NER) systems to extract chemical entities from recent\nliterature, which are then augmented with external knowledge bases to form a\ncomprehensive knowledge graph. By generating multi-hop questions across these\ngraphs, we assess LLM performance in both context-augmented and non-context\naugmented settings. Our experiments reveal that even state-of-the-art models\nface significant challenges in multi-hop compositional reasoning. The results\nreflect the importance of augmenting LLMs with document retrieval, which can\nhave a substantial impact on improving their performance. However, even perfect\nretrieval accuracy with full context does not eliminate reasoning errors,\nunderscoring the complexity of compositional reasoning. This work not only\nbenchmarks and highlights the limitations of current LLMs but also presents a\nnovel data generation pipeline capable of producing challenging reasoning\ndatasets across various domains. Overall, this research advances our\nunderstanding of reasoning in computational linguistics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we introduced a new benchmark consisting of a curated dataset\nand a defined evaluation process to assess the compositional reasoning\ncapabilities of large language models within the chemistry domain. We designed\nand validated a fully automated pipeline, verified by subject matter experts,\nto facilitate this task. Our approach integrates OpenAI reasoning models with\nnamed entity recognition (NER) systems to extract chemical entities from recent\nliterature, which are then augmented with external knowledge bases to form a\ncomprehensive knowledge graph. By generating multi-hop questions across these\ngraphs, we assess LLM performance in both context-augmented and non-context\naugmented settings. Our experiments reveal that even state-of-the-art models\nface significant challenges in multi-hop compositional reasoning. The results\nreflect the importance of augmenting LLMs with document retrieval, which can\nhave a substantial impact on improving their performance. However, even perfect\nretrieval accuracy with full context does not eliminate reasoning errors,\nunderscoring the complexity of compositional reasoning. This work not only\nbenchmarks and highlights the limitations of current LLMs but also presents a\nnovel data generation pipeline capable of producing challenging reasoning\ndatasets across various domains. Overall, this research advances our\nunderstanding of reasoning in computational linguistics."
                },
                "authors": [
                    {
                        "name": "Mohammad Khodadad"
                    },
                    {
                        "name": "Ali Shiraee Kasmaee"
                    },
                    {
                        "name": "Mahdi Astaraki"
                    },
                    {
                        "name": "Nicholas Sherck"
                    },
                    {
                        "name": "Hamidreza Mahyar"
                    },
                    {
                        "name": "Soheila Samiee"
                    }
                ],
                "author_detail": {
                    "name": "Soheila Samiee"
                },
                "author": "Soheila Samiee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16414v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16414v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21696v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21696v3",
                "updated": "2025-08-06T11:17:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    17,
                    33,
                    2,
                    218,
                    0
                ],
                "published": "2025-07-29T11:20:03Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    11,
                    20,
                    3,
                    1,
                    210,
                    0
                ],
                "title": "Edge Agentic AI Framework for Autonomous Network Optimisation in O-RAN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Agentic AI Framework for Autonomous Network Optimisation in O-RAN"
                },
                "summary": "The deployment of AI agents within legacy Radio Access Network (RAN)\ninfrastructure poses significant safety and reliability challenges for future\n6G networks. This paper presents a novel Edge AI framework for autonomous\nnetwork optimisation in Open RAN environments, addressing these challenges\nthrough three core innovations: (1) a persona-based multi-tools architecture\nenabling distributed, context-aware decision-making; (2) proactive anomaly\ndetection agent powered by traffic predictive tool; and (3) a safety, aligned\nreward mechanism that balances performance with operational stability.\nIntegrated into the RAN Intelligent Controller (RIC), our framework leverages\nmultimodal data fusion, including network KPIs, a traffic prediction model, and\nexternal information sources, to anticipate and respond to dynamic network\nconditions. Extensive evaluation using realistic 5G scenarios demonstrates that\nthe edge framework achieves zero network outages under high-stress conditions,\ncompared to 8.4% for traditional fixed-power networks and 3.3% for large\nlanguage model (LLM) agent-based approaches, while maintaining near real-time\nresponsiveness and consistent QoS. These results establish that, when equipped\nwith the right tools and contextual awareness, AI agents can be safely and\neffectively deployed in critical network infrastructure, laying the framework\nfor intelligent and autonomous 5G and beyond network operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of AI agents within legacy Radio Access Network (RAN)\ninfrastructure poses significant safety and reliability challenges for future\n6G networks. This paper presents a novel Edge AI framework for autonomous\nnetwork optimisation in Open RAN environments, addressing these challenges\nthrough three core innovations: (1) a persona-based multi-tools architecture\nenabling distributed, context-aware decision-making; (2) proactive anomaly\ndetection agent powered by traffic predictive tool; and (3) a safety, aligned\nreward mechanism that balances performance with operational stability.\nIntegrated into the RAN Intelligent Controller (RIC), our framework leverages\nmultimodal data fusion, including network KPIs, a traffic prediction model, and\nexternal information sources, to anticipate and respond to dynamic network\nconditions. Extensive evaluation using realistic 5G scenarios demonstrates that\nthe edge framework achieves zero network outages under high-stress conditions,\ncompared to 8.4% for traditional fixed-power networks and 3.3% for large\nlanguage model (LLM) agent-based approaches, while maintaining near real-time\nresponsiveness and consistent QoS. These results establish that, when equipped\nwith the right tools and contextual awareness, AI agents can be safely and\neffectively deployed in critical network infrastructure, laying the framework\nfor intelligent and autonomous 5G and beyond network operations."
                },
                "authors": [
                    {
                        "name": "Abdelaziz Salama"
                    },
                    {
                        "name": "Zeinab Nezami"
                    },
                    {
                        "name": "Mohammed M. H. Qazzaz"
                    },
                    {
                        "name": "Maryam Hafeez"
                    },
                    {
                        "name": "Syed Ali Raza Zaidi"
                    }
                ],
                "author_detail": {
                    "name": "Syed Ali Raza Zaidi"
                },
                "author": "Syed Ali Raza Zaidi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21696v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21696v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04325v1",
                "updated": "2025-08-06T11:11:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    11,
                    40,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T11:11:40Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    11,
                    40,
                    2,
                    218,
                    0
                ],
                "title": "Beyond the Leaderboard: Rethinking Medical Benchmarks for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Leaderboard: Rethinking Medical Benchmarks for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) show significant potential in healthcare,\nprompting numerous benchmarks to evaluate their capabilities. However, concerns\npersist regarding the reliability of these benchmarks, which often lack\nclinical fidelity, robust data management, and safety-oriented evaluation\nmetrics. To address these shortcomings, we introduce MedCheck, the first\nlifecycle-oriented assessment framework specifically designed for medical\nbenchmarks. Our framework deconstructs a benchmark's development into five\ncontinuous stages, from design to governance, and provides a comprehensive\nchecklist of 46 medically-tailored criteria. Using MedCheck, we conducted an\nin-depth empirical evaluation of 53 medical LLM benchmarks. Our analysis\nuncovers widespread, systemic issues, including a profound disconnect from\nclinical practice, a crisis of data integrity due to unmitigated contamination\nrisks, and a systematic neglect of safety-critical evaluation dimensions like\nmodel robustness and uncertainty awareness. Based on these findings, MedCheck\nserves as both a diagnostic tool for existing benchmarks and an actionable\nguideline to foster a more standardized, reliable, and transparent approach to\nevaluating AI in healthcare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) show significant potential in healthcare,\nprompting numerous benchmarks to evaluate their capabilities. However, concerns\npersist regarding the reliability of these benchmarks, which often lack\nclinical fidelity, robust data management, and safety-oriented evaluation\nmetrics. To address these shortcomings, we introduce MedCheck, the first\nlifecycle-oriented assessment framework specifically designed for medical\nbenchmarks. Our framework deconstructs a benchmark's development into five\ncontinuous stages, from design to governance, and provides a comprehensive\nchecklist of 46 medically-tailored criteria. Using MedCheck, we conducted an\nin-depth empirical evaluation of 53 medical LLM benchmarks. Our analysis\nuncovers widespread, systemic issues, including a profound disconnect from\nclinical practice, a crisis of data integrity due to unmitigated contamination\nrisks, and a systematic neglect of safety-critical evaluation dimensions like\nmodel robustness and uncertainty awareness. Based on these findings, MedCheck\nserves as both a diagnostic tool for existing benchmarks and an actionable\nguideline to foster a more standardized, reliable, and transparent approach to\nevaluating AI in healthcare."
                },
                "authors": [
                    {
                        "name": "Zizhan Ma"
                    },
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Guo Yu"
                    },
                    {
                        "name": "Yiu-Fai Cheung"
                    },
                    {
                        "name": "Meidan Ding"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Wenting Chen"
                    },
                    {
                        "name": "Linlin Shen"
                    }
                ],
                "author_detail": {
                    "name": "Linlin Shen"
                },
                "author": "Linlin Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13287v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13287v5",
                "updated": "2025-08-06T11:02:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    2,
                    1,
                    2,
                    218,
                    0
                ],
                "published": "2024-10-17T07:33:35Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    7,
                    33,
                    35,
                    3,
                    291,
                    0
                ],
                "title": "PAK-UCB Contextual Bandit: An Online Learning Approach to Prompt-Aware\n  Selection of Generative Models and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAK-UCB Contextual Bandit: An Online Learning Approach to Prompt-Aware\n  Selection of Generative Models and LLMs"
                },
                "summary": "Selecting a sample generation scheme from multiple prompt-based generative\nmodels, including large language models (LLMs) and prompt-guided image and\nvideo generation models, is typically addressed by choosing the model that\nmaximizes an averaged evaluation score. However, this score-based selection\noverlooks the possibility that different models achieve the best generation\nperformance for different types of text prompts. An online identification of\nthe best generation model for various input prompts can reduce the costs\nassociated with querying sub-optimal models. In this work, we explore the\npossibility of varying rankings of text-based generative models for different\ntext prompts and propose an online learning framework to predict the best data\ngeneration model for a given input prompt. The proposed PAK-UCB algorithm\naddresses a contextual bandit (CB) setting with shared context variables across\nthe arms, utilizing the generated data to update kernel-based functions that\npredict the score of each model available for unseen text prompts.\nAdditionally, we leverage random Fourier features (RFF) to accelerate the\nonline learning process of PAK-UCB. Our numerical experiments on real and\nsimulated text-to-image and image-to-text generative models show that RFF-UCB\nperforms successfully in identifying the best generation model across different\nsample types. The code is available at:\ngithub.com/yannxiaoyanhu/dgm-online-select.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selecting a sample generation scheme from multiple prompt-based generative\nmodels, including large language models (LLMs) and prompt-guided image and\nvideo generation models, is typically addressed by choosing the model that\nmaximizes an averaged evaluation score. However, this score-based selection\noverlooks the possibility that different models achieve the best generation\nperformance for different types of text prompts. An online identification of\nthe best generation model for various input prompts can reduce the costs\nassociated with querying sub-optimal models. In this work, we explore the\npossibility of varying rankings of text-based generative models for different\ntext prompts and propose an online learning framework to predict the best data\ngeneration model for a given input prompt. The proposed PAK-UCB algorithm\naddresses a contextual bandit (CB) setting with shared context variables across\nthe arms, utilizing the generated data to update kernel-based functions that\npredict the score of each model available for unseen text prompts.\nAdditionally, we leverage random Fourier features (RFF) to accelerate the\nonline learning process of PAK-UCB. Our numerical experiments on real and\nsimulated text-to-image and image-to-text generative models show that RFF-UCB\nperforms successfully in identifying the best generation model across different\nsample types. The code is available at:\ngithub.com/yannxiaoyanhu/dgm-online-select."
                },
                "authors": [
                    {
                        "name": "Xiaoyan Hu"
                    },
                    {
                        "name": "Ho-fung Leung"
                    },
                    {
                        "name": "Farzan Farnia"
                    }
                ],
                "author_detail": {
                    "name": "Farzan Farnia"
                },
                "author": "Farzan Farnia",
                "arxiv_comment": "accepted to ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13287v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13287v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04306v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04306v1",
                "updated": "2025-08-06T10:45:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    10,
                    45,
                    52,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T10:45:52Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    10,
                    45,
                    52,
                    2,
                    218,
                    0
                ],
                "title": "Multi-Agent Taskforce Collaboration: Self-Correction of Compounding\n  Errors in Long-Form Literature Review Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Taskforce Collaboration: Self-Correction of Compounding\n  Errors in Long-Form Literature Review Generation"
                },
                "summary": "Literature reviews play an important role in scientific research. Recent\nadvances in large language models (LLMs) have boosted the development of\nautomated systems for the entire literature review workflow, from retrieval to\nmanuscript drafting. However, a key challenge is that mistakes made in early\nstages can propagate and amplify in subsequent steps, leading to compounding\nerrors that undermine the faithfulness of the final review. To tackle this\nissue, we propose the Multi-Agent Taskforce Collaboration (MATC) framework,\nwhich consists of a manager agent and four executor agents for literature\nsearching, outline generation, fact localization, and manuscript drafting. We\npropose three novel collaboration paradigms, forming exploration, exploitation,\nand experience taskforces, to effectively organize agents and mitigate\ncompounding errors both between and within executor agents. Experimental\nresults show that MATC achieves state-of-the-art performance on existing\nbenchmarks. We further propose a new benchmark dataset featuring more diverse\ntopics for faithful literature review generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Literature reviews play an important role in scientific research. Recent\nadvances in large language models (LLMs) have boosted the development of\nautomated systems for the entire literature review workflow, from retrieval to\nmanuscript drafting. However, a key challenge is that mistakes made in early\nstages can propagate and amplify in subsequent steps, leading to compounding\nerrors that undermine the faithfulness of the final review. To tackle this\nissue, we propose the Multi-Agent Taskforce Collaboration (MATC) framework,\nwhich consists of a manager agent and four executor agents for literature\nsearching, outline generation, fact localization, and manuscript drafting. We\npropose three novel collaboration paradigms, forming exploration, exploitation,\nand experience taskforces, to effectively organize agents and mitigate\ncompounding errors both between and within executor agents. Experimental\nresults show that MATC achieves state-of-the-art performance on existing\nbenchmarks. We further propose a new benchmark dataset featuring more diverse\ntopics for faithful literature review generation."
                },
                "authors": [
                    {
                        "name": "Zhi Zhang"
                    },
                    {
                        "name": "Yan Liu"
                    },
                    {
                        "name": "Zhejing Hu"
                    },
                    {
                        "name": "Gong Chen"
                    },
                    {
                        "name": "Sheng-hua Zhong"
                    },
                    {
                        "name": "Jiannong Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jiannong Cao"
                },
                "author": "Jiannong Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04306v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04306v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04295v1",
                "updated": "2025-08-06T10:31:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    10,
                    31,
                    23,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T10:31:23Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    10,
                    31,
                    23,
                    2,
                    218,
                    0
                ],
                "title": "EVOC2RUST: A Skeleton-guided Framework for Project-Level C-to-Rust\n  Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVOC2RUST: A Skeleton-guided Framework for Project-Level C-to-Rust\n  Translation"
                },
                "summary": "Rust's compile-time safety guarantees make it ideal for safety-critical\nsystems, creating demand for translating legacy C codebases to Rust. While\nvarious approaches have emerged for this task, they face inherent trade-offs:\nrule-based solutions face challenges in meeting code safety and idiomaticity\nrequirements, while LLM-based solutions often fail to generate semantically\nequivalent Rust code, due to the heavy dependencies of modules across the\nentire codebase. Recent studies have revealed that both solutions are limited\nto small-scale programs. In this paper, we propose EvoC2Rust, an automated\nframework for converting entire C projects to equivalent Rust ones. EvoC2Rust\nemploys a skeleton-guided translation strategy for project-level translation.\nThe pipeline consists of three evolutionary stages: 1) it first decomposes the\nC project into functional modules, employs a feature-mapping-enhanced LLM to\ntransform definitions and macros and generates type-checked function stubs,\nwhich form a compilable Rust skeleton; 2) it then incrementally translates the\nfunction, replacing the corresponding stub placeholder; 3) finally, it repairs\ncompilation errors by integrating LLM and static analysis. Through evolutionary\naugmentation, EvoC2Rust combines the advantages of both rule-based and\nLLM-based solutions. Our evaluation on open-source benchmarks and six\nindustrial projects demonstrates EvoC2Rust's superior performance in\nproject-level C-to-Rust translation. On average, it achieves 17.24% and 14.32%\nimprovements in syntax and semantic accuracy over the LLM-based approaches,\nalong with a 96.79% higher code safety rate than the rule-based tools. At the\nmodule level, EvoC2Rust reaches 92.25% compilation and 89.53% test pass rates\non industrial projects, even for complex codebases and long functions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rust's compile-time safety guarantees make it ideal for safety-critical\nsystems, creating demand for translating legacy C codebases to Rust. While\nvarious approaches have emerged for this task, they face inherent trade-offs:\nrule-based solutions face challenges in meeting code safety and idiomaticity\nrequirements, while LLM-based solutions often fail to generate semantically\nequivalent Rust code, due to the heavy dependencies of modules across the\nentire codebase. Recent studies have revealed that both solutions are limited\nto small-scale programs. In this paper, we propose EvoC2Rust, an automated\nframework for converting entire C projects to equivalent Rust ones. EvoC2Rust\nemploys a skeleton-guided translation strategy for project-level translation.\nThe pipeline consists of three evolutionary stages: 1) it first decomposes the\nC project into functional modules, employs a feature-mapping-enhanced LLM to\ntransform definitions and macros and generates type-checked function stubs,\nwhich form a compilable Rust skeleton; 2) it then incrementally translates the\nfunction, replacing the corresponding stub placeholder; 3) finally, it repairs\ncompilation errors by integrating LLM and static analysis. Through evolutionary\naugmentation, EvoC2Rust combines the advantages of both rule-based and\nLLM-based solutions. Our evaluation on open-source benchmarks and six\nindustrial projects demonstrates EvoC2Rust's superior performance in\nproject-level C-to-Rust translation. On average, it achieves 17.24% and 14.32%\nimprovements in syntax and semantic accuracy over the LLM-based approaches,\nalong with a 96.79% higher code safety rate than the rule-based tools. At the\nmodule level, EvoC2Rust reaches 92.25% compilation and 89.53% test pass rates\non industrial projects, even for complex codebases and long functions."
                },
                "authors": [
                    {
                        "name": "Chaofan Wang"
                    },
                    {
                        "name": "Tingrui Yu"
                    },
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Dong Chen"
                    },
                    {
                        "name": "Wenrui Zhang"
                    },
                    {
                        "name": "Yuling Shi"
                    },
                    {
                        "name": "Xiaodong Gu"
                    },
                    {
                        "name": "Beijun Shen"
                    }
                ],
                "author_detail": {
                    "name": "Beijun Shen"
                },
                "author": "Beijun Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04291v1",
                "updated": "2025-08-06T10:29:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    10,
                    29,
                    46,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T10:29:46Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    10,
                    29,
                    46,
                    2,
                    218,
                    0
                ],
                "title": "Less Signals, More Understanding: Channel-Capacity Codebook Design for\n  Digital Task-Oriented Semantic Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Less Signals, More Understanding: Channel-Capacity Codebook Design for\n  Digital Task-Oriented Semantic Communication"
                },
                "summary": "Discrete representation has emerged as a powerful tool in task-oriented\nsemantic communication (ToSC), offering compact, interpretable, and efficient\nrepresentations well-suited for low-power edge intelligence scenarios. Its\ninherent digital nature aligns seamlessly with hardware-friendly deployment and\nrobust storage/transmission protocols. However, despite its strengths, current\nToSC frameworks often decouple semantic-aware discrete mapping from the\nunderlying channel characteristics and task demands. This mismatch leads to\nsuboptimal communication performance, degraded task utility, and limited\ngeneralization under variable wireless conditions. Moreover, conventional\ndesigns frequently overlook channel-awareness in codebook construction,\nrestricting the effectiveness of semantic symbol selection under constrained\nresources. To address these limitations, this paper proposes a channel-aware\ndiscrete semantic coding framework tailored for low-power edge networks.\nLeveraging a Wasserstein-regularized objective, our approach aligns discrete\ncode activations with optimal input distributions, thereby improving semantic\nfidelity, robustness, and task accuracy. Extensive experiments on the inference\ntasks across diverse signal-to-noise ratio (SNR) regimes show that our method\nachieves notable gains in accuracy and communication efficiency. This work\nprovides new insights into integrating discrete semantics and channel\noptimization, paving the way for the widespread adoption of semantic\ncommunication in future digital infrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discrete representation has emerged as a powerful tool in task-oriented\nsemantic communication (ToSC), offering compact, interpretable, and efficient\nrepresentations well-suited for low-power edge intelligence scenarios. Its\ninherent digital nature aligns seamlessly with hardware-friendly deployment and\nrobust storage/transmission protocols. However, despite its strengths, current\nToSC frameworks often decouple semantic-aware discrete mapping from the\nunderlying channel characteristics and task demands. This mismatch leads to\nsuboptimal communication performance, degraded task utility, and limited\ngeneralization under variable wireless conditions. Moreover, conventional\ndesigns frequently overlook channel-awareness in codebook construction,\nrestricting the effectiveness of semantic symbol selection under constrained\nresources. To address these limitations, this paper proposes a channel-aware\ndiscrete semantic coding framework tailored for low-power edge networks.\nLeveraging a Wasserstein-regularized objective, our approach aligns discrete\ncode activations with optimal input distributions, thereby improving semantic\nfidelity, robustness, and task accuracy. Extensive experiments on the inference\ntasks across diverse signal-to-noise ratio (SNR) regimes show that our method\nachieves notable gains in accuracy and communication efficiency. This work\nprovides new insights into integrating discrete semantics and channel\noptimization, paving the way for the widespread adoption of semantic\ncommunication in future digital infrastructures."
                },
                "authors": [
                    {
                        "name": "Anbang Zhang"
                    },
                    {
                        "name": "Shuaishuai Guo"
                    },
                    {
                        "name": "Chenyuan Feng"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Haojin Li"
                    },
                    {
                        "name": "Chen Sun"
                    },
                    {
                        "name": "Haijun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Haijun Zhang"
                },
                "author": "Haijun Zhang",
                "arxiv_comment": "submitted to IEEE Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04289v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04289v2",
                "updated": "2025-08-07T04:14:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    4,
                    14,
                    31,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-06T10:26:52Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    10,
                    26,
                    52,
                    2,
                    218,
                    0
                ],
                "title": "Method-Based Reasoning for Large Language Models: Extraction, Reuse, and\n  Continuous Improvement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Method-Based Reasoning for Large Language Models: Extraction, Reuse, and\n  Continuous Improvement"
                },
                "summary": "Large language models (LLMs) have shown impressive capabilities across a wide\nrange of language tasks. However, their reasoning process is primarily guided\nby statistical patterns in training data, which limits their ability to handle\nnovel problems and perform consistent logical reasoning. In this paper, we\npropose a method-based model that enhances LLMs with explicit, reusable\nprocedures extracted from training content, generated responses, and user\ninteractions. Each method is represented as a pair consisting of a problem and\nits corresponding solution, stored externally and ranked based on feedback.\nWhen a new query is received, the system retrieves and applies the most\nrelevant methods to guide the LLM's response. Our model enables continual\nlearning, method reuse, and logical consistency beyond next-token prediction.\nExperimental results demonstrate that the system improves factual verification\nand generalization in complex prompts, and that newly learned methods can\noutperform earlier ones through user-driven refinement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown impressive capabilities across a wide\nrange of language tasks. However, their reasoning process is primarily guided\nby statistical patterns in training data, which limits their ability to handle\nnovel problems and perform consistent logical reasoning. In this paper, we\npropose a method-based model that enhances LLMs with explicit, reusable\nprocedures extracted from training content, generated responses, and user\ninteractions. Each method is represented as a pair consisting of a problem and\nits corresponding solution, stored externally and ranked based on feedback.\nWhen a new query is received, the system retrieves and applies the most\nrelevant methods to guide the LLM's response. Our model enables continual\nlearning, method reuse, and logical consistency beyond next-token prediction.\nExperimental results demonstrate that the system improves factual verification\nand generalization in complex prompts, and that newly learned methods can\noutperform earlier ones through user-driven refinement."
                },
                "authors": [
                    {
                        "name": "Hong Su"
                    }
                ],
                "author_detail": {
                    "name": "Hong Su"
                },
                "author": "Hong Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04289v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04289v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04281v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04281v1",
                "updated": "2025-08-06T10:10:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    10,
                    10,
                    1,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T10:10:01Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    10,
                    10,
                    1,
                    2,
                    218,
                    0
                ],
                "title": "Prompt Injection Vulnerability of Consensus Generating Applications in\n  Digital Democracy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Injection Vulnerability of Consensus Generating Applications in\n  Digital Democracy"
                },
                "summary": "Large Language Models (LLMs) are gaining traction as a method to generate\nconsensus statements and aggregate preferences in digital democracy\nexperiments. Yet, LLMs may introduce critical vulnerabilities in these systems.\nHere, we explore the impact of prompt-injection attacks targeting consensus\ngenerating systems by introducing a four-dimensional taxonomy of attacks. We\ntest these attacks using LLaMA 3.1 8B and Chat GPT 4.1 Nano finding the LLMs\nmore vulnerable to criticism attacks -- attacks using disagreeable prompts --\nand more effective at tilting ambiguous consensus statements. We also find\nevidence of more effective manipulation when using explicit imperatives and\nrational-sounding arguments compared to emotional language or fabricated\nstatistics. To mitigate these vulnerabilities, we apply Direct Preference\nOptimization (DPO), an alignment method that fine-tunes LLMs to prefer\nunperturbed consensus statements. While DPO significantly improves robustness,\nit still offers limited protection against attacks targeting ambiguous\nconsensus. These results advance our understanding of the vulnerability and\nrobustness of consensus generating LLMs in digital democracy applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are gaining traction as a method to generate\nconsensus statements and aggregate preferences in digital democracy\nexperiments. Yet, LLMs may introduce critical vulnerabilities in these systems.\nHere, we explore the impact of prompt-injection attacks targeting consensus\ngenerating systems by introducing a four-dimensional taxonomy of attacks. We\ntest these attacks using LLaMA 3.1 8B and Chat GPT 4.1 Nano finding the LLMs\nmore vulnerable to criticism attacks -- attacks using disagreeable prompts --\nand more effective at tilting ambiguous consensus statements. We also find\nevidence of more effective manipulation when using explicit imperatives and\nrational-sounding arguments compared to emotional language or fabricated\nstatistics. To mitigate these vulnerabilities, we apply Direct Preference\nOptimization (DPO), an alignment method that fine-tunes LLMs to prefer\nunperturbed consensus statements. While DPO significantly improves robustness,\nit still offers limited protection against attacks targeting ambiguous\nconsensus. These results advance our understanding of the vulnerability and\nrobustness of consensus generating LLMs in digital democracy applications."
                },
                "authors": [
                    {
                        "name": "Jairo Gudio-Rosero"
                    },
                    {
                        "name": "Clment Contet"
                    },
                    {
                        "name": "Umberto Grandi"
                    },
                    {
                        "name": "Csar A. Hidalgo"
                    }
                ],
                "author_detail": {
                    "name": "Csar A. Hidalgo"
                },
                "author": "Csar A. Hidalgo",
                "arxiv_comment": "24 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04281v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04280v1",
                "updated": "2025-08-06T10:08:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    10,
                    8,
                    48,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T10:08:48Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    10,
                    8,
                    48,
                    2,
                    218,
                    0
                ],
                "title": "Enhancing Vision-Language Model Training with Reinforcement Learning in\n  Synthetic Worlds for Real-World Success",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Vision-Language Model Training with Reinforcement Learning in\n  Synthetic Worlds for Real-World Success"
                },
                "summary": "Interactive multimodal agents must convert raw visual observations into\ncoherent sequences of language-conditioned actions -- a capability that current\nvision-language models (VLMs) still lack. Earlier reinforcement-learning (RL)\nefforts could, in principle, endow VLMs with such skills, but they have seldom\ntested whether the learned behaviours generalize beyond their training\nsimulators, and they depend either on brittle hyperparameter tuning or on\ndense-reward environments with low state variability. We introduce\nVision-Language Decoupled Actor-Critic (VL-DAC), a lightweight,\nhyperparameter-free RL algorithm. VL-DAC applies PPO updates to action tokens\nwhile learning value only at the environment-step level: an arrangement, to our\nknowledge, not previously explored for large VLMs or LLMs. This simple\ndecoupling removes unstable weighting terms and yields faster, more reliable\nconvergence. Training a single VLM with VL-DAC in one inexpensive simulator at\na time (MiniWorld, Gym-Cards, ALFWorld, or WebShop) already produces policies\nthat generalize widely: +50\\% relative on BALROG (game-centric agentic\ncontrol), +5\\% relative on the hardest part of VSI-Bench (spatial planning),\nand +2\\% on VisualWebBench (web navigation), all without degrading general\nimage understanding accuracy. These results provide the first evidence that a\nsimple RL algorithm can train VLMs entirely in cheap synthetic worlds while\ndelivering measurable gains on real-image agentic, spatial-reasoning, and\nweb-navigation benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactive multimodal agents must convert raw visual observations into\ncoherent sequences of language-conditioned actions -- a capability that current\nvision-language models (VLMs) still lack. Earlier reinforcement-learning (RL)\nefforts could, in principle, endow VLMs with such skills, but they have seldom\ntested whether the learned behaviours generalize beyond their training\nsimulators, and they depend either on brittle hyperparameter tuning or on\ndense-reward environments with low state variability. We introduce\nVision-Language Decoupled Actor-Critic (VL-DAC), a lightweight,\nhyperparameter-free RL algorithm. VL-DAC applies PPO updates to action tokens\nwhile learning value only at the environment-step level: an arrangement, to our\nknowledge, not previously explored for large VLMs or LLMs. This simple\ndecoupling removes unstable weighting terms and yields faster, more reliable\nconvergence. Training a single VLM with VL-DAC in one inexpensive simulator at\na time (MiniWorld, Gym-Cards, ALFWorld, or WebShop) already produces policies\nthat generalize widely: +50\\% relative on BALROG (game-centric agentic\ncontrol), +5\\% relative on the hardest part of VSI-Bench (spatial planning),\nand +2\\% on VisualWebBench (web navigation), all without degrading general\nimage understanding accuracy. These results provide the first evidence that a\nsimple RL algorithm can train VLMs entirely in cheap synthetic worlds while\ndelivering measurable gains on real-image agentic, spatial-reasoning, and\nweb-navigation benchmarks."
                },
                "authors": [
                    {
                        "name": "George Bredis"
                    },
                    {
                        "name": "Stanislav Dereka"
                    },
                    {
                        "name": "Viacheslav Sinii"
                    },
                    {
                        "name": "Ruslan Rakhimov"
                    },
                    {
                        "name": "Daniil Gavrilov"
                    }
                ],
                "author_detail": {
                    "name": "Daniil Gavrilov"
                },
                "author": "Daniil Gavrilov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04279v1",
                "updated": "2025-08-06T10:08:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    10,
                    8,
                    47,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T10:08:47Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    10,
                    8,
                    47,
                    2,
                    218,
                    0
                ],
                "title": "Mockingbird: How does LLM perform in general machine learning tasks?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mockingbird: How does LLM perform in general machine learning tasks?"
                },
                "summary": "Large language models (LLMs) are now being used with increasing frequency as\nchat bots, tasked with the summarizing information or generating text and code\nin accordance with user instructions. The rapid increase in reasoning\ncapabilities and inference speed of LLMs has revealed their remarkable\npotential for applications extending beyond the domain of chat bots to general\nmachine learning tasks. This work is conducted out of the curiosity about such\npotential. In this work, we propose a framework Mockingbird to adapt LLMs to\ngeneral machine learning tasks and evaluate its performance and scalability on\nseveral general machine learning tasks. The core concept of this framework is\ninstructing LLMs to role-play functions and reflect on its mistakes to improve\nitself. Our evaluation and analysis result shows that LLM-driven machine\nlearning methods, such as Mockingbird, can achieve acceptable results on common\nmachine learning tasks; however, solely reflecting on its own currently cannot\noutperform the effect of domain-specific documents and feedback from human\nexperts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are now being used with increasing frequency as\nchat bots, tasked with the summarizing information or generating text and code\nin accordance with user instructions. The rapid increase in reasoning\ncapabilities and inference speed of LLMs has revealed their remarkable\npotential for applications extending beyond the domain of chat bots to general\nmachine learning tasks. This work is conducted out of the curiosity about such\npotential. In this work, we propose a framework Mockingbird to adapt LLMs to\ngeneral machine learning tasks and evaluate its performance and scalability on\nseveral general machine learning tasks. The core concept of this framework is\ninstructing LLMs to role-play functions and reflect on its mistakes to improve\nitself. Our evaluation and analysis result shows that LLM-driven machine\nlearning methods, such as Mockingbird, can achieve acceptable results on common\nmachine learning tasks; however, solely reflecting on its own currently cannot\noutperform the effect of domain-specific documents and feedback from human\nexperts."
                },
                "authors": [
                    {
                        "name": "Haoyu Jia"
                    },
                    {
                        "name": "Yoshiki Obinata"
                    },
                    {
                        "name": "Kento Kawaharazuka"
                    },
                    {
                        "name": "Kei Okada"
                    }
                ],
                "author_detail": {
                    "name": "Kei Okada"
                },
                "author": "Kei Okada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04278v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04278v1",
                "updated": "2025-08-06T10:06:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    10,
                    6,
                    11,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T10:06:11Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    10,
                    6,
                    11,
                    2,
                    218,
                    0
                ],
                "title": "Large Language Model's Multi-Capability Alignment in Biomedical Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model's Multi-Capability Alignment in Biomedical Domain"
                },
                "summary": "BalancedBio is a theoretically grounded framework for parameter-efficient\nbiomedical reasoning, addressing multi-capability integration in\ndomain-specific AI alignment. It establishes the Biomedical Multi-Capability\nConvergence Theorem, proving orthogonal gradient spaces are essential to\nprevent capability interference for safe deployment. Key innovations include:\n(1) Medical Knowledge Grounded Synthetic Generation (MKGSG), extending\nSource2Synth with clinical workflow constraints and medical ontology validation\nfor factual accuracy and safety; and (2) Capability Aware Group Relative Policy\nOptimization, deriving optimal hybrid reward weighting to maintain\northogonality in RL, using a reward model with rule-based and model-based\nscores adapted to biomedical tasks. Mathematical analysis proves Pareto-optimal\nconvergence, preserving performance across capabilities. It achieves\nstate-of-the-art results in its parameter class: domain expertise (80.95%\nBIOMED-MMLU, +15.32% over baseline), reasoning (61.94%, +7.75%), instruction\nfollowing (67.95%, +6.44%), and integration (86.7%, +18.5%). Theoretical safety\nguarantees include bounds on capability preservation and clinical accuracy.\nReal-world deployment yields 78% cost reduction, 23% improved diagnostic\naccuracy, and 89% clinician acceptance. This work provides a principled\nmethodology for biomedical AI alignment, enabling efficient reasoning with\nessential safety and reliability, with the 0.5B model version to be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BalancedBio is a theoretically grounded framework for parameter-efficient\nbiomedical reasoning, addressing multi-capability integration in\ndomain-specific AI alignment. It establishes the Biomedical Multi-Capability\nConvergence Theorem, proving orthogonal gradient spaces are essential to\nprevent capability interference for safe deployment. Key innovations include:\n(1) Medical Knowledge Grounded Synthetic Generation (MKGSG), extending\nSource2Synth with clinical workflow constraints and medical ontology validation\nfor factual accuracy and safety; and (2) Capability Aware Group Relative Policy\nOptimization, deriving optimal hybrid reward weighting to maintain\northogonality in RL, using a reward model with rule-based and model-based\nscores adapted to biomedical tasks. Mathematical analysis proves Pareto-optimal\nconvergence, preserving performance across capabilities. It achieves\nstate-of-the-art results in its parameter class: domain expertise (80.95%\nBIOMED-MMLU, +15.32% over baseline), reasoning (61.94%, +7.75%), instruction\nfollowing (67.95%, +6.44%), and integration (86.7%, +18.5%). Theoretical safety\nguarantees include bounds on capability preservation and clinical accuracy.\nReal-world deployment yields 78% cost reduction, 23% improved diagnostic\naccuracy, and 89% clinician acceptance. This work provides a principled\nmethodology for biomedical AI alignment, enabling efficient reasoning with\nessential safety and reliability, with the 0.5B model version to be released."
                },
                "authors": [
                    {
                        "name": "Wentao Wu"
                    },
                    {
                        "name": "Linqing Chen"
                    },
                    {
                        "name": "Hanmeng Zhong"
                    },
                    {
                        "name": "Weilei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weilei Wang"
                },
                "author": "Weilei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04278v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04276v1",
                "updated": "2025-08-06T10:01:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    10,
                    1,
                    26,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T10:01:26Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    10,
                    1,
                    26,
                    2,
                    218,
                    0
                ],
                "title": "A Few Words Can Distort Graphs: Knowledge Poisoning Attacks on\n  Graph-based Retrieval-Augmented Generation of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Few Words Can Distort Graphs: Knowledge Poisoning Attacks on\n  Graph-based Retrieval-Augmented Generation of Large Language Models"
                },
                "summary": "Graph-based Retrieval-Augmented Generation (GraphRAG) has recently emerged as\na promising paradigm for enhancing large language models (LLMs) by converting\nraw text into structured knowledge graphs, improving both accuracy and\nexplainability. However, GraphRAG relies on LLMs to extract knowledge from raw\ntext during graph construction, and this process can be maliciously manipulated\nto implant misleading information. Targeting this attack surface, we propose\ntwo knowledge poisoning attacks (KPAs) and demonstrate that modifying only a\nfew words in the source text can significantly change the constructed graph,\npoison the GraphRAG, and severely mislead downstream reasoning. The first\nattack, named Targeted KPA (TKPA), utilizes graph-theoretic analysis to locate\nvulnerable nodes in the generated graphs and rewrites the corresponding\nnarratives with LLMs, achieving precise control over specific\nquestion-answering (QA) outcomes with a success rate of 93.1\\%, while keeping\nthe poisoned text fluent and natural. The second attack, named Universal KPA\n(UKPA), exploits linguistic cues such as pronouns and dependency relations to\ndisrupt the structural integrity of the generated graph by altering globally\ninfluential words. With fewer than 0.05\\% of full text modified, the QA\naccuracy collapses from 95\\% to 50\\%. Furthermore, experiments show that\nstate-of-the-art defense methods fail to detect these attacks, highlighting\nthat securing GraphRAG pipelines against knowledge poisoning remains largely\nunexplored.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-based Retrieval-Augmented Generation (GraphRAG) has recently emerged as\na promising paradigm for enhancing large language models (LLMs) by converting\nraw text into structured knowledge graphs, improving both accuracy and\nexplainability. However, GraphRAG relies on LLMs to extract knowledge from raw\ntext during graph construction, and this process can be maliciously manipulated\nto implant misleading information. Targeting this attack surface, we propose\ntwo knowledge poisoning attacks (KPAs) and demonstrate that modifying only a\nfew words in the source text can significantly change the constructed graph,\npoison the GraphRAG, and severely mislead downstream reasoning. The first\nattack, named Targeted KPA (TKPA), utilizes graph-theoretic analysis to locate\nvulnerable nodes in the generated graphs and rewrites the corresponding\nnarratives with LLMs, achieving precise control over specific\nquestion-answering (QA) outcomes with a success rate of 93.1\\%, while keeping\nthe poisoned text fluent and natural. The second attack, named Universal KPA\n(UKPA), exploits linguistic cues such as pronouns and dependency relations to\ndisrupt the structural integrity of the generated graph by altering globally\ninfluential words. With fewer than 0.05\\% of full text modified, the QA\naccuracy collapses from 95\\% to 50\\%. Furthermore, experiments show that\nstate-of-the-art defense methods fail to detect these attacks, highlighting\nthat securing GraphRAG pipelines against knowledge poisoning remains largely\nunexplored."
                },
                "authors": [
                    {
                        "name": "Jiayi Wen"
                    },
                    {
                        "name": "Tianxin Chen"
                    },
                    {
                        "name": "Zhirun Zheng"
                    },
                    {
                        "name": "Cheng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Cheng Huang"
                },
                "author": "Cheng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04266v1",
                "updated": "2025-08-06T09:51:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    9,
                    51,
                    30,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T09:51:30Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    9,
                    51,
                    30,
                    2,
                    218,
                    0
                ],
                "title": "ShoppingBench: A Real-World Intent-Grounded Shopping Benchmark for\n  LLM-based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShoppingBench: A Real-World Intent-Grounded Shopping Benchmark for\n  LLM-based Agents"
                },
                "summary": "Existing benchmarks in e-commerce primarily focus on basic user intents, such\nas finding or purchasing products. However, real-world users often pursue more\ncomplex goals, such as applying vouchers, managing budgets, and finding\nmulti-products seller. To bridge this gap, we propose ShoppingBench, a novel\nend-to-end shopping benchmark designed to encompass increasingly challenging\nlevels of grounded intent. Specifically, we propose a scalable framework to\nsimulate user instructions based on various intents derived from sampled\nreal-world products. To facilitate consistent and reliable evaluations, we\nprovide a large-scale shopping sandbox that serves as an interactive simulated\nenvironment, incorporating over 2.5 million real-world products. Experimental\nresults demonstrate that even state-of-the-art language agents (such as\nGPT-4.1) achieve absolute success rates under 50% on our benchmark tasks,\nhighlighting the significant challenges posed by our ShoppingBench. In\naddition, we propose a trajectory distillation strategy and leverage supervised\nfine-tuning, along with reinforcement learning on synthetic trajectories, to\ndistill the capabilities of a large language agent into a smaller one. As a\nresult, our trained agent achieves competitive performance compared to GPT-4.1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing benchmarks in e-commerce primarily focus on basic user intents, such\nas finding or purchasing products. However, real-world users often pursue more\ncomplex goals, such as applying vouchers, managing budgets, and finding\nmulti-products seller. To bridge this gap, we propose ShoppingBench, a novel\nend-to-end shopping benchmark designed to encompass increasingly challenging\nlevels of grounded intent. Specifically, we propose a scalable framework to\nsimulate user instructions based on various intents derived from sampled\nreal-world products. To facilitate consistent and reliable evaluations, we\nprovide a large-scale shopping sandbox that serves as an interactive simulated\nenvironment, incorporating over 2.5 million real-world products. Experimental\nresults demonstrate that even state-of-the-art language agents (such as\nGPT-4.1) achieve absolute success rates under 50% on our benchmark tasks,\nhighlighting the significant challenges posed by our ShoppingBench. In\naddition, we propose a trajectory distillation strategy and leverage supervised\nfine-tuning, along with reinforcement learning on synthetic trajectories, to\ndistill the capabilities of a large language agent into a smaller one. As a\nresult, our trained agent achieves competitive performance compared to GPT-4.1."
                },
                "authors": [
                    {
                        "name": "Jiangyuan Wang"
                    },
                    {
                        "name": "Kejun Xiao"
                    },
                    {
                        "name": "Qi Sun"
                    },
                    {
                        "name": "Huaipeng Zhao"
                    },
                    {
                        "name": "Tao Luo"
                    },
                    {
                        "name": "Jiandong Zhang"
                    },
                    {
                        "name": "Xiaoyi Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyi Zeng"
                },
                "author": "Xiaoyi Zeng",
                "arxiv_comment": "submit to AAAI2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04265v1",
                "updated": "2025-08-06T09:50:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    9,
                    50,
                    39,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T09:50:39Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    9,
                    50,
                    39,
                    2,
                    218,
                    0
                ],
                "title": "SelectiveShield: Lightweight Hybrid Defense Against Gradient Leakage in\n  Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectiveShield: Lightweight Hybrid Defense Against Gradient Leakage in\n  Federated Learning"
                },
                "summary": "Federated Learning (FL) enables collaborative model training on decentralized\ndata but remains vulnerable to gradient leakage attacks that can reconstruct\nsensitive user information. Existing defense mechanisms, such as differential\nprivacy (DP) and homomorphic encryption (HE), often introduce a trade-off\nbetween privacy, model utility, and system overhead, a challenge that is\nexacerbated in heterogeneous environments with non-IID data and varying client\ncapabilities. To address these limitations, we propose SelectiveShield, a\nlightweight hybrid defense framework that adaptively integrates selective\nhomomorphic encryption and differential privacy. SelectiveShield leverages\nFisher information to quantify parameter sensitivity, allowing clients to\nidentify critical parameters locally. Through a collaborative negotiation\nprotocol, clients agree on a shared set of the most sensitive parameters for\nprotection via homomorphic encryption. Parameters that are uniquely important\nto individual clients are retained locally, fostering personalization, while\nnon-critical parameters are protected with adaptive differential privacy noise.\nExtensive experiments demonstrate that SelectiveShield maintains strong model\nutility while significantly mitigating gradient leakage risks, offering a\npractical and scalable defense mechanism for real-world federated learning\ndeployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) enables collaborative model training on decentralized\ndata but remains vulnerable to gradient leakage attacks that can reconstruct\nsensitive user information. Existing defense mechanisms, such as differential\nprivacy (DP) and homomorphic encryption (HE), often introduce a trade-off\nbetween privacy, model utility, and system overhead, a challenge that is\nexacerbated in heterogeneous environments with non-IID data and varying client\ncapabilities. To address these limitations, we propose SelectiveShield, a\nlightweight hybrid defense framework that adaptively integrates selective\nhomomorphic encryption and differential privacy. SelectiveShield leverages\nFisher information to quantify parameter sensitivity, allowing clients to\nidentify critical parameters locally. Through a collaborative negotiation\nprotocol, clients agree on a shared set of the most sensitive parameters for\nprotection via homomorphic encryption. Parameters that are uniquely important\nto individual clients are retained locally, fostering personalization, while\nnon-critical parameters are protected with adaptive differential privacy noise.\nExtensive experiments demonstrate that SelectiveShield maintains strong model\nutility while significantly mitigating gradient leakage risks, offering a\npractical and scalable defense mechanism for real-world federated learning\ndeployments."
                },
                "authors": [
                    {
                        "name": "Borui Li"
                    },
                    {
                        "name": "Li Yan"
                    },
                    {
                        "name": "Jianmin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jianmin Liu"
                },
                "author": "Jianmin Liu",
                "arxiv_comment": "19 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14448v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14448v2",
                "updated": "2025-08-06T09:42:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    9,
                    42,
                    36,
                    2,
                    218,
                    0
                ],
                "published": "2025-06-17T12:13:56Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    12,
                    13,
                    56,
                    1,
                    168,
                    0
                ],
                "title": "How Far Can LLMs Improve from Experience? Measuring Test-Time Learning\n  Ability in LLMs with Human Comparison",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Far Can LLMs Improve from Experience? Measuring Test-Time Learning\n  Ability in LLMs with Human Comparison"
                },
                "summary": "As evaluation designs of large language models may shape our trajectory\ntoward artificial general intelligence, comprehensive and forward-looking\nassessment is essential. Existing benchmarks primarily assess static knowledge,\nwhile intelligence also entails the ability to rapidly learn from experience.\nTo this end, we advocate for the evaluation of Test-time Learning, the capacity\nto improve performance in experience-based, reasoning-intensive tasks during\ntest time. In this work, we propose semantic games as effective testbeds for\nevaluating test-time learning, due to their resistance to saturation and\ninherent demand for strategic reasoning. We introduce an objective evaluation\nframework that compares model performance under both limited and cumulative\nexperience settings, and contains four forms of experience representation. To\nprovide a comparative baseline, we recruit eight human participants to complete\nthe same task. Results show that LLMs exhibit measurable test-time learning\ncapabilities; however, their improvements are less stable under cumulative\nexperience and progress more slowly than those observed in humans. These\nfindings underscore the potential of LLMs as general-purpose learning machines,\nwhile also revealing a substantial intellectual gap between models and humans,\nirrespective of how well LLMs perform on static benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As evaluation designs of large language models may shape our trajectory\ntoward artificial general intelligence, comprehensive and forward-looking\nassessment is essential. Existing benchmarks primarily assess static knowledge,\nwhile intelligence also entails the ability to rapidly learn from experience.\nTo this end, we advocate for the evaluation of Test-time Learning, the capacity\nto improve performance in experience-based, reasoning-intensive tasks during\ntest time. In this work, we propose semantic games as effective testbeds for\nevaluating test-time learning, due to their resistance to saturation and\ninherent demand for strategic reasoning. We introduce an objective evaluation\nframework that compares model performance under both limited and cumulative\nexperience settings, and contains four forms of experience representation. To\nprovide a comparative baseline, we recruit eight human participants to complete\nthe same task. Results show that LLMs exhibit measurable test-time learning\ncapabilities; however, their improvements are less stable under cumulative\nexperience and progress more slowly than those observed in humans. These\nfindings underscore the potential of LLMs as general-purpose learning machines,\nwhile also revealing a substantial intellectual gap between models and humans,\nirrespective of how well LLMs perform on static benchmarks."
                },
                "authors": [
                    {
                        "name": "Jiayin Wang"
                    },
                    {
                        "name": "Zhiquang Guo"
                    },
                    {
                        "name": "Weizhi Ma"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14448v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14448v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04257v1",
                "updated": "2025-08-06T09:40:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    9,
                    40,
                    9,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T09:40:09Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    9,
                    40,
                    9,
                    2,
                    218,
                    0
                ],
                "title": "KVSink: Understanding and Enhancing the Preservation of Attention Sinks\n  in KV Cache Quantization for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVSink: Understanding and Enhancing the Preservation of Attention Sinks\n  in KV Cache Quantization for LLMs"
                },
                "summary": "Key-Value (KV) cache quantization has become a widely adopted optimization\ntechnique for efficient large language models (LLMs) inference by reducing KV\ncache memory usage and mitigating memory-bound constraints. Recent studies have\nemphasized the importance of preserving the original precision of KVs for the\nfirst few tokens to ensure the protection of attention sinks. While this\napproach has proven effective in mitigating performance degradation, its\nunderlying principles remain insufficiently understood. Moreover, it fails to\naddress the recent discovery that attention sinks can emerge beyond the initial\ntoken positions. In this work, we elucidate the underlying mechanisms of\nattention sinks during inference by examining their role in the cross-layer\nevolution of extreme activation outliers. Additionally, we provide a\ncomprehensive analysis of the interplay between attention sinks and KV cache\nquantization. Based on our enhanced understanding, we introduce\n\\textit{\\textbf{KVSink}}, a plug-and-play method that effectively predicts sink\ntokens with negligible overhead, enabling more thorough preservation. Extensive\nexperiments demonstrate that KVSink outperforms the existing Preserve-First-N\n(PFN) strategy, offering more effective preservation of attention sinks during\nKV cache quantization. Moreover, when applied to the well-established KVQuant\nmethod, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit\nnumerical outliers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache quantization has become a widely adopted optimization\ntechnique for efficient large language models (LLMs) inference by reducing KV\ncache memory usage and mitigating memory-bound constraints. Recent studies have\nemphasized the importance of preserving the original precision of KVs for the\nfirst few tokens to ensure the protection of attention sinks. While this\napproach has proven effective in mitigating performance degradation, its\nunderlying principles remain insufficiently understood. Moreover, it fails to\naddress the recent discovery that attention sinks can emerge beyond the initial\ntoken positions. In this work, we elucidate the underlying mechanisms of\nattention sinks during inference by examining their role in the cross-layer\nevolution of extreme activation outliers. Additionally, we provide a\ncomprehensive analysis of the interplay between attention sinks and KV cache\nquantization. Based on our enhanced understanding, we introduce\n\\textit{\\textbf{KVSink}}, a plug-and-play method that effectively predicts sink\ntokens with negligible overhead, enabling more thorough preservation. Extensive\nexperiments demonstrate that KVSink outperforms the existing Preserve-First-N\n(PFN) strategy, offering more effective preservation of attention sinks during\nKV cache quantization. Moreover, when applied to the well-established KVQuant\nmethod, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit\nnumerical outliers."
                },
                "authors": [
                    {
                        "name": "Zunhai Su"
                    },
                    {
                        "name": "Kehong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Kehong Yuan"
                },
                "author": "Kehong Yuan",
                "arxiv_comment": "Published as a conference paper at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04251v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04251v1",
                "updated": "2025-08-06T09:31:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    9,
                    31,
                    44,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T09:31:44Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    9,
                    31,
                    44,
                    2,
                    218,
                    0
                ],
                "title": "T3Time: Tri-Modal Time Series Forecasting via Adaptive Multi-Head\n  Alignment and Residual Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T3Time: Tri-Modal Time Series Forecasting via Adaptive Multi-Head\n  Alignment and Residual Fusion"
                },
                "summary": "Multivariate time series forecasting (MTSF) seeks to model temporal dynamics\namong variables to predict future trends. Transformer-based models and large\nlanguage models (LLMs) have shown promise due to their ability to capture\nlong-range dependencies and patterns. However, current methods often rely on\nrigid inductive biases, ignore intervariable interactions, or apply static\nfusion strategies that limit adaptability across forecast horizons. These\nlimitations create bottlenecks in capturing nuanced, horizon-specific\nrelationships in time-series data. To solve this problem, we propose T3Time, a\nnovel trimodal framework consisting of time, spectral, and prompt branches,\nwhere the dedicated frequency encoding branch captures the periodic structures\nalong with a gating mechanism that learns prioritization between temporal and\nspectral features based on the prediction horizon. We also proposed a mechanism\nwhich adaptively aggregates multiple cross-modal alignment heads by dynamically\nweighting the importance of each head based on the features. Extensive\nexperiments on benchmark datasets demonstrate that our model consistently\noutperforms state-of-the-art baselines, achieving an average reduction of 3.28%\nin MSE and 2.29% in MAE. Furthermore, it shows strong generalization in\nfew-shot learning settings: with 5% training data, we see a reduction in MSE\nand MAE by 4.13% and 1.91%, respectively; and with 10% data, by 3.62% and 1.98%\non average. Code - https://github.com/monaf-chowdhury/T3Time/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multivariate time series forecasting (MTSF) seeks to model temporal dynamics\namong variables to predict future trends. Transformer-based models and large\nlanguage models (LLMs) have shown promise due to their ability to capture\nlong-range dependencies and patterns. However, current methods often rely on\nrigid inductive biases, ignore intervariable interactions, or apply static\nfusion strategies that limit adaptability across forecast horizons. These\nlimitations create bottlenecks in capturing nuanced, horizon-specific\nrelationships in time-series data. To solve this problem, we propose T3Time, a\nnovel trimodal framework consisting of time, spectral, and prompt branches,\nwhere the dedicated frequency encoding branch captures the periodic structures\nalong with a gating mechanism that learns prioritization between temporal and\nspectral features based on the prediction horizon. We also proposed a mechanism\nwhich adaptively aggregates multiple cross-modal alignment heads by dynamically\nweighting the importance of each head based on the features. Extensive\nexperiments on benchmark datasets demonstrate that our model consistently\noutperforms state-of-the-art baselines, achieving an average reduction of 3.28%\nin MSE and 2.29% in MAE. Furthermore, it shows strong generalization in\nfew-shot learning settings: with 5% training data, we see a reduction in MSE\nand MAE by 4.13% and 1.91%, respectively; and with 10% data, by 3.62% and 1.98%\non average. Code - https://github.com/monaf-chowdhury/T3Time/"
                },
                "authors": [
                    {
                        "name": "Abdul Monaf Chowdhury"
                    },
                    {
                        "name": "Rabeya Akter"
                    },
                    {
                        "name": "Safaeid Hossain Arib"
                    }
                ],
                "author_detail": {
                    "name": "Safaeid Hossain Arib"
                },
                "author": "Safaeid Hossain Arib",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04251v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]