[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.04965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04965v1",
                "updated": "2024-11-07T18:41:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    41,
                    50,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T18:41:50Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    41,
                    50,
                    3,
                    312,
                    0
                ],
                "title": "BitNet a4.8: 4-bit Activations for 1-bit LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitNet a4.8: 4-bit Activations for 1-bit LLMs"
                },
                "summary": "Recent research on the 1-bit Large Language Models (LLMs), such as BitNet\nb1.58, presents a promising direction for reducing the inference cost of LLMs\nwhile maintaining their performance. In this work, we introduce BitNet a4.8,\nenabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid\nquantization and sparsification strategy to mitigate the quantization errors\nintroduced by the outlier channels. Specifically, we utilize 4-bit activations\nfor inputs to the attention and feed-forward network layers, while sparsifying\nintermediate states followed with 8-bit quantization. Extensive experiments\ndemonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58\nwith equivalent training costs, while being faster in inference with enabling\n4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of\nparameters and supports 3-bit KV cache, further enhancing the efficiency of\nlarge-scale LLM deployment and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research on the 1-bit Large Language Models (LLMs), such as BitNet\nb1.58, presents a promising direction for reducing the inference cost of LLMs\nwhile maintaining their performance. In this work, we introduce BitNet a4.8,\nenabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid\nquantization and sparsification strategy to mitigate the quantization errors\nintroduced by the outlier channels. Specifically, we utilize 4-bit activations\nfor inputs to the attention and feed-forward network layers, while sparsifying\nintermediate states followed with 8-bit quantization. Extensive experiments\ndemonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58\nwith equivalent training costs, while being faster in inference with enabling\n4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of\nparameters and supports 3-bit KV cache, further enhancing the efficiency of\nlarge-scale LLM deployment and inference."
                },
                "authors": [
                    {
                        "name": "Hongyu Wang"
                    },
                    {
                        "name": "Shuming Ma"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02397v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02397v2",
                "updated": "2024-11-07T17:06:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    17,
                    6,
                    32,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-04T18:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    59,
                    44,
                    0,
                    309,
                    0
                ],
                "title": "Adaptive Caching for Faster Video Generation with Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Caching for Faster Video Generation with Diffusion Transformers"
                },
                "summary": "Generating temporally-consistent high-fidelity videos can be computationally\nexpensive, especially over longer temporal spans. More-recent Diffusion\nTransformers (DiTs) -- despite making significant headway in this context --\nhave only heightened such challenges as they rely on larger models and heavier\nattention mechanisms, resulting in slower inference speeds. In this paper, we\nintroduce a training-free method to accelerate video DiTs, termed Adaptive\nCaching (AdaCache), which is motivated by the fact that \"not all videos are\ncreated equal\": meaning, some videos require fewer denoising steps to attain a\nreasonable quality than others. Building on this, we not only cache\ncomputations through the diffusion process, but also devise a caching schedule\ntailored to each video generation, maximizing the quality-latency trade-off. We\nfurther introduce a Motion Regularization (MoReg) scheme to utilize video\ninformation within AdaCache, essentially controlling the compute allocation\nbased on motion content. Altogether, our plug-and-play contributions grant\nsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video\ngeneration) without sacrificing the generation quality, across multiple video\nDiT baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating temporally-consistent high-fidelity videos can be computationally\nexpensive, especially over longer temporal spans. More-recent Diffusion\nTransformers (DiTs) -- despite making significant headway in this context --\nhave only heightened such challenges as they rely on larger models and heavier\nattention mechanisms, resulting in slower inference speeds. In this paper, we\nintroduce a training-free method to accelerate video DiTs, termed Adaptive\nCaching (AdaCache), which is motivated by the fact that \"not all videos are\ncreated equal\": meaning, some videos require fewer denoising steps to attain a\nreasonable quality than others. Building on this, we not only cache\ncomputations through the diffusion process, but also devise a caching schedule\ntailored to each video generation, maximizing the quality-latency trade-off. We\nfurther introduce a Motion Regularization (MoReg) scheme to utilize video\ninformation within AdaCache, essentially controlling the compute allocation\nbased on motion content. Altogether, our plug-and-play contributions grant\nsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video\ngeneration) without sacrificing the generation quality, across multiple video\nDiT baselines."
                },
                "authors": [
                    {
                        "name": "Kumara Kahatapitiya"
                    },
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Sen He"
                    },
                    {
                        "name": "Ding Liu"
                    },
                    {
                        "name": "Menglin Jia"
                    },
                    {
                        "name": "Chenyang Zhang"
                    },
                    {
                        "name": "Michael S. Ryoo"
                    },
                    {
                        "name": "Tian Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tian Xie"
                },
                "author": "Tian Xie",
                "arxiv_comment": "Project-page is available at https://adacache-dit.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02397v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04762v1",
                "updated": "2024-11-07T14:59:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    59,
                    44,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T14:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    59,
                    44,
                    3,
                    312,
                    0
                ],
                "title": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems"
                },
                "summary": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms."
                },
                "authors": [
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Jiaxu Wu"
                    },
                    {
                        "name": "Long He"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Abbas Jamalipour"
                    },
                    {
                        "name": "Shiwen Mao"
                    }
                ],
                "author_detail": {
                    "name": "Shiwen Mao"
                },
                "author": "Shiwen Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16591v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16591v2",
                "updated": "2024-11-07T09:33:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    9,
                    33,
                    40,
                    3,
                    312,
                    0
                ],
                "published": "2024-05-26T14:50:40Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    14,
                    50,
                    40,
                    6,
                    147,
                    0
                ],
                "title": "CapS-Adapter: Caption-based MultiModal Adapter in Zero-Shot\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CapS-Adapter: Caption-based MultiModal Adapter in Zero-Shot\n  Classification"
                },
                "summary": "Recent advances in vision-language foundational models, such as CLIP, have\ndemonstrated significant strides in zero-shot classification. However, the\nextensive parameterization of models like CLIP necessitates a\nresource-intensive fine-tuning process. In response, TIP-Adapter and SuS-X have\nintroduced training-free methods aimed at bolstering the efficacy of downstream\ntasks. While these approaches incorporate support sets to maintain data\ndistribution consistency between knowledge cache and test sets, they often fall\nshort in terms of generalization on the test set, particularly when faced with\ntest data exhibiting substantial distributional variations. In this work, we\npresent CapS-Adapter, an innovative method that employs a caption-based support\nset, effectively harnessing both image and caption features to exceed existing\nstate-of-the-art techniques in training-free scenarios. CapS-Adapter adeptly\nconstructs support sets that closely mirror target distributions, utilizing\ninstance-level distribution features extracted from multimodal large models. By\nleveraging CLIP's single and cross-modal strengths, CapS-Adapter enhances\npredictive accuracy through the use of multimodal support sets. Our method\nachieves outstanding zero-shot classification results across 19 benchmark\ndatasets, improving accuracy by 2.19\\% over the previous leading method. Our\ncontributions are substantiated through extensive validation on multiple\nbenchmark datasets, demonstrating superior performance and robust\ngeneralization capabilities. Our code is made publicly available at\nhttps://github.com/WLuLi/CapS-Adapter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in vision-language foundational models, such as CLIP, have\ndemonstrated significant strides in zero-shot classification. However, the\nextensive parameterization of models like CLIP necessitates a\nresource-intensive fine-tuning process. In response, TIP-Adapter and SuS-X have\nintroduced training-free methods aimed at bolstering the efficacy of downstream\ntasks. While these approaches incorporate support sets to maintain data\ndistribution consistency between knowledge cache and test sets, they often fall\nshort in terms of generalization on the test set, particularly when faced with\ntest data exhibiting substantial distributional variations. In this work, we\npresent CapS-Adapter, an innovative method that employs a caption-based support\nset, effectively harnessing both image and caption features to exceed existing\nstate-of-the-art techniques in training-free scenarios. CapS-Adapter adeptly\nconstructs support sets that closely mirror target distributions, utilizing\ninstance-level distribution features extracted from multimodal large models. By\nleveraging CLIP's single and cross-modal strengths, CapS-Adapter enhances\npredictive accuracy through the use of multimodal support sets. Our method\nachieves outstanding zero-shot classification results across 19 benchmark\ndatasets, improving accuracy by 2.19\\% over the previous leading method. Our\ncontributions are substantiated through extensive validation on multiple\nbenchmark datasets, demonstrating superior performance and robust\ngeneralization capabilities. Our code is made publicly available at\nhttps://github.com/WLuLi/CapS-Adapter."
                },
                "authors": [
                    {
                        "name": "Qijie Wang"
                    },
                    {
                        "name": "Guandu Liu"
                    },
                    {
                        "name": "Bin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bin Wang"
                },
                "author": "Bin Wang",
                "arxiv_doi": "10.1145/3664647.3681566",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3664647.3681566",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.16591v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16591v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ACM Multimedia 2024 Poster",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01288v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01288v2",
                "updated": "2024-11-07T06:40:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    6,
                    40,
                    40,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-02T15:45:54Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "title": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01288v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01288v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02265v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02265v3",
                "updated": "2024-11-06T09:15:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    9,
                    15,
                    27,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-04T16:56:26Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    16,
                    56,
                    26,
                    0,
                    309,
                    0
                ],
                "title": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated\n  Parameters by Tencent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated\n  Parameters by Tencent"
                },
                "summary": "In this paper, we introduce Hunyuan-Large, which is currently the largest\nopen-source Transformer-based mixture of experts model, with a total of 389\nbillion parameters and 52 billion activation parameters, capable of handling up\nto 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior\nperformance across various benchmarks including language understanding and\ngeneration, logical reasoning, mathematical problem-solving, coding,\nlong-context, and aggregated tasks, where it outperforms LLama3.1-70B and\nexhibits comparable performance when compared to the significantly larger\nLLama3.1-405B model. Key practice of Hunyuan-Large include large-scale\nsynthetic data that is orders larger than in previous literature, a mixed\nexpert routing strategy, a key-value cache compression technique, and an\nexpert-specific learning rate strategy. Additionally, we also investigate the\nscaling laws and learning rate schedule of mixture of experts models, providing\nvaluable insights and guidances for future model development and optimization.\nThe code and checkpoints of Hunyuan-Large are released to facilitate future\ninnovations and applications.\n  Codes: https://github.com/Tencent/Hunyuan-Large\n  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Hunyuan-Large, which is currently the largest\nopen-source Transformer-based mixture of experts model, with a total of 389\nbillion parameters and 52 billion activation parameters, capable of handling up\nto 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior\nperformance across various benchmarks including language understanding and\ngeneration, logical reasoning, mathematical problem-solving, coding,\nlong-context, and aggregated tasks, where it outperforms LLama3.1-70B and\nexhibits comparable performance when compared to the significantly larger\nLLama3.1-405B model. Key practice of Hunyuan-Large include large-scale\nsynthetic data that is orders larger than in previous literature, a mixed\nexpert routing strategy, a key-value cache compression technique, and an\nexpert-specific learning rate strategy. Additionally, we also investigate the\nscaling laws and learning rate schedule of mixture of experts models, providing\nvaluable insights and guidances for future model development and optimization.\nThe code and checkpoints of Hunyuan-Large are released to facilitate future\ninnovations and applications.\n  Codes: https://github.com/Tencent/Hunyuan-Large\n  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large"
                },
                "authors": [
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Yanfeng Chen"
                    },
                    {
                        "name": "Yiqing Huang"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "Jiaqi Zhu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Shuaipeng Li"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Jonny Han"
                    },
                    {
                        "name": "Xiaobo Shu"
                    },
                    {
                        "name": "Jiahao Bu"
                    },
                    {
                        "name": "Zhongzhi Chen"
                    },
                    {
                        "name": "Xuemeng Huang"
                    },
                    {
                        "name": "Fengzong Lian"
                    },
                    {
                        "name": "Saiyong Yang"
                    },
                    {
                        "name": "Jianfeng Yan"
                    },
                    {
                        "name": "Yuyuan Zeng"
                    },
                    {
                        "name": "Xiaoqin Ren"
                    },
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Lulu Wu"
                    },
                    {
                        "name": "Yue Mao"
                    },
                    {
                        "name": "Jun Xia"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Suncong Zheng"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Dian Jiao"
                    },
                    {
                        "name": "Jinbao Xue"
                    },
                    {
                        "name": "Xipeng Zhang"
                    },
                    {
                        "name": "Decheng Wu"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Dengpeng Wu"
                    },
                    {
                        "name": "Guanghui Xu"
                    },
                    {
                        "name": "Shaohua Chen"
                    },
                    {
                        "name": "Shuang Chen"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Yigeng Hong"
                    },
                    {
                        "name": "Junqiang Zheng"
                    },
                    {
                        "name": "Chengcheng Xu"
                    },
                    {
                        "name": "Zongwei Li"
                    },
                    {
                        "name": "Xiong Kuang"
                    },
                    {
                        "name": "Jianglu Hu"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Yuchi Deng"
                    },
                    {
                        "name": "Guiyang Li"
                    },
                    {
                        "name": "Ao Liu"
                    },
                    {
                        "name": "Chenchen Zhang"
                    },
                    {
                        "name": "Shihui Hu"
                    },
                    {
                        "name": "Zilong Zhao"
                    },
                    {
                        "name": "Zifan Wu"
                    },
                    {
                        "name": "Yao Ding"
                    },
                    {
                        "name": "Weichao Wang"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Roberts Wang"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Peijie Yu"
                    },
                    {
                        "name": "Ze Zhao"
                    },
                    {
                        "name": "Xun Cao"
                    },
                    {
                        "name": "Hai Wang"
                    },
                    {
                        "name": "Fusheng Xiang"
                    },
                    {
                        "name": "Mengyuan Huang"
                    },
                    {
                        "name": "Zhiyuan Xiong"
                    },
                    {
                        "name": "Bin Hu"
                    },
                    {
                        "name": "Xuebin Hou"
                    },
                    {
                        "name": "Lei Jiang"
                    },
                    {
                        "name": "Jianqiang Ma"
                    },
                    {
                        "name": "Jiajia Wu"
                    },
                    {
                        "name": "Yaping Deng"
                    },
                    {
                        "name": "Yi Shen"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Liang Dong"
                    },
                    {
                        "name": "Weiwen Jia"
                    },
                    {
                        "name": "Hu Chen"
                    },
                    {
                        "name": "Feifei Liu"
                    },
                    {
                        "name": "Rui Yuan"
                    },
                    {
                        "name": "Huilin Xu"
                    },
                    {
                        "name": "Zhenxiang Yan"
                    },
                    {
                        "name": "Tengfei Cao"
                    },
                    {
                        "name": "Zhichao Hu"
                    },
                    {
                        "name": "Xinhua Feng"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Tinghao Yu"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Feng Zhang"
                    },
                    {
                        "name": "Jianchen Zhu"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Xirui Li"
                    },
                    {
                        "name": "Chong Zha"
                    },
                    {
                        "name": "Wen Ouyang"
                    },
                    {
                        "name": "Yinben Xia"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Zekun He"
                    },
                    {
                        "name": "Rongpeng Chen"
                    },
                    {
                        "name": "Jiawei Song"
                    },
                    {
                        "name": "Ruibin Chen"
                    },
                    {
                        "name": "Fan Jiang"
                    },
                    {
                        "name": "Chongqing Zhao"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Hao Gong"
                    },
                    {
                        "name": "Rong Gan"
                    },
                    {
                        "name": "Winston Hu"
                    },
                    {
                        "name": "Zhanhui Kang"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Jie Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Jiang"
                },
                "author": "Jie Jiang",
                "arxiv_comment": "17 pages, 4 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02265v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02265v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03731v1",
                "updated": "2024-11-06T07:53:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    53,
                    4,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T07:53:04Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    53,
                    4,
                    2,
                    311,
                    0
                ],
                "title": "Reducing Hyperparameter Tuning Costs in ML, Vision and Language Model\n  Training Pipelines via Memoization-Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing Hyperparameter Tuning Costs in ML, Vision and Language Model\n  Training Pipelines via Memoization-Awareness"
                },
                "summary": "The training or fine-tuning of machine learning, vision, and language models\nis often implemented as a pipeline: a sequence of stages encompassing data\npreparation, model training and evaluation. In this paper, we exploit pipeline\nstructures to reduce the cost of hyperparameter tuning for model\ntraining/fine-tuning, which is particularly valuable for language models given\ntheir high costs in GPU-days. We propose a \"memoization-aware\" Bayesian\nOptimization (BO) algorithm, EEIPU, that works in tandem with a pipeline\ncaching system, allowing it to evaluate significantly more hyperparameter\ncandidates per GPU-day than other tuning algorithms. The result is\nbetter-quality hyperparameters in the same amount of search time, or\nequivalently, reduced search time to reach the same hyperparameter quality. In\nour benchmarks on machine learning (model ensembles), vision (convolutional\narchitecture) and language (T5 architecture) pipelines, we compare EEIPU\nagainst recent BO algorithms: EEIPU produces an average of $103\\%$ more\nhyperparameter candidates (within the same budget), and increases the\nvalidation metric by an average of $108\\%$ more than other algorithms (where\nthe increase is measured starting from the end of warm-up iterations).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The training or fine-tuning of machine learning, vision, and language models\nis often implemented as a pipeline: a sequence of stages encompassing data\npreparation, model training and evaluation. In this paper, we exploit pipeline\nstructures to reduce the cost of hyperparameter tuning for model\ntraining/fine-tuning, which is particularly valuable for language models given\ntheir high costs in GPU-days. We propose a \"memoization-aware\" Bayesian\nOptimization (BO) algorithm, EEIPU, that works in tandem with a pipeline\ncaching system, allowing it to evaluate significantly more hyperparameter\ncandidates per GPU-day than other tuning algorithms. The result is\nbetter-quality hyperparameters in the same amount of search time, or\nequivalently, reduced search time to reach the same hyperparameter quality. In\nour benchmarks on machine learning (model ensembles), vision (convolutional\narchitecture) and language (T5 architecture) pipelines, we compare EEIPU\nagainst recent BO algorithms: EEIPU produces an average of $103\\%$ more\nhyperparameter candidates (within the same budget), and increases the\nvalidation metric by an average of $108\\%$ more than other algorithms (where\nthe increase is measured starting from the end of warm-up iterations)."
                },
                "authors": [
                    {
                        "name": "Abdelmajid Essofi"
                    },
                    {
                        "name": "Ridwan Salahuddeen"
                    },
                    {
                        "name": "Munachiso Nwadike"
                    },
                    {
                        "name": "Elnura Zhalieva"
                    },
                    {
                        "name": "Kun Zhang"
                    },
                    {
                        "name": "Eric Xing"
                    },
                    {
                        "name": "Willie Neiswanger"
                    },
                    {
                        "name": "Qirong Ho"
                    }
                ],
                "author_detail": {
                    "name": "Qirong Ho"
                },
                "author": "Qirong Ho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v2",
                "updated": "2024-11-06T07:12:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    12,
                    55,
                    2,
                    311,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "This work was submitted for review on Sept. 5, 2024, and the initial\n  version was uploaded to Arxiv on Sept. 30, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01433v2",
                "updated": "2024-11-06T01:49:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    1,
                    49,
                    45,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-03T04:25:46Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    4,
                    25,
                    46,
                    6,
                    308,
                    0
                ],
                "title": "HOBBIT: A Mixed Precision Expert Offloading System for Fast MoE\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HOBBIT: A Mixed Precision Expert Offloading System for Fast MoE\n  Inference"
                },
                "summary": "The Mixture-of-Experts (MoE) architecture has demonstrated significant\nadvantages in the era of Large Language Models (LLMs), offering enhanced\ncapabilities with reduced inference costs. However, deploying MoE-based LLMs on\nmemoryconstrained edge devices remains challenging due to their substantial\nmemory requirements. While existing expertoffloading methods alleviate the\nmemory requirements, they often incur significant expert-loading costs or\ncompromise model accuracy. We present HOBBIT, a mixed precision expert\noffloading system to enable flexible and efficient MoE inference. Our key\ninsight is that dynamically replacing less critical cache-miss experts with low\nprecision versions can substantially reduce expert-loading latency while\npreserving model accuracy. HOBBIT introduces three innovative techniques that\nmap the natural hierarchy of MoE computation: (1) a token-level dynamic expert\nloading mechanism, (2) a layer-level adaptive expert prefetching technique, and\n(3) a sequence-level multidimensional expert caching policy. These innovations\nfully leverage the benefits of mixedprecision expert inference. By implementing\nHOBBIT on top of the renowned LLM inference framework Llama.cpp, we evaluate\nits performance across different edge devices with representative MoE models.\nThe results demonstrate that HOBBIT achieves up to a 9.93x speedup in decoding\ncompared to state-of-the-art MoE offloading systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture-of-Experts (MoE) architecture has demonstrated significant\nadvantages in the era of Large Language Models (LLMs), offering enhanced\ncapabilities with reduced inference costs. However, deploying MoE-based LLMs on\nmemoryconstrained edge devices remains challenging due to their substantial\nmemory requirements. While existing expertoffloading methods alleviate the\nmemory requirements, they often incur significant expert-loading costs or\ncompromise model accuracy. We present HOBBIT, a mixed precision expert\noffloading system to enable flexible and efficient MoE inference. Our key\ninsight is that dynamically replacing less critical cache-miss experts with low\nprecision versions can substantially reduce expert-loading latency while\npreserving model accuracy. HOBBIT introduces three innovative techniques that\nmap the natural hierarchy of MoE computation: (1) a token-level dynamic expert\nloading mechanism, (2) a layer-level adaptive expert prefetching technique, and\n(3) a sequence-level multidimensional expert caching policy. These innovations\nfully leverage the benefits of mixedprecision expert inference. By implementing\nHOBBIT on top of the renowned LLM inference framework Llama.cpp, we evaluate\nits performance across different edge devices with representative MoE models.\nThe results demonstrate that HOBBIT achieves up to a 9.93x speedup in decoding\ncompared to state-of-the-art MoE offloading systems."
                },
                "authors": [
                    {
                        "name": "Peng Tang"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Xiaofeng Hou"
                    },
                    {
                        "name": "Yifei Pu"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Pheng-Ann Heng"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03174v1",
                "updated": "2024-11-05T15:22:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    22,
                    11,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T15:22:11Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    22,
                    11,
                    1,
                    310,
                    0
                ],
                "title": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression"
                },
                "summary": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Alex Zhong"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.05591v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.05591v3",
                "updated": "2024-11-05T08:34:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    8,
                    34,
                    44,
                    1,
                    310,
                    0
                ],
                "published": "2023-08-10T13:57:37Z",
                "published_parsed": [
                    2023,
                    8,
                    10,
                    13,
                    57,
                    37,
                    3,
                    222,
                    0
                ],
                "title": "Wireless Edge Content Broadcast via Integrated Terrestrial and\n  Non-terrestrial Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless Edge Content Broadcast via Integrated Terrestrial and\n  Non-terrestrial Networks"
                },
                "summary": "Non-terrestrial networks (NTN) have emerged as a transformative solution to\nbridge the digital divide and deliver essential services to remote and\nunderserved areas. In this context, low Earth orbit (LEO) satellite\nconstellations offer remarkable potential for efficient cache content broadcast\nin remote regions, thereby extending the reach of digital services. In this\npaper, we introduce a novel approach to optimize wireless edge content\nplacement using NTN. Despite wide coverage, the varying NTN transmission\ncapabilities must be carefully aligned with each content placement to maximize\nbroadcast efficiency. In this paper, we introduce a novel approach to optimize\nwireless edge content placement using NTN, positioning NTN as a complement to\nTN for achieving optimal content broadcasting. Specifically, we dynamically\nselect content for placement via NTN links. This selection is based on\npopularity and suitability for delivery through NTN, while considering the\norbital motion of LEO satellites. Our system-level case studies, based on a\npractical LEO constellation, demonstrate the significant improvement in\nplacement speed compared to existing methods, which neglect network mobility.\nWe also demonstrate that NTN links significantly outperform standalone wireless\nTN solutions, particularly in the early stages of content delivery. This\nadvantage is amplified when there is a higher correlation of content popularity\nacross geographical regions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-terrestrial networks (NTN) have emerged as a transformative solution to\nbridge the digital divide and deliver essential services to remote and\nunderserved areas. In this context, low Earth orbit (LEO) satellite\nconstellations offer remarkable potential for efficient cache content broadcast\nin remote regions, thereby extending the reach of digital services. In this\npaper, we introduce a novel approach to optimize wireless edge content\nplacement using NTN. Despite wide coverage, the varying NTN transmission\ncapabilities must be carefully aligned with each content placement to maximize\nbroadcast efficiency. In this paper, we introduce a novel approach to optimize\nwireless edge content placement using NTN, positioning NTN as a complement to\nTN for achieving optimal content broadcasting. Specifically, we dynamically\nselect content for placement via NTN links. This selection is based on\npopularity and suitability for delivery through NTN, while considering the\norbital motion of LEO satellites. Our system-level case studies, based on a\npractical LEO constellation, demonstrate the significant improvement in\nplacement speed compared to existing methods, which neglect network mobility.\nWe also demonstrate that NTN links significantly outperform standalone wireless\nTN solutions, particularly in the early stages of content delivery. This\nadvantage is amplified when there is a higher correlation of content popularity\nacross geographical regions."
                },
                "authors": [
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Giovanni Geraci"
                    },
                    {
                        "name": "Lingxiang Li"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Q. S. Quek"
                },
                "author": "Tony Q. S. Quek",
                "arxiv_comment": "This work is expanded on our paper presented at IEEE Globecom 2023:\n  F. Wang, G. Geraci and T. Q. S. Quek, \"Optimizing Cache Content Placement in\n  Integrated Terrestrial and Non-terrestrial Networks,\" GLOBECOM 2023 - 2023\n  IEEE Global Communications Conference, Kuala Lumpur, Malaysia, 2023, pp.\n  6609-6614",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.05591v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.05591v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v1",
                "updated": "2024-11-05T07:56:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "With the development of large language models (LLMs), the ability to handle\nlonger contexts has become a key capability for Web applications such as\ncross-document understanding and LLM-powered search systems. However, this\nprogress faces two major challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues hinder the\napplication of LLMs in long-context scenarios. In this paper, we propose\nDynamic Token-Level KV Cache Selection (TokenSelect), a model-agnostic,\ntraining-free method for efficient and accurate long-context inference.\nTokenSelect builds upon the observation of non-contiguous attention sparsity,\nusing Query-Key dot products to measure per-head KV Cache criticality at\ntoken-level. By per-head soft voting mechanism, TokenSelect selectively\ninvolves a small number of critical KV cache tokens in the attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesigned the Selection Cache based on observations of consecutive Query\nsimilarity and implemented efficient dot product kernel, significantly reducing\nthe overhead of token selection. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), the ability to handle\nlonger contexts has become a key capability for Web applications such as\ncross-document understanding and LLM-powered search systems. However, this\nprogress faces two major challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues hinder the\napplication of LLMs in long-context scenarios. In this paper, we propose\nDynamic Token-Level KV Cache Selection (TokenSelect), a model-agnostic,\ntraining-free method for efficient and accurate long-context inference.\nTokenSelect builds upon the observation of non-contiguous attention sparsity,\nusing Query-Key dot products to measure per-head KV Cache criticality at\ntoken-level. By per-head soft voting mechanism, TokenSelect selectively\ninvolves a small number of critical KV cache tokens in the attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesigned the Selection Cache based on observations of consecutive Query\nsimilarity and implemented efficient dot product kernel, significantly reducing\nthe overhead of token selection. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v1",
                "updated": "2024-11-05T05:41:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: Enhancing Cross-LLM Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: Enhancing Cross-LLM Communication"
                },
                "summary": "In multi-agent systems utilizing Large Language Models (LLMs), communication\nbetween agents traditionally relies on natural language. This communication\noften includes the full context of the query so far, which can introduce\nsignificant prefill-phase latency, especially with long contexts.\n  We introduce DroidSpeak, a novel framework to target this cross-LLM\ncommunication by leveraging the reuse of intermediate data, such as input\nembeddings (E-cache) and key-value caches (KV-cache). We efficiently bypass the\nneed to reprocess entire contexts for fine-tuned versions of the same\nfoundational model. This approach allows faster context integration while\nmaintaining the quality of task performance. Experimental evaluations\ndemonstrate DroidSpeak's ability to significantly accelerate inter-agent\ncommunication, achieving up to a 2.78x speedup in prefill latency with\nnegligible loss in accuracy. Our findings underscore the potential to create\nmore efficient and scalable multi-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In multi-agent systems utilizing Large Language Models (LLMs), communication\nbetween agents traditionally relies on natural language. This communication\noften includes the full context of the query so far, which can introduce\nsignificant prefill-phase latency, especially with long contexts.\n  We introduce DroidSpeak, a novel framework to target this cross-LLM\ncommunication by leveraging the reuse of intermediate data, such as input\nembeddings (E-cache) and key-value caches (KV-cache). We efficiently bypass the\nneed to reprocess entire contexts for fine-tuned versions of the same\nfoundational model. This approach allows faster context integration while\nmaintaining the quality of task performance. Experimental evaluations\ndemonstrate DroidSpeak's ability to significantly accelerate inter-agent\ncommunication, achieving up to a 2.78x speedup in prefill latency with\nnegligible loss in accuracy. Our findings underscore the potential to create\nmore efficient and scalable multi-agent systems."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Esha Choukse"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Madan Musuvathi"
                    }
                ],
                "author_detail": {
                    "name": "Madan Musuvathi"
                },
                "author": "Madan Musuvathi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02295v1",
                "updated": "2024-11-04T17:21:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    21,
                    58,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T17:21:58Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    21,
                    58,
                    0,
                    309,
                    0
                ],
                "title": "Kilovolt Pyroelectric Voltage Generation and Electrostatic Actuation\n  With Fluidic Heating",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kilovolt Pyroelectric Voltage Generation and Electrostatic Actuation\n  With Fluidic Heating"
                },
                "summary": "Integrated micro power generators are crucial components for micro robotic\nplatforms to demonstrate untethered operation and to achieve autonomy. Current\nmicro robotic electrostatic actuators typically require hundreds to thousands\nof voltages to output sufficient work. Pyroelectricity is one such source of\nhigh voltages that can be scaled to small form factors. This paper demonstrates\na distributed pyroelectric high voltage generation mechanism to power kV\nactuators using alternating exposure of crystals to hot and cold water (300C to\n900C water temperature). Using this fluidic temperature control, a\npyroelectrically generated voltage of 2470 V was delivered to a 2 pF storage\ncapacitor yielding a 6.10 {\\mu}J stored energy. A maximum energy of 17.46\n{\\mu}J was delivered to a 47 pF capacitor at 861 V. The recirculating water can\nbe used to heat a distributed array of converters to generate electricity in\ndistant robotic actuator sections. The development of this distributed system\nwould enable untethered micro-robot to be operated with a flexible body and\nfree of battery recharging, which advances its applications in the real world.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrated micro power generators are crucial components for micro robotic\nplatforms to demonstrate untethered operation and to achieve autonomy. Current\nmicro robotic electrostatic actuators typically require hundreds to thousands\nof voltages to output sufficient work. Pyroelectricity is one such source of\nhigh voltages that can be scaled to small form factors. This paper demonstrates\na distributed pyroelectric high voltage generation mechanism to power kV\nactuators using alternating exposure of crystals to hot and cold water (300C to\n900C water temperature). Using this fluidic temperature control, a\npyroelectrically generated voltage of 2470 V was delivered to a 2 pF storage\ncapacitor yielding a 6.10 {\\mu}J stored energy. A maximum energy of 17.46\n{\\mu}J was delivered to a 47 pF capacitor at 861 V. The recirculating water can\nbe used to heat a distributed array of converters to generate electricity in\ndistant robotic actuator sections. The development of this distributed system\nwould enable untethered micro-robot to be operated with a flexible body and\nfree of battery recharging, which advances its applications in the real world."
                },
                "authors": [
                    {
                        "name": "Di Ni"
                    },
                    {
                        "name": "Ved Gund"
                    },
                    {
                        "name": "Landon Ivy"
                    },
                    {
                        "name": "Amit Lal"
                    }
                ],
                "author_detail": {
                    "name": "Amit Lal"
                },
                "author": "Amit Lal",
                "arxiv_doi": "10.31438/trf.hh2022.16",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.31438/trf.hh2022.16",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.02295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted and published at Hilton Head Workshop 2022: A Solid-State\n  Sensors, Actuators and Microsystems Workshop",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10740v2",
                "updated": "2024-11-04T12:14:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    12,
                    14,
                    7,
                    0,
                    309,
                    0
                ],
                "published": "2024-07-15T14:09:00Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    14,
                    9,
                    0,
                    0,
                    197,
                    0
                ],
                "title": "TME-Box: Scalable In-Process Isolation through Intel TME-MK Memory\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TME-Box: Scalable In-Process Isolation through Intel TME-MK Memory\n  Encryption"
                },
                "summary": "Efficient cloud computing relies on in-process isolation to optimize\nperformance by running workloads within a single process. Without heavy-weight\nprocess isolation, memory safety errors pose a significant security threat by\nallowing an adversary to extract or corrupt the private data of other\nco-located tenants. Existing in-process isolation mechanisms are not suitable\nfor modern cloud requirements, e.g., MPK's 16 protection domains are\ninsufficient to isolate thousands of cloud workers per process. Consequently,\ncloud service providers have a strong need for lightweight in-process isolation\non commodity x86 machines.\n  This paper presents TME-Box, a novel isolation technique that enables\nfine-grained and scalable sandboxing on commodity x86 CPUs. By repurposing\nIntel TME-MK, which is intended for the encryption of virtual machines, TME-Box\noffers lightweight and efficient in-process isolation. TME-Box enforces that\nsandboxes use their designated encryption keys for memory interactions through\ncompiler instrumentation. This cryptographic isolation enables fine-grained\naccess control, from single cache lines to full pages, and supports flexible\ndata relocation. In addition, the design of TME-Box allows the efficient\nisolation of up to 32K concurrent sandboxes. We present a performance-optimized\nTME-Box prototype, utilizing x86 segment-based addressing, that showcases\ngeomean performance overheads of 5.2 % for data isolation and 9.7 % for code\nand data isolation, evaluated with the SPEC CPU2017 benchmark suite.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient cloud computing relies on in-process isolation to optimize\nperformance by running workloads within a single process. Without heavy-weight\nprocess isolation, memory safety errors pose a significant security threat by\nallowing an adversary to extract or corrupt the private data of other\nco-located tenants. Existing in-process isolation mechanisms are not suitable\nfor modern cloud requirements, e.g., MPK's 16 protection domains are\ninsufficient to isolate thousands of cloud workers per process. Consequently,\ncloud service providers have a strong need for lightweight in-process isolation\non commodity x86 machines.\n  This paper presents TME-Box, a novel isolation technique that enables\nfine-grained and scalable sandboxing on commodity x86 CPUs. By repurposing\nIntel TME-MK, which is intended for the encryption of virtual machines, TME-Box\noffers lightweight and efficient in-process isolation. TME-Box enforces that\nsandboxes use their designated encryption keys for memory interactions through\ncompiler instrumentation. This cryptographic isolation enables fine-grained\naccess control, from single cache lines to full pages, and supports flexible\ndata relocation. In addition, the design of TME-Box allows the efficient\nisolation of up to 32K concurrent sandboxes. We present a performance-optimized\nTME-Box prototype, utilizing x86 segment-based addressing, that showcases\ngeomean performance overheads of 5.2 % for data isolation and 9.7 % for code\nand data isolation, evaluated with the SPEC CPU2017 benchmark suite."
                },
                "authors": [
                    {
                        "name": "Martin Unterguggenberger"
                    },
                    {
                        "name": "Lukas Lamster"
                    },
                    {
                        "name": "David Schrammel"
                    },
                    {
                        "name": "Martin Schwarzl"
                    },
                    {
                        "name": "Stefan Mangard"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Mangard"
                },
                "author": "Stefan Mangard",
                "arxiv_comment": "To appear in the Network and Distributed System Security (NDSS)\n  Symposium, February 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00601v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00601v2",
                "updated": "2024-11-04T09:40:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    9,
                    40,
                    27,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-01T14:03:21Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    3,
                    21,
                    4,
                    306,
                    0
                ],
                "title": "Diversity in Network-Friendly Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity in Network-Friendly Recommendations"
                },
                "summary": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms."
                },
                "authors": [
                    {
                        "name": "Evangelia Tzimpimpaki"
                    },
                    {
                        "name": "Thrasyvoulos Spyropoulos"
                    }
                ],
                "author_detail": {
                    "name": "Thrasyvoulos Spyropoulos"
                },
                "author": "Thrasyvoulos Spyropoulos",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00601v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00601v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01783v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01783v1",
                "updated": "2024-11-04T04:15:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    4,
                    15,
                    36,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T04:15:36Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    4,
                    15,
                    36,
                    0,
                    309,
                    0
                ],
                "title": "Context Parallelism for Scalable Million-Token Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context Parallelism for Scalable Million-Token Inference"
                },
                "summary": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth."
                },
                "authors": [
                    {
                        "name": "Amy Yang"
                    },
                    {
                        "name": "Jingyi Yang"
                    },
                    {
                        "name": "Aya Ibrahim"
                    },
                    {
                        "name": "Xinfeng Xie"
                    },
                    {
                        "name": "Bangsheng Tang"
                    },
                    {
                        "name": "Grigory Sizov"
                    },
                    {
                        "name": "Jongsoo Park"
                    },
                    {
                        "name": "Jianyu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jianyu Huang"
                },
                "author": "Jianyu Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01783v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01783v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01754v1",
                "updated": "2024-11-04T02:35:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    35,
                    3,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T02:35:03Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    35,
                    3,
                    0,
                    309,
                    0
                ],
                "title": "Experimental demonstration of dark current mitigation by an\n  over-inserted plug in a normal conducting VHF gun",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimental demonstration of dark current mitigation by an\n  over-inserted plug in a normal conducting VHF gun"
                },
                "summary": "The room temperature continuous wave (CW) very-high-frequency (VHF) gun is\none of the candidates for the electron gun of the high-repetition-rate\nfree-electron lasers (FELs). The VHF gun operates with a cathode gradient of ~\n20 MV/m and an accelerating voltage of ~ 750 kV. The gun dark current emission\nleads to beam loss along the FEL machine, therefore is a critical parameter for\nthe performance of the CW gun. In this paper, we presents a systematic study of\nthe dark current reduction of the VHF gun, including cathode region\noptimizations, dark current tracking simulations and measurements.\nOver-inserted cathode plugs were tested in two VHF guns of different\nacceleration gap sizes, and both demonstrated significant dark current\nreduction ratios of more than two orders of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The room temperature continuous wave (CW) very-high-frequency (VHF) gun is\none of the candidates for the electron gun of the high-repetition-rate\nfree-electron lasers (FELs). The VHF gun operates with a cathode gradient of ~\n20 MV/m and an accelerating voltage of ~ 750 kV. The gun dark current emission\nleads to beam loss along the FEL machine, therefore is a critical parameter for\nthe performance of the CW gun. In this paper, we presents a systematic study of\nthe dark current reduction of the VHF gun, including cathode region\noptimizations, dark current tracking simulations and measurements.\nOver-inserted cathode plugs were tested in two VHF guns of different\nacceleration gap sizes, and both demonstrated significant dark current\nreduction ratios of more than two orders of magnitude."
                },
                "authors": [
                    {
                        "name": "X. -H. Wang"
                    },
                    {
                        "name": "G. Shu"
                    },
                    {
                        "name": "H. Qian"
                    },
                    {
                        "name": "X. Li"
                    },
                    {
                        "name": "Z. Liu"
                    },
                    {
                        "name": "Z. Jiang"
                    },
                    {
                        "name": "H. Meng"
                    },
                    {
                        "name": "C. Xing"
                    },
                    {
                        "name": "Q. Zhou"
                    },
                    {
                        "name": "H. Deng"
                    }
                ],
                "author_detail": {
                    "name": "H. Deng"
                },
                "author": "H. Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21118v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21118v2",
                "updated": "2024-11-04T02:08:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    8,
                    55,
                    0,
                    309,
                    0
                ],
                "published": "2024-07-30T18:19:38Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "title": "Palu: Compressing KV-Cache with Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Palu: Compressing KV-Cache with Low-Rank Projection"
                },
                "summary": "Post-training KV-Cache compression methods typically either sample a subset\nof effectual tokens or quantize the data into lower numerical bit width.\nHowever, these methods cannot exploit redundancy in the hidden dimension of the\nKV tensors. This paper presents a hidden dimension compression approach called\nPalu, a KV-Cache compression framework that utilizes low-rank projection to\nreduce inference-time LLM memory usage. Palu decomposes the linear layers into\nlow-rank matrices, caches compressed intermediate states, and reconstructs the\nfull keys and values on the fly. To improve accuracy, compression rate, and\nefficiency, Palu further encompasses (1) a medium-grained low-rank\ndecomposition scheme, (2) an efficient rank search algorithm, (3)\nlow-rank-aware quantization compatibility enhancements, and (4) optimized GPU\nkernels with operators fusion. Extensive experiments with popular LLMs show\nthat Palu compresses KV-Cache by 50% while maintaining strong accuracy and\ndelivering up to 1.89x on the RoPE-based attention module. When combined with\nquantization, Palu's inherent quantization-friendly design yields small to\nnegligible extra accuracy degradation while saving additional memory than\nquantization-only methods and achieving up to 2.91x speedup for the RoPE-based\nattention. Moreover, it maintains comparable or even better accuracy (up to\n1.19 lower perplexity) compared to quantization-only methods. These results\ndemonstrate Palu's superior capability to effectively address the efficiency\nand memory challenges of LLM inference posed by KV-Cache. Our code is publicly\navailable at: https://github.com/shadowpa0327/Palu",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training KV-Cache compression methods typically either sample a subset\nof effectual tokens or quantize the data into lower numerical bit width.\nHowever, these methods cannot exploit redundancy in the hidden dimension of the\nKV tensors. This paper presents a hidden dimension compression approach called\nPalu, a KV-Cache compression framework that utilizes low-rank projection to\nreduce inference-time LLM memory usage. Palu decomposes the linear layers into\nlow-rank matrices, caches compressed intermediate states, and reconstructs the\nfull keys and values on the fly. To improve accuracy, compression rate, and\nefficiency, Palu further encompasses (1) a medium-grained low-rank\ndecomposition scheme, (2) an efficient rank search algorithm, (3)\nlow-rank-aware quantization compatibility enhancements, and (4) optimized GPU\nkernels with operators fusion. Extensive experiments with popular LLMs show\nthat Palu compresses KV-Cache by 50% while maintaining strong accuracy and\ndelivering up to 1.89x on the RoPE-based attention module. When combined with\nquantization, Palu's inherent quantization-friendly design yields small to\nnegligible extra accuracy degradation while saving additional memory than\nquantization-only methods and achieving up to 2.91x speedup for the RoPE-based\nattention. Moreover, it maintains comparable or even better accuracy (up to\n1.19 lower perplexity) compared to quantization-only methods. These results\ndemonstrate Palu's superior capability to effectively address the efficiency\nand memory challenges of LLM inference posed by KV-Cache. Our code is publicly\navailable at: https://github.com/shadowpa0327/Palu"
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Chong-Yan Chen"
                    },
                    {
                        "name": "Yu-Fang Hu"
                    },
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21118v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21118v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11430v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11430v4",
                "updated": "2024-11-03T09:42:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    3,
                    9,
                    42,
                    35,
                    6,
                    308,
                    0
                ],
                "published": "2024-06-17T11:35:16Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    11,
                    35,
                    16,
                    0,
                    169,
                    0
                ],
                "title": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression"
                },
                "summary": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability."
                },
                "authors": [
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Pasquale Minervini"
                    }
                ],
                "author_detail": {
                    "name": "Pasquale Minervini"
                },
                "author": "Pasquale Minervini",
                "arxiv_comment": "This is an extended version of a paper published in the proceedings\n  of the 2024 Conference on Empirical Methods in Natural Language Processing\n  (EMNLP 2024); this version was presented at the 4th NeurIPS Workshop on\n  Efficient Natural Language and Speech Processing (ENLSP-IV)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11430v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11430v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01458v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01458v1",
                "updated": "2024-11-03T07:01:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    3,
                    7,
                    1,
                    13,
                    6,
                    308,
                    0
                ],
                "published": "2024-11-03T07:01:13Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    7,
                    1,
                    13,
                    6,
                    308,
                    0
                ],
                "title": "Two-Timescale Model Caching and Resource Allocation for Edge-Enabled\n  AI-Generated Content Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-Timescale Model Caching and Resource Allocation for Edge-Enabled\n  AI-Generated Content Services"
                },
                "summary": "Generative AI (GenAI) has emerged as a transformative technology, enabling\ncustomized and personalized AI-generated content (AIGC) services. In this\npaper, we address challenges of edge-enabled AIGC service provisioning, which\nremain underexplored in the literature. These services require executing GenAI\nmodels with billions of parameters, posing significant obstacles to\nresource-limited wireless edge. We subsequently introduce the formulation of\njoint model caching and resource allocation for AIGC services to balance a\ntrade-off between AIGC quality and latency metrics. We obtain mathematical\nrelationships of these metrics with the computational resources required by\nGenAI models via experimentation. Afterward, we decompose the formulation into\na model caching subproblem on a long-timescale and a resource allocation\nsubproblem on a short-timescale. Since the variables to be solved are discrete\nand continuous, respectively, we leverage a double deep Q-network (DDQN)\nalgorithm to solve the former subproblem and propose a diffusion-based deep\ndeterministic policy gradient (D3PG) algorithm to solve the latter. The\nproposed D3PG algorithm makes an innovative use of diffusion models as the\nactor network to determine optimal resource allocation decisions. Consequently,\nwe integrate these two learning methods within the overarching two-timescale\ndeep reinforcement learning (T2DRL) algorithm, the performance of which is\nstudied through comparative numerical simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI (GenAI) has emerged as a transformative technology, enabling\ncustomized and personalized AI-generated content (AIGC) services. In this\npaper, we address challenges of edge-enabled AIGC service provisioning, which\nremain underexplored in the literature. These services require executing GenAI\nmodels with billions of parameters, posing significant obstacles to\nresource-limited wireless edge. We subsequently introduce the formulation of\njoint model caching and resource allocation for AIGC services to balance a\ntrade-off between AIGC quality and latency metrics. We obtain mathematical\nrelationships of these metrics with the computational resources required by\nGenAI models via experimentation. Afterward, we decompose the formulation into\na model caching subproblem on a long-timescale and a resource allocation\nsubproblem on a short-timescale. Since the variables to be solved are discrete\nand continuous, respectively, we leverage a double deep Q-network (DDQN)\nalgorithm to solve the former subproblem and propose a diffusion-based deep\ndeterministic policy gradient (D3PG) algorithm to solve the latter. The\nproposed D3PG algorithm makes an innovative use of diffusion models as the\nactor network to determine optimal resource allocation decisions. Consequently,\nwe integrate these two learning methods within the overarching two-timescale\ndeep reinforcement learning (T2DRL) algorithm, the performance of which is\nstudied through comparative numerical simulations."
                },
                "authors": [
                    {
                        "name": "Zhang Liu"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Xiangwang Hou"
                    },
                    {
                        "name": "Lianfen Huang"
                    },
                    {
                        "name": "Seyyedali Hosseinalipour"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Khaled Ben Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled Ben Letaief"
                },
                "author": "Khaled Ben Letaief",
                "arxiv_comment": "14 pages, 8 figures, 39 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01458v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01269v1",
                "updated": "2024-11-02T14:40:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    14,
                    40,
                    36,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T14:40:36Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    14,
                    40,
                    36,
                    5,
                    307,
                    0
                ],
                "title": "Disaggregated Database Management Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated Database Management Systems"
                },
                "summary": "Modern applications demand high performance and cost efficient database\nmanagement systems (DBMSs). Their workloads may be diverse, ranging from online\ntransaction processing to analytics and decision support. The cloud\ninfrastructure enables disaggregation of monolithic DBMSs into components that\nfacilitate software-hardware co-design. This is realized using pools of\nhardware resources, i.e., CPUs, GPUs, memory, FPGA, NVM, etc., connected using\nhigh-speed networks. This disaggregation trend is being adopted by cloud DBMSs\nbecause hardware re-provisioning can be achieved by simply invoking software\nAPIs. Disaggregated DBMSs separate processing from storage, enabling each to\nscale elastically and independently. They may disaggregate compute usage based\non functionality, e.g., compute needed for writes from compute needed for\nqueries and compute needed for compaction. They may also use disaggregated\nmemory, e.g., for intermediate results in a shuffle or for remote caching. The\nDBMS monitors the characteristics of a workload and dynamically assembles its\ncomponents that are most efficient and cost effective for the workload. This\npaper is a summary of a panel session that discussed the capability,\nchallenges, and opportunities of these emerging DBMSs and disaggregated\nhardware systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern applications demand high performance and cost efficient database\nmanagement systems (DBMSs). Their workloads may be diverse, ranging from online\ntransaction processing to analytics and decision support. The cloud\ninfrastructure enables disaggregation of monolithic DBMSs into components that\nfacilitate software-hardware co-design. This is realized using pools of\nhardware resources, i.e., CPUs, GPUs, memory, FPGA, NVM, etc., connected using\nhigh-speed networks. This disaggregation trend is being adopted by cloud DBMSs\nbecause hardware re-provisioning can be achieved by simply invoking software\nAPIs. Disaggregated DBMSs separate processing from storage, enabling each to\nscale elastically and independently. They may disaggregate compute usage based\non functionality, e.g., compute needed for writes from compute needed for\nqueries and compute needed for compaction. They may also use disaggregated\nmemory, e.g., for intermediate results in a shuffle or for remote caching. The\nDBMS monitors the characteristics of a workload and dynamically assembles its\ncomponents that are most efficient and cost effective for the workload. This\npaper is a summary of a panel session that discussed the capability,\nchallenges, and opportunities of these emerging DBMSs and disaggregated\nhardware systems."
                },
                "authors": [
                    {
                        "name": "Shahram Ghandeharizadeh"
                    },
                    {
                        "name": "Philip A. Bernstein"
                    },
                    {
                        "name": "Dhruba Borthakur"
                    },
                    {
                        "name": "Haoyu Huang"
                    },
                    {
                        "name": "Jai Menon"
                    },
                    {
                        "name": "Sumit Puri"
                    }
                ],
                "author_detail": {
                    "name": "Sumit Puri"
                },
                "author": "Sumit Puri",
                "arxiv_comment": "This paper appeared in the {\\em Performance Evaluation and\n  Benchmarking} - 14th TPC Technology Conference, TPCTC 2022, Sydney, NSW,\n  Australia, September 5, 2022, Revised Selected Papers. Lecture Notes in\n  Computer Science 13860, Springer 2023, ISBN 978-3-031-29575-1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01246v1",
                "updated": "2024-11-02T13:52:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    13,
                    52,
                    49,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T13:52:49Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    13,
                    52,
                    49,
                    5,
                    307,
                    0
                ],
                "title": "CAMP: A Cost Adaptive Multi-Queue Eviction Policy for Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAMP: A Cost Adaptive Multi-Queue Eviction Policy for Key-Value Stores"
                },
                "summary": "Cost Adaptive Multi-queue eviction Policy (CAMP) is an algorithm for a\ngeneral purpose key-value store (KVS) that manages key-value pairs computed by\napplications with different access patterns, key-value sizes, and varying costs\nfor each key-value pair. CAMP is an approximation of the Greedy Dual Size (GDS)\nalgorithm that can be implemented as efficiently as LRU. In particular, CAMP's\neviction policies are as effective as those of GDS but require only a small\nfraction of the updates to an internal data structure in order to make those\ndecisions. Similar to an implementation of LRU using queues, it adapts to\nchanging workload patterns based on the history of requests for different\nkey-value pairs. It is superior to LRU because it considers both the size and\ncost of key-value pairs to maximize the utility of the available memory across\ncompeting applications. We compare CAMP with both LRU and an alternative that\nrequires human intervention to partition memory into pools and assign grouping\nof key-value pairs to different pools. The results demonstrate CAMP is as fast\nas LRU while outperforming both LRU and the pooled alternative. We also present\nresults from an implementation of CAMP using Twitter's version of memcached.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost Adaptive Multi-queue eviction Policy (CAMP) is an algorithm for a\ngeneral purpose key-value store (KVS) that manages key-value pairs computed by\napplications with different access patterns, key-value sizes, and varying costs\nfor each key-value pair. CAMP is an approximation of the Greedy Dual Size (GDS)\nalgorithm that can be implemented as efficiently as LRU. In particular, CAMP's\neviction policies are as effective as those of GDS but require only a small\nfraction of the updates to an internal data structure in order to make those\ndecisions. Similar to an implementation of LRU using queues, it adapts to\nchanging workload patterns based on the history of requests for different\nkey-value pairs. It is superior to LRU because it considers both the size and\ncost of key-value pairs to maximize the utility of the available memory across\ncompeting applications. We compare CAMP with both LRU and an alternative that\nrequires human intervention to partition memory into pools and assign grouping\nof key-value pairs to different pools. The results demonstrate CAMP is as fast\nas LRU while outperforming both LRU and the pooled alternative. We also present\nresults from an implementation of CAMP using Twitter's version of memcached."
                },
                "authors": [
                    {
                        "name": "Shahram Ghandeharizadeh"
                    },
                    {
                        "name": "Sandy Irani"
                    },
                    {
                        "name": "Jenny Lam"
                    },
                    {
                        "name": "Jason Yap"
                    }
                ],
                "author_detail": {
                    "name": "Jason Yap"
                },
                "author": "Jason Yap",
                "arxiv_doi": "10.1145/2663165.2663317",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/2663165.2663317",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.01246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "A shorter version of CAMP appeared in the Proceedings of the\n  ACM/IFIP/USENIX Middleware Conference, Bordeaux, France, December 2014. See\n  https://github.com/scdblab/CAMP for an implementation",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01142v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01142v1",
                "updated": "2024-11-02T05:15:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    5,
                    15,
                    44,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T05:15:44Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    5,
                    15,
                    44,
                    5,
                    307,
                    0
                ],
                "title": "NEO: Saving GPU Memory Crisis with CPU Offloading for Online LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NEO: Saving GPU Memory Crisis with CPU Offloading for Online LLM\n  Inference"
                },
                "summary": "Online LLM inference powers many exciting applications such as intelligent\nchatbots and autonomous agents. Modern LLM inference engines widely rely on\nrequest batching to improve inference throughput, aiming to make it\ncost-efficient when running on expensive GPU accelerators. However, the limited\nGPU memory has largely limited the batch size achieved in practice, leaving\nsignificant GPU compute resources wasted.\n  We present NEO, an online LLM inference system that offloads part of\nattention compute and KV cache states from the GPU to the local host CPU,\neffectively increasing the GPU batch size and thus inference throughput. To\nthis end, NEO proposes asymmetric GPU-CPU pipelining and load-aware scheduling\nto balance GPU and CPU loads and fully utilize their compute and memory\nresources. We evaluate NEO on a wide range of workloads (i.e., code generation,\ntext summarization), GPUs (i.e., T4, A10G, H100), and LLM models (i.e., 7B, 8B,\n70B). NEO achieves up to 7.5$\\times$, 26%, and 14% higher throughput compared\nto GPU-only approach on T4, A10G, and H100 GPUs, respectively, while\nmaintaining the same latency; with more powerful CPUs, NEO achieves up to 79.3%\nthroughput gain on A10G GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online LLM inference powers many exciting applications such as intelligent\nchatbots and autonomous agents. Modern LLM inference engines widely rely on\nrequest batching to improve inference throughput, aiming to make it\ncost-efficient when running on expensive GPU accelerators. However, the limited\nGPU memory has largely limited the batch size achieved in practice, leaving\nsignificant GPU compute resources wasted.\n  We present NEO, an online LLM inference system that offloads part of\nattention compute and KV cache states from the GPU to the local host CPU,\neffectively increasing the GPU batch size and thus inference throughput. To\nthis end, NEO proposes asymmetric GPU-CPU pipelining and load-aware scheduling\nto balance GPU and CPU loads and fully utilize their compute and memory\nresources. We evaluate NEO on a wide range of workloads (i.e., code generation,\ntext summarization), GPUs (i.e., T4, A10G, H100), and LLM models (i.e., 7B, 8B,\n70B). NEO achieves up to 7.5$\\times$, 26%, and 14% higher throughput compared\nto GPU-only approach on T4, A10G, and H100 GPUs, respectively, while\nmaintaining the same latency; with more powerful CPUs, NEO achieves up to 79.3%\nthroughput gain on A10G GPU."
                },
                "authors": [
                    {
                        "name": "Xuanlin Jiang"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Minlan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Minlan Yu"
                },
                "author": "Minlan Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01142v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.15420v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.15420v3",
                "updated": "2024-11-01T14:56:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    56,
                    52,
                    4,
                    306,
                    0
                ],
                "published": "2024-04-23T18:10:42Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    18,
                    10,
                    42,
                    1,
                    114,
                    0
                ],
                "title": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference"
                },
                "summary": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude."
                },
                "authors": [
                    {
                        "name": "Joo Monteiro"
                    },
                    {
                        "name": "tienne Marcotte"
                    },
                    {
                        "name": "Pierre-Andr Nol"
                    },
                    {
                        "name": "Valentina Zantedeschi"
                    },
                    {
                        "name": "David Vzquez"
                    },
                    {
                        "name": "Nicolas Chapados"
                    },
                    {
                        "name": "Christopher Pal"
                    },
                    {
                        "name": "Perouz Taslakian"
                    }
                ],
                "author_detail": {
                    "name": "Perouz Taslakian"
                },
                "author": "Perouz Taslakian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.15420v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.15420v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02657v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02657v2",
                "updated": "2024-11-01T08:52:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    8,
                    52,
                    18,
                    4,
                    306,
                    0
                ],
                "published": "2024-06-04T17:45:26Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    45,
                    26,
                    1,
                    156,
                    0
                ],
                "title": "Block Transformer: Global-to-Local Language Modeling for Fast Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block Transformer: Global-to-Local Language Modeling for Fast Inference"
                },
                "summary": "We introduce the Block Transformer which adopts hierarchical global-to-local\nmodeling to autoregressive transformers to mitigate the inference bottlenecks\nassociated with self-attention. Self-attention requires the key-value (KV)\ncache of all previous sequences to be retrieved from memory at every decoding\nstep to retrieve context information, leading to two primary bottlenecks during\nbatch inference. First, there is a significant delay in obtaining the first\ntoken, as the information of the entire prompt must first be processed to\nprefill the KV cache. Second, computation of subsequent tokens is bottlenecked\nby the high memory I/O demand of fetching the entire KV cache, which grows\nlinearly with sequence length, incurring quadratic memory reads overall. We\ndesign the Block Transformer to strategically mitigate these costs, by\nincorporating coarsity and locality into an integrated global-to-local\narchitecture. At the lower layers, we aggregate tokens into fixed size blocks\nto apply attention across the entire sequence at coarse-grained detail, to\ncapture the global context while minimizing KV cache overhead. At upper layers,\nwe apply attention within each block to decode individual tokens, to model\nfine-grained details with a lightweight local KV cache. We pretrain vanilla and\nBlock Transformers from scratch and demonstrate that Block Transformers reach\n10--20x inference throughput compared to vanilla transformers with equivalent\nperplexity and zero-shot task performance. Code is available at\nhttps://github.com/itsnamgyu/block-transformer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Block Transformer which adopts hierarchical global-to-local\nmodeling to autoregressive transformers to mitigate the inference bottlenecks\nassociated with self-attention. Self-attention requires the key-value (KV)\ncache of all previous sequences to be retrieved from memory at every decoding\nstep to retrieve context information, leading to two primary bottlenecks during\nbatch inference. First, there is a significant delay in obtaining the first\ntoken, as the information of the entire prompt must first be processed to\nprefill the KV cache. Second, computation of subsequent tokens is bottlenecked\nby the high memory I/O demand of fetching the entire KV cache, which grows\nlinearly with sequence length, incurring quadratic memory reads overall. We\ndesign the Block Transformer to strategically mitigate these costs, by\nincorporating coarsity and locality into an integrated global-to-local\narchitecture. At the lower layers, we aggregate tokens into fixed size blocks\nto apply attention across the entire sequence at coarse-grained detail, to\ncapture the global context while minimizing KV cache overhead. At upper layers,\nwe apply attention within each block to decode individual tokens, to model\nfine-grained details with a lightweight local KV cache. We pretrain vanilla and\nBlock Transformers from scratch and demonstrate that Block Transformers reach\n10--20x inference throughput compared to vanilla transformers with equivalent\nperplexity and zero-shot task performance. Code is available at\nhttps://github.com/itsnamgyu/block-transformer."
                },
                "authors": [
                    {
                        "name": "Namgyu Ho"
                    },
                    {
                        "name": "Sangmin Bae"
                    },
                    {
                        "name": "Taehyeon Kim"
                    },
                    {
                        "name": "Hyunjik Jo"
                    },
                    {
                        "name": "Yireun Kim"
                    },
                    {
                        "name": "Tal Schuster"
                    },
                    {
                        "name": "Adam Fisch"
                    },
                    {
                        "name": "James Thorne"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "37 pages, 24 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02657v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02657v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00131v1",
                "updated": "2024-10-31T18:31:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    18,
                    31,
                    13,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T18:31:13Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    18,
                    31,
                    13,
                    3,
                    305,
                    0
                ],
                "title": "Two Dimensional Hidden Surface Removal with Frame-to-frame Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two Dimensional Hidden Surface Removal with Frame-to-frame Coherence"
                },
                "summary": "We describe a hidden surface removal algorithm for two-dimensional layered\nscenes built from arbitrary primitives, particularly suited to interaction and\nanimation in rich scenes (for example, in illustration). The method makes use\nof a set-based raster representation to implement a front-to-back rendering\nmodel which analyses and dramatically reduces the amount of rasterization and\ncomposition required to render a scene. The method is extended to add\nframe-to-frame coherence analysis and caching for interactive or animated\nscenes. A powerful system of primitive-combiners called filters is described,\nwhich preserves the efficiencies of the algorithm in highly complicated scenes.\nThe set representation is extended to solve the problem of correlated mattes,\nleading to an efficient solution for high quality antialiasing. A prototype\nimplementation has been prepared.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe a hidden surface removal algorithm for two-dimensional layered\nscenes built from arbitrary primitives, particularly suited to interaction and\nanimation in rich scenes (for example, in illustration). The method makes use\nof a set-based raster representation to implement a front-to-back rendering\nmodel which analyses and dramatically reduces the amount of rasterization and\ncomposition required to render a scene. The method is extended to add\nframe-to-frame coherence analysis and caching for interactive or animated\nscenes. A powerful system of primitive-combiners called filters is described,\nwhich preserves the efficiencies of the algorithm in highly complicated scenes.\nThe set representation is extended to solve the problem of correlated mattes,\nleading to an efficient solution for high quality antialiasing. A prototype\nimplementation has been prepared."
                },
                "authors": [
                    {
                        "name": "John Whitington"
                    }
                ],
                "author_detail": {
                    "name": "John Whitington"
                },
                "author": "John Whitington",
                "arxiv_doi": "10.1145/2788539.27885",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/2788539.27885",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.00131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages, 14 figures",
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24174v1",
                "updated": "2024-10-31T17:41:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    41,
                    14,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T17:41:14Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    41,
                    14,
                    3,
                    305,
                    0
                ],
                "title": "Novel Architecture for Distributed Travel Data Integration and Service\n  Provision Using Microservices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Novel Architecture for Distributed Travel Data Integration and Service\n  Provision Using Microservices"
                },
                "summary": "This paper introduces a microservices architecture for the purpose of\nenhancing the flexibility and performance of an airline reservation system. The\narchitectural design incorporates Redis cache technologies, two different\nmessaging systems (Kafka and RabbitMQ), two types of storages (MongoDB, and\nPostgreSQL). It also introduces authorization techniques, including secure\ncommunication through OAuth2 and JWT which is essential with the management of\nhigh-demand travel services. According to selected indicators, the architecture\nprovides an impressive level of data consistency at 99.5% and a latency of data\npropagation of less than 75 ms allowing rapid and reliable intercommunication\nbetween microservices. A system throughput of 1050 events per second was\nachieved so that the acceptability level was maintained even during peak time.\nRedis caching reduced a 92% cache hit ratio on the database thereby lowering\nthe burden on the database and increasing the speed of response. Further\nimprovement of the systems scalability was done through the use of Docker and\nKubernetes which enabled services to be expanded horizontally to cope with the\nchanges in demand. The error rates were very low, at 0.2% further enhancing the\nefficiency of the system in handling real-time data integration. This approach\nis suggested to meet the specific needs of the airline reservation system. It\nis secure, fast, scalable, all serving to improve the user experience as well\nas the efficiency of operations. The low latency and high data integration\nlevels and prevaiing efficient usage of the resources demonstrates the\narchitecture ability to offer continued support in the ever growing high demand\nsituations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a microservices architecture for the purpose of\nenhancing the flexibility and performance of an airline reservation system. The\narchitectural design incorporates Redis cache technologies, two different\nmessaging systems (Kafka and RabbitMQ), two types of storages (MongoDB, and\nPostgreSQL). It also introduces authorization techniques, including secure\ncommunication through OAuth2 and JWT which is essential with the management of\nhigh-demand travel services. According to selected indicators, the architecture\nprovides an impressive level of data consistency at 99.5% and a latency of data\npropagation of less than 75 ms allowing rapid and reliable intercommunication\nbetween microservices. A system throughput of 1050 events per second was\nachieved so that the acceptability level was maintained even during peak time.\nRedis caching reduced a 92% cache hit ratio on the database thereby lowering\nthe burden on the database and increasing the speed of response. Further\nimprovement of the systems scalability was done through the use of Docker and\nKubernetes which enabled services to be expanded horizontally to cope with the\nchanges in demand. The error rates were very low, at 0.2% further enhancing the\nefficiency of the system in handling real-time data integration. This approach\nis suggested to meet the specific needs of the airline reservation system. It\nis secure, fast, scalable, all serving to improve the user experience as well\nas the efficiency of operations. The low latency and high data integration\nlevels and prevaiing efficient usage of the resources demonstrates the\narchitecture ability to offer continued support in the ever growing high demand\nsituations."
                },
                "authors": [
                    {
                        "name": "Biman Barua"
                    },
                    {
                        "name": "M. Shamim Kaiser"
                    }
                ],
                "author_detail": {
                    "name": "M. Shamim Kaiser"
                },
                "author": "M. Shamim Kaiser",
                "arxiv_comment": "20 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23805v1",
                "updated": "2024-10-31T10:45:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    10,
                    45,
                    2,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T10:45:02Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    10,
                    45,
                    2,
                    3,
                    305,
                    0
                ],
                "title": "MemANNS: Enhancing Billion-Scale ANNS Efficiency with Practical PIM\n  Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemANNS: Enhancing Billion-Scale ANNS Efficiency with Practical PIM\n  Hardware"
                },
                "summary": "In numerous production environments, Approximate Nearest Neighbor Search\n(ANNS) plays an indispensable role, particularly when dealing with massive\ndatasets that can contain billions of entries. The necessity for rapid response\ntimes in these applications makes the efficiency of ANNS algorithms crucial.\nHowever, traditional ANNS approaches encounter substantial challenges at the\nbillion-scale level. CPU-based methods are hindered by the limitations of\nmemory bandwidth, while GPU-based methods struggle with memory capacity and\nresource utilization efficiency. This paper introduces MemANNS, an innovative\nframework that utilizes UPMEM PIM architecture to address the memory\nbottlenecks in ANNS algorithms at scale. We concentrate on optimizing a\nwell-known ANNS algorithm, IVFPQ, for PIM hardware through several techniques.\nFirst, we introduce an architecture-aware strategy for data placement and query\nscheduling that ensures an even distribution of workload across PIM chips,\nthereby maximizing the use of aggregated memory bandwidth. Additionally, we\nhave developed an efficient thread scheduling mechanism that capitalizes on\nPIM's multi-threading capabilities and enhances memory management to boost\ncache efficiency. Moreover, we have recognized that real-world datasets often\nfeature vectors with frequently co-occurring items. To address this, we propose\na novel encoding method for IVFPQ that minimizes memory accesses during query\nprocessing. Our comprehensive evaluation using actual PIM hardware and\nreal-world datasets at the billion-scale, show that MemANNS offers a\nsignificant 4.3x increase in QPS over CPU-based Faiss, and it matches the\nperformance of GPU-based Faiss implementations. Additionally, MemANNS improves\nenergy efficiency, with a 2.3x enhancement in QPS/Watt compared to GPU\nsolutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In numerous production environments, Approximate Nearest Neighbor Search\n(ANNS) plays an indispensable role, particularly when dealing with massive\ndatasets that can contain billions of entries. The necessity for rapid response\ntimes in these applications makes the efficiency of ANNS algorithms crucial.\nHowever, traditional ANNS approaches encounter substantial challenges at the\nbillion-scale level. CPU-based methods are hindered by the limitations of\nmemory bandwidth, while GPU-based methods struggle with memory capacity and\nresource utilization efficiency. This paper introduces MemANNS, an innovative\nframework that utilizes UPMEM PIM architecture to address the memory\nbottlenecks in ANNS algorithms at scale. We concentrate on optimizing a\nwell-known ANNS algorithm, IVFPQ, for PIM hardware through several techniques.\nFirst, we introduce an architecture-aware strategy for data placement and query\nscheduling that ensures an even distribution of workload across PIM chips,\nthereby maximizing the use of aggregated memory bandwidth. Additionally, we\nhave developed an efficient thread scheduling mechanism that capitalizes on\nPIM's multi-threading capabilities and enhances memory management to boost\ncache efficiency. Moreover, we have recognized that real-world datasets often\nfeature vectors with frequently co-occurring items. To address this, we propose\na novel encoding method for IVFPQ that minimizes memory accesses during query\nprocessing. Our comprehensive evaluation using actual PIM hardware and\nreal-world datasets at the billion-scale, show that MemANNS offers a\nsignificant 4.3x increase in QPS over CPU-based Faiss, and it matches the\nperformance of GPU-based Faiss implementations. Additionally, MemANNS improves\nenergy efficiency, with a 2.3x enhancement in QPS/Watt compared to GPU\nsolutions."
                },
                "authors": [
                    {
                        "name": "Sitian Chen"
                    },
                    {
                        "name": "Amelie Chi Zhou"
                    },
                    {
                        "name": "Yucheng Shi"
                    },
                    {
                        "name": "Yusen Li"
                    },
                    {
                        "name": "Xin Yao"
                    }
                ],
                "author_detail": {
                    "name": "Xin Yao"
                },
                "author": "Xin Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23537v1",
                "updated": "2024-10-31T00:58:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    0,
                    58,
                    11,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T00:58:11Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    0,
                    58,
                    11,
                    3,
                    305,
                    0
                ],
                "title": "ALISE: Accelerating Large Language Model Serving with Speculative\n  Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALISE: Accelerating Large Language Model Serving with Speculative\n  Scheduling"
                },
                "summary": "Large Language Models (LLMs) represent a revolutionary advancement in the\ncontemporary landscape of artificial general intelligence (AGI). As exemplified\nby ChatGPT, LLM-based applications necessitate minimal response latency and\nmaximal throughput for inference serving. However, due to the unpredictability\nof LLM execution, the first-come-first-serve (FCFS) scheduling policy employed\nby current LLM serving systems suffers from head-of-line (HoL) blocking issues\nand long job response times.\n  In this paper, we propose a new efficient LLM inference serving framework,\nnamed ALISE. The key design paradigm of ALISE is to leverage a novel\nspeculative scheduler by estimating the execution time for each job and\nexploiting such prior knowledge to assign appropriate job priority orders, thus\nminimizing potential queuing delays for heterogeneous workloads. Furthermore,\nto mitigate the memory overhead of the intermediate key-value (KV) cache, we\nemploy a priority-based adaptive memory management protocol and\nquantization-based compression techniques. Evaluations demonstrate that in\ncomparison to the state-of-the-art solution vLLM, ALISE improves the throughput\nof inference serving by up to 1.8x and 2.1x under the same latency constraint\non the Alpaca and ShareGPT datasets, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) represent a revolutionary advancement in the\ncontemporary landscape of artificial general intelligence (AGI). As exemplified\nby ChatGPT, LLM-based applications necessitate minimal response latency and\nmaximal throughput for inference serving. However, due to the unpredictability\nof LLM execution, the first-come-first-serve (FCFS) scheduling policy employed\nby current LLM serving systems suffers from head-of-line (HoL) blocking issues\nand long job response times.\n  In this paper, we propose a new efficient LLM inference serving framework,\nnamed ALISE. The key design paradigm of ALISE is to leverage a novel\nspeculative scheduler by estimating the execution time for each job and\nexploiting such prior knowledge to assign appropriate job priority orders, thus\nminimizing potential queuing delays for heterogeneous workloads. Furthermore,\nto mitigate the memory overhead of the intermediate key-value (KV) cache, we\nemploy a priority-based adaptive memory management protocol and\nquantization-based compression techniques. Evaluations demonstrate that in\ncomparison to the state-of-the-art solution vLLM, ALISE improves the throughput\nof inference serving by up to 1.8x and 2.1x under the same latency constraint\non the Alpaca and ShareGPT datasets, respectively."
                },
                "authors": [
                    {
                        "name": "Youpeng Zhao"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "arxiv_comment": "ICCAD 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18400v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18400v6",
                "updated": "2024-10-30T21:22:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    21,
                    22,
                    54,
                    2,
                    304,
                    0
                ],
                "published": "2024-05-28T17:40:48Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    17,
                    40,
                    48,
                    1,
                    149,
                    0
                ],
                "title": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass"
                },
                "summary": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding."
                },
                "authors": [
                    {
                        "name": "Ethan Shen"
                    },
                    {
                        "name": "Alan Fan"
                    },
                    {
                        "name": "Sarah M. Pratt"
                    },
                    {
                        "name": "Jae Sung Park"
                    },
                    {
                        "name": "Matthew Wallingford"
                    },
                    {
                        "name": "Sham M. Kakade"
                    },
                    {
                        "name": "Ari Holtzman"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Ali Farhadi"
                    },
                    {
                        "name": "Aditya Kusupati"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Kusupati"
                },
                "author": "Aditya Kusupati",
                "arxiv_comment": "23 pages, 16 figures, accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18400v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18400v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14576v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14576v3",
                "updated": "2024-10-30T16:06:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    6,
                    21,
                    2,
                    304,
                    0
                ],
                "published": "2024-02-08T17:17:46Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    17,
                    17,
                    46,
                    3,
                    39,
                    0
                ],
                "title": "Attention-Enhanced Prioritized Proximal Policy Optimization for Adaptive\n  Edge Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention-Enhanced Prioritized Proximal Policy Optimization for Adaptive\n  Edge Caching"
                },
                "summary": "This paper tackles the growing issue of excessive data transmission in\nnetworks. With increasing traffic, backhaul links and core networks are under\nsignificant traffic, leading to the investigation of caching solutions at edge\nrouters. Many existing studies utilize Markov Decision Processes (MDP) to\ntackle caching problems, often assuming decision points at fixed intervals;\nhowever, real-world environments are characterized by random request arrivals.\nAdditionally, critical file attributes such as lifetime, size, and priority\nsignificantly impact the effectiveness of caching policies, yet existing\nresearch fails to integrate all these attributes in policy design. In this\nwork, we model the caching problem using a Semi-Markov Decision Process (SMDP)\nto better capture the continuous-time nature of real-world applications,\nenabling caching decisions to be triggered by random file requests. We then\nintroduce a Proximal Policy Optimization (PPO)--based caching strategy that\nfully considers file attributes like lifetime, size, and priority. Simulations\nshow that our method outperforms a recent Deep Reinforcement Learning-based\ntechnique. To further advance our research, we improved the convergence rate of\nPPO by prioritizing transitions within the replay buffer through an attention\nmechanism. This mechanism evaluates the similarity between the current state\nand all stored transitions, assigning higher priorities to transitions that\nexhibit greater similarity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper tackles the growing issue of excessive data transmission in\nnetworks. With increasing traffic, backhaul links and core networks are under\nsignificant traffic, leading to the investigation of caching solutions at edge\nrouters. Many existing studies utilize Markov Decision Processes (MDP) to\ntackle caching problems, often assuming decision points at fixed intervals;\nhowever, real-world environments are characterized by random request arrivals.\nAdditionally, critical file attributes such as lifetime, size, and priority\nsignificantly impact the effectiveness of caching policies, yet existing\nresearch fails to integrate all these attributes in policy design. In this\nwork, we model the caching problem using a Semi-Markov Decision Process (SMDP)\nto better capture the continuous-time nature of real-world applications,\nenabling caching decisions to be triggered by random file requests. We then\nintroduce a Proximal Policy Optimization (PPO)--based caching strategy that\nfully considers file attributes like lifetime, size, and priority. Simulations\nshow that our method outperforms a recent Deep Reinforcement Learning-based\ntechnique. To further advance our research, we improved the convergence rate of\nPPO by prioritizing transitions within the replay buffer through an attention\nmechanism. This mechanism evaluates the similarity between the current state\nand all stored transitions, assigning higher priorities to transitions that\nexhibit greater similarity."
                },
                "authors": [
                    {
                        "name": "Farnaz Niknia"
                    },
                    {
                        "name": "Ping Wang"
                    },
                    {
                        "name": "Zixu Wang"
                    },
                    {
                        "name": "Aakash Agarwal"
                    },
                    {
                        "name": "Adib S. Rezaei"
                    }
                ],
                "author_detail": {
                    "name": "Adib S. Rezaei"
                },
                "author": "Adib S. Rezaei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14576v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14576v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23079v1",
                "updated": "2024-10-30T14:53:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    53,
                    37,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T14:53:37Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    53,
                    37,
                    2,
                    304,
                    0
                ],
                "title": "BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters\n  for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters\n  for Efficient LLM Inference"
                },
                "summary": "Large language models (LLMs) are essential in natural language processing but\noften struggle with inference speed and computational efficiency, limiting\nreal-time deployment. The key-value (KV) cache mechanism reduces computational\noverhead in transformer models, but challenges in maintaining contextual\nunderstanding remain. In this paper, we propose BUZZ, a novel KV caching\nalgorithm that leverages structured contextual information to minimize cache\nmemory usage while enhancing inference speed. BUZZ employs a beehive-structured\nsparse cache, incorporating a sliding window to capture recent information and\ndynamically segmenting historical tokens into chunks to prioritize important\ntokens in local neighborhoods. We evaluate BUZZ on four real-world datasets:\nCNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ\n(1) reduces cache memory usage by $\\textbf{2.5}\\times$ in LLM inference while\nmaintaining over 99% accuracy in long-text summarization, and (2) surpasses\nstate-of-the-art performance in multi-document question answering by\n$\\textbf{7.69%}$ under the same memory limit, where full cache methods\nencounter out-of-memory issues. Additionally, BUZZ achieves significant\ninference speedup with a $\\log{n}$ time complexity. The code is available at\nhttps://github.com/JunqiZhao888/buzz-llm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are essential in natural language processing but\noften struggle with inference speed and computational efficiency, limiting\nreal-time deployment. The key-value (KV) cache mechanism reduces computational\noverhead in transformer models, but challenges in maintaining contextual\nunderstanding remain. In this paper, we propose BUZZ, a novel KV caching\nalgorithm that leverages structured contextual information to minimize cache\nmemory usage while enhancing inference speed. BUZZ employs a beehive-structured\nsparse cache, incorporating a sliding window to capture recent information and\ndynamically segmenting historical tokens into chunks to prioritize important\ntokens in local neighborhoods. We evaluate BUZZ on four real-world datasets:\nCNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ\n(1) reduces cache memory usage by $\\textbf{2.5}\\times$ in LLM inference while\nmaintaining over 99% accuracy in long-text summarization, and (2) surpasses\nstate-of-the-art performance in multi-document question answering by\n$\\textbf{7.69%}$ under the same memory limit, where full cache methods\nencounter out-of-memory issues. Additionally, BUZZ achieves significant\ninference speedup with a $\\log{n}$ time complexity. The code is available at\nhttps://github.com/JunqiZhao888/buzz-llm."
                },
                "authors": [
                    {
                        "name": "Junqi Zhao"
                    },
                    {
                        "name": "Zhijin Fang"
                    },
                    {
                        "name": "Shu Li"
                    },
                    {
                        "name": "Shaohui Yang"
                    },
                    {
                        "name": "Shichao He"
                    }
                ],
                "author_detail": {
                    "name": "Shichao He"
                },
                "author": "Shichao He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17808v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17808v2",
                "updated": "2024-10-30T03:31:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    3,
                    31,
                    9,
                    2,
                    304,
                    0
                ],
                "published": "2024-06-24T03:59:17Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    3,
                    59,
                    17,
                    0,
                    176,
                    0
                ],
                "title": "Training-Free Exponential Context Extension via Cascading KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Exponential Context Extension via Cascading KV Cache"
                },
                "summary": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency."
                },
                "authors": [
                    {
                        "name": "Jeffrey Willette"
                    },
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17808v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17808v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22649v1",
                "updated": "2024-10-30T02:36:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    36,
                    55,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T02:36:55Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    36,
                    55,
                    2,
                    304,
                    0
                ],
                "title": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting"
                },
                "summary": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs."
                },
                "authors": [
                    {
                        "name": "Aobo Liang"
                    },
                    {
                        "name": "Yan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yan Sun"
                },
                "author": "Yan Sun",
                "arxiv_comment": "The code is coming soon! For sure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23317v1",
                "updated": "2024-10-29T20:04:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    20,
                    4,
                    34,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T20:04:34Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    20,
                    4,
                    34,
                    1,
                    303,
                    0
                ],
                "title": "VL-Cache: Sparsity and Modality-Aware KV Cache Compression for\n  Vision-Language Model Inference Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VL-Cache: Sparsity and Modality-Aware KV Cache Compression for\n  Vision-Language Model Inference Acceleration"
                },
                "summary": "Vision-Language Models (VLMs) have demonstrated impressive performance across\na versatile set of tasks. A key challenge in accelerating VLMs is storing and\naccessing the large Key-Value (KV) cache that encodes long visual contexts,\nsuch as images or videos. While existing KV cache compression methods are\neffective for Large Language Models (LLMs), directly migrating them to VLMs\nyields suboptimal accuracy and speedup. To bridge the gap, we propose VL-Cache,\na novel KV cache compression recipe tailored for accelerating VLM inference. In\nthis paper, we first investigate the unique sparsity pattern of VLM attention\nby distinguishing visual and text tokens in prefill and decoding phases. Based\non these observations, we introduce a layer-adaptive sparsity-aware cache\nbudget allocation method that effectively distributes the limited cache budget\nacross different layers, further reducing KV cache size without compromising\naccuracy. Additionally, we develop a modality-aware token scoring policy to\nbetter evaluate the token importance. Empirical results on multiple benchmark\ndatasets demonstrate that retaining only 10% of KV cache achieves accuracy\ncomparable to that with full cache. In a speed benchmark, our method\naccelerates end-to-end latency of generating 100 tokens by up to 2.33x and\nspeeds up decoding by up to 7.08x, while reducing the memory footprint of KV\ncache in GPU by 90%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) have demonstrated impressive performance across\na versatile set of tasks. A key challenge in accelerating VLMs is storing and\naccessing the large Key-Value (KV) cache that encodes long visual contexts,\nsuch as images or videos. While existing KV cache compression methods are\neffective for Large Language Models (LLMs), directly migrating them to VLMs\nyields suboptimal accuracy and speedup. To bridge the gap, we propose VL-Cache,\na novel KV cache compression recipe tailored for accelerating VLM inference. In\nthis paper, we first investigate the unique sparsity pattern of VLM attention\nby distinguishing visual and text tokens in prefill and decoding phases. Based\non these observations, we introduce a layer-adaptive sparsity-aware cache\nbudget allocation method that effectively distributes the limited cache budget\nacross different layers, further reducing KV cache size without compromising\naccuracy. Additionally, we develop a modality-aware token scoring policy to\nbetter evaluate the token importance. Empirical results on multiple benchmark\ndatasets demonstrate that retaining only 10% of KV cache achieves accuracy\ncomparable to that with full cache. In a speed benchmark, our method\naccelerates end-to-end latency of generating 100 tokens by up to 2.33x and\nspeeds up decoding by up to 7.08x, while reducing the memory footprint of KV\ncache in GPU by 90%."
                },
                "authors": [
                    {
                        "name": "Dezhan Tu"
                    },
                    {
                        "name": "Danylo Vashchilenko"
                    },
                    {
                        "name": "Yuzhe Lu"
                    },
                    {
                        "name": "Panpan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Panpan Xu"
                },
                "author": "Panpan Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.01801v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.01801v4",
                "updated": "2024-10-29T18:26:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    18,
                    26,
                    9,
                    1,
                    303,
                    0
                ],
                "published": "2023-10-03T05:17:08Z",
                "published_parsed": [
                    2023,
                    10,
                    3,
                    5,
                    17,
                    8,
                    1,
                    276,
                    0
                ],
                "title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs"
                },
                "summary": "In this study, we introduce adaptive KV cache compression, a plug-and-play\nmethod that reduces the memory footprint of generative inference for Large\nLanguage Models (LLMs). Different from the conventional KV cache that retains\nkey and value vectors for all context tokens, we conduct targeted profiling to\ndiscern the intrinsic structure of attention modules. Based on the recognized\nstructure, we then construct the KV cache in an adaptive manner: evicting\nlong-range contexts on attention heads emphasizing local contexts, discarding\nnon-special tokens on attention heads centered on special tokens, and only\nemploying the standard KV cache for attention heads that broadly attend to all\ntokens. Moreover, with the lightweight attention profiling used to guide the\nconstruction of the adaptive KV cache, FastGen can be deployed without\nresource-intensive fine-tuning or re-training. In our experiments across\nvarious asks, FastGen demonstrates substantial reduction on GPU memory\nconsumption with negligible generation quality loss. We will release our code\nand the compatible CUDA kernel for reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we introduce adaptive KV cache compression, a plug-and-play\nmethod that reduces the memory footprint of generative inference for Large\nLanguage Models (LLMs). Different from the conventional KV cache that retains\nkey and value vectors for all context tokens, we conduct targeted profiling to\ndiscern the intrinsic structure of attention modules. Based on the recognized\nstructure, we then construct the KV cache in an adaptive manner: evicting\nlong-range contexts on attention heads emphasizing local contexts, discarding\nnon-special tokens on attention heads centered on special tokens, and only\nemploying the standard KV cache for attention heads that broadly attend to all\ntokens. Moreover, with the lightweight attention profiling used to guide the\nconstruction of the adaptive KV cache, FastGen can be deployed without\nresource-intensive fine-tuning or re-training. In our experiments across\nvarious asks, FastGen demonstrates substantial reduction on GPU memory\nconsumption with negligible generation quality loss. We will release our code\nand the compatible CUDA kernel for reproducibility."
                },
                "authors": [
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Liyuan Liu"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Jianfeng Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Gao"
                },
                "author": "Jianfeng Gao",
                "arxiv_comment": "ICLR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.01801v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.01801v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19274v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19274v2",
                "updated": "2024-10-29T17:33:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    33,
                    19,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-25T03:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    3,
                    1,
                    19,
                    4,
                    299,
                    0
                ],
                "title": "Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware\n  Neuron Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware\n  Neuron Management"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Ripple, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory. Ripple\nleverages the concept of Neuron Co-Activation, where neurons frequently\nactivated together are linked to facilitate continuous read access and optimize\ndata transfer efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Ripple achieves up to\n5.93x improvements in I/O latency compared to the state-of-the-art. As the\nfirst solution to optimize storage placement under sparsity, Ripple explores a\nnew optimization space at the intersection of sparsity-driven algorithm and\nstorage-level system co-design in LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Ripple, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory. Ripple\nleverages the concept of Neuron Co-Activation, where neurons frequently\nactivated together are linked to facilitate continuous read access and optimize\ndata transfer efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Ripple achieves up to\n5.93x improvements in I/O latency compared to the state-of-the-art. As the\nfirst solution to optimize storage placement under sparsity, Ripple explores a\nnew optimization space at the intersection of sparsity-driven algorithm and\nstorage-level system co-design in LLM inference."
                },
                "authors": [
                    {
                        "name": "Tuowei Wang"
                    },
                    {
                        "name": "Ruwen Fan"
                    },
                    {
                        "name": "Minxing Huang"
                    },
                    {
                        "name": "Zixu Hao"
                    },
                    {
                        "name": "Kun Li"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Youyou Lu"
                    },
                    {
                        "name": "Yaoxue Zhang"
                    },
                    {
                        "name": "Ju Ren"
                    }
                ],
                "author_detail": {
                    "name": "Ju Ren"
                },
                "author": "Ju Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19274v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19274v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21142v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21142v2",
                "updated": "2024-10-29T16:55:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    16,
                    55,
                    23,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-28T15:43:33Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    43,
                    33,
                    0,
                    302,
                    0
                ],
                "title": "Modeling and Monitoring of Indoor Populations using Sparse Positioning\n  Data (Extension)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling and Monitoring of Indoor Populations using Sparse Positioning\n  Data (Extension)"
                },
                "summary": "In large venues like shopping malls and airports, knowledge on the indoor\npopulations fuels applications such as business analytics, venue management,\nand safety control. In this work, we provide means of modeling populations in\npartitions of indoor space offline and of monitoring indoor populations\ncontinuously, by using indoor positioning data. However, the low-sampling rates\nof indoor positioning render the data temporally and spatially sparse, which in\nturn renders the offline capture of indoor populations challenging. It is even\nmore challenging to continuously monitor indoor populations, as positioning\ndata may be missing or not ready yet at the current moment. To address these\nchallenges, we first enable probabilistic modeling of populations in indoor\nspace partitions as Normal distributions. Based on that, we propose two\nlearning-based estimators for on-the-fly prediction of population\ndistributions. Leveraging the prediction-based schemes, we provide a unified\ncontinuous query processing framework for a type of query that enables\ncontinuous monitoring of populated partitions. The framework encompasses\ncaching and result validity mechanisms to reduce cost and maintain monitoring\neffectiveness. Extensive experiments on two real data sets show that the\nproposed estimators are able to outperform the state-of-the-art alternatives\nand that the query processing framework is effective and efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large venues like shopping malls and airports, knowledge on the indoor\npopulations fuels applications such as business analytics, venue management,\nand safety control. In this work, we provide means of modeling populations in\npartitions of indoor space offline and of monitoring indoor populations\ncontinuously, by using indoor positioning data. However, the low-sampling rates\nof indoor positioning render the data temporally and spatially sparse, which in\nturn renders the offline capture of indoor populations challenging. It is even\nmore challenging to continuously monitor indoor populations, as positioning\ndata may be missing or not ready yet at the current moment. To address these\nchallenges, we first enable probabilistic modeling of populations in indoor\nspace partitions as Normal distributions. Based on that, we propose two\nlearning-based estimators for on-the-fly prediction of population\ndistributions. Leveraging the prediction-based schemes, we provide a unified\ncontinuous query processing framework for a type of query that enables\ncontinuous monitoring of populated partitions. The framework encompasses\ncaching and result validity mechanisms to reduce cost and maintain monitoring\neffectiveness. Extensive experiments on two real data sets show that the\nproposed estimators are able to outperform the state-of-the-art alternatives\nand that the query processing framework is effective and efficient."
                },
                "authors": [
                    {
                        "name": "Xiao Li"
                    },
                    {
                        "name": "Huan Li"
                    },
                    {
                        "name": "Hua Lu"
                    },
                    {
                        "name": "Christian S. Jensen"
                    }
                ],
                "author_detail": {
                    "name": "Christian S. Jensen"
                },
                "author": "Christian S. Jensen",
                "arxiv_comment": "Accepted at TKDE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21142v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21142v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22134v1",
                "updated": "2024-10-29T15:31:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:31:27Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching"
                },
                "summary": "The promising applications of large language models are often constrained by\nthe limited GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help mitigate this issue by activating only a subset of the\nmodel's parameters during computation, allowing the unused parameters to be\noffloaded to host memory and reducing overall GPU memory demand. However,\nexisting cache-based offloading solutions handle cache misses reactively and\nsignificantly impact system performance. In this paper, we propose ProMoE, a\nnovel proactive caching system that leverages intermediate model results to\npredict subsequent parameter usage. By proactively fetching experts in advance,\nProMoE removes the loading time from the critical path and diminishes the\nperformance overhead of offloading. Our evaluations demonstrate that ProMoE\nachieves an average speedup of 2.13x and 2.84x in the prefill and decode stages\nrespectively, compared to existing offloading solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promising applications of large language models are often constrained by\nthe limited GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help mitigate this issue by activating only a subset of the\nmodel's parameters during computation, allowing the unused parameters to be\noffloaded to host memory and reducing overall GPU memory demand. However,\nexisting cache-based offloading solutions handle cache misses reactively and\nsignificantly impact system performance. In this paper, we propose ProMoE, a\nnovel proactive caching system that leverages intermediate model results to\npredict subsequent parameter usage. By proactively fetching experts in advance,\nProMoE removes the loading time from the critical path and diminishes the\nperformance overhead of offloading. Our evaluations demonstrate that ProMoE\nachieves an average speedup of 2.13x and 2.84x in the prefill and decode stages\nrespectively, compared to existing offloading solutions."
                },
                "authors": [
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Zihang Zhong"
                    },
                    {
                        "name": "Rong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Rong Chen"
                },
                "author": "Rong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22118v1",
                "updated": "2024-10-29T15:19:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:19:13Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "title": "The Impact of Inference Acceleration Strategies on Bias of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Inference Acceleration Strategies on Bias of LLMs"
                },
                "summary": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to deeply benefit a vast\narray of application domains. However, due to their immense size, performing\ninference with LLMs is both costly and slow. Consequently, a plethora of recent\nwork has proposed strategies to enhance inference efficiency, e.g.,\nquantization, pruning, and caching. These acceleration strategies reduce the\ninference cost and latency, often by several factors, while maintaining much of\nthe predictive performance measured via common benchmarks. In this work, we\nexplore another critical aspect of LLM performance: demographic bias in model\ngenerations due to inference acceleration optimizations. Using a wide range of\nmetrics, we probe bias in model outputs from a number of angles. Analysis of\noutputs before and after inference acceleration shows significant change in\nbias. Worryingly, these bias effects are complex and unpredictable. A\ncombination of an acceleration strategy and bias type may show little bias\nchange in one model but may lead to a large effect in another. Our results\nhighlight a need for in-depth and case-by-case evaluation of model bias after\nit has been modified to accelerate inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to deeply benefit a vast\narray of application domains. However, due to their immense size, performing\ninference with LLMs is both costly and slow. Consequently, a plethora of recent\nwork has proposed strategies to enhance inference efficiency, e.g.,\nquantization, pruning, and caching. These acceleration strategies reduce the\ninference cost and latency, often by several factors, while maintaining much of\nthe predictive performance measured via common benchmarks. In this work, we\nexplore another critical aspect of LLM performance: demographic bias in model\ngenerations due to inference acceleration optimizations. Using a wide range of\nmetrics, we probe bias in model outputs from a number of angles. Analysis of\noutputs before and after inference acceleration shows significant change in\nbias. Worryingly, these bias effects are complex and unpredictable. A\ncombination of an acceleration strategy and bias type may show little bias\nchange in one model but may lead to a large effect in another. Our results\nhighlight a need for in-depth and case-by-case evaluation of model bias after\nit has been modified to accelerate inference."
                },
                "authors": [
                    {
                        "name": "Elisabeth Kirsten"
                    },
                    {
                        "name": "Ivan Habernal"
                    },
                    {
                        "name": "Vedant Nanda"
                    },
                    {
                        "name": "Muhammad Bilal Zafar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Bilal Zafar"
                },
                "author": "Muhammad Bilal Zafar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09526v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09526v2",
                "updated": "2024-10-29T13:04:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    13,
                    4,
                    42,
                    1,
                    303,
                    0
                ],
                "published": "2024-04-15T07:45:04Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    7,
                    45,
                    4,
                    0,
                    106,
                    0
                ],
                "title": "LoongServe: Efficiently Serving Long-Context Large Language Models with\n  Elastic Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoongServe: Efficiently Serving Long-Context Large Language Models with\n  Elastic Sequence Parallelism"
                },
                "summary": "The context window of large language models (LLMs) is rapidly increasing,\nleading to a huge variance in resource usage between different requests as well\nas between different phases of the same request. Restricted by static\nparallelism strategies, existing LLM serving systems cannot efficiently utilize\nthe underlying resources to serve variable-length requests in different phases.\nTo address this problem, we propose a new parallelism paradigm, elastic\nsequence parallelism (ESP), to elastically adapt to the variance between\ndifferent requests and phases. Based on ESP, we design and build LoongServe, an\nLLM serving system that (1) improves computation efficiency by elastically\nadjusting the degree of parallelism in real-time, (2) improves communication\nefficiency by reducing key-value cache migration overhead and overlapping\npartial decoding communication with computation, and (3) improves GPU memory\nefficiency by reducing key-value cache fragmentation across instances. Our\nevaluation under diverse real-world datasets shows that LoongServe improves the\nmaximum throughput by up to 3.85$\\times$ compared to the chunked prefill and\n5.81$\\times$ compared to the prefill-decoding disaggregation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The context window of large language models (LLMs) is rapidly increasing,\nleading to a huge variance in resource usage between different requests as well\nas between different phases of the same request. Restricted by static\nparallelism strategies, existing LLM serving systems cannot efficiently utilize\nthe underlying resources to serve variable-length requests in different phases.\nTo address this problem, we propose a new parallelism paradigm, elastic\nsequence parallelism (ESP), to elastically adapt to the variance between\ndifferent requests and phases. Based on ESP, we design and build LoongServe, an\nLLM serving system that (1) improves computation efficiency by elastically\nadjusting the degree of parallelism in real-time, (2) improves communication\nefficiency by reducing key-value cache migration overhead and overlapping\npartial decoding communication with computation, and (3) improves GPU memory\nefficiency by reducing key-value cache fragmentation across instances. Our\nevaluation under diverse real-world datasets shows that LoongServe improves the\nmaximum throughput by up to 3.85$\\times$ compared to the chunked prefill and\n5.81$\\times$ compared to the prefill-decoding disaggregation."
                },
                "authors": [
                    {
                        "name": "Bingyang Wu"
                    },
                    {
                        "name": "Shengyu Liu"
                    },
                    {
                        "name": "Yinmin Zhong"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09526v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09526v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05821v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05821v4",
                "updated": "2024-10-29T12:28:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    28,
                    58,
                    1,
                    303,
                    0
                ],
                "published": "2023-12-10T08:41:24Z",
                "published_parsed": [
                    2023,
                    12,
                    10,
                    8,
                    41,
                    24,
                    6,
                    344,
                    0
                ],
                "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models"
                },
                "summary": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from the distribution variance in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by transforming the\nweight matrix based on the activation distribution. This transformation allows\nthe outliers in the activation matrix to be absorbed into the transformed\nweight matrix, thereby enhancing decomposition accuracy. Additionally, we\npropose an efficient iterative calibration process to optimize layer-specific\ndecomposition by addressing the varying sensitivity of different LLM layers. In\nthis way, ASVD can compress a network by 10%-30%. Based on the success of the\nlow-rank decomposition of projection matrices in the self-attention module, we\nfurther introduce ASVD to compress the KV cache. By reducing the channel\ndimension of KV activations, memory requirements for KV cache can be largely\nreduced. ASVD can further achieve 50% KV cache reductions without performance\ndrop in a training-free manner. Code is anonymously available in supplementary\nmaterials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from the distribution variance in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by transforming the\nweight matrix based on the activation distribution. This transformation allows\nthe outliers in the activation matrix to be absorbed into the transformed\nweight matrix, thereby enhancing decomposition accuracy. Additionally, we\npropose an efficient iterative calibration process to optimize layer-specific\ndecomposition by addressing the varying sensitivity of different LLM layers. In\nthis way, ASVD can compress a network by 10%-30%. Based on the success of the\nlow-rank decomposition of projection matrices in the self-attention module, we\nfurther introduce ASVD to compress the KV cache. By reducing the channel\ndimension of KV activations, memory requirements for KV cache can be largely\nreduced. ASVD can further achieve 50% KV cache reductions without performance\ndrop in a training-free manner. Code is anonymously available in supplementary\nmaterials."
                },
                "authors": [
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05821v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05821v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18627v2",
                "updated": "2024-10-29T12:03:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    3,
                    14,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-24T10:36:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits"
                },
                "summary": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the greedy policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the greedy policy."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00456v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00456v2",
                "updated": "2024-10-29T11:09:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    11,
                    9,
                    12,
                    1,
                    303,
                    0
                ],
                "published": "2024-03-30T19:20:06Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    19,
                    20,
                    6,
                    5,
                    90,
                    0
                ],
                "title": "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs"
                },
                "summary": "We introduce QuaRot, a new Quantization scheme based on Rotations, which is\nable to quantize LLMs end-to-end, including all weights, activations, and KV\ncache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the\nhidden state without changing the output, making quantization easier. This\ncomputational invariance is applied to the hidden state (residual) of the LLM,\nas well as to the activations of the feed-forward components, aspects of the\nattention mechanism, and to the KV cache. The result is a quantized model where\nall matrix multiplications are performed in 4 bits, without any channels\nidentified for retention in higher precision. Our 4-bit quantized LLaMa2-70B\nmodel has losses of at most 0.47 WikiText-2 perplexity and retains 99% of the\nzero-shot performance. We also show that QuaRot can provide lossless 6 and 8\nbit LLaMa2 models without any calibration data using round-to-nearest\nquantization. Code is available at: https://github.com/spcl/QuaRot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce QuaRot, a new Quantization scheme based on Rotations, which is\nable to quantize LLMs end-to-end, including all weights, activations, and KV\ncache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the\nhidden state without changing the output, making quantization easier. This\ncomputational invariance is applied to the hidden state (residual) of the LLM,\nas well as to the activations of the feed-forward components, aspects of the\nattention mechanism, and to the KV cache. The result is a quantized model where\nall matrix multiplications are performed in 4 bits, without any channels\nidentified for retention in higher precision. Our 4-bit quantized LLaMa2-70B\nmodel has losses of at most 0.47 WikiText-2 perplexity and retains 99% of the\nzero-shot performance. We also show that QuaRot can provide lossless 6 and 8\nbit LLaMa2 models without any calibration data using round-to-nearest\nquantization. Code is available at: https://github.com/spcl/QuaRot."
                },
                "authors": [
                    {
                        "name": "Saleh Ashkboos"
                    },
                    {
                        "name": "Amirkeivan Mohtashami"
                    },
                    {
                        "name": "Maximilian L. Croci"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Pashmina Cameron"
                    },
                    {
                        "name": "Martin Jaggi"
                    },
                    {
                        "name": "Dan Alistarh"
                    },
                    {
                        "name": "Torsten Hoefler"
                    },
                    {
                        "name": "James Hensman"
                    }
                ],
                "author_detail": {
                    "name": "James Hensman"
                },
                "author": "James Hensman",
                "arxiv_comment": "21 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00456v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00456v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02369v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02369v3",
                "updated": "2024-10-29T04:21:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    4,
                    21,
                    30,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-03T10:33:49Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    10,
                    33,
                    49,
                    3,
                    277,
                    0
                ],
                "title": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation"
                },
                "summary": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings."
                },
                "authors": [
                    {
                        "name": "Muzhi Zhu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Zekai Luo"
                    },
                    {
                        "name": "Chenchen Jing"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Guangkai Xu"
                    },
                    {
                        "name": "Xinlong Wang"
                    },
                    {
                        "name": "Chunhua Shen"
                    }
                ],
                "author_detail": {
                    "name": "Chunhua Shen"
                },
                "author": "Chunhua Shen",
                "arxiv_comment": "Accepted to Proc. Annual Conference on Neural Information Processing\n  Systems (NeurIPS) 2024. Webpage: https://github.com/aim-uofa/DiffewS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02369v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02369v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19291v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19291v3",
                "updated": "2024-10-29T02:52:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    2,
                    52,
                    24,
                    1,
                    303,
                    0
                ],
                "published": "2024-07-27T16:20:21Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "title": "Symmetric Locality: Definition and Initial Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Locality: Definition and Initial Results"
                },
                "summary": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs."
                },
                "authors": [
                    {
                        "name": "Giordan Escalona"
                    },
                    {
                        "name": "Dylan McKellips"
                    },
                    {
                        "name": "Chen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ding"
                },
                "author": "Chen Ding",
                "arxiv_comment": "6 pages, 2nd ver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19291v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19291v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19258v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19258v2",
                "updated": "2024-10-28T19:32:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    32,
                    23,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-25T02:22:00Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    2,
                    22,
                    0,
                    4,
                    299,
                    0
                ],
                "title": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning"
                },
                "summary": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark."
                },
                "authors": [
                    {
                        "name": "Yu Fu"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "arxiv_comment": "18pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19258v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19258v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21465v1",
                "updated": "2024-10-28T19:08:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T19:08:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference"
                },
                "summary": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Wenlei Bao"
                    },
                    {
                        "name": "Size Zheng"
                    },
                    {
                        "name": "Ningxin Zheng"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Harry Dong"
                    },
                    {
                        "name": "Yuejie Chi"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21266v1",
                "updated": "2024-10-28T17:57:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    57,
                    40,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T17:57:40Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    57,
                    40,
                    0,
                    302,
                    0
                ],
                "title": "Online Weighted Paging with Unknown Weights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Weighted Paging with Unknown Weights"
                },
                "summary": "Online paging is a fundamental problem in the field of online algorithms, in\nwhich one maintains a cache of $k$ slots as requests for fetching pages arrive\nonline. In the weighted variant of this problem, each page has its own fetching\ncost; a substantial line of work on this problem culminated in an (optimal)\n$O(\\log k)$-competitive randomized algorithm, due to Bansal, Buchbinder and\nNaor (FOCS'07).\n  Existing work for weighted paging assumes that page weights are known in\nadvance, which is not always the case in practice. For example, in multi-level\ncaching architectures, the expected cost of fetching a memory block is a\nfunction of its probability of being in a mid-level cache rather than the main\nmemory. This complex property cannot be predicted in advance; over time,\nhowever, one may glean information about page weights through sampling their\nfetching cost multiple times.\n  We present the first algorithm for online weighted paging that does not know\npage weights in advance, but rather learns from weight samples. In terms of\ntechniques, this requires providing (integral) samples to a fractional solver,\nrequiring a delicate interface between this solver and the randomized rounding\nscheme; we believe that our work can inspire online algorithms to other\nproblems that involve cost sampling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online paging is a fundamental problem in the field of online algorithms, in\nwhich one maintains a cache of $k$ slots as requests for fetching pages arrive\nonline. In the weighted variant of this problem, each page has its own fetching\ncost; a substantial line of work on this problem culminated in an (optimal)\n$O(\\log k)$-competitive randomized algorithm, due to Bansal, Buchbinder and\nNaor (FOCS'07).\n  Existing work for weighted paging assumes that page weights are known in\nadvance, which is not always the case in practice. For example, in multi-level\ncaching architectures, the expected cost of fetching a memory block is a\nfunction of its probability of being in a mid-level cache rather than the main\nmemory. This complex property cannot be predicted in advance; over time,\nhowever, one may glean information about page weights through sampling their\nfetching cost multiple times.\n  We present the first algorithm for online weighted paging that does not know\npage weights in advance, but rather learns from weight samples. In terms of\ntechniques, this requires providing (integral) samples to a fractional solver,\nrequiring a delicate interface between this solver and the randomized rounding\nscheme; we believe that our work can inspire online algorithms to other\nproblems that involve cost sampling."
                },
                "authors": [
                    {
                        "name": "Orin Levy"
                    },
                    {
                        "name": "Noam Touitou"
                    },
                    {
                        "name": "Aviv Rosenberg"
                    }
                ],
                "author_detail": {
                    "name": "Aviv Rosenberg"
                },
                "author": "Aviv Rosenberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08141v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08141v2",
                "updated": "2024-10-28T16:42:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    16,
                    42,
                    11,
                    0,
                    302,
                    0
                ],
                "published": "2024-09-12T15:34:23Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "title": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects"
                },
                "summary": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. Like others\nbefore us, we argue that the assumptions that led to this model are obsolete,\nand in many use-cases use of Programmed I/O (PIO), where the CPU explicitly\ntransfers data and control information to and from a device via loads and\nstores, actually results in a more efficient system. However, unlike others to\ndate, we push this idea further and show, in a real implementation, the gains\nin average and tail latency for fine-grained communication achievable using an\nopen cache-coherence protocol which exposes cache transitions to a smart\ndevice. We show this using three use-cases: fine-grained RPC-style invocation\nof functions on an accelerator, offloading of operators in a streaming dataflow\nengine, and a network interface targeting for serverless functions, comparing\nour use of coherence with both traditional DMA-style interaction and a\nhighly-optimized implementation using PIO over PCI Express (PCIe).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. Like others\nbefore us, we argue that the assumptions that led to this model are obsolete,\nand in many use-cases use of Programmed I/O (PIO), where the CPU explicitly\ntransfers data and control information to and from a device via loads and\nstores, actually results in a more efficient system. However, unlike others to\ndate, we push this idea further and show, in a real implementation, the gains\nin average and tail latency for fine-grained communication achievable using an\nopen cache-coherence protocol which exposes cache transitions to a smart\ndevice. We show this using three use-cases: fine-grained RPC-style invocation\nof functions on an accelerator, offloading of operators in a streaming dataflow\nengine, and a network interface targeting for serverless functions, comparing\nour use of coherence with both traditional DMA-style interaction and a\nhighly-optimized implementation using PIO over PCI Express (PCIe)."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ruzhanskaia"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "David Cock"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08141v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08141v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16179v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16179v2",
                "updated": "2024-10-28T14:44:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    44,
                    22,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-21T16:44:51Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    44,
                    51,
                    0,
                    295,
                    0
                ],
                "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicPIG: LSH Sampling for Efficient LLM Generation"
                },
                "summary": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\n\\url{https://github.com/Infini-AI-Lab/MagicPIG}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\n\\url{https://github.com/Infini-AI-Lab/MagicPIG}."
                },
                "authors": [
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jianyu Zhang"
                    },
                    {
                        "name": "Niklas Nolte"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Matthijs Douze"
                    },
                    {
                        "name": "Leon Bottou"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16179v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16179v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21073v1",
                "updated": "2024-10-28T14:35:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    35,
                    12,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T14:35:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    35,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "Skip2-LoRA: A Lightweight On-device DNN Fine-tuning Method for Low-cost\n  Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip2-LoRA: A Lightweight On-device DNN Fine-tuning Method for Low-cost\n  Edge Devices"
                },
                "summary": "This paper proposes Skip2-LoRA as a lightweight fine-tuning method for deep\nneural networks to address the gap between pre-trained and deployed models. In\nour approach, trainable LoRA (low-rank adaptation) adapters are inserted\nbetween the last layer and every other layer to enhance the network expressive\npower while keeping the backward computation cost low. This architecture is\nwell-suited to cache intermediate computation results of the forward pass and\nthen can skip the forward computation of seen samples as training epochs\nprogress. We implemented the combination of the proposed architecture and\ncache, denoted as Skip2-LoRA, and tested it on a $15 single board computer. Our\nresults show that Skip2-LoRA reduces the fine-tuning time by 90.0% on average\ncompared to the counterpart that has the same number of trainable parameters\nwhile preserving the accuracy, while taking only a few seconds on the\nmicrocontroller board.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes Skip2-LoRA as a lightweight fine-tuning method for deep\nneural networks to address the gap between pre-trained and deployed models. In\nour approach, trainable LoRA (low-rank adaptation) adapters are inserted\nbetween the last layer and every other layer to enhance the network expressive\npower while keeping the backward computation cost low. This architecture is\nwell-suited to cache intermediate computation results of the forward pass and\nthen can skip the forward computation of seen samples as training epochs\nprogress. We implemented the combination of the proposed architecture and\ncache, denoted as Skip2-LoRA, and tested it on a $15 single board computer. Our\nresults show that Skip2-LoRA reduces the fine-tuning time by 90.0% on average\ncompared to the counterpart that has the same number of trainable parameters\nwhile preserving the accuracy, while taking only a few seconds on the\nmicrocontroller board."
                },
                "authors": [
                    {
                        "name": "Hiroki Matsutani"
                    },
                    {
                        "name": "Masaaki Kondo"
                    },
                    {
                        "name": "Kazuki Sunaga"
                    },
                    {
                        "name": "Radu Marculescu"
                    }
                ],
                "author_detail": {
                    "name": "Radu Marculescu"
                },
                "author": "Radu Marculescu",
                "arxiv_comment": "ASP-DAC 2025 (accepted)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21035v1",
                "updated": "2024-10-28T13:56:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    56,
                    30,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T13:56:30Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    56,
                    30,
                    0,
                    302,
                    0
                ],
                "title": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time"
                },
                "summary": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, our models, even without caching, can\ngenerate tokens at a rate that is up to 8 times faster than AR models employing\nKV caching, and we anticipate further improvements with the inclusion of\ncaching. Moreover, we demonstrate the efficacy of our approach for diffusion\nlanguage models with up to 860M parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, our models, even without caching, can\ngenerate tokens at a rate that is up to 8 times faster than AR models employing\nKV caching, and we anticipate further improvements with the inclusion of\ncaching. Moreover, we demonstrate the efficacy of our approach for diffusion\nlanguage models with up to 860M parameters."
                },
                "authors": [
                    {
                        "name": "Justin Deschenaux"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20790v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20790v1",
                "updated": "2024-10-28T07:13:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    13,
                    25,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T07:13:25Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    13,
                    25,
                    0,
                    302,
                    0
                ],
                "title": "SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by\n  Exploiting Temporal Continuity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by\n  Exploiting Temporal Continuity"
                },
                "summary": "Deep learning models have become pivotal in the field of video processing and\nis increasingly critical in practical applications such as autonomous driving\nand object detection. Although Vision Transformers (ViTs) have demonstrated\ntheir power, Convolutional Neural Networks (CNNs) remain a highly efficient and\nhigh-performance choice for feature extraction and encoding. However, the\nintensive computational demands of convolution operations hinder its broader\nadoption as a video encoder. Given the inherent temporal continuity in video\nframes, changes between consecutive frames are minimal, allowing for the\nskipping of redundant computations. This technique, which we term as Diff\nComputation, presents two primary challenges. First, Diff Computation requires\nto cache intermediate feature maps to ensure the correctness of non-linear\ncomputations, leading to significant memory consumption. Second, the imbalance\nof sparsity among layers, introduced by Diff Computation, incurs accuracy\ndegradation. To address these issues, we propose a memory-efficient scheduling\nmethod to eliminate memory overhead and an online adjustment mechanism to\nminimize accuracy degradation. We integrate these techniques into our\nframework, SparseTem, to seamlessly support various CNN-based video encoders.\nSparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with\nminimal accuracy drop and no additional memory overhead. Extensive experimental\nresults demonstrate that SparseTem sets a new state-of-the-art by effectively\nutilizing temporal continuity to accelerate CNN-based video encoders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models have become pivotal in the field of video processing and\nis increasingly critical in practical applications such as autonomous driving\nand object detection. Although Vision Transformers (ViTs) have demonstrated\ntheir power, Convolutional Neural Networks (CNNs) remain a highly efficient and\nhigh-performance choice for feature extraction and encoding. However, the\nintensive computational demands of convolution operations hinder its broader\nadoption as a video encoder. Given the inherent temporal continuity in video\nframes, changes between consecutive frames are minimal, allowing for the\nskipping of redundant computations. This technique, which we term as Diff\nComputation, presents two primary challenges. First, Diff Computation requires\nto cache intermediate feature maps to ensure the correctness of non-linear\ncomputations, leading to significant memory consumption. Second, the imbalance\nof sparsity among layers, introduced by Diff Computation, incurs accuracy\ndegradation. To address these issues, we propose a memory-efficient scheduling\nmethod to eliminate memory overhead and an online adjustment mechanism to\nminimize accuracy degradation. We integrate these techniques into our\nframework, SparseTem, to seamlessly support various CNN-based video encoders.\nSparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with\nminimal accuracy drop and no additional memory overhead. Extensive experimental\nresults demonstrate that SparseTem sets a new state-of-the-art by effectively\nutilizing temporal continuity to accelerate CNN-based video encoders."
                },
                "authors": [
                    {
                        "name": "Kunyun Wang"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Wenchao Ding"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "arxiv_comment": "9 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20790v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20790v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01847v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01847v3",
                "updated": "2024-10-27T14:40:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    27,
                    14,
                    40,
                    8,
                    6,
                    301,
                    0
                ],
                "published": "2024-04-02T11:12:42Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    11,
                    12,
                    42,
                    1,
                    93,
                    0
                ],
                "title": "Accelerating Transformer Pre-training with 2:4 Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Transformer Pre-training with 2:4 Sparsity"
                },
                "summary": "Training large transformers is slow, but recent innovations on GPU\narchitecture give us an advantage. NVIDIA Ampere GPUs can execute a\nfine-grained 2:4 sparse matrix multiplication twice as fast as its dense\nequivalent. In the light of this property, we comprehensively investigate the\nfeasibility of accelerating feed-forward networks (FFNs) of transformers in\npre-training. First, we define a ``flip rate'' to monitor the stability of a\n2:4 training process. Utilizing this metric, we propose three techniques to\npreserve accuracy: to modify the sparse-refined straight-through estimator by\napplying the masked decay term on gradients, to determine a feasible decay\nfactor in warm-up stage, and to enhance the model's quality by a dense\nfine-tuning procedure near the end of pre-training. Besides, we devise two\ntechniques to practically accelerate training: to calculate transposable 2:4\nmasks by convolution, and to accelerate gated activation functions by reducing\nGPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm\nachieves similar convergence to dense training algorithms on several\ntransformer pre-training tasks, while actual acceleration can be observed on\ndifferent shapes of transformer block apparently. Our toolkit is available at\nhttps://github.com/huyz2023/2by4-pretrain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large transformers is slow, but recent innovations on GPU\narchitecture give us an advantage. NVIDIA Ampere GPUs can execute a\nfine-grained 2:4 sparse matrix multiplication twice as fast as its dense\nequivalent. In the light of this property, we comprehensively investigate the\nfeasibility of accelerating feed-forward networks (FFNs) of transformers in\npre-training. First, we define a ``flip rate'' to monitor the stability of a\n2:4 training process. Utilizing this metric, we propose three techniques to\npreserve accuracy: to modify the sparse-refined straight-through estimator by\napplying the masked decay term on gradients, to determine a feasible decay\nfactor in warm-up stage, and to enhance the model's quality by a dense\nfine-tuning procedure near the end of pre-training. Besides, we devise two\ntechniques to practically accelerate training: to calculate transposable 2:4\nmasks by convolution, and to accelerate gated activation functions by reducing\nGPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm\nachieves similar convergence to dense training algorithms on several\ntransformer pre-training tasks, while actual acceleration can be observed on\ndifferent shapes of transformer block apparently. Our toolkit is available at\nhttps://github.com/huyz2023/2by4-pretrain."
                },
                "authors": [
                    {
                        "name": "Yuezhou Hu"
                    },
                    {
                        "name": "Kang Zhao"
                    },
                    {
                        "name": "Weiyu Huang"
                    },
                    {
                        "name": "Jianfei Chen"
                    },
                    {
                        "name": "Jun Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhu"
                },
                "author": "Jun Zhu",
                "arxiv_journal_ref": "Proceedings of the 41st International Conference on Machine\n  Learning (2024), in Proceedings of Machine Learning Research 235:19531-19543",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01847v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01847v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20337v1",
                "updated": "2024-10-27T04:31:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    27,
                    4,
                    31,
                    35,
                    6,
                    301,
                    0
                ],
                "published": "2024-10-27T04:31:35Z",
                "published_parsed": [
                    2024,
                    10,
                    27,
                    4,
                    31,
                    35,
                    6,
                    301,
                    0
                ],
                "title": "On the I/O Complexity of the CYK Algorithm and of a Family of Related DP\n  Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the I/O Complexity of the CYK Algorithm and of a Family of Related DP\n  Algorithms"
                },
                "summary": "Asymptotically tight lower bounds are derived for the Input/Output (I/O)\ncomplexity of a class of dynamic programming algorithms including matrix chain\nmultiplication, optimal polygon triangulation, and the construction of optimal\nbinary search trees. Assuming no recomputation of intermediate values, we\nestablish an $\\Omega\\left(\\frac{n^3}{\\sqrt{M}B}\\right)$ I/O lower bound, where\n$n$ denotes the size of the input and $M$ denotes the size of the available\nfast memory (cache). When recomputation is allowed, we show the same bound\nholds for $M < cn$, where $c$ is a positive constant. In the case where $M \\ge\n2n$, we show an $\\Omega\\left(n/B\\right)$ I/O lower bound. We also discuss\nalgorithms for which the number of executed I/O operations matches\nasymptotically each of the presented lower bounds, which are thus\nasymptotically tight.\n  Additionally, we refine our general method to obtain a lower bound for the\nI/O complexity of the Cocke-Younger-Kasami algorithm, where the size of the\ngrammar impacts the I/O complexity. An upper bound with asymptotically matching\nperformance in many cases is also provided.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asymptotically tight lower bounds are derived for the Input/Output (I/O)\ncomplexity of a class of dynamic programming algorithms including matrix chain\nmultiplication, optimal polygon triangulation, and the construction of optimal\nbinary search trees. Assuming no recomputation of intermediate values, we\nestablish an $\\Omega\\left(\\frac{n^3}{\\sqrt{M}B}\\right)$ I/O lower bound, where\n$n$ denotes the size of the input and $M$ denotes the size of the available\nfast memory (cache). When recomputation is allowed, we show the same bound\nholds for $M < cn$, where $c$ is a positive constant. In the case where $M \\ge\n2n$, we show an $\\Omega\\left(n/B\\right)$ I/O lower bound. We also discuss\nalgorithms for which the number of executed I/O operations matches\nasymptotically each of the presented lower bounds, which are thus\nasymptotically tight.\n  Additionally, we refine our general method to obtain a lower bound for the\nI/O complexity of the Cocke-Younger-Kasami algorithm, where the size of the\ngrammar impacts the I/O complexity. An upper bound with asymptotically matching\nperformance in many cases is also provided."
                },
                "authors": [
                    {
                        "name": "Lorenzo De Stefani"
                    },
                    {
                        "name": "Vedant Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Vedant Gupta"
                },
                "author": "Vedant Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04216v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04216v3",
                "updated": "2024-10-26T22:19:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    26,
                    22,
                    19,
                    4,
                    5,
                    300,
                    0
                ],
                "published": "2024-02-06T18:17:02Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    18,
                    17,
                    2,
                    1,
                    37,
                    0
                ],
                "title": "Resource-Aware Hierarchical Federated Learning in Wireless Video Caching\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource-Aware Hierarchical Federated Learning in Wireless Video Caching\n  Networks"
                },
                "summary": "Backhaul traffic congestion caused by the video traffic of a few popular\nfiles can be alleviated by storing the to-be-requested content at various\nlevels in wireless video caching networks. Typically, content service providers\n(CSPs) own the content, and the users request their preferred content from the\nCSPs using their (wireless) internet service providers (ISPs). As these parties\ndo not reveal their private information and business secrets, traditional\ntechniques may not be readily used to predict the dynamic changes in users'\nfuture demands. Motivated by this, we propose a novel resource-aware\nhierarchical federated learning (RawHFL) solution for predicting user's future\ncontent requests. A practical data acquisition technique is used that allows\nthe user to update its local training dataset based on its requested content.\nBesides, since networking and other computational resources are limited,\nconsidering that only a subset of the users participate in the model training,\nwe derive the convergence bound of the proposed algorithm. Based on this bound,\nwe minimize a weighted utility function for jointly configuring the\ncontrollable parameters to train the RawHFL energy efficiently under practical\nresource constraints. Our extensive simulation results validate the proposed\nalgorithm's superiority, in terms of test accuracy and energy cost, over\nexisting baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backhaul traffic congestion caused by the video traffic of a few popular\nfiles can be alleviated by storing the to-be-requested content at various\nlevels in wireless video caching networks. Typically, content service providers\n(CSPs) own the content, and the users request their preferred content from the\nCSPs using their (wireless) internet service providers (ISPs). As these parties\ndo not reveal their private information and business secrets, traditional\ntechniques may not be readily used to predict the dynamic changes in users'\nfuture demands. Motivated by this, we propose a novel resource-aware\nhierarchical federated learning (RawHFL) solution for predicting user's future\ncontent requests. A practical data acquisition technique is used that allows\nthe user to update its local training dataset based on its requested content.\nBesides, since networking and other computational resources are limited,\nconsidering that only a subset of the users participate in the model training,\nwe derive the convergence bound of the proposed algorithm. Based on this bound,\nwe minimize a weighted utility function for jointly configuring the\ncontrollable parameters to train the RawHFL energy efficiently under practical\nresource constraints. Our extensive simulation results validate the proposed\nalgorithm's superiority, in terms of test accuracy and energy cost, over\nexisting baselines."
                },
                "authors": [
                    {
                        "name": "Md Ferdous Pervej"
                    },
                    {
                        "name": "Andreas F. Molisch"
                    }
                ],
                "author_detail": {
                    "name": "Andreas F. Molisch"
                },
                "author": "Andreas F. Molisch",
                "arxiv_comment": "Under review for possible publication in IEEE Transactions on\n  Wireless Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04216v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04216v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20149v1",
                "updated": "2024-10-26T11:20:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    26,
                    11,
                    20,
                    2,
                    5,
                    300,
                    0
                ],
                "published": "2024-10-26T11:20:02Z",
                "published_parsed": [
                    2024,
                    10,
                    26,
                    11,
                    20,
                    2,
                    5,
                    300,
                    0
                ],
                "title": "AdaNeg: Adaptive Negative Proxy Guided OOD Detection with\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaNeg: Adaptive Negative Proxy Guided OOD Detection with\n  Vision-Language Models"
                },
                "summary": "Recent research has shown that pre-trained vision-language models are\neffective at identifying out-of-distribution (OOD) samples by using negative\nlabels as guidance. However, employing consistent negative labels across\ndifferent OOD datasets often results in semantic misalignments, as these text\nlabels may not accurately reflect the actual space of OOD images. To overcome\nthis issue, we introduce \\textit{adaptive negative proxies}, which are\ndynamically generated during testing by exploring actual OOD images, to align\nmore closely with the underlying OOD label space and enhance the efficacy of\nnegative proxy guidance. Specifically, our approach utilizes a feature memory\nbank to selectively cache discriminative features from test images,\nrepresenting the targeted OOD distribution. This facilitates the creation of\nproxies that can better align with specific OOD datasets. While task-adaptive\nproxies average features to reflect the unique characteristics of each dataset,\nthe sample-adaptive proxies weight features based on their similarity to\nindividual test samples, exploring detailed sample-level nuances. The final\nscore for identifying OOD samples integrates static negative labels with our\nproposed adaptive proxies, effectively combining textual and visual knowledge\nfor enhanced performance. Our method is training-free and annotation-free, and\nit maintains fast testing speed. Extensive experiments across various\nbenchmarks demonstrate the effectiveness of our approach, abbreviated as\nAdaNeg. Notably, on the large-scale ImageNet benchmark, our AdaNeg\nsignificantly outperforms existing methods, with a 2.45\\% increase in AUROC and\na 6.48\\% reduction in FPR95. Codes are available at\n\\url{https://github.com/YBZh/OpenOOD-VLM}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has shown that pre-trained vision-language models are\neffective at identifying out-of-distribution (OOD) samples by using negative\nlabels as guidance. However, employing consistent negative labels across\ndifferent OOD datasets often results in semantic misalignments, as these text\nlabels may not accurately reflect the actual space of OOD images. To overcome\nthis issue, we introduce \\textit{adaptive negative proxies}, which are\ndynamically generated during testing by exploring actual OOD images, to align\nmore closely with the underlying OOD label space and enhance the efficacy of\nnegative proxy guidance. Specifically, our approach utilizes a feature memory\nbank to selectively cache discriminative features from test images,\nrepresenting the targeted OOD distribution. This facilitates the creation of\nproxies that can better align with specific OOD datasets. While task-adaptive\nproxies average features to reflect the unique characteristics of each dataset,\nthe sample-adaptive proxies weight features based on their similarity to\nindividual test samples, exploring detailed sample-level nuances. The final\nscore for identifying OOD samples integrates static negative labels with our\nproposed adaptive proxies, effectively combining textual and visual knowledge\nfor enhanced performance. Our method is training-free and annotation-free, and\nit maintains fast testing speed. Extensive experiments across various\nbenchmarks demonstrate the effectiveness of our approach, abbreviated as\nAdaNeg. Notably, on the large-scale ImageNet benchmark, our AdaNeg\nsignificantly outperforms existing methods, with a 2.45\\% increase in AUROC and\na 6.48\\% reduction in FPR95. Codes are available at\n\\url{https://github.com/YBZh/OpenOOD-VLM}."
                },
                "authors": [
                    {
                        "name": "Yabin Zhang"
                    },
                    {
                        "name": "Lei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhang"
                },
                "author": "Lei Zhang",
                "arxiv_comment": "NIPS 2024 Camera Ready, Codes are available at\n  \\url{https://github.com/YBZh/OpenOOD-VLM}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20004v1",
                "updated": "2024-10-25T23:17:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    23,
                    17,
                    56,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T23:17:56Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    23,
                    17,
                    56,
                    4,
                    299,
                    0
                ],
                "title": "Lightweight, Secure and Stateful Serverless Computing with PSL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight, Secure and Stateful Serverless Computing with PSL"
                },
                "summary": "We present PSL, a lightweight, secure and stateful Function-as-a-Serivce\n(FaaS) framework for Trusted Execution Environments (TEEs). The framework\nprovides rich programming language support on heterogeneous TEE hardware for\nstatically compiled binaries and/or WebAssembly (WASM) bytecodes, with a\nfamiliar Key-Value Store (KVS) interface to secure, performant,\nnetwork-embedded storage. It achieves near-native execution speeds by utilizing\nthe dynamic memory mapping capabilities of Intel SGX2 to create an in-enclave\nWASM runtime with Just-In-Time (JIT) compilation. PSL is designed to\nefficiently operate within an asynchronous environment with a distributed\ntamper-proof confidential storage system, assuming minority failures. The\nsystem exchanges eventually-consistent state updates across nodes while\nutilizing release-consistent locking mechanisms to enhance transactional\ncapabilities. The execution of PSL is up to 3.7x faster than the\nstate-of-the-art SGX WASM runtime. PSL reaches 95k ops/s with YCSB 100% read\nworkload and 89k ops/s with 50% read/write workload. We demonstrate the\nscalability and adaptivity of PSL through a case study of secure and\ndistributed training of deep neural networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present PSL, a lightweight, secure and stateful Function-as-a-Serivce\n(FaaS) framework for Trusted Execution Environments (TEEs). The framework\nprovides rich programming language support on heterogeneous TEE hardware for\nstatically compiled binaries and/or WebAssembly (WASM) bytecodes, with a\nfamiliar Key-Value Store (KVS) interface to secure, performant,\nnetwork-embedded storage. It achieves near-native execution speeds by utilizing\nthe dynamic memory mapping capabilities of Intel SGX2 to create an in-enclave\nWASM runtime with Just-In-Time (JIT) compilation. PSL is designed to\nefficiently operate within an asynchronous environment with a distributed\ntamper-proof confidential storage system, assuming minority failures. The\nsystem exchanges eventually-consistent state updates across nodes while\nutilizing release-consistent locking mechanisms to enhance transactional\ncapabilities. The execution of PSL is up to 3.7x faster than the\nstate-of-the-art SGX WASM runtime. PSL reaches 95k ops/s with YCSB 100% read\nworkload and 89k ops/s with 50% read/write workload. We demonstrate the\nscalability and adaptivity of PSL through a case study of secure and\ndistributed training of deep neural networks."
                },
                "authors": [
                    {
                        "name": "Alexander Thomas"
                    },
                    {
                        "name": "Shubham Mishra"
                    },
                    {
                        "name": "Kaiyuan Chen"
                    },
                    {
                        "name": "John Kubiatowicz"
                    }
                ],
                "author_detail": {
                    "name": "John Kubiatowicz"
                },
                "author": "John Kubiatowicz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05317v2",
                "updated": "2024-10-25T21:09:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    21,
                    9,
                    59,
                    4,
                    299,
                    0
                ],
                "published": "2024-06-08T01:35:11Z",
                "published_parsed": [
                    2024,
                    6,
                    8,
                    1,
                    35,
                    11,
                    5,
                    160,
                    0
                ],
                "title": "LoCoCo: Dropping In Convolutions for Long Context Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoCoCo: Dropping In Convolutions for Long Context Compression"
                },
                "summary": "This paper tackles the memory hurdle of processing long context sequences in\nLarge Language Models (LLMs), by presenting a novel approach, Dropping In\nConvolutions for Long Context Compression (LoCoCo). LoCoCo employs only a\nfixed-size Key-Value (KV) cache, and can enhance efficiency in both inference\nand fine-tuning stages. Diverging from prior methods that selectively drop KV\npairs based on heuristics, LoCoCo leverages a data-driven adaptive fusion\ntechnique, blending previous KV pairs with incoming tokens to minimize the loss\nof contextual information and ensure accurate attention modeling. This token\nintegration is achieved through injecting one-dimensional convolutional kernels\nthat dynamically calculate mixing weights for each KV cache slot. Designed for\nbroad compatibility with existing LLM frameworks, LoCoCo allows for\nstraightforward \"drop-in\" integration without needing architectural\nmodifications, while incurring minimal tuning overhead. Experiments demonstrate\nthat LoCoCo maintains consistently outstanding performance across various\ncontext lengths and can achieve a high context compression rate during both\ninference and fine-tuning phases. During inference, we successfully compressed\nup to 3482 tokens into a 128-size KV cache, while retaining comparable\nperformance to the full sequence - an accuracy improvement of up to 0.2791\ncompared to baselines at the same cache size. During post-training tuning, we\nalso effectively extended the context length from 4K to 32K using a KV cache of\nfixed size 512, achieving performance similar to fine-tuning with entire\nsequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper tackles the memory hurdle of processing long context sequences in\nLarge Language Models (LLMs), by presenting a novel approach, Dropping In\nConvolutions for Long Context Compression (LoCoCo). LoCoCo employs only a\nfixed-size Key-Value (KV) cache, and can enhance efficiency in both inference\nand fine-tuning stages. Diverging from prior methods that selectively drop KV\npairs based on heuristics, LoCoCo leverages a data-driven adaptive fusion\ntechnique, blending previous KV pairs with incoming tokens to minimize the loss\nof contextual information and ensure accurate attention modeling. This token\nintegration is achieved through injecting one-dimensional convolutional kernels\nthat dynamically calculate mixing weights for each KV cache slot. Designed for\nbroad compatibility with existing LLM frameworks, LoCoCo allows for\nstraightforward \"drop-in\" integration without needing architectural\nmodifications, while incurring minimal tuning overhead. Experiments demonstrate\nthat LoCoCo maintains consistently outstanding performance across various\ncontext lengths and can achieve a high context compression rate during both\ninference and fine-tuning phases. During inference, we successfully compressed\nup to 3482 tokens into a 128-size KV cache, while retaining comparable\nperformance to the full sequence - an accuracy improvement of up to 0.2791\ncompared to baselines at the same cache size. During post-training tuning, we\nalso effectively extended the context length from 4K to 32K using a KV cache of\nfixed size 512, achieving performance similar to fine-tuning with entire\nsequences."
                },
                "authors": [
                    {
                        "name": "Ruisi Cai"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03766v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03766v2",
                "updated": "2024-10-25T19:45:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    45,
                    33,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-02T15:22:08Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    22,
                    8,
                    2,
                    276,
                    0
                ],
                "title": "FutureFill: Fast Generation from Convolutional Sequence Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FutureFill: Fast Generation from Convolutional Sequence Models"
                },
                "summary": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill - a method for fast generation that\napplies to any sequence prediction algorithm based on convolutional operators.\nOur approach reduces the generation time requirement from quadratic to\nquasilinear relative to the context length. Additionally, FutureFill requires a\nprefill cache sized only by the number of tokens generated, which is smaller\nthan the cache requirements for standard convolutional and attention-based\nmodels. We validate our theoretical findings with experimental evidence\ndemonstrating correctness and efficiency gains in a synthetic generation task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill - a method for fast generation that\napplies to any sequence prediction algorithm based on convolutional operators.\nOur approach reduces the generation time requirement from quadratic to\nquasilinear relative to the context length. Additionally, FutureFill requires a\nprefill cache sized only by the number of tokens generated, which is smaller\nthan the cache requirements for standard convolutional and attention-based\nmodels. We validate our theoretical findings with experimental evidence\ndemonstrating correctness and efficiency gains in a synthetic generation task."
                },
                "authors": [
                    {
                        "name": "Naman Agarwal"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Evan Dogariu"
                    },
                    {
                        "name": "Vlad Feinberg"
                    },
                    {
                        "name": "Daniel Suo"
                    },
                    {
                        "name": "Peter Bartlett"
                    },
                    {
                        "name": "Elad Hazan"
                    }
                ],
                "author_detail": {
                    "name": "Elad Hazan"
                },
                "author": "Elad Hazan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03766v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03766v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19937v1",
                "updated": "2024-10-25T19:18:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    18,
                    22,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T19:18:22Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    18,
                    22,
                    4,
                    299,
                    0
                ],
                "title": "RobustKV: Defending Large Language Models against Jailbreak Attacks via\n  KV Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RobustKV: Defending Large Language Models against Jailbreak Attacks via\n  KV Eviction"
                },
                "summary": "Jailbreak attacks circumvent LLMs' built-in safeguards by concealing harmful\nqueries within jailbreak prompts. While existing defenses primarily focus on\nmitigating the effects of jailbreak prompts, they often prove inadequate as\njailbreak prompts can take arbitrary, adaptive forms. This paper presents\nRobustKV, a novel defense that adopts a fundamentally different approach by\nselectively removing critical tokens of harmful queries from key-value (KV)\ncaches. Intuitively, for a jailbreak prompt to be effective, its tokens must\nachieve sufficient `importance' (as measured by attention scores), which\ninevitably lowers the importance of tokens in the concealed harmful query.\nThus, by strategically evicting the KVs of the lowest-ranked tokens, RobustKV\ndiminishes the presence of the harmful query in the KV cache, thus preventing\nthe LLM from generating malicious responses. Extensive evaluation using\nbenchmark datasets and models demonstrates that RobustKV effectively counters\nstate-of-the-art jailbreak attacks while maintaining the LLM's general\nperformance on benign queries. Moreover, RobustKV creates an intriguing\nevasiveness dilemma for adversaries, forcing them to balance between evading\nRobustKV and bypassing the LLM's built-in safeguards. This trade-off\ncontributes to RobustKV's robustness against adaptive attacks. (warning: this\npaper contains potentially harmful content generated by LLMs.)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreak attacks circumvent LLMs' built-in safeguards by concealing harmful\nqueries within jailbreak prompts. While existing defenses primarily focus on\nmitigating the effects of jailbreak prompts, they often prove inadequate as\njailbreak prompts can take arbitrary, adaptive forms. This paper presents\nRobustKV, a novel defense that adopts a fundamentally different approach by\nselectively removing critical tokens of harmful queries from key-value (KV)\ncaches. Intuitively, for a jailbreak prompt to be effective, its tokens must\nachieve sufficient `importance' (as measured by attention scores), which\ninevitably lowers the importance of tokens in the concealed harmful query.\nThus, by strategically evicting the KVs of the lowest-ranked tokens, RobustKV\ndiminishes the presence of the harmful query in the KV cache, thus preventing\nthe LLM from generating malicious responses. Extensive evaluation using\nbenchmark datasets and models demonstrates that RobustKV effectively counters\nstate-of-the-art jailbreak attacks while maintaining the LLM's general\nperformance on benign queries. Moreover, RobustKV creates an intriguing\nevasiveness dilemma for adversaries, forcing them to balance between evading\nRobustKV and bypassing the LLM's built-in safeguards. This trade-off\ncontributes to RobustKV's robustness against adaptive attacks. (warning: this\npaper contains potentially harmful content generated by LLMs.)"
                },
                "authors": [
                    {
                        "name": "Tanqiu Jiang"
                    },
                    {
                        "name": "Zian Wang"
                    },
                    {
                        "name": "Jiacheng Liang"
                    },
                    {
                        "name": "Changjiang Li"
                    },
                    {
                        "name": "Yuhui Wang"
                    },
                    {
                        "name": "Ting Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ting Wang"
                },
                "author": "Ting Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18248v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18248v2",
                "updated": "2024-10-25T19:18:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    18,
                    0,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-23T19:53:30Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    19,
                    53,
                    30,
                    2,
                    297,
                    0
                ],
                "title": "Fast Inference for Augmented Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Inference for Augmented Large Language Models"
                },
                "summary": "Augmented Large Language Models (LLMs) enhance the capabilities of standalone\nLLMs by integrating external data sources through API calls. In interactive LLM\napplications, efficient scheduling is crucial for maintaining low request\ncompletion times, directly impacting user engagement. However, these\naugmentations introduce scheduling challenges due to the need to manage limited\nmemory for cached information (KV caches). As a result, traditional size-based\nscheduling algorithms, such as Shortest Job First (SJF), become less effective\nat minimizing completion times. Existing work focuses only on handling requests\nduring API calls by preserving, discarding, or swapping memory without\nconsidering how to schedule requests with API calls. In this paper, we propose\nLAMPS, a novel LLM inference framework for augmented LLMs. LAMPS minimizes\nrequest completion time through a unified scheduling approach that considers\nthe total length of requests and their handling strategies during API calls.\nRecognizing that LLM inference is memory-bound, our approach ranks requests\nbased on their consumption of memory over time, which depends on both the\noutput sizes and how a request is managed during its API calls. To implement\nour scheduling, LAMPS predicts the strategy that minimizes memory waste of a\nrequest during its API calls, aligning with but improving upon existing\napproaches. We also propose starvation prevention techniques and optimizations\nto mitigate the overhead of our scheduling. We implement LAMPS on top of vLLM\nand evaluate its performance against baseline LLM inference systems,\ndemonstrating improvements in end-to-end latency by 27%-85% and reductions in\nTTFT by 4%-96% compared to the existing augmented-LLM system, with even greater\ngains over vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmented Large Language Models (LLMs) enhance the capabilities of standalone\nLLMs by integrating external data sources through API calls. In interactive LLM\napplications, efficient scheduling is crucial for maintaining low request\ncompletion times, directly impacting user engagement. However, these\naugmentations introduce scheduling challenges due to the need to manage limited\nmemory for cached information (KV caches). As a result, traditional size-based\nscheduling algorithms, such as Shortest Job First (SJF), become less effective\nat minimizing completion times. Existing work focuses only on handling requests\nduring API calls by preserving, discarding, or swapping memory without\nconsidering how to schedule requests with API calls. In this paper, we propose\nLAMPS, a novel LLM inference framework for augmented LLMs. LAMPS minimizes\nrequest completion time through a unified scheduling approach that considers\nthe total length of requests and their handling strategies during API calls.\nRecognizing that LLM inference is memory-bound, our approach ranks requests\nbased on their consumption of memory over time, which depends on both the\noutput sizes and how a request is managed during its API calls. To implement\nour scheduling, LAMPS predicts the strategy that minimizes memory waste of a\nrequest during its API calls, aligning with but improving upon existing\napproaches. We also propose starvation prevention techniques and optimizations\nto mitigate the overhead of our scheduling. We implement LAMPS on top of vLLM\nand evaluate its performance against baseline LLM inference systems,\ndemonstrating improvements in end-to-end latency by 27%-85% and reductions in\nTTFT by 4%-96% compared to the existing augmented-LLM system, with even greater\ngains over vLLM."
                },
                "authors": [
                    {
                        "name": "Rana Shahout"
                    },
                    {
                        "name": "Cong Liang"
                    },
                    {
                        "name": "Shiji Xin"
                    },
                    {
                        "name": "Qianru Lao"
                    },
                    {
                        "name": "Yong Cui"
                    },
                    {
                        "name": "Minlan Yu"
                    },
                    {
                        "name": "Michael Mitzenmacher"
                    }
                ],
                "author_detail": {
                    "name": "Michael Mitzenmacher"
                },
                "author": "Michael Mitzenmacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18248v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18248v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.18079v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.18079v5",
                "updated": "2024-10-25T18:29:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    18,
                    29,
                    43,
                    4,
                    299,
                    0
                ],
                "published": "2024-01-31T18:58:14Z",
                "published_parsed": [
                    2024,
                    1,
                    31,
                    18,
                    58,
                    14,
                    2,
                    31,
                    0
                ],
                "title": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache\n  Quantization"
                },
                "summary": "LLMs are seeing growing use for applications which require large context\nwindows, and with these large context windows KV cache activations surface as\nthe dominant contributor to memory consumption during inference. Quantization\nis a promising approach for compressing KV cache activations; however, existing\nsolutions fail to represent activations accurately in sub-4-bit precision. Our\nwork, KVQuant, facilitates low precision KV cache quantization by incorporating\nseveral novel methods: (i) Per-Channel Key Quantization, where we adjust the\ndimension along which we quantize the Key activations to better match the\ndistribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations\nbefore the rotary positional embedding to mitigate its impact on quantization;\n(iii) Non-Uniform KV Cache Quantization, where we derive per-layer\nsensitivity-weighted non-uniform datatypes that better represent the\ndistributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we\nisolate outliers separately for each vector to minimize skews in quantization\nranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral\nmodels, we achieve < 0.1 perplexity degradation with 3-bit quantization on both\nWikitext-2 and C4, outperforming existing approaches. Our method enables\nserving LLaMA-7B with a context length of up to 1 million on a single A100-80GB\nGPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for\nKVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline\nfp16 matrix-vector multiplications, for the LLaMA-7B model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are seeing growing use for applications which require large context\nwindows, and with these large context windows KV cache activations surface as\nthe dominant contributor to memory consumption during inference. Quantization\nis a promising approach for compressing KV cache activations; however, existing\nsolutions fail to represent activations accurately in sub-4-bit precision. Our\nwork, KVQuant, facilitates low precision KV cache quantization by incorporating\nseveral novel methods: (i) Per-Channel Key Quantization, where we adjust the\ndimension along which we quantize the Key activations to better match the\ndistribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations\nbefore the rotary positional embedding to mitigate its impact on quantization;\n(iii) Non-Uniform KV Cache Quantization, where we derive per-layer\nsensitivity-weighted non-uniform datatypes that better represent the\ndistributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we\nisolate outliers separately for each vector to minimize skews in quantization\nranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral\nmodels, we achieve < 0.1 perplexity degradation with 3-bit quantization on both\nWikitext-2 and C4, outperforming existing approaches. Our method enables\nserving LLaMA-7B with a context length of up to 1 million on a single A100-80GB\nGPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for\nKVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline\nfp16 matrix-vector multiplications, for the LLaMA-7B model."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Yakun Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.18079v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.18079v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19355v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19355v1",
                "updated": "2024-10-25T07:24:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    24,
                    38,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T07:24:38Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    24,
                    38,
                    4,
                    299,
                    0
                ],
                "title": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality"
                },
                "summary": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality."
                },
                "authors": [
                    {
                        "name": "Zhengyao Lv"
                    },
                    {
                        "name": "Chenyang Si"
                    },
                    {
                        "name": "Junhao Song"
                    },
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Kwan-Yee K. Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Yee K. Wong"
                },
                "author": "Kwan-Yee K. Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19355v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19355v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19123v1",
                "updated": "2024-10-24T19:48:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    19,
                    48,
                    51,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T19:48:51Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    19,
                    48,
                    51,
                    3,
                    298,
                    0
                ],
                "title": "Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with\n  System Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with\n  System Co-Design"
                },
                "summary": "The proliferation of large language models (LLMs) has led to the adoption of\nMixture-of-Experts (MoE) architectures that dynamically leverage specialized\nsubnetworks for improved efficiency and performance. Despite their benefits,\nMoE models face significant challenges during inference, including inefficient\nmemory management and suboptimal batching, due to misaligned design choices\nbetween the model architecture and the system policies. Furthermore, the\nconventional approach of training MoEs from scratch is increasingly prohibitive\nin terms of cost. In this paper, we propose a novel framework Read-ME that\ntransforms pre-trained dense LLMs into smaller MoE models (in contrast to\n\"upcycling\" generalist MoEs), avoiding the high costs of ground-up training.\nOur approach employs activation sparsity to extract experts. To compose\nexperts, we examine the widely-adopted layer-wise router design and show its\nredundancy, and thus we introduce the pre-gating router decoupled from the MoE\nbackbone that facilitates system-friendly pre-computing and lookahead\nscheduling, enhancing expert-aware batching and caching. Our codesign therefore\naddresses critical gaps on both the algorithmic and system fronts, establishing\na scalable and efficient alternative for LLM inference in resource-constrained\nsettings. Read-ME outperforms other popular open-source dense models of similar\nscales, achieving improvements of up to 10.1% on MMLU, and improving mean\nend-to-end latency up to 6.1%. Codes are available at:\nhttps://github.com/VITA-Group/READ-ME.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of large language models (LLMs) has led to the adoption of\nMixture-of-Experts (MoE) architectures that dynamically leverage specialized\nsubnetworks for improved efficiency and performance. Despite their benefits,\nMoE models face significant challenges during inference, including inefficient\nmemory management and suboptimal batching, due to misaligned design choices\nbetween the model architecture and the system policies. Furthermore, the\nconventional approach of training MoEs from scratch is increasingly prohibitive\nin terms of cost. In this paper, we propose a novel framework Read-ME that\ntransforms pre-trained dense LLMs into smaller MoE models (in contrast to\n\"upcycling\" generalist MoEs), avoiding the high costs of ground-up training.\nOur approach employs activation sparsity to extract experts. To compose\nexperts, we examine the widely-adopted layer-wise router design and show its\nredundancy, and thus we introduce the pre-gating router decoupled from the MoE\nbackbone that facilitates system-friendly pre-computing and lookahead\nscheduling, enhancing expert-aware batching and caching. Our codesign therefore\naddresses critical gaps on both the algorithmic and system fronts, establishing\na scalable and efficient alternative for LLM inference in resource-constrained\nsettings. Read-ME outperforms other popular open-source dense models of similar\nscales, achieving improvements of up to 10.1% on MMLU, and improving mean\nend-to-end latency up to 6.1%. Codes are available at:\nhttps://github.com/VITA-Group/READ-ME."
                },
                "authors": [
                    {
                        "name": "Ruisi Cai"
                    },
                    {
                        "name": "Yeonju Ro"
                    },
                    {
                        "name": "Geon-Woo Kim"
                    },
                    {
                        "name": "Peihao Wang"
                    },
                    {
                        "name": "Babak Ehteshami Bejnordi"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Zhangyang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhangyang Wang"
                },
                "author": "Zhangyang Wang",
                "arxiv_comment": "38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18517v1",
                "updated": "2024-10-24T08:06:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    8,
                    6,
                    41,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T08:06:41Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    8,
                    6,
                    41,
                    3,
                    298,
                    0
                ],
                "title": "KVSharer: Efficient Inference via Layer-Wise Dissimilar KV Cache Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVSharer: Efficient Inference via Layer-Wise Dissimilar KV Cache Sharing"
                },
                "summary": "The development of large language models (LLMs) has significantly expanded\nmodel sizes, resulting in substantial GPU memory requirements during inference.\nThe key and value storage of the attention map in the KV (key-value) cache\naccounts for more than 80\\% of this memory consumption. Nowadays, most existing\nKV cache compression methods focus on intra-layer compression within a single\nTransformer layer but few works consider layer-wise compression. In this paper,\nwe propose a plug-and-play method called \\textit{KVSharer}, which shares the KV\ncache between layers to achieve layer-wise compression. Rather than intuitively\nsharing based on higher similarity, we discover a counterintuitive phenomenon:\nsharing dissimilar KV caches better preserves the model performance.\nExperiments show that \\textit{KVSharer} can reduce KV cache computation by\n30\\%, thereby lowering memory consumption without significantly impacting model\nperformance and it can also achieve at least 1.3 times generation acceleration.\nAdditionally, we verify that \\textit{KVSharer} is compatible with existing\nintra-layer KV cache compression methods, and combining both can further save\nmemory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of large language models (LLMs) has significantly expanded\nmodel sizes, resulting in substantial GPU memory requirements during inference.\nThe key and value storage of the attention map in the KV (key-value) cache\naccounts for more than 80\\% of this memory consumption. Nowadays, most existing\nKV cache compression methods focus on intra-layer compression within a single\nTransformer layer but few works consider layer-wise compression. In this paper,\nwe propose a plug-and-play method called \\textit{KVSharer}, which shares the KV\ncache between layers to achieve layer-wise compression. Rather than intuitively\nsharing based on higher similarity, we discover a counterintuitive phenomenon:\nsharing dissimilar KV caches better preserves the model performance.\nExperiments show that \\textit{KVSharer} can reduce KV cache computation by\n30\\%, thereby lowering memory consumption without significantly impacting model\nperformance and it can also achieve at least 1.3 times generation acceleration.\nAdditionally, we verify that \\textit{KVSharer} is compatible with existing\nintra-layer KV cache compression methods, and combining both can further save\nmemory."
                },
                "authors": [
                    {
                        "name": "Yifei Yang"
                    },
                    {
                        "name": "Zouying Cao"
                    },
                    {
                        "name": "Qiguang Chen"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Dongjie Yang"
                    },
                    {
                        "name": "Hai Zhao"
                    },
                    {
                        "name": "Zhi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Chen"
                },
                "author": "Zhi Chen",
                "arxiv_comment": "Under Review by ICLR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18441v1",
                "updated": "2024-10-24T05:29:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    5,
                    29,
                    20,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T05:29:20Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    5,
                    29,
                    20,
                    3,
                    298,
                    0
                ],
                "title": "The Nature of Mathematical Modeling and Probabilistic Optimization\n  Engineering in Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Nature of Mathematical Modeling and Probabilistic Optimization\n  Engineering in Generative AI"
                },
                "summary": "In this paper, we give an in-depth analysis on the mathematical problem\nformulations and the probabilistic optimization explorations for some of the\nkey components in Transformer model [33] in the field of generative AI. We\nexplore and discuss some potential further enhancement for current state of the\nart methods for some key underlying technologies of generative AI models from\nalgorithmic and probabilistic optimization perspective. In particular, we\npresent an optimal solution for sub-word encoding (SWE) based on similar\ninitial settings as that of byte-pair encoding (BPE) algorithm in [9] with\nsimilar objectives as that of WordPiece approach in [28, 31] to maximize the\nlikelihood of the training data. We also present cross entropy optimization\nmethod to optimize hyperparameters for word2vec model [17]. In addition, we\npropose a factored combination of rotary positional encoding (RoPE) [32] and\nattention with linear biases (ALiBi) [23] with a harmonic series. We also\npresent a probabilistic FlashAttention [6, 7] (PrFlashAttention) method with a\nprobability distribution over block distances in the matrix to decide which\nblock is likely to participate in a given round of attention computation while\nmaintaining the lower triangle shape of the tensor for autoregressive language\nmodels by re-shaping the tensors. Finally, we present staircase adaptive\nquantization (SAQ) of key-value (KV) cache for multi-query attention (MQA)\nbased on the framework presented in [16] to have gradual quantization\ndegradation while achieving reasonable model quality and cost savings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we give an in-depth analysis on the mathematical problem\nformulations and the probabilistic optimization explorations for some of the\nkey components in Transformer model [33] in the field of generative AI. We\nexplore and discuss some potential further enhancement for current state of the\nart methods for some key underlying technologies of generative AI models from\nalgorithmic and probabilistic optimization perspective. In particular, we\npresent an optimal solution for sub-word encoding (SWE) based on similar\ninitial settings as that of byte-pair encoding (BPE) algorithm in [9] with\nsimilar objectives as that of WordPiece approach in [28, 31] to maximize the\nlikelihood of the training data. We also present cross entropy optimization\nmethod to optimize hyperparameters for word2vec model [17]. In addition, we\npropose a factored combination of rotary positional encoding (RoPE) [32] and\nattention with linear biases (ALiBi) [23] with a harmonic series. We also\npresent a probabilistic FlashAttention [6, 7] (PrFlashAttention) method with a\nprobability distribution over block distances in the matrix to decide which\nblock is likely to participate in a given round of attention computation while\nmaintaining the lower triangle shape of the tensor for autoregressive language\nmodels by re-shaping the tensors. Finally, we present staircase adaptive\nquantization (SAQ) of key-value (KV) cache for multi-query attention (MQA)\nbased on the framework presented in [16] to have gradual quantization\ndegradation while achieving reasonable model quality and cost savings."
                },
                "authors": [
                    {
                        "name": "Fulu Li"
                    }
                ],
                "author_detail": {
                    "name": "Fulu Li"
                },
                "author": "Fulu Li",
                "arxiv_comment": "19 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18002v1",
                "updated": "2024-10-23T16:25:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    22,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T16:25:22Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    22,
                    2,
                    297,
                    0
                ],
                "title": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges"
                },
                "summary": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks."
                },
                "authors": [
                    {
                        "name": "Yuchen Liu"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Zifan Zhang"
                    },
                    {
                        "name": "Hanzhi Yu"
                    },
                    {
                        "name": "Mingzhe Chen"
                    }
                ],
                "author_detail": {
                    "name": "Mingzhe Chen"
                },
                "author": "Mingzhe Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08437v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08437v2",
                "updated": "2024-10-23T15:44:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    44,
                    9,
                    2,
                    297,
                    0
                ],
                "published": "2023-10-12T16:01:46Z",
                "published_parsed": [
                    2023,
                    10,
                    12,
                    16,
                    1,
                    46,
                    3,
                    285,
                    0
                ],
                "title": "Cold Start Latency in Serverless Computing: A Systematic Review,\n  Taxonomy, and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cold Start Latency in Serverless Computing: A Systematic Review,\n  Taxonomy, and Future Directions"
                },
                "summary": "Recently, academics and the corporate sector have paid attention to\nserverless computing, which enables dynamic scalability and an economic model.\nIn serverless computing, users only pay for the time they actually use\nresources, enabling zero scaling to optimise cost and resource utilisation.\nHowever, this approach also introduces the serverless cold start problem.\nResearchers have developed various solutions to address the cold start problem,\nyet it remains an unresolved research area. In this article, we propose a\nsystematic literature review on clod start latency in serverless computing.\nFurthermore, we create a detailed taxonomy of approaches to cold start latency,\nwhich we use to investigate existing techniques for reducing the cold start\ntime and frequency. We have classified the current studies on cold start\nlatency into several categories such as caching and application-level\noptimisation-based solutions, as well as Artificial Intelligence (AI)/Machine\nLearning (ML)-based solutions. Moreover, we have analyzed the impact of cold\nstart latency on quality of service, explored current cold start latency\nmitigation methods, datasets, and implementation platforms, and classified them\ninto categories based on their common characteristics and features. Finally, we\noutline the open challenges and highlight the possible future directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, academics and the corporate sector have paid attention to\nserverless computing, which enables dynamic scalability and an economic model.\nIn serverless computing, users only pay for the time they actually use\nresources, enabling zero scaling to optimise cost and resource utilisation.\nHowever, this approach also introduces the serverless cold start problem.\nResearchers have developed various solutions to address the cold start problem,\nyet it remains an unresolved research area. In this article, we propose a\nsystematic literature review on clod start latency in serverless computing.\nFurthermore, we create a detailed taxonomy of approaches to cold start latency,\nwhich we use to investigate existing techniques for reducing the cold start\ntime and frequency. We have classified the current studies on cold start\nlatency into several categories such as caching and application-level\noptimisation-based solutions, as well as Artificial Intelligence (AI)/Machine\nLearning (ML)-based solutions. Moreover, we have analyzed the impact of cold\nstart latency on quality of service, explored current cold start latency\nmitigation methods, datasets, and implementation platforms, and classified them\ninto categories based on their common characteristics and features. Finally, we\noutline the open challenges and highlight the possible future directions."
                },
                "authors": [
                    {
                        "name": "Muhammed Golec"
                    },
                    {
                        "name": "Guneet Kaur Walia"
                    },
                    {
                        "name": "Mohit Kumar"
                    },
                    {
                        "name": "Felix Cuadrado"
                    },
                    {
                        "name": "Sukhpal Singh Gill"
                    },
                    {
                        "name": "Steve Uhlig"
                    }
                ],
                "author_detail": {
                    "name": "Steve Uhlig"
                },
                "author": "Steve Uhlig",
                "arxiv_doi": "10.1145/3700875",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3700875",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.08437v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08437v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Preprint Version Accepted for Publication in ACM Computing Survey,\n  2024",
                "arxiv_journal_ref": "ACM Computing Surveys 2024",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17954v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17954v1",
                "updated": "2024-10-23T15:24:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    24,
                    54,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T15:24:54Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    24,
                    54,
                    2,
                    297,
                    0
                ],
                "title": "ExpertFlow: Optimized Expert Activation and Token Allocation for\n  Efficient Mixture-of-Experts Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpertFlow: Optimized Expert Activation and Token Allocation for\n  Efficient Mixture-of-Experts Inference"
                },
                "summary": "Sparse Mixture of Experts (MoE) models, while outperforming dense Large\nLanguage Models (LLMs) in terms of performance, face significant deployment\nchallenges during inference due to their high memory demands. Existing\noffloading techniques, which involve swapping activated and idle experts\nbetween the GPU and CPU, often suffer from rigid expert caching mechanisms.\nThese mechanisms fail to adapt to dynamic routing, leading to inefficient cache\nutilization, or incur prohibitive costs for prediction training. To tackle\nthese inference-specific challenges, we introduce ExpertFlow, a comprehensive\nsystem specifically designed to enhance inference efficiency by accommodating\nflexible routing and enabling efficient expert scheduling between CPU and GPU.\nThis reduces overhead and boosts system performance. Central to our approach is\na predictive routing path-based offloading mechanism that utilizes a\nlightweight predictor to accurately forecast routing paths before computation\nbegins. This proactive strategy allows for real-time error correction in expert\ncaching, significantly increasing cache hit ratios and reducing the frequency\nof expert transfers, thereby minimizing I/O overhead. Additionally, we\nimplement a dynamic token scheduling strategy that optimizes MoE inference by\nrearranging input tokens across different batches. This method not only reduces\nthe number of activated experts per batch but also improves computational\nefficiency. Our extensive experiments demonstrate that ExpertFlow achieves up\nto 93.72\\% GPU memory savings and enhances inference speed by 2 to 10 times\ncompared to baseline methods, highlighting its effectiveness and utility as a\nrobust solution for resource-constrained inference scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Mixture of Experts (MoE) models, while outperforming dense Large\nLanguage Models (LLMs) in terms of performance, face significant deployment\nchallenges during inference due to their high memory demands. Existing\noffloading techniques, which involve swapping activated and idle experts\nbetween the GPU and CPU, often suffer from rigid expert caching mechanisms.\nThese mechanisms fail to adapt to dynamic routing, leading to inefficient cache\nutilization, or incur prohibitive costs for prediction training. To tackle\nthese inference-specific challenges, we introduce ExpertFlow, a comprehensive\nsystem specifically designed to enhance inference efficiency by accommodating\nflexible routing and enabling efficient expert scheduling between CPU and GPU.\nThis reduces overhead and boosts system performance. Central to our approach is\na predictive routing path-based offloading mechanism that utilizes a\nlightweight predictor to accurately forecast routing paths before computation\nbegins. This proactive strategy allows for real-time error correction in expert\ncaching, significantly increasing cache hit ratios and reducing the frequency\nof expert transfers, thereby minimizing I/O overhead. Additionally, we\nimplement a dynamic token scheduling strategy that optimizes MoE inference by\nrearranging input tokens across different batches. This method not only reduces\nthe number of activated experts per batch but also improves computational\nefficiency. Our extensive experiments demonstrate that ExpertFlow achieves up\nto 93.72\\% GPU memory savings and enhances inference speed by 2 to 10 times\ncompared to baseline methods, highlighting its effectiveness and utility as a\nrobust solution for resource-constrained inference scenarios."
                },
                "authors": [
                    {
                        "name": "Xin He"
                    },
                    {
                        "name": "Shunkang Zhang"
                    },
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Haiyan Yin"
                    },
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Shaohuai Shi"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Ivor Tsang"
                    },
                    {
                        "name": "Ong Yew Soon"
                    }
                ],
                "author_detail": {
                    "name": "Ong Yew Soon"
                },
                "author": "Ong Yew Soon",
                "arxiv_comment": "Mixture-of-Experts, Inference, Offloading",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17954v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v1",
                "updated": "2024-10-23T14:15:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers"
                },
                "summary": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the KV cache by nearly 50%. Comprehensive empirical\nevidence demonstrates that ResFormer mitigates attention concentration problem\nin deeper layers and enhances representation across most layers, outperforming\nthe vanilla Transformer, DenseFormer, and NeuTRENO in training error as well as\ndownstream tasks. SVFormer trains significantly faster than the vanilla\nTransformer and performs better than other methods like GQA and CLA, with\nperformance influenced by sequence length and cumulative learning rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the KV cache by nearly 50%. Comprehensive empirical\nevidence demonstrates that ResFormer mitigates attention concentration problem\nin deeper layers and enhances representation across most layers, outperforming\nthe vanilla Transformer, DenseFormer, and NeuTRENO in training error as well as\ndownstream tasks. SVFormer trains significantly faster than the vanilla\nTransformer and performs better than other methods like GQA and CLA, with\nperformance influenced by sequence length and cumulative learning rate."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05118v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05118v3",
                "updated": "2024-10-23T10:39:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    10,
                    39,
                    15,
                    2,
                    297,
                    0
                ],
                "published": "2024-05-08T15:16:02Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    15,
                    16,
                    2,
                    2,
                    129,
                    0
                ],
                "title": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms"
                },
                "summary": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning."
                },
                "authors": [
                    {
                        "name": "Ari Rasch"
                    }
                ],
                "author_detail": {
                    "name": "Ari Rasch"
                },
                "author": "Ari Rasch",
                "arxiv_doi": "10.1145/3665643",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3665643",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.05118v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05118v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "A short version of this paper is published at ACM TOPLAS and\n  presented at PLDI'24",
                "arxiv_journal_ref": "ACM Trans. Program. Lang. Syst. (May 2024)",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17635v1",
                "updated": "2024-10-23T07:53:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    53,
                    29,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T07:53:29Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    53,
                    29,
                    2,
                    297,
                    0
                ],
                "title": "Markov Chain of Thought for Efficient Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Markov Chain of Thought for Efficient Mathematical Reasoning"
                },
                "summary": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, ``derive, then reduce'', we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the \\texttt{MCoTInstruct} dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, ``derive, then reduce'', we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the \\texttt{MCoTInstruct} dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Wen Yang"
                    },
                    {
                        "name": "Kai Fan"
                    },
                    {
                        "name": "Minpeng Liao"
                    }
                ],
                "author_detail": {
                    "name": "Minpeng Liao"
                },
                "author": "Minpeng Liao",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v5",
                "updated": "2024-10-23T05:55:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    5,
                    55,
                    31,
                    2,
                    297,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14740v2",
                "updated": "2024-10-23T01:08:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    1,
                    8,
                    59,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-17T08:33:39Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    8,
                    33,
                    39,
                    3,
                    291,
                    0
                ],
                "title": "Harnessing Your DRAM and SSD for Sustainable and Accessible LLM\n  Inference with Mixed-Precision and Multi-level Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Your DRAM and SSD for Sustainable and Accessible LLM\n  Inference with Mixed-Precision and Multi-level Caching"
                },
                "summary": "Although Large Language Models (LLMs) have demonstrated remarkable\ncapabilities, their massive parameter counts and associated extensive computing\nmake LLMs' deployment the main part of carbon emission from nowadays AI\napplications. Compared to modern GPUs like H$100$, it would be significantly\ncarbon-sustainable if we could leverage old-fashioned GPUs such as M$40$ (as\nshown in Figure 1, M$40$ only has one third carbon emission of H$100$'s) for\nLLM servings. However, the limited High Bandwidth Memory (HBM) available on\nsuch GPU often cannot support the loading of LLMs due to the gigantic model\nsize and intermediate activation data, making their serving challenging. For\ninstance, a LLaMA2 model with $70$B parameters typically requires $128$GB for\ninference, which substantially surpasses $24$GB HBM in a $3090$ GPU and remains\ninfeasible even considering the additional $64$GB DRAM. To address this\nchallenge, this paper proposes a mixed-precision with a model modularization\nalgorithm to enable LLM inference on outdated hardware with resource\nconstraints. (The precision denotes the numerical precision like FP16, INT8,\nINT4) and multi-level caching (M2Cache).)\n  Specifically, our M2Cache first modulizes neurons in LLM and creates their\nimportance ranking. Then, it adopts a dynamic sparse mixed-precision\nquantization mechanism in weight space to reduce computational demands and\ncommunication overhead at each decoding step. It collectively lowers the\noperational carbon emissions associated with LLM inference. Moreover, M2Cache\nintroduces a three-level cache management system with HBM, DRAM, and SSDs that\ncomplements the dynamic sparse mixed-precision inference. To enhance\ncommunication efficiency, M2Cache maintains a neuron-level mixed-precision LRU\ncache in HBM, a larger layer-aware cache in DRAM, and a full model in SSD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Large Language Models (LLMs) have demonstrated remarkable\ncapabilities, their massive parameter counts and associated extensive computing\nmake LLMs' deployment the main part of carbon emission from nowadays AI\napplications. Compared to modern GPUs like H$100$, it would be significantly\ncarbon-sustainable if we could leverage old-fashioned GPUs such as M$40$ (as\nshown in Figure 1, M$40$ only has one third carbon emission of H$100$'s) for\nLLM servings. However, the limited High Bandwidth Memory (HBM) available on\nsuch GPU often cannot support the loading of LLMs due to the gigantic model\nsize and intermediate activation data, making their serving challenging. For\ninstance, a LLaMA2 model with $70$B parameters typically requires $128$GB for\ninference, which substantially surpasses $24$GB HBM in a $3090$ GPU and remains\ninfeasible even considering the additional $64$GB DRAM. To address this\nchallenge, this paper proposes a mixed-precision with a model modularization\nalgorithm to enable LLM inference on outdated hardware with resource\nconstraints. (The precision denotes the numerical precision like FP16, INT8,\nINT4) and multi-level caching (M2Cache).)\n  Specifically, our M2Cache first modulizes neurons in LLM and creates their\nimportance ranking. Then, it adopts a dynamic sparse mixed-precision\nquantization mechanism in weight space to reduce computational demands and\ncommunication overhead at each decoding step. It collectively lowers the\noperational carbon emissions associated with LLM inference. Moreover, M2Cache\nintroduces a three-level cache management system with HBM, DRAM, and SSDs that\ncomplements the dynamic sparse mixed-precision inference. To enhance\ncommunication efficiency, M2Cache maintains a neuron-level mixed-precision LRU\ncache in HBM, a larger layer-aware cache in DRAM, and a full model in SSD."
                },
                "authors": [
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Zhang Cao"
                    },
                    {
                        "name": "Huaizhi Qu"
                    },
                    {
                        "name": "Zhengyu Zhang"
                    },
                    {
                        "name": "Chang Guo"
                    },
                    {
                        "name": "Yanyong Zhang"
                    },
                    {
                        "name": "Zhichao Cao"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "24 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.11724v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.11724v2",
                "updated": "2024-10-22T19:07:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    22,
                    19,
                    7,
                    8,
                    1,
                    296,
                    0
                ],
                "published": "2024-05-20T01:57:34Z",
                "published_parsed": [
                    2024,
                    5,
                    20,
                    1,
                    57,
                    34,
                    0,
                    141,
                    0
                ],
                "title": "Token-wise Influential Training Data Retrieval for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-wise Influential Training Data Retrieval for Large Language Models"
                },
                "summary": "Given a Large Language Model (LLM) generation, how can we identify which\ntraining data led to this generation? In this paper, we proposed RapidIn, a\nscalable framework adapting to LLMs for estimating the influence of each\ntraining data. The proposed framework consists of two stages: caching and\nretrieval. First, we compress the gradient vectors by over 200,000x, allowing\nthem to be cached on disk or in GPU/CPU memory. Then, given a generation,\nRapidIn efficiently traverses the cached gradients to estimate the influence\nwithin minutes, achieving over a 6,326x speedup. Moreover, RapidIn supports\nmulti-GPU parallelization to substantially accelerate caching and retrieval.\nOur empirical result confirms the efficiency and effectiveness of RapidIn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given a Large Language Model (LLM) generation, how can we identify which\ntraining data led to this generation? In this paper, we proposed RapidIn, a\nscalable framework adapting to LLMs for estimating the influence of each\ntraining data. The proposed framework consists of two stages: caching and\nretrieval. First, we compress the gradient vectors by over 200,000x, allowing\nthem to be cached on disk or in GPU/CPU memory. Then, given a generation,\nRapidIn efficiently traverses the cached gradients to estimate the influence\nwithin minutes, achieving over a 6,326x speedup. Moreover, RapidIn supports\nmulti-GPU parallelization to substantially accelerate caching and retrieval.\nOur empirical result confirms the efficiency and effectiveness of RapidIn."
                },
                "authors": [
                    {
                        "name": "Huawei Lin"
                    },
                    {
                        "name": "Jikai Long"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Weijie Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Weijie Zhao"
                },
                "author": "Weijie Zhao",
                "arxiv_comment": "Accepted to ACL 2024. Keywords: Influence Function, Influence\n  Estimation, Training Data Attribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.11724v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.11724v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16218v1",
                "updated": "2024-10-21T17:23:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    17,
                    23,
                    3,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-21T17:23:03Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    17,
                    23,
                    3,
                    0,
                    295,
                    0
                ],
                "title": "3 kV Monolithic Bidirectional GaN HEMT on Sapphire",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3 kV Monolithic Bidirectional GaN HEMT on Sapphire"
                },
                "summary": "More than 3 kV breakdown voltage was demonstrated in monolithic bidirectional\nGaN HEMTs for the first time having potential applications in 1200V or\n1700V-class novel power converters. The on resistance of the fabricated\ntransistors was ~20 ohm.mm or ~11 mili ohm.cm^2. Breakdown voltage was\noptimized by utilizing two field plates in either side of the transistor and\noptimizing their geometry. Shorter first field plate lengths (less than 2\nmicron) resulted in higher breakdown voltage and the possible reason for this\nwas discussed. The transistors had a steep subthreshold swing of 92 mV / dec.\nThe on/off ratio was greater than 10^5 and it was limited by the tool capacity.\nThe fabricated 3 kV transistor was benchmarked against the state-of-the-art\nmonolithic bidirectional GaN HEMTs in the performance matrices of breakdown\nvoltage and on resistance, that showed crucial progress.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More than 3 kV breakdown voltage was demonstrated in monolithic bidirectional\nGaN HEMTs for the first time having potential applications in 1200V or\n1700V-class novel power converters. The on resistance of the fabricated\ntransistors was ~20 ohm.mm or ~11 mili ohm.cm^2. Breakdown voltage was\noptimized by utilizing two field plates in either side of the transistor and\noptimizing their geometry. Shorter first field plate lengths (less than 2\nmicron) resulted in higher breakdown voltage and the possible reason for this\nwas discussed. The transistors had a steep subthreshold swing of 92 mV / dec.\nThe on/off ratio was greater than 10^5 and it was limited by the tool capacity.\nThe fabricated 3 kV transistor was benchmarked against the state-of-the-art\nmonolithic bidirectional GaN HEMTs in the performance matrices of breakdown\nvoltage and on resistance, that showed crucial progress."
                },
                "authors": [
                    {
                        "name": "Md Tahmidul Alam"
                    },
                    {
                        "name": "Swarnav Mukhopadhyay"
                    },
                    {
                        "name": "Md Mobinul Haque"
                    },
                    {
                        "name": "Shubhra S. Pasayat"
                    },
                    {
                        "name": "Chirag Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Chirag Gupta"
                },
                "author": "Chirag Gupta",
                "arxiv_comment": "4 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13761v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13761v2",
                "updated": "2024-10-21T15:59:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    15,
                    59,
                    18,
                    0,
                    295,
                    0
                ],
                "published": "2024-09-16T18:46:24Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    18,
                    46,
                    24,
                    0,
                    260,
                    0
                ],
                "title": "Do Large Language Models Need a Content Delivery Network?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Need a Content Delivery Network?"
                },
                "summary": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache."
                },
                "authors": [
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13761v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13761v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15908v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15908v1",
                "updated": "2024-10-21T11:29:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    11,
                    29,
                    49,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-21T11:29:49Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    11,
                    29,
                    49,
                    0,
                    295,
                    0
                ],
                "title": "Formalising CXL Cache Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formalising CXL Cache Coherence"
                },
                "summary": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs."
                },
                "authors": [
                    {
                        "name": "Chengsong Tan"
                    },
                    {
                        "name": "Alastair F. Donaldson"
                    },
                    {
                        "name": "John Wickerson"
                    }
                ],
                "author_detail": {
                    "name": "John Wickerson"
                },
                "author": "John Wickerson",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15908v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15908v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14142v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14142v2",
                "updated": "2024-10-21T07:24:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    7,
                    24,
                    53,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-18T03:30:25Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    3,
                    30,
                    25,
                    4,
                    292,
                    0
                ],
                "title": "Secure Collaborative Computation Offloading and Resource Allocation in\n  Cache-Assisted Ultra-Dense IoT Networks With Multi-Slope Channels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure Collaborative Computation Offloading and Resource Allocation in\n  Cache-Assisted Ultra-Dense IoT Networks With Multi-Slope Channels"
                },
                "summary": "Cache-assisted ultra-dense mobile edge computing (MEC) networks are a\npromising solution for meeting the increasing demands of numerous\nInternet-of-Things mobile devices (IMDs). To address the complex interferences\ncaused by small base stations (SBSs) deployed densely in such networks, this\npaper explores the combination of orthogonal frequency division multiple access\n(OFDMA), non-orthogonal multiple access (NOMA), and base station (BS)\nclustering. Additionally, security measures are introduced to protect IMDs'\ntasks offloaded to BSs from potential eavesdropping and malicious attacks. As\nfor such a network framework, a computation offloading scheme is proposed to\nminimize IMDs' energy consumption while considering constraints such as delay,\npower, computing resources, and security costs, optimizing channel selections,\ntask execution decisions, device associations, power controls, security service\nassignments, and computing resource allocations. To solve the formulated\nproblem efficiently, we develop a further improved hierarchical adaptive search\n(FIHAS) algorithm, giving some insights into its parallel implementation,\ncomputation complexity, and convergence. Simulation results demonstrate that\nthe proposed algorithms can achieve lower total energy consumption and delay\ncompared to other algorithms when strict latency and cost constraints are\nimposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-assisted ultra-dense mobile edge computing (MEC) networks are a\npromising solution for meeting the increasing demands of numerous\nInternet-of-Things mobile devices (IMDs). To address the complex interferences\ncaused by small base stations (SBSs) deployed densely in such networks, this\npaper explores the combination of orthogonal frequency division multiple access\n(OFDMA), non-orthogonal multiple access (NOMA), and base station (BS)\nclustering. Additionally, security measures are introduced to protect IMDs'\ntasks offloaded to BSs from potential eavesdropping and malicious attacks. As\nfor such a network framework, a computation offloading scheme is proposed to\nminimize IMDs' energy consumption while considering constraints such as delay,\npower, computing resources, and security costs, optimizing channel selections,\ntask execution decisions, device associations, power controls, security service\nassignments, and computing resource allocations. To solve the formulated\nproblem efficiently, we develop a further improved hierarchical adaptive search\n(FIHAS) algorithm, giving some insights into its parallel implementation,\ncomputation complexity, and convergence. Simulation results demonstrate that\nthe proposed algorithms can achieve lower total energy consumption and delay\ncompared to other algorithms when strict latency and cost constraints are\nimposed."
                },
                "authors": [
                    {
                        "name": "Tianqing Zhou"
                    },
                    {
                        "name": "Bobo Wang"
                    },
                    {
                        "name": "Dong Qin"
                    },
                    {
                        "name": "Xuefang Nie"
                    },
                    {
                        "name": "Nan Jiang"
                    },
                    {
                        "name": "Chunguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Chunguo Li"
                },
                "author": "Chunguo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14142v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14142v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15704v1",
                "updated": "2024-10-21T07:20:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    7,
                    20,
                    41,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-21T07:20:41Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    7,
                    20,
                    41,
                    0,
                    295,
                    0
                ],
                "title": "Residual vector quantization for KV cache compression in large language\n  model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Residual vector quantization for KV cache compression in large language\n  model"
                },
                "summary": "KV cache compression methods have mainly relied on scalar quantization\ntechniques to reduce the memory requirements during decoding. In this work, we\napply residual vector quantization, which has been widely used for high\nfidelity audio compression, to compress KV cache in large language models\n(LLM). We adapt the standard recipe with minimal changes to compress the output\nof any key or value projection matrix in a pretrained LLM: we scale the vector\nby its standard deviation, divide channels into groups and then quantize each\ngroup with the same residual vector quantizer. We learn the codebook using\nexponential moving average and there are no other learnable parameters\nincluding the input and output projections normally used in a vector\nquantization set up. We find that a residual depth of 8 recovers most of the\nperformance of the unquantized model. We also find that grouping non-contiguous\nchannels together works better than grouping contiguous channels for\ncompressing key matrix and the method further benefits from a light weight\nfinetuning of LLM together with the quantization. Overall, the proposed\ntechnique is competitive with existing quantization methods while being much\nsimpler and results in 5.5x compression compared to half precision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache compression methods have mainly relied on scalar quantization\ntechniques to reduce the memory requirements during decoding. In this work, we\napply residual vector quantization, which has been widely used for high\nfidelity audio compression, to compress KV cache in large language models\n(LLM). We adapt the standard recipe with minimal changes to compress the output\nof any key or value projection matrix in a pretrained LLM: we scale the vector\nby its standard deviation, divide channels into groups and then quantize each\ngroup with the same residual vector quantizer. We learn the codebook using\nexponential moving average and there are no other learnable parameters\nincluding the input and output projections normally used in a vector\nquantization set up. We find that a residual depth of 8 recovers most of the\nperformance of the unquantized model. We also find that grouping non-contiguous\nchannels together works better than grouping contiguous channels for\ncompressing key matrix and the method further benefits from a light weight\nfinetuning of LLM together with the quantization. Overall, the proposed\ntechnique is competitive with existing quantization methods while being much\nsimpler and results in 5.5x compression compared to half precision."
                },
                "authors": [
                    {
                        "name": "Ankur Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Ankur Kumar"
                },
                "author": "Ankur Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16546v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16546v2",
                "updated": "2024-10-21T05:06:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    5,
                    6,
                    1,
                    0,
                    295,
                    0
                ],
                "published": "2024-09-25T01:39:02Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    1,
                    39,
                    2,
                    2,
                    269,
                    0
                ],
                "title": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization"
                },
                "summary": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision."
                },
                "authors": [
                    {
                        "name": "Yifan Tan"
                    },
                    {
                        "name": "Haoze Wang"
                    },
                    {
                        "name": "Chao Yan"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16546v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16546v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09202v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09202v2",
                "updated": "2024-10-21T02:35:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    2,
                    35,
                    8,
                    0,
                    295,
                    0
                ],
                "published": "2024-09-13T21:31:45Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    21,
                    31,
                    45,
                    4,
                    257,
                    0
                ],
                "title": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions"
                },
                "summary": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. In those tests, WarmSwap\naccelerates dependency loading for serverless functions with large dependency\nrequirements by a factor ranging from 2.2 to 3.2. Simulation experiments using\nAzure traces indicate that WarmSwap can save 88\\% of optimization space when\nsharing a dependency image among ten different functions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. In those tests, WarmSwap\naccelerates dependency loading for serverless functions with large dependency\nrequirements by a factor ranging from 2.2 to 3.2. Simulation experiments using\nAzure traces indicate that WarmSwap can save 88\\% of optimization space when\nsharing a dependency image among ten different functions."
                },
                "authors": [
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Devesh Tiwari"
                    },
                    {
                        "name": "Gene Cooperman"
                    }
                ],
                "author_detail": {
                    "name": "Gene Cooperman"
                },
                "author": "Gene Cooperman",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09202v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09202v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04053v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04053v2",
                "updated": "2024-10-20T13:37:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    13,
                    37,
                    46,
                    6,
                    294,
                    0
                ],
                "published": "2024-07-04T16:51:17Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    16,
                    51,
                    17,
                    3,
                    186,
                    0
                ],
                "title": "Edge AI: A Taxonomy, Systematic Review and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge AI: A Taxonomy, Systematic Review and Future Directions"
                },
                "summary": "Edge Artificial Intelligence (AI) incorporates a network of interconnected\nsystems and devices that receive, cache, process, and analyze data in close\ncommunication with the location where the data is captured with AI technology.\nRecent advancements in AI efficiency, the widespread use of Internet of Things\n(IoT) devices, and the emergence of edge computing have unlocked the enormous\nscope of Edge AI. Edge AI aims to optimize data processing efficiency and\nvelocity while ensuring data confidentiality and integrity. Despite being a\nrelatively new field of research from 2014 to the present, it has shown\nsignificant and rapid development over the last five years. This article\npresents a systematic literature review for Edge AI to discuss the existing\nresearch, recent advancements, and future research directions. We created a\ncollaborative edge AI learning system for cloud and edge computing analysis,\nincluding an in-depth study of the architectures that facilitate this\nmechanism. The taxonomy for Edge AI facilitates the classification and\nconfiguration of Edge AI systems while examining its potential influence across\nmany fields through compassing infrastructure, cloud computing, fog computing,\nservices, use cases, ML and deep learning, and resource management. This study\nhighlights the significance of Edge AI in processing real-time data at the edge\nof the network. Additionally, it emphasizes the research challenges encountered\nby Edge AI systems, including constraints on resources, vulnerabilities to\nsecurity threats, and problems with scalability. Finally, this study highlights\nthe potential future research directions that aim to address the current\nlimitations of Edge AI by providing innovative solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Artificial Intelligence (AI) incorporates a network of interconnected\nsystems and devices that receive, cache, process, and analyze data in close\ncommunication with the location where the data is captured with AI technology.\nRecent advancements in AI efficiency, the widespread use of Internet of Things\n(IoT) devices, and the emergence of edge computing have unlocked the enormous\nscope of Edge AI. Edge AI aims to optimize data processing efficiency and\nvelocity while ensuring data confidentiality and integrity. Despite being a\nrelatively new field of research from 2014 to the present, it has shown\nsignificant and rapid development over the last five years. This article\npresents a systematic literature review for Edge AI to discuss the existing\nresearch, recent advancements, and future research directions. We created a\ncollaborative edge AI learning system for cloud and edge computing analysis,\nincluding an in-depth study of the architectures that facilitate this\nmechanism. The taxonomy for Edge AI facilitates the classification and\nconfiguration of Edge AI systems while examining its potential influence across\nmany fields through compassing infrastructure, cloud computing, fog computing,\nservices, use cases, ML and deep learning, and resource management. This study\nhighlights the significance of Edge AI in processing real-time data at the edge\nof the network. Additionally, it emphasizes the research challenges encountered\nby Edge AI systems, including constraints on resources, vulnerabilities to\nsecurity threats, and problems with scalability. Finally, this study highlights\nthe potential future research directions that aim to address the current\nlimitations of Edge AI by providing innovative solutions."
                },
                "authors": [
                    {
                        "name": "Sukhpal Singh Gill"
                    },
                    {
                        "name": "Muhammed Golec"
                    },
                    {
                        "name": "Jianmin Hu"
                    },
                    {
                        "name": "Minxian Xu"
                    },
                    {
                        "name": "Junhui Du"
                    },
                    {
                        "name": "Huaming Wu"
                    },
                    {
                        "name": "Guneet Kaur Walia"
                    },
                    {
                        "name": "Subramaniam Subramanian Murugesan"
                    },
                    {
                        "name": "Babar Ali"
                    },
                    {
                        "name": "Mohit Kumar"
                    },
                    {
                        "name": "Kejiang Ye"
                    },
                    {
                        "name": "Prabal Verma"
                    },
                    {
                        "name": "Surendra Kumar"
                    },
                    {
                        "name": "Felix Cuadrado"
                    },
                    {
                        "name": "Steve Uhlig"
                    }
                ],
                "author_detail": {
                    "name": "Steve Uhlig"
                },
                "author": "Steve Uhlig",
                "arxiv_doi": "10.1007/s10586-024-04686-y",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10586-024-04686-y",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.04053v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04053v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Preprint Version Accepted for Publication in Springer Cluster\n  Computing, 2024",
                "arxiv_journal_ref": "Springer Cluster Computing, Volume 28, article number 18, pages\n  11953 - 11981, (2025)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15344v1",
                "updated": "2024-10-20T09:37:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    9,
                    37,
                    7,
                    6,
                    294,
                    0
                ],
                "published": "2024-10-20T09:37:07Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    9,
                    37,
                    7,
                    6,
                    294,
                    0
                ],
                "title": "LLC Intra-set Write Balancing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLC Intra-set Write Balancing"
                },
                "summary": "The increasing use of Non-Volatile Memory (NVM) in computer architecture has\nbrought about new challenges, one of which is the write endurance problem.\nFrequent writes to a particular cache cell in NVM can lead to degradation of\nthe memory cell and reduce its lifespan. To solve this problem, we propose a\nsample-based blocking technique for the Last Level Cache (LLC). Our approach\ninvolves defining a threshold value and sampling a subset of cache sets. If the\nnumber of writes to a way in a sampled set exceeds the threshold, the way is\nblocked, and writes are redirected to other ways. We also maintain a history\nstructure to record the number of writes in a set and a PC-Table to use for\nblocking in unsampled sets. Based on blocking on sampled sets, variance of\nvalues stored in history is used to determine whether blocking had a positive\nimpact or not, and on this basis, value corresponding to instruction pointer is\nincremented or decremented. This value is later used for blocking in unsampled\nsets. Our results show that our approach significantly balances write traffic\nto the cache and improves the overall lifespan of the memory cells while having\nbetter performance to the base-line system. Our approach can also be applied to\nother cache hierarchies and NVM technologies to mitigate the problem of write\nendurance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing use of Non-Volatile Memory (NVM) in computer architecture has\nbrought about new challenges, one of which is the write endurance problem.\nFrequent writes to a particular cache cell in NVM can lead to degradation of\nthe memory cell and reduce its lifespan. To solve this problem, we propose a\nsample-based blocking technique for the Last Level Cache (LLC). Our approach\ninvolves defining a threshold value and sampling a subset of cache sets. If the\nnumber of writes to a way in a sampled set exceeds the threshold, the way is\nblocked, and writes are redirected to other ways. We also maintain a history\nstructure to record the number of writes in a set and a PC-Table to use for\nblocking in unsampled sets. Based on blocking on sampled sets, variance of\nvalues stored in history is used to determine whether blocking had a positive\nimpact or not, and on this basis, value corresponding to instruction pointer is\nincremented or decremented. This value is later used for blocking in unsampled\nsets. Our results show that our approach significantly balances write traffic\nto the cache and improves the overall lifespan of the memory cells while having\nbetter performance to the base-line system. Our approach can also be applied to\nother cache hierarchies and NVM technologies to mitigate the problem of write\nendurance."
                },
                "authors": [
                    {
                        "name": "Keshav Krishna"
                    },
                    {
                        "name": "Ayush Verma"
                    }
                ],
                "author_detail": {
                    "name": "Ayush Verma"
                },
                "author": "Ayush Verma",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15332v1",
                "updated": "2024-10-20T08:42:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    8,
                    42,
                    29,
                    6,
                    294,
                    0
                ],
                "published": "2024-10-20T08:42:29Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    8,
                    42,
                    29,
                    6,
                    294,
                    0
                ],
                "title": "EPIC: Efficient Position-Independent Context Caching for Serving Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPIC: Efficient Position-Independent Context Caching for Serving Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) are critical for a wide range of applications,\nbut serving them efficiently becomes increasingly challenging as inputs become\nmore complex. Context caching improves serving performance by exploiting\ninter-request dependency and reusing key-value (KV) cache across requests, thus\nimproving time-to-first-token (TTFT). However, existing prefix-based context\ncaching requires exact token prefix matches, limiting cache reuse in few-shot\nlearning, multi-document QA, or retrieval-augmented generation, where prefixes\nmay vary. In this paper, we present EPIC, an LLM serving system that introduces\nposition-independent context caching (PIC), enabling modular KV cache reuse\nregardless of token chunk position (or prefix). EPIC features two key designs:\nAttnLink, which leverages static attention sparsity to minimize recomputation\nfor accuracy recovery, and KVSplit, a customizable chunking method that\npreserves semantic coherence. Our experiments demonstrate that Epic delivers up\nto 8x improvements in TTFT and 7x throughput over existing systems, with\nnegligible or no accuracy loss. By addressing the limitations of traditional\ncaching approaches, Epic enables more scalable and efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are critical for a wide range of applications,\nbut serving them efficiently becomes increasingly challenging as inputs become\nmore complex. Context caching improves serving performance by exploiting\ninter-request dependency and reusing key-value (KV) cache across requests, thus\nimproving time-to-first-token (TTFT). However, existing prefix-based context\ncaching requires exact token prefix matches, limiting cache reuse in few-shot\nlearning, multi-document QA, or retrieval-augmented generation, where prefixes\nmay vary. In this paper, we present EPIC, an LLM serving system that introduces\nposition-independent context caching (PIC), enabling modular KV cache reuse\nregardless of token chunk position (or prefix). EPIC features two key designs:\nAttnLink, which leverages static attention sparsity to minimize recomputation\nfor accuracy recovery, and KVSplit, a customizable chunking method that\npreserves semantic coherence. Our experiments demonstrate that Epic delivers up\nto 8x improvements in TTFT and 7x throughput over existing systems, with\nnegligible or no accuracy loss. By addressing the limitations of traditional\ncaching approaches, Epic enables more scalable and efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Wenrui Huang"
                    },
                    {
                        "name": "Haoyi Wang"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Qin Zhang"
                    },
                    {
                        "name": "Hao Feng"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Tao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tao Xie"
                },
                "author": "Tao Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15252v1",
                "updated": "2024-10-20T02:17:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    2,
                    17,
                    35,
                    6,
                    294,
                    0
                ],
                "published": "2024-10-20T02:17:35Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    2,
                    17,
                    35,
                    6,
                    294,
                    0
                ],
                "title": "Lossless KV Cache Compression to 2%",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lossless KV Cache Compression to 2%"
                },
                "summary": "Large language models have revolutionized data processing in numerous\ndomains, with their ability to handle extended context reasoning receiving\nnotable recognition. To speed up inference, maintaining a key-value (KV) cache\nmemory is essential. Nonetheless, the growing demands for KV cache memory\ncreate significant hurdles for efficient implementation. This work introduces a\nnovel architecture, Cross-Layer Latent Attention (CLLA), aimed at compressing\nthe KV cache to less than 2% of its original size while maintaining comparable\nperformance levels. CLLA integrates multiple aspects of KV cache compression,\nincluding attention head/dimension reduction, layer sharing, and quantization\ntechniques, into a cohesive framework. Our extensive experiments demonstrate\nthat CLLA achieves lossless performance on most tasks while utilizing minimal\nKV cache, marking a significant advancement in practical KV cache compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have revolutionized data processing in numerous\ndomains, with their ability to handle extended context reasoning receiving\nnotable recognition. To speed up inference, maintaining a key-value (KV) cache\nmemory is essential. Nonetheless, the growing demands for KV cache memory\ncreate significant hurdles for efficient implementation. This work introduces a\nnovel architecture, Cross-Layer Latent Attention (CLLA), aimed at compressing\nthe KV cache to less than 2% of its original size while maintaining comparable\nperformance levels. CLLA integrates multiple aspects of KV cache compression,\nincluding attention head/dimension reduction, layer sharing, and quantization\ntechniques, into a cohesive framework. Our extensive experiments demonstrate\nthat CLLA achieves lossless performance on most tasks while utilizing minimal\nKV cache, marking a significant advancement in practical KV cache compression."
                },
                "authors": [
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "J. N. Han"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "An Wang"
                    },
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Zhanhui Kang"
                    }
                ],
                "author_detail": {
                    "name": "Zhanhui Kang"
                },
                "author": "Zhanhui Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2206.05579v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2206.05579v4",
                "updated": "2024-10-19T12:15:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    19,
                    12,
                    15,
                    50,
                    5,
                    293,
                    0
                ],
                "published": "2022-06-11T17:52:10Z",
                "published_parsed": [
                    2022,
                    6,
                    11,
                    17,
                    52,
                    10,
                    5,
                    162,
                    0
                ],
                "title": "Online Paging with Heterogeneous Cache Slots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Paging with Heterogeneous Cache Slots"
                },
                "summary": "It is natural to generalize the online $k$-Server problem by allowing each\nrequest to specify not only a point $p$, but also a subset $S$ of servers that\nmay serve it. For uniform metrics, the problem is equivalent to a\ngeneralization of Paging in which each request specifies not only a page $p$,\nbut also a subset $S$ of cache slots, and is satisfied by having a copy of $p$\nin some slot in $S$. We call this problem Slot-Heterogenous Paging.\n  We parameterize the problem by specifying a family $\\mathcal S \\subseteq\n2^{[k]}$ of requestable slot sets, and we establish bounds on the competitive\nratio as a function of the cache size $k$ and family $\\mathcal S$:\n  - If all request sets are allowed ($\\mathcal\nS=2^{[k]}\\setminus\\{\\emptyset\\}$), the optimal deterministic and randomized\ncompetitive ratios are exponentially worse than for standard \\Paging ($\\mathcal\nS=\\{[k]\\}$).\n  - As a function of $|\\mathcal S|$ and $k$, the optimal deterministic ratio is\npolynomial: at most $O(k^2|\\mathcal S|)$ and at least $\\Omega(\\sqrt{|\\mathcal\nS|})$.\n  - For any laminar family $\\mathcal S$ of height $h$, the optimal ratios are\n$O(hk)$ (deterministic) and $O(h^2\\log k)$ (randomized).\n  - The special case of laminar $\\mathcal S$ that we call All-or-One Paging\nextends standard Paging by allowing each request to specify a specific slot to\nput the requested page in. The optimal deterministic ratio for weighted\nAll-or-One Paging is $\\Theta(k)$. Offline All-or-One Paging is NP-hard.\n  Some results for the laminar case are shown via a reduction to the\ngeneralization of Paging in which each request specifies a set $\\mathcal P of\npages, and is satisfied by fetching any page from $\\mathcal P into the cache.\nThe optimal ratios for the latter problem (with laminar family of height $h$)\nare at most $hk$ (deterministic) and $h\\,H_k$ (randomized).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is natural to generalize the online $k$-Server problem by allowing each\nrequest to specify not only a point $p$, but also a subset $S$ of servers that\nmay serve it. For uniform metrics, the problem is equivalent to a\ngeneralization of Paging in which each request specifies not only a page $p$,\nbut also a subset $S$ of cache slots, and is satisfied by having a copy of $p$\nin some slot in $S$. We call this problem Slot-Heterogenous Paging.\n  We parameterize the problem by specifying a family $\\mathcal S \\subseteq\n2^{[k]}$ of requestable slot sets, and we establish bounds on the competitive\nratio as a function of the cache size $k$ and family $\\mathcal S$:\n  - If all request sets are allowed ($\\mathcal\nS=2^{[k]}\\setminus\\{\\emptyset\\}$), the optimal deterministic and randomized\ncompetitive ratios are exponentially worse than for standard \\Paging ($\\mathcal\nS=\\{[k]\\}$).\n  - As a function of $|\\mathcal S|$ and $k$, the optimal deterministic ratio is\npolynomial: at most $O(k^2|\\mathcal S|)$ and at least $\\Omega(\\sqrt{|\\mathcal\nS|})$.\n  - For any laminar family $\\mathcal S$ of height $h$, the optimal ratios are\n$O(hk)$ (deterministic) and $O(h^2\\log k)$ (randomized).\n  - The special case of laminar $\\mathcal S$ that we call All-or-One Paging\nextends standard Paging by allowing each request to specify a specific slot to\nput the requested page in. The optimal deterministic ratio for weighted\nAll-or-One Paging is $\\Theta(k)$. Offline All-or-One Paging is NP-hard.\n  Some results for the laminar case are shown via a reduction to the\ngeneralization of Paging in which each request specifies a set $\\mathcal P of\npages, and is satisfied by fetching any page from $\\mathcal P into the cache.\nThe optimal ratios for the latter problem (with laminar family of height $h$)\nare at most $hk$ (deterministic) and $h\\,H_k$ (randomized)."
                },
                "authors": [
                    {
                        "name": "Marek Chrobak"
                    },
                    {
                        "name": "Samuel Haney"
                    },
                    {
                        "name": "Mehraneh Liaee"
                    },
                    {
                        "name": "Debmalya Panigrahi"
                    },
                    {
                        "name": "Rajmohan Rajaraman"
                    },
                    {
                        "name": "Ravi Sundaram"
                    },
                    {
                        "name": "Neal E. Young"
                    }
                ],
                "author_detail": {
                    "name": "Neal E. Young"
                },
                "author": "Neal E. Young",
                "arxiv_doi": "10.1007/s00453-024-01270-z",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s00453-024-01270-z",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2206.05579v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2206.05579v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "conference and journal versions appear in STACS 2023 and Algorithmica\n  (2004)",
                "arxiv_journal_ref": "Algorithmica (2004)",
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.0; F.1.2; C.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12876v2",
                "updated": "2024-10-19T08:45:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    19,
                    8,
                    45,
                    11,
                    5,
                    293,
                    0
                ],
                "published": "2024-10-15T05:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    1,
                    19,
                    1,
                    289,
                    0
                ],
                "title": "In-context KV-Cache Eviction for LLMs via Attention-Gate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context KV-Cache Eviction for LLMs via Attention-Gate"
                },
                "summary": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). It caches states of self-attention to avoid\nrecomputation. Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system, especially when confronted with\nultra-large models and long-context queries. A natural remedy is to discard the\nKV-Cache for less important tokens, with StreamingLLM as an example, but the\nused static eviction strategies cannot flexibly adapt to varying contexts.\nRemedies like H2O leverage accumulative attention scores to perform dynamic\neviction but suffer from the attention bias issue in capturing contextual\ninformation. This paper bridges this gap by devising a parameterized KV-Cache\neviction mechanism, dubbed as Attention-Gate, which accepts the whole context\nas input and yields eviction flags for each token to realize in-context\neviction. The subsequent self-attention module proceeds according to the flags\nand only the KV states for the remaining tokens need to be cached. The\nAttention-Gates can vary among different heads and layers and be trivially\nplugged into pre-trained LLMs, tuned by cost-effective continual pre-training\nor supervised fine-tuning objectives to acquire what to discard. The\ncomputational and memory overhead introduced by Attention-Gates is minimal. Our\nmethod is validated across multiple tasks, demonstrating both efficiency and\nadaptability. After a highly efficient continual pre-training, it achieves\nhigher average accuracy and evicts more tokens compared to traditional\ntraining-free methods. In supervised fine-tuning, it not only evicts many\ntokens but also outperforms LoRA-finetuned LLMs on some datasets, such as RTE,\nwhere it improves accuracy by 13.9% while evicting 62.8% of tokens, showing\nthat effective eviction of redundant tokens can even enhance performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). It caches states of self-attention to avoid\nrecomputation. Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system, especially when confronted with\nultra-large models and long-context queries. A natural remedy is to discard the\nKV-Cache for less important tokens, with StreamingLLM as an example, but the\nused static eviction strategies cannot flexibly adapt to varying contexts.\nRemedies like H2O leverage accumulative attention scores to perform dynamic\neviction but suffer from the attention bias issue in capturing contextual\ninformation. This paper bridges this gap by devising a parameterized KV-Cache\neviction mechanism, dubbed as Attention-Gate, which accepts the whole context\nas input and yields eviction flags for each token to realize in-context\neviction. The subsequent self-attention module proceeds according to the flags\nand only the KV states for the remaining tokens need to be cached. The\nAttention-Gates can vary among different heads and layers and be trivially\nplugged into pre-trained LLMs, tuned by cost-effective continual pre-training\nor supervised fine-tuning objectives to acquire what to discard. The\ncomputational and memory overhead introduced by Attention-Gates is minimal. Our\nmethod is validated across multiple tasks, demonstrating both efficiency and\nadaptability. After a highly efficient continual pre-training, it achieves\nhigher average accuracy and evicts more tokens compared to traditional\ntraining-free methods. In supervised fine-tuning, it not only evicts many\ntokens but also outperforms LoRA-finetuned LLMs on some datasets, such as RTE,\nwhere it improves accuracy by 13.9% while evicting 62.8% of tokens, showing\nthat effective eviction of redundant tokens can even enhance performance."
                },
                "authors": [
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Bokai Lin"
                    },
                    {
                        "name": "Tianqi Hou"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10593v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10593v3",
                "updated": "2024-10-18T19:30:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    19,
                    30,
                    35,
                    4,
                    292,
                    0
                ],
                "published": "2024-09-16T17:36:50Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    36,
                    50,
                    0,
                    260,
                    0
                ],
                "title": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios"
                },
                "summary": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%. Code is\navailable at https://github.com/wln20/CSKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%. Code is\navailable at https://github.com/wln20/CSKV."
                },
                "authors": [
                    {
                        "name": "Luning Wang"
                    },
                    {
                        "name": "Shiyao Li"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "arxiv_comment": "4th NeurIPS Efficient Natural Language and Speech Processing Workshop\n  (ENLSP-IV 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10593v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10593v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14346v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14346v2",
                "updated": "2024-10-18T13:59:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    59,
                    54,
                    4,
                    292,
                    0
                ],
                "published": "2024-07-19T14:28:53Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    14,
                    28,
                    53,
                    4,
                    201,
                    0
                ],
                "title": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals"
                },
                "summary": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue."
                },
                "authors": [
                    {
                        "name": "Akash Kumar Mohankumar"
                    },
                    {
                        "name": "Gururaj K"
                    },
                    {
                        "name": "Gagan Madan"
                    },
                    {
                        "name": "Amit Singh"
                    }
                ],
                "author_detail": {
                    "name": "Amit Singh"
                },
                "author": "Amit Singh",
                "arxiv_comment": "Accepted to EMNLP 2024 Industry Track. 10 pages, 10 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14346v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14346v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14442v1",
                "updated": "2024-10-18T13:01:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    1,
                    14,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T13:01:14Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    1,
                    14,
                    4,
                    292,
                    0
                ],
                "title": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference"
                },
                "summary": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2x, most configurations can achieve competitive performance to and\nhigher throughput than standard transformers, but when further reducing the\nsize of the KV cache, pairing queries of all layers with KVs of upper layers\ncan better maintain performance, although it also introduces additional\ntraining cost and prefilling latency. We hope that this work will help users\nchoose the appropriate approach according to their requirements and facilitate\nresearch on the acceleration of LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2x, most configurations can achieve competitive performance to and\nhigher throughput than standard transformers, but when further reducing the\nsize of the KV cache, pairing queries of all layers with KVs of upper layers\ncan better maintain performance, although it also introduces additional\ntraining cost and prefilling latency. We hope that this work will help users\nchoose the appropriate approach according to their requirements and facilitate\nresearch on the acceleration of LLM inference."
                },
                "authors": [
                    {
                        "name": "You Wu"
                    },
                    {
                        "name": "Haoyi Wu"
                    },
                    {
                        "name": "Kewei Tu"
                    }
                ],
                "author_detail": {
                    "name": "Kewei Tu"
                },
                "author": "Kewei Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10859v2",
                "updated": "2024-10-18T10:02:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    10,
                    2,
                    3,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-07T13:46:06Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    46,
                    6,
                    0,
                    281,
                    0
                ],
                "title": "FAME: Towards Factual Multi-Task Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAME: Towards Factual Multi-Task Model Editing"
                },
                "summary": "Large language models (LLMs) embed extensive knowledge and utilize it to\nperform exceptionally well across various tasks. Nevertheless, outdated\nknowledge or factual errors within LLMs can lead to misleading or incorrect\nresponses, causing significant issues in practical applications. To rectify the\nfatal flaw without the necessity for costly model retraining, various model\nediting approaches have been proposed to correct inaccurate knowledge within\nLLMs in a cost-efficient way. To evaluate these model editing methods, previous\nwork introduced a series of datasets. However, most of the previous datasets\nonly contain fabricated data in a single format, which diverges from real-world\nmodel editing scenarios, raising doubts about their usability in practice. To\nfacilitate the application of model editing in real-world scenarios, we propose\nthe challenge of practicality. To resolve such challenges and effectively\nenhance the capabilities of LLMs, we present FAME, an factual, comprehensive,\nand multi-task dataset, which is designed to enhance the practicality of model\nediting. We then propose SKEME, a model editing method that uses a novel\ncaching mechanism to ensure synchronization with the real world. The\nexperiments demonstrate that SKEME performs excellently across various tasks\nand scenarios, confirming its practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) embed extensive knowledge and utilize it to\nperform exceptionally well across various tasks. Nevertheless, outdated\nknowledge or factual errors within LLMs can lead to misleading or incorrect\nresponses, causing significant issues in practical applications. To rectify the\nfatal flaw without the necessity for costly model retraining, various model\nediting approaches have been proposed to correct inaccurate knowledge within\nLLMs in a cost-efficient way. To evaluate these model editing methods, previous\nwork introduced a series of datasets. However, most of the previous datasets\nonly contain fabricated data in a single format, which diverges from real-world\nmodel editing scenarios, raising doubts about their usability in practice. To\nfacilitate the application of model editing in real-world scenarios, we propose\nthe challenge of practicality. To resolve such challenges and effectively\nenhance the capabilities of LLMs, we present FAME, an factual, comprehensive,\nand multi-task dataset, which is designed to enhance the practicality of model\nediting. We then propose SKEME, a model editing method that uses a novel\ncaching mechanism to ensure synchronization with the real world. The\nexperiments demonstrate that SKEME performs excellently across various tasks\nand scenarios, confirming its practicality."
                },
                "authors": [
                    {
                        "name": "Li Zeng"
                    },
                    {
                        "name": "Yingyu Shan"
                    },
                    {
                        "name": "Zeming Liu"
                    },
                    {
                        "name": "Jiashu Yao"
                    },
                    {
                        "name": "Yuhang Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yuhang Guo"
                },
                "author": "Yuhang Guo",
                "arxiv_comment": "9 pages, 3 figures. This paper has been accepted by EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14003v1",
                "updated": "2024-10-17T20:11:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    20,
                    11,
                    34,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T20:11:34Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    20,
                    11,
                    34,
                    3,
                    291,
                    0
                ],
                "title": "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time\n  Systems"
                },
                "summary": "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\nmemory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial for high performance. To support high MLP, shared last-level caches\n(LLCs) are divided into multiple banks, allowing parallel access. However,\nuneven distribution of cache requests from the cores, especially when requests\nfrom multiple cores are concentrated on a single bank, can result in\nsignificant contention affecting all cores that access the cache. Such cache\nbank contention can even be maliciously induced -- known as cache bank-aware\ndenial-of-service (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked shared LLC based multicore real-time systems. By regulating\nbandwidth on a per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache accesses to non-contended banks, thus improving overall\nperformance (throughput) without compromising isolation benefits of throttling.\nWe implement our approach on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively using both synthetic and real-world workloads.\nOur evaluation results show that the proposed per-bank regulation approach\neffectively protects real-time tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\times$ performance improvement for the\nthrottled benign best-effort tasks compared to prior bank-oblivious bandwidth\nthrottling approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\nmemory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial for high performance. To support high MLP, shared last-level caches\n(LLCs) are divided into multiple banks, allowing parallel access. However,\nuneven distribution of cache requests from the cores, especially when requests\nfrom multiple cores are concentrated on a single bank, can result in\nsignificant contention affecting all cores that access the cache. Such cache\nbank contention can even be maliciously induced -- known as cache bank-aware\ndenial-of-service (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked shared LLC based multicore real-time systems. By regulating\nbandwidth on a per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache accesses to non-contended banks, thus improving overall\nperformance (throughput) without compromising isolation benefits of throttling.\nWe implement our approach on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively using both synthetic and real-world workloads.\nOur evaluation results show that the proposed per-bank regulation approach\neffectively protects real-time tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\times$ performance improvement for the\nthrottled benign best-effort tasks compared to prior bank-oblivious bandwidth\nthrottling approaches."
                },
                "authors": [
                    {
                        "name": "Connor Sullivan"
                    },
                    {
                        "name": "Alex Manley"
                    },
                    {
                        "name": "Mohammad Alian"
                    },
                    {
                        "name": "Heechul Yun"
                    }
                ],
                "author_detail": {
                    "name": "Heechul Yun"
                },
                "author": "Heechul Yun",
                "arxiv_comment": "RTSS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13846v1",
                "updated": "2024-10-17T17:58:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    58,
                    14,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T17:58:14Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    58,
                    14,
                    3,
                    291,
                    0
                ],
                "title": "SimLayerKV: A Simple Framework for Layer-Level KV Cache Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimLayerKV: A Simple Framework for Layer-Level KV Cache Reduction"
                },
                "summary": "Recent advancements in large language models (LLMs) have extended their\ncapabilities to handle long contexts. However, increasing the number of model\nlayers and the length of input sequences significantly escalates the memory\nrequired to store key-value (KV) cache, posing challenges for efficient\ninference. To mitigate this issue, we present SimLayerKV, a simple yet\neffective method that reduces inter-layer KV cache redundancies by selectively\ndropping cache in identified lazy layers. Our approach is based on the\nobservation that certain layers in long-context LLMs exhibit \"lazy\" behavior,\ncontributing less to modeling long-range dependencies compared to non-lazy\nlayers. By analyzing attention weight patterns, we find that the behavior of\nthese lazy layers is consistent across tokens during generation for a given\ninput. This insight motivates our SimLayerKV, which identifies lazy layers and\nreduces their KV cache accordingly. SimLayerKV is training-free, generalizable,\nand can be implemented with only seven lines of code. We conduct extensive\nexperiments on three representative LLMs, e.g., LLaMA2-7B, LLaMA3-8B, and\nMistral-7B across 16 tasks from the LongBench benchmark. The results\ndemonstrate that SimLayerKV achieves a KV cache compression ratio of 5$\\times$\nwith only a 1.2% performance drop when combined with 4-bit quantization. Our\ncode is available at https://github.com/sail-sg/SimLayerKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have extended their\ncapabilities to handle long contexts. However, increasing the number of model\nlayers and the length of input sequences significantly escalates the memory\nrequired to store key-value (KV) cache, posing challenges for efficient\ninference. To mitigate this issue, we present SimLayerKV, a simple yet\neffective method that reduces inter-layer KV cache redundancies by selectively\ndropping cache in identified lazy layers. Our approach is based on the\nobservation that certain layers in long-context LLMs exhibit \"lazy\" behavior,\ncontributing less to modeling long-range dependencies compared to non-lazy\nlayers. By analyzing attention weight patterns, we find that the behavior of\nthese lazy layers is consistent across tokens during generation for a given\ninput. This insight motivates our SimLayerKV, which identifies lazy layers and\nreduces their KV cache accordingly. SimLayerKV is training-free, generalizable,\nand can be implemented with only seven lines of code. We conduct extensive\nexperiments on three representative LLMs, e.g., LLaMA2-7B, LLaMA3-8B, and\nMistral-7B across 16 tasks from the LongBench benchmark. The results\ndemonstrate that SimLayerKV achieves a KV cache compression ratio of 5$\\times$\nwith only a 1.2% performance drop when combined with 4-bit quantization. Our\ncode is available at https://github.com/sail-sg/SimLayerKV."
                },
                "authors": [
                    {
                        "name": "Xuan Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15355v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15355v4",
                "updated": "2024-10-17T15:27:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    15,
                    27,
                    30,
                    3,
                    291,
                    0
                ],
                "published": "2024-09-14T02:34:26Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    2,
                    34,
                    26,
                    5,
                    258,
                    0
                ],
                "title": "Block-Attention for Efficient RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-Attention for Efficient RAG"
                },
                "summary": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "East Sun"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Lan Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lan Tian"
                },
                "author": "Lan Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15355v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15355v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07979v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07979v2",
                "updated": "2024-10-17T08:54:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    8,
                    54,
                    37,
                    3,
                    291,
                    0
                ],
                "published": "2024-04-11T17:57:22Z",
                "published_parsed": [
                    2024,
                    4,
                    11,
                    17,
                    57,
                    22,
                    3,
                    102,
                    0
                ],
                "title": "LLoCO: Learning Long Contexts Offline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLoCO: Learning Long Contexts Offline"
                },
                "summary": "Processing long contexts remains a challenge for large language models (LLMs)\ndue to the quadratic computational and memory overhead of the self-attention\nmechanism and the substantial KV cache sizes during generation. We propose\nLLoCO, a novel approach to address this problem by learning contexts offline\nthrough context compression and in-domain parameter-efficient finetuning with\nLoRA. Our method enables an LLM to create a concise representation of the\noriginal context and efficiently retrieve relevant information to answer\nquestions accurately. Our approach extends the effective context window of a 4k\ntoken LLaMA2-7B model to handle up to 128k tokens. We evaluate our approach on\nseveral long-context question-answering datasets, demonstrating that LLoCO\nsignificantly outperforms in-context learning while using $30\\times$ fewer\ntokens during inference. LLoCO achieves up to $7.62\\times$ speed-up during\ninference and $11.52\\times$ higher throughput during finetuning, substantially\nreduces the cost of long document question answering. This makes it a promising\nsolution for efficient long context processing. Our code is publicly available\non https://github.com/jeffreysijuntan/lloco.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts remains a challenge for large language models (LLMs)\ndue to the quadratic computational and memory overhead of the self-attention\nmechanism and the substantial KV cache sizes during generation. We propose\nLLoCO, a novel approach to address this problem by learning contexts offline\nthrough context compression and in-domain parameter-efficient finetuning with\nLoRA. Our method enables an LLM to create a concise representation of the\noriginal context and efficiently retrieve relevant information to answer\nquestions accurately. Our approach extends the effective context window of a 4k\ntoken LLaMA2-7B model to handle up to 128k tokens. We evaluate our approach on\nseveral long-context question-answering datasets, demonstrating that LLoCO\nsignificantly outperforms in-context learning while using $30\\times$ fewer\ntokens during inference. LLoCO achieves up to $7.62\\times$ speed-up during\ninference and $11.52\\times$ higher throughput during finetuning, substantially\nreduces the cost of long document question answering. This makes it a promising\nsolution for efficient long context processing. Our code is publicly available\non https://github.com/jeffreysijuntan/lloco."
                },
                "authors": [
                    {
                        "name": "Sijun Tan"
                    },
                    {
                        "name": "Xiuyu Li"
                    },
                    {
                        "name": "Shishir Patil"
                    },
                    {
                        "name": "Ziyang Wu"
                    },
                    {
                        "name": "Tianjun Zhang"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Raluca Ada Popa"
                    }
                ],
                "author_detail": {
                    "name": "Raluca Ada Popa"
                },
                "author": "Raluca Ada Popa",
                "arxiv_comment": "EMNLP 2024. The first two authors contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07979v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07979v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18126v1",
                "updated": "2024-10-17T04:37:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    4,
                    37,
                    43,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T04:37:43Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    4,
                    37,
                    43,
                    3,
                    291,
                    0
                ],
                "title": "Leveraging Hardware Performance Counters for Predicting Workload\n  Interference in Vector Supercomputers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Hardware Performance Counters for Predicting Workload\n  Interference in Vector Supercomputers"
                },
                "summary": "In the rapidly evolving domain of high-performance computing (HPC),\nheterogeneous architectures such as the SX-Aurora TSUBASA (SX-AT) system\narchitecture, which integrate diverse processor types, present both\nopportunities and challenges for optimizing resource utilization. This paper\ninvestigates workload interference within an SX-AT system, with a specific\nfocus on resource contention between Vector Hosts (VHs) and Vector Engines\n(VEs). Through comprehensive empirical analysis, the study identifies key\nfactors contributing to performance degradation, such as cache and memory\nbandwidth contention, when jobs with varying computational demands share\nresources. To address these issues, we develop a predictive model that\nleverages hardware performance counters (HCs) and machine learning (ML)\nalgorithms to classify and predict workload interference. Our results\ndemonstrate that the model accurately forecasts performance degradation,\noffering valuable insights for future research on optimizing job scheduling and\nresource allocation. This approach highlights the importance of adaptive\nresource management strategies in maintaining system efficiency and provides a\nfoundation for future enhancements in heterogeneous supercomputing\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the rapidly evolving domain of high-performance computing (HPC),\nheterogeneous architectures such as the SX-Aurora TSUBASA (SX-AT) system\narchitecture, which integrate diverse processor types, present both\nopportunities and challenges for optimizing resource utilization. This paper\ninvestigates workload interference within an SX-AT system, with a specific\nfocus on resource contention between Vector Hosts (VHs) and Vector Engines\n(VEs). Through comprehensive empirical analysis, the study identifies key\nfactors contributing to performance degradation, such as cache and memory\nbandwidth contention, when jobs with varying computational demands share\nresources. To address these issues, we develop a predictive model that\nleverages hardware performance counters (HCs) and machine learning (ML)\nalgorithms to classify and predict workload interference. Our results\ndemonstrate that the model accurately forecasts performance degradation,\noffering valuable insights for future research on optimizing job scheduling and\nresource allocation. This approach highlights the importance of adaptive\nresource management strategies in maintaining system efficiency and provides a\nfoundation for future enhancements in heterogeneous supercomputing\nenvironments."
                },
                "authors": [
                    {
                        "name": "Shubham"
                    },
                    {
                        "name": "Keichi Takahashi"
                    },
                    {
                        "name": "Hiroyuki Takizawa"
                    }
                ],
                "author_detail": {
                    "name": "Hiroyuki Takizawa"
                },
                "author": "Hiroyuki Takizawa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.05007v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05007v1",
                "updated": "2024-11-07T18:59:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    59,
                    58,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T18:59:58Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    59,
                    58,
                    3,
                    312,
                    0
                ],
                "title": "SVDQunat: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SVDQunat: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion\n  Models"
                },
                "summary": "Diffusion models have been proven highly effective at generating high-quality\nimages. However, as these models grow larger, they require significantly more\nmemory and suffer from higher latency, posing substantial challenges for\ndeployment. In this work, we aim to accelerate diffusion models by quantizing\ntheir weights and activations to 4 bits. At such an aggressive level, both\nweights and activations are highly sensitive, where conventional post-training\nquantization methods for large language models like smoothing become\ninsufficient. To overcome this limitation, we propose SVDQuant, a new 4-bit\nquantization paradigm. Different from smoothing which redistributes outliers\nbetween weights and activations, our approach absorbs these outliers using a\nlow-rank branch. We first consolidate the outliers by shifting them from\nactivations to weights, then employ a high-precision low-rank branch to take in\nthe weight outliers with Singular Value Decomposition (SVD). This process eases\nthe quantization on both sides. However, na\\\"{\\i}vely running the low-rank\nbranch independently incurs significant overhead due to extra data movement of\nactivations, negating the quantization speedup. To address this, we co-design\nan inference engine Nunchaku that fuses the kernels of the low-rank branch into\nthose of the low-bit branch to cut off redundant memory access. It can also\nseamlessly support off-the-shelf low-rank adapters (LoRAs) without the need for\nre-quantization. Extensive experiments on SDXL, PixArt-$\\Sigma$, and FLUX.1\nvalidate the effectiveness of SVDQuant in preserving image quality. We reduce\nthe memory usage for the 12B FLUX.1 models by 3.5$\\times$, achieving\n3.0$\\times$ speedup over the 4-bit weight-only quantized baseline on the 16GB\nlaptop 4090 GPU, paving the way for more interactive applications on PCs. Our\nquantization library and inference engine are open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have been proven highly effective at generating high-quality\nimages. However, as these models grow larger, they require significantly more\nmemory and suffer from higher latency, posing substantial challenges for\ndeployment. In this work, we aim to accelerate diffusion models by quantizing\ntheir weights and activations to 4 bits. At such an aggressive level, both\nweights and activations are highly sensitive, where conventional post-training\nquantization methods for large language models like smoothing become\ninsufficient. To overcome this limitation, we propose SVDQuant, a new 4-bit\nquantization paradigm. Different from smoothing which redistributes outliers\nbetween weights and activations, our approach absorbs these outliers using a\nlow-rank branch. We first consolidate the outliers by shifting them from\nactivations to weights, then employ a high-precision low-rank branch to take in\nthe weight outliers with Singular Value Decomposition (SVD). This process eases\nthe quantization on both sides. However, na\\\"{\\i}vely running the low-rank\nbranch independently incurs significant overhead due to extra data movement of\nactivations, negating the quantization speedup. To address this, we co-design\nan inference engine Nunchaku that fuses the kernels of the low-rank branch into\nthose of the low-bit branch to cut off redundant memory access. It can also\nseamlessly support off-the-shelf low-rank adapters (LoRAs) without the need for\nre-quantization. Extensive experiments on SDXL, PixArt-$\\Sigma$, and FLUX.1\nvalidate the effectiveness of SVDQuant in preserving image quality. We reduce\nthe memory usage for the 12B FLUX.1 models by 3.5$\\times$, achieving\n3.0$\\times$ speedup over the 4-bit weight-only quantized baseline on the 16GB\nlaptop 4090 GPU, paving the way for more interactive applications on PCs. Our\nquantization library and inference engine are open-sourced."
                },
                "authors": [
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Zhekai Zhang"
                    },
                    {
                        "name": "Tianle Cai"
                    },
                    {
                        "name": "Xiuyu Li"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Enze Xie"
                    },
                    {
                        "name": "Chenlin Meng"
                    },
                    {
                        "name": "Jun-Yan Zhu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Quantization Library: https://github.com/mit-han-lab/deepcompressor\n  Inference Engine: https://github.com/mit-han-lab/nunchaku Website:\n  https://hanlab.mit.edu/projects/svdquant Demo: https://svdquant.mit.edu Blog:\n  https://hanlab.mit.edu/blog/svdquant",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05007v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.00922v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.00922v3",
                "updated": "2024-11-07T18:59:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    59,
                    30,
                    3,
                    312,
                    0
                ],
                "published": "2024-06-03T01:32:52Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    1,
                    32,
                    52,
                    0,
                    155,
                    0
                ],
                "title": "MediQ: Question-Asking LLMs and a Benchmark for Reliable Interactive\n  Clinical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MediQ: Question-Asking LLMs and a Benchmark for Reliable Interactive\n  Clinical Reasoning"
                },
                "summary": "Users typically engage with LLMs interactively, yet most existing benchmarks\nevaluate them in a static, single-turn format, posing reliability concerns in\ninteractive scenarios. We identify a key obstacle towards reliability: LLMs are\ntrained to answer any question, even with incomplete context or insufficient\nknowledge. In this paper, we propose to change the static paradigm to an\ninteractive one, develop systems that proactively ask questions to gather more\ninformation and respond reliably, and introduce an benchmark - MediQ - to\nevaluate question-asking ability in LLMs. MediQ simulates clinical interactions\nconsisting of a Patient System and an adaptive Expert System; with potentially\nincomplete initial information, the Expert refrains from making diagnostic\ndecisions when unconfident, and instead elicits missing details via follow-up\nquestions. We provide a pipeline to convert single-turn medical benchmarks into\nan interactive format. Our results show that directly prompting\nstate-of-the-art LLMs to ask questions degrades performance, indicating that\nadapting LLMs to proactive information-seeking settings is nontrivial. We\nexperiment with abstention strategies to better estimate model confidence and\ndecide when to ask questions, improving diagnostic accuracy by 22.3%; however,\nperformance still lags compared to an (unrealistic in practice) upper bound\nwith complete information upfront. Further analyses show improved interactive\nperformance with filtering irrelevant contexts and reformatting conversations.\nOverall, we introduce a novel problem towards LLM reliability, an interactive\nMediQ benchmark and a novel question-asking system, and highlight directions to\nextend LLMs' information-seeking abilities in critical domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Users typically engage with LLMs interactively, yet most existing benchmarks\nevaluate them in a static, single-turn format, posing reliability concerns in\ninteractive scenarios. We identify a key obstacle towards reliability: LLMs are\ntrained to answer any question, even with incomplete context or insufficient\nknowledge. In this paper, we propose to change the static paradigm to an\ninteractive one, develop systems that proactively ask questions to gather more\ninformation and respond reliably, and introduce an benchmark - MediQ - to\nevaluate question-asking ability in LLMs. MediQ simulates clinical interactions\nconsisting of a Patient System and an adaptive Expert System; with potentially\nincomplete initial information, the Expert refrains from making diagnostic\ndecisions when unconfident, and instead elicits missing details via follow-up\nquestions. We provide a pipeline to convert single-turn medical benchmarks into\nan interactive format. Our results show that directly prompting\nstate-of-the-art LLMs to ask questions degrades performance, indicating that\nadapting LLMs to proactive information-seeking settings is nontrivial. We\nexperiment with abstention strategies to better estimate model confidence and\ndecide when to ask questions, improving diagnostic accuracy by 22.3%; however,\nperformance still lags compared to an (unrealistic in practice) upper bound\nwith complete information upfront. Further analyses show improved interactive\nperformance with filtering irrelevant contexts and reformatting conversations.\nOverall, we introduce a novel problem towards LLM reliability, an interactive\nMediQ benchmark and a novel question-asking system, and highlight directions to\nextend LLMs' information-seeking abilities in critical domains."
                },
                "authors": [
                    {
                        "name": "Shuyue Stella Li"
                    },
                    {
                        "name": "Vidhisha Balachandran"
                    },
                    {
                        "name": "Shangbin Feng"
                    },
                    {
                        "name": "Jonathan S. Ilgen"
                    },
                    {
                        "name": "Emma Pierson"
                    },
                    {
                        "name": "Pang Wei Koh"
                    },
                    {
                        "name": "Yulia Tsvetkov"
                    }
                ],
                "author_detail": {
                    "name": "Yulia Tsvetkov"
                },
                "author": "Yulia Tsvetkov",
                "arxiv_comment": "29 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.00922v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.00922v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04999v1",
                "updated": "2024-11-07T18:59:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    59,
                    27,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T18:59:27Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    59,
                    27,
                    3,
                    312,
                    0
                ],
                "title": "DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile\n  Manipulation"
                },
                "summary": "Significant progress has been made in open-vocabulary mobile manipulation,\nwhere the goal is for a robot to perform tasks in any environment given a\nnatural language description. However, most current systems assume a static\nenvironment, which limits the system's applicability in real-world scenarios\nwhere environments frequently change due to human intervention or the robot's\nown actions. In this work, we present DynaMem, a new approach to open-world\nmobile manipulation that uses a dynamic spatio-semantic memory to represent a\nrobot's environment. DynaMem constructs a 3D data structure to maintain a\ndynamic memory of point clouds, and answers open-vocabulary object localization\nqueries using multimodal LLMs or open-vocabulary features generated by\nstate-of-the-art vision-language models. Powered by DynaMem, our robots can\nexplore novel environments, search for objects not found in memory, and\ncontinuously update the memory as objects move, appear, or disappear in the\nscene. We run extensive experiments on the Stretch SE3 robots in three real and\nnine offline scenes, and achieve an average pick-and-drop success rate of 70%\non non-stationary objects, which is more than a 2x improvement over\nstate-of-the-art static systems. Our code as well as our experiment and\ndeployment videos are open sourced and can be found on our project website:\nhttps://dynamem.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Significant progress has been made in open-vocabulary mobile manipulation,\nwhere the goal is for a robot to perform tasks in any environment given a\nnatural language description. However, most current systems assume a static\nenvironment, which limits the system's applicability in real-world scenarios\nwhere environments frequently change due to human intervention or the robot's\nown actions. In this work, we present DynaMem, a new approach to open-world\nmobile manipulation that uses a dynamic spatio-semantic memory to represent a\nrobot's environment. DynaMem constructs a 3D data structure to maintain a\ndynamic memory of point clouds, and answers open-vocabulary object localization\nqueries using multimodal LLMs or open-vocabulary features generated by\nstate-of-the-art vision-language models. Powered by DynaMem, our robots can\nexplore novel environments, search for objects not found in memory, and\ncontinuously update the memory as objects move, appear, or disappear in the\nscene. We run extensive experiments on the Stretch SE3 robots in three real and\nnine offline scenes, and achieve an average pick-and-drop success rate of 70%\non non-stationary objects, which is more than a 2x improvement over\nstate-of-the-art static systems. Our code as well as our experiment and\ndeployment videos are open sourced and can be found on our project website:\nhttps://dynamem.github.io/"
                },
                "authors": [
                    {
                        "name": "Peiqi Liu"
                    },
                    {
                        "name": "Zhanqiu Guo"
                    },
                    {
                        "name": "Mohit Warke"
                    },
                    {
                        "name": "Soumith Chintala"
                    },
                    {
                        "name": "Chris Paxton"
                    },
                    {
                        "name": "Nur Muhammad Mahi Shafiullah"
                    },
                    {
                        "name": "Lerrel Pinto"
                    }
                ],
                "author_detail": {
                    "name": "Lerrel Pinto"
                },
                "author": "Lerrel Pinto",
                "arxiv_comment": "Website: https://dynamem.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05000v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05000v1",
                "updated": "2024-11-07T18:59:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    59,
                    27,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T18:59:27Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    59,
                    27,
                    3,
                    312,
                    0
                ],
                "title": "Needle Threading: Can LLMs Follow Threads through Near-Million-Scale\n  Haystacks?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Needle Threading: Can LLMs Follow Threads through Near-Million-Scale\n  Haystacks?"
                },
                "summary": "As the context limits of Large Language Models (LLMs) increase, the range of\npossible applications and downstream functions broadens. In many real-world\ntasks, decisions depend on details scattered across collections of often\ndisparate documents containing mostly irrelevant information. Long-context LLMs\nappear well-suited to this form of complex information retrieval and reasoning,\nwhich has traditionally proven costly and time-consuming. However, although the\ndevelopment of longer context models has seen rapid gains in recent years, our\nunderstanding of how effectively LLMs use their context has not kept pace. To\naddress this, we conduct a set of retrieval experiments designed to evaluate\nthe capabilities of 17 leading LLMs, such as their ability to follow threads of\ninformation through the context window. Strikingly, we find that many models\nare remarkably threadsafe: capable of simultaneously following multiple threads\nwithout significant loss in performance. Still, for many models, we find the\neffective context limit is significantly shorter than the supported context\nlength, with accuracy decreasing as the context window grows. Our study also\nhighlights the important point that token counts from different tokenizers\nshould not be directly compared -- they often correspond to substantially\ndifferent numbers of written characters. We release our code and long-context\nexperimental data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the context limits of Large Language Models (LLMs) increase, the range of\npossible applications and downstream functions broadens. In many real-world\ntasks, decisions depend on details scattered across collections of often\ndisparate documents containing mostly irrelevant information. Long-context LLMs\nappear well-suited to this form of complex information retrieval and reasoning,\nwhich has traditionally proven costly and time-consuming. However, although the\ndevelopment of longer context models has seen rapid gains in recent years, our\nunderstanding of how effectively LLMs use their context has not kept pace. To\naddress this, we conduct a set of retrieval experiments designed to evaluate\nthe capabilities of 17 leading LLMs, such as their ability to follow threads of\ninformation through the context window. Strikingly, we find that many models\nare remarkably threadsafe: capable of simultaneously following multiple threads\nwithout significant loss in performance. Still, for many models, we find the\neffective context limit is significantly shorter than the supported context\nlength, with accuracy decreasing as the context window grows. Our study also\nhighlights the important point that token counts from different tokenizers\nshould not be directly compared -- they often correspond to substantially\ndifferent numbers of written characters. We release our code and long-context\nexperimental data."
                },
                "authors": [
                    {
                        "name": "Jonathan Roberts"
                    },
                    {
                        "name": "Kai Han"
                    },
                    {
                        "name": "Samuel Albanie"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Albanie"
                },
                "author": "Samuel Albanie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05000v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05000v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04997v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04997v1",
                "updated": "2024-11-07T18:59:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    59,
                    16,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T18:59:16Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    59,
                    16,
                    3,
                    312,
                    0
                ],
                "title": "LLM2CLIP: Powerful Language Model Unlock Richer Visual Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM2CLIP: Powerful Language Model Unlock Richer Visual Representation"
                },
                "summary": "CLIP is one of the most important multimodal foundational models today. What\npowers CLIP's capabilities? The rich supervision signals provided by natural\nlanguage, the carrier of human knowledge, shape a powerful cross-modal\nrepresentation space. However, with the rapid advancements in large language\nmodels LLMs like GPT-4 and LLaMA, the boundaries of language comprehension and\ngeneration are continually being pushed. This raises an intriguing question:\ncan the capabilities of LLMs be harnessed to further improve multimodal\nrepresentation learning? The potential benefits of incorporating LLMs into CLIP\nare clear. LLMs' strong textual understanding can fundamentally improve CLIP's\nability to handle image captions, drastically enhancing its ability to process\nlong and complex texts, a well-known limitation of vanilla CLIP. Moreover, LLMs\nare trained on a vast corpus of text, possessing open-world knowledge. This\nallows them to expand on caption information during training, increasing the\nefficiency of the learning process. In this paper, we propose LLM2CLIP, a novel\napproach that embraces the power of LLMs to unlock CLIP's potential. By\nfine-tuning the LLM in the caption space with contrastive learning, we extract\nits textual capabilities into the output embeddings, significantly improving\nthe output layer's textual discriminability. We then design an efficient\ntraining process where the fine-tuned LLM acts as a powerful teacher for CLIP's\nvisual encoder. Thanks to the LLM's presence, we can now incorporate longer and\nmore complex captions without being restricted by vanilla CLIP's text encoder's\ncontext window and ability limitations. Our experiments demonstrate that this\napproach brings substantial improvements in cross-modal tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLIP is one of the most important multimodal foundational models today. What\npowers CLIP's capabilities? The rich supervision signals provided by natural\nlanguage, the carrier of human knowledge, shape a powerful cross-modal\nrepresentation space. However, with the rapid advancements in large language\nmodels LLMs like GPT-4 and LLaMA, the boundaries of language comprehension and\ngeneration are continually being pushed. This raises an intriguing question:\ncan the capabilities of LLMs be harnessed to further improve multimodal\nrepresentation learning? The potential benefits of incorporating LLMs into CLIP\nare clear. LLMs' strong textual understanding can fundamentally improve CLIP's\nability to handle image captions, drastically enhancing its ability to process\nlong and complex texts, a well-known limitation of vanilla CLIP. Moreover, LLMs\nare trained on a vast corpus of text, possessing open-world knowledge. This\nallows them to expand on caption information during training, increasing the\nefficiency of the learning process. In this paper, we propose LLM2CLIP, a novel\napproach that embraces the power of LLMs to unlock CLIP's potential. By\nfine-tuning the LLM in the caption space with contrastive learning, we extract\nits textual capabilities into the output embeddings, significantly improving\nthe output layer's textual discriminability. We then design an efficient\ntraining process where the fine-tuned LLM acts as a powerful teacher for CLIP's\nvisual encoder. Thanks to the LLM's presence, we can now incorporate longer and\nmore complex captions without being restricted by vanilla CLIP's text encoder's\ncontext window and ability limitations. Our experiments demonstrate that this\napproach brings substantial improvements in cross-modal tasks."
                },
                "authors": [
                    {
                        "name": "Weiquan Huang"
                    },
                    {
                        "name": "Aoqi Wu"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Liang Hu"
                    },
                    {
                        "name": "Qi Dai"
                    },
                    {
                        "name": "Xiyang Dai"
                    },
                    {
                        "name": "Dongdong Chen"
                    },
                    {
                        "name": "Chong Luo"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04997v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04997v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04996v1",
                "updated": "2024-11-07T18:59:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    59,
                    6,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T18:59:06Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    59,
                    6,
                    3,
                    312,
                    0
                ],
                "title": "Mixture-of-Transformers: A Sparse and Scalable Architecture for\n  Multi-Modal Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Transformers: A Sparse and Scalable Architecture for\n  Multi-Modal Foundation Models"
                },
                "summary": "The development of large language models (LLMs) has expanded to multi-modal\nsystems capable of processing text, images, and speech within a unified\nframework. Training these models demands significantly larger datasets and\ncomputational resources compared to text-only LLMs. To address the scaling\nchallenges, we introduce Mixture-of-Transformers (MoT), a sparse multi-modal\ntransformer architecture that significantly reduces pretraining computational\ncosts. MoT decouples non-embedding parameters of the model by modality --\nincluding feed-forward networks, attention matrices, and layer normalization --\nenabling modality-specific processing with global self-attention over the full\ninput sequence. We evaluate MoT across multiple settings and model scales. In\nthe Chameleon 7B setting (autoregressive text-and-image generation), MoT\nmatches the dense baseline's performance using only 55.8\\% of the FLOPs. When\nextended to include speech, MoT reaches speech performance comparable to the\ndense baseline with only 37.2\\% of the FLOPs. In the Transfusion setting, where\ntext and image are trained with different objectives, a 7B MoT model matches\nthe image modality performance of the dense baseline with one third of the\nFLOPs, and a 760M MoT model outperforms a 1.4B dense baseline across key image\ngeneration metrics. System profiling further highlights MoT's practical\nbenefits, achieving dense baseline image quality in 47.2\\% of the wall-clock\ntime and text quality in 75.6\\% of the wall-clock time (measured on AWS\np4de.24xlarge instances with NVIDIA A100 GPUs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of large language models (LLMs) has expanded to multi-modal\nsystems capable of processing text, images, and speech within a unified\nframework. Training these models demands significantly larger datasets and\ncomputational resources compared to text-only LLMs. To address the scaling\nchallenges, we introduce Mixture-of-Transformers (MoT), a sparse multi-modal\ntransformer architecture that significantly reduces pretraining computational\ncosts. MoT decouples non-embedding parameters of the model by modality --\nincluding feed-forward networks, attention matrices, and layer normalization --\nenabling modality-specific processing with global self-attention over the full\ninput sequence. We evaluate MoT across multiple settings and model scales. In\nthe Chameleon 7B setting (autoregressive text-and-image generation), MoT\nmatches the dense baseline's performance using only 55.8\\% of the FLOPs. When\nextended to include speech, MoT reaches speech performance comparable to the\ndense baseline with only 37.2\\% of the FLOPs. In the Transfusion setting, where\ntext and image are trained with different objectives, a 7B MoT model matches\nthe image modality performance of the dense baseline with one third of the\nFLOPs, and a 760M MoT model outperforms a 1.4B dense baseline across key image\ngeneration metrics. System profiling further highlights MoT's practical\nbenefits, achieving dense baseline image quality in 47.2\\% of the wall-clock\ntime and text quality in 75.6\\% of the wall-clock time (measured on AWS\np4de.24xlarge instances with NVIDIA A100 GPUs)."
                },
                "authors": [
                    {
                        "name": "Weixin Liang"
                    },
                    {
                        "name": "Lili Yu"
                    },
                    {
                        "name": "Liang Luo"
                    },
                    {
                        "name": "Srinivasan Iyer"
                    },
                    {
                        "name": "Ning Dong"
                    },
                    {
                        "name": "Chunting Zhou"
                    },
                    {
                        "name": "Gargi Ghosh"
                    },
                    {
                        "name": "Mike Lewis"
                    },
                    {
                        "name": "Wen-tau Yih"
                    },
                    {
                        "name": "Luke Zettlemoyer"
                    },
                    {
                        "name": "Xi Victoria Lin"
                    }
                ],
                "author_detail": {
                    "name": "Xi Victoria Lin"
                },
                "author": "Xi Victoria Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21723v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21723v2",
                "updated": "2024-11-07T18:57:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    57,
                    27,
                    3,
                    312,
                    0
                ],
                "published": "2024-10-29T04:22:28Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    4,
                    22,
                    28,
                    1,
                    303,
                    0
                ],
                "title": "Fine-tuning Large Language Models for DGA and DNS Exfiltration Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning Large Language Models for DGA and DNS Exfiltration Detection"
                },
                "summary": "Domain Generation Algorithms (DGAs) are malicious techniques used by malware\nto dynamically generate seemingly random domain names for communication with\nCommand & Control (C&C) servers. Due to the fast and simple generation of DGA\ndomains, detection methods must be highly efficient and precise to be\neffective. Large Language Models (LLMs) have demonstrated their proficiency in\nreal-time detection tasks, making them ideal candidates for detecting DGAs. Our\nwork validates the effectiveness of fine-tuned LLMs for detecting DGAs and DNS\nexfiltration attacks. We developed LLM models and conducted comprehensive\nevaluation using a diverse dataset comprising 59 distinct real-world DGA\nmalware families and normal domain data. Our LLM model significantly\noutperformed traditional natural language processing techniques, especially in\ndetecting unknown DGAs. We also evaluated its performance on DNS exfiltration\ndatasets, demonstrating its effectiveness in enhancing cybersecurity measures.\nTo the best of our knowledge, this is the first work that empirically applies\nLLMs for DGA and DNS exfiltration detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain Generation Algorithms (DGAs) are malicious techniques used by malware\nto dynamically generate seemingly random domain names for communication with\nCommand & Control (C&C) servers. Due to the fast and simple generation of DGA\ndomains, detection methods must be highly efficient and precise to be\neffective. Large Language Models (LLMs) have demonstrated their proficiency in\nreal-time detection tasks, making them ideal candidates for detecting DGAs. Our\nwork validates the effectiveness of fine-tuned LLMs for detecting DGAs and DNS\nexfiltration attacks. We developed LLM models and conducted comprehensive\nevaluation using a diverse dataset comprising 59 distinct real-world DGA\nmalware families and normal domain data. Our LLM model significantly\noutperformed traditional natural language processing techniques, especially in\ndetecting unknown DGAs. We also evaluated its performance on DNS exfiltration\ndatasets, demonstrating its effectiveness in enhancing cybersecurity measures.\nTo the best of our knowledge, this is the first work that empirically applies\nLLMs for DGA and DNS exfiltration detection."
                },
                "authors": [
                    {
                        "name": "Md Abu Sayed"
                    },
                    {
                        "name": "Asif Rahman"
                    },
                    {
                        "name": "Christopher Kiekintveld"
                    },
                    {
                        "name": "Sebastian Garcia"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Garcia"
                },
                "author": "Sebastian Garcia",
                "arxiv_comment": "Accepted in Proceedings of the Workshop at AI for Cyber Threat\n  Intelligence (WAITI), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21723v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21723v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04991v1",
                "updated": "2024-11-07T18:57:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    57,
                    3,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T18:57:03Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    57,
                    3,
                    3,
                    312,
                    0
                ],
                "title": "Rethinking Bradley-Terry Models in Preference-Based Reward Modeling:\n  Foundations, Theory, and Alternatives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Bradley-Terry Models in Preference-Based Reward Modeling:\n  Foundations, Theory, and Alternatives"
                },
                "summary": "The Bradley-Terry (BT) model is a common and successful practice in reward\nmodeling for Large Language Model (LLM) alignment. However, it remains unclear\nwhy this model -- originally developed for multi-player stochastic game\nmatching -- can be adopted to convert pairwise response comparisons to reward\nvalues and make predictions. Especially given the fact that only a limited\nnumber of prompt-response pairs are sparsely compared with others. In this\npaper, we first revisit the foundations of using BT models in reward modeling,\nand establish the convergence rate of BT reward models based on deep neural\nnetworks using embeddings, providing a theoretical foundation for their use.\nDespite theoretically sound, we argue that the BT model is not a necessary\nchoice from the perspective of downstream optimization. This is because a\nreward model only needs to preserve the correct ranking predictions through a\nmonotonic transformation of the true reward. We highlight the critical concept\nof order consistency in reward modeling and demonstrate that the BT model\npossesses this property. Consequently, we propose a simple and straightforward\nupper-bound algorithm, compatible with off-the-shelf binary classifiers, as an\nalternative order-consistent reward modeling objective. To offer practical\ninsights, we empirically evaluate the performance of these different reward\nmodeling approaches across more than 12,000 experimental setups, using $6$ base\nLLMs, $2$ datasets, and diverse annotation designs that vary in quantity,\nquality, and pairing choices in preference annotations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bradley-Terry (BT) model is a common and successful practice in reward\nmodeling for Large Language Model (LLM) alignment. However, it remains unclear\nwhy this model -- originally developed for multi-player stochastic game\nmatching -- can be adopted to convert pairwise response comparisons to reward\nvalues and make predictions. Especially given the fact that only a limited\nnumber of prompt-response pairs are sparsely compared with others. In this\npaper, we first revisit the foundations of using BT models in reward modeling,\nand establish the convergence rate of BT reward models based on deep neural\nnetworks using embeddings, providing a theoretical foundation for their use.\nDespite theoretically sound, we argue that the BT model is not a necessary\nchoice from the perspective of downstream optimization. This is because a\nreward model only needs to preserve the correct ranking predictions through a\nmonotonic transformation of the true reward. We highlight the critical concept\nof order consistency in reward modeling and demonstrate that the BT model\npossesses this property. Consequently, we propose a simple and straightforward\nupper-bound algorithm, compatible with off-the-shelf binary classifiers, as an\nalternative order-consistent reward modeling objective. To offer practical\ninsights, we empirically evaluate the performance of these different reward\nmodeling approaches across more than 12,000 experimental setups, using $6$ base\nLLMs, $2$ datasets, and diverse annotation designs that vary in quantity,\nquality, and pairing choices in preference annotations."
                },
                "authors": [
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Yunyi Shen"
                    },
                    {
                        "name": "Jean-Francois Ton"
                    }
                ],
                "author_detail": {
                    "name": "Jean-Francois Ton"
                },
                "author": "Jean-Francois Ton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04981v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04981v1",
                "updated": "2024-11-07T18:54:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    54,
                    31,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T18:54:31Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    54,
                    31,
                    3,
                    312,
                    0
                ],
                "title": "Enhancing Reverse Engineering: Investigating and Benchmarking Large\n  Language Models for Vulnerability Analysis in Decompiled Binaries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Reverse Engineering: Investigating and Benchmarking Large\n  Language Models for Vulnerability Analysis in Decompiled Binaries"
                },
                "summary": "Security experts reverse engineer (decompile) binary code to identify\ncritical security vulnerabilities. The limited access to source code in vital\nsystems - such as firmware, drivers, and proprietary software used in Critical\nInfrastructures (CI) - makes this analysis even more crucial on the binary\nlevel. Even with available source code, a semantic gap persists after\ncompilation between the source and the binary code executed by the processor.\nThis gap may hinder the detection of vulnerabilities in source code. That being\nsaid, current research on Large Language Models (LLMs) overlooks the\nsignificance of decompiled binaries in this area by focusing solely on source\ncode. In this work, we are the first to empirically uncover the substantial\nsemantic limitations of state-of-the-art LLMs when it comes to analyzing\nvulnerabilities in decompiled binaries, largely due to the absence of relevant\ndatasets. To bridge the gap, we introduce DeBinVul, a novel decompiled binary\ncode vulnerability dataset. Our dataset is multi-architecture and\nmulti-optimization, focusing on C/C++ due to their wide usage in CI and\nassociation with numerous vulnerabilities. Specifically, we curate 150,872\nsamples of vulnerable and non-vulnerable decompiled binary code for the task of\n(i) identifying; (ii) classifying; (iii) describing vulnerabilities; and (iv)\nrecovering function names in the domain of decompiled binaries. Subsequently,\nwe fine-tune state-of-the-art LLMs using DeBinVul and report on a performance\nincrease of 19%, 24%, and 21% in the capabilities of CodeLlama, Llama3, and\nCodeGen2 respectively, in detecting binary code vulnerabilities. Additionally,\nusing DeBinVul, we report a high performance of 80-90% on the vulnerability\nclassification task. Furthermore, we report improved performance in function\nname recovery and vulnerability description tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Security experts reverse engineer (decompile) binary code to identify\ncritical security vulnerabilities. The limited access to source code in vital\nsystems - such as firmware, drivers, and proprietary software used in Critical\nInfrastructures (CI) - makes this analysis even more crucial on the binary\nlevel. Even with available source code, a semantic gap persists after\ncompilation between the source and the binary code executed by the processor.\nThis gap may hinder the detection of vulnerabilities in source code. That being\nsaid, current research on Large Language Models (LLMs) overlooks the\nsignificance of decompiled binaries in this area by focusing solely on source\ncode. In this work, we are the first to empirically uncover the substantial\nsemantic limitations of state-of-the-art LLMs when it comes to analyzing\nvulnerabilities in decompiled binaries, largely due to the absence of relevant\ndatasets. To bridge the gap, we introduce DeBinVul, a novel decompiled binary\ncode vulnerability dataset. Our dataset is multi-architecture and\nmulti-optimization, focusing on C/C++ due to their wide usage in CI and\nassociation with numerous vulnerabilities. Specifically, we curate 150,872\nsamples of vulnerable and non-vulnerable decompiled binary code for the task of\n(i) identifying; (ii) classifying; (iii) describing vulnerabilities; and (iv)\nrecovering function names in the domain of decompiled binaries. Subsequently,\nwe fine-tune state-of-the-art LLMs using DeBinVul and report on a performance\nincrease of 19%, 24%, and 21% in the capabilities of CodeLlama, Llama3, and\nCodeGen2 respectively, in detecting binary code vulnerabilities. Additionally,\nusing DeBinVul, we report a high performance of 80-90% on the vulnerability\nclassification task. Furthermore, we report improved performance in function\nname recovery and vulnerability description tasks."
                },
                "authors": [
                    {
                        "name": "Dylan Manuel"
                    },
                    {
                        "name": "Nafis Tanveer Islam"
                    },
                    {
                        "name": "Joseph Khoury"
                    },
                    {
                        "name": "Ana Nunez"
                    },
                    {
                        "name": "Elias Bou-Harb"
                    },
                    {
                        "name": "Peyman Najafirad"
                    }
                ],
                "author_detail": {
                    "name": "Peyman Najafirad"
                },
                "author": "Peyman Najafirad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04981v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04981v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04975v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04975v1",
                "updated": "2024-11-07T18:49:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    49,
                    33,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T18:49:33Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    49,
                    33,
                    3,
                    312,
                    0
                ],
                "title": "SuffixDecoding: A Model-Free Approach to Speeding Up Large Language\n  Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuffixDecoding: A Model-Free Approach to Speeding Up Large Language\n  Model Inference"
                },
                "summary": "We present SuffixDecoding, a novel model-free approach to accelerating large\nlanguage model (LLM) inference through speculative decoding. Unlike existing\nmethods that rely on draft models or specialized decoding heads, SuffixDecoding\nleverages suffix trees built from previously generated outputs to efficiently\npredict candidate token sequences. Our approach enables flexible\ntree-structured speculation without the overhead of maintaining and\norchestrating additional models. SuffixDecoding builds and dynamically updates\nsuffix trees to capture patterns in the generated text, using them to construct\nspeculation trees through a principled scoring mechanism based on empirical\ntoken frequencies. SuffixDecoding requires only CPU memory which is plentiful\nand underutilized on typical LLM serving nodes. We demonstrate that\nSuffixDecoding achieves competitive speedups compared to model-based approaches\nacross diverse workloads including open-domain chat, code generation, and\ntext-to-SQL tasks. For open-ended chat and code generation tasks,\nSuffixDecoding achieves up to $1.4\\times$ higher output throughput than\nSpecInfer and up to $1.1\\times$ lower time-per-token (TPOT) latency. For a\nproprietary multi-LLM text-to-SQL application, SuffixDecoding achieves up to\n$2.9\\times$ higher output throughput and $3\\times$ lower latency than\nspeculative decoding. Our evaluation shows that SuffixDecoding maintains high\nacceptance rates even with small reference corpora of 256 examples, while\ncontinuing to improve performance as more historical outputs are incorporated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present SuffixDecoding, a novel model-free approach to accelerating large\nlanguage model (LLM) inference through speculative decoding. Unlike existing\nmethods that rely on draft models or specialized decoding heads, SuffixDecoding\nleverages suffix trees built from previously generated outputs to efficiently\npredict candidate token sequences. Our approach enables flexible\ntree-structured speculation without the overhead of maintaining and\norchestrating additional models. SuffixDecoding builds and dynamically updates\nsuffix trees to capture patterns in the generated text, using them to construct\nspeculation trees through a principled scoring mechanism based on empirical\ntoken frequencies. SuffixDecoding requires only CPU memory which is plentiful\nand underutilized on typical LLM serving nodes. We demonstrate that\nSuffixDecoding achieves competitive speedups compared to model-based approaches\nacross diverse workloads including open-domain chat, code generation, and\ntext-to-SQL tasks. For open-ended chat and code generation tasks,\nSuffixDecoding achieves up to $1.4\\times$ higher output throughput than\nSpecInfer and up to $1.1\\times$ lower time-per-token (TPOT) latency. For a\nproprietary multi-LLM text-to-SQL application, SuffixDecoding achieves up to\n$2.9\\times$ higher output throughput and $3\\times$ lower latency than\nspeculative decoding. Our evaluation shows that SuffixDecoding maintains high\nacceptance rates even with small reference corpora of 256 examples, while\ncontinuing to improve performance as more historical outputs are incorporated."
                },
                "authors": [
                    {
                        "name": "Gabriele Oliaro"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Daniel Campos"
                    },
                    {
                        "name": "Aurick Qiao"
                    }
                ],
                "author_detail": {
                    "name": "Aurick Qiao"
                },
                "author": "Aurick Qiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04975v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04967v1",
                "updated": "2024-11-07T18:43:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    43,
                    17,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T18:43:17Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    43,
                    17,
                    3,
                    312,
                    0
                ],
                "title": "AsCAN: Asymmetric Convolution-Attention Networks for Efficient\n  Recognition and Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsCAN: Asymmetric Convolution-Attention Networks for Efficient\n  Recognition and Generation"
                },
                "summary": "Neural network architecture design requires making many crucial decisions.\nThe common desiderata is that similar decisions, with little modifications, can\nbe reused in a variety of tasks and applications. To satisfy that,\narchitectures must provide promising latency and performance trade-offs,\nsupport a variety of tasks, scale efficiently with respect to the amounts of\ndata and compute, leverage available data from other tasks, and efficiently\nsupport various hardware. To this end, we introduce AsCAN -- a hybrid\narchitecture, combining both convolutional and transformer blocks. We revisit\nthe key design principles of hybrid architectures and propose a simple and\neffective \\emph{asymmetric} architecture, where the distribution of\nconvolutional and transformer blocks is \\emph{asymmetric}, containing more\nconvolutional blocks in the earlier stages, followed by more transformer blocks\nin later stages. AsCAN supports a variety of tasks: recognition, segmentation,\nclass-conditional image generation, and features a superior trade-off between\nperformance and latency. We then scale the same architecture to solve a\nlarge-scale text-to-image task and show state-of-the-art performance compared\nto the most recent public and commercial models. Notably, even without any\ncomputation optimization for transformer blocks, our models still yield faster\ninference speed than existing works featuring efficient attention mechanisms,\nhighlighting the advantages and the value of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural network architecture design requires making many crucial decisions.\nThe common desiderata is that similar decisions, with little modifications, can\nbe reused in a variety of tasks and applications. To satisfy that,\narchitectures must provide promising latency and performance trade-offs,\nsupport a variety of tasks, scale efficiently with respect to the amounts of\ndata and compute, leverage available data from other tasks, and efficiently\nsupport various hardware. To this end, we introduce AsCAN -- a hybrid\narchitecture, combining both convolutional and transformer blocks. We revisit\nthe key design principles of hybrid architectures and propose a simple and\neffective \\emph{asymmetric} architecture, where the distribution of\nconvolutional and transformer blocks is \\emph{asymmetric}, containing more\nconvolutional blocks in the earlier stages, followed by more transformer blocks\nin later stages. AsCAN supports a variety of tasks: recognition, segmentation,\nclass-conditional image generation, and features a superior trade-off between\nperformance and latency. We then scale the same architecture to solve a\nlarge-scale text-to-image task and show state-of-the-art performance compared\nto the most recent public and commercial models. Notably, even without any\ncomputation optimization for transformer blocks, our models still yield faster\ninference speed than existing works featuring efficient attention mechanisms,\nhighlighting the advantages and the value of our approach."
                },
                "authors": [
                    {
                        "name": "Anil Kag"
                    },
                    {
                        "name": "Huseyin Coskun"
                    },
                    {
                        "name": "Jierun Chen"
                    },
                    {
                        "name": "Junli Cao"
                    },
                    {
                        "name": "Willi Menapace"
                    },
                    {
                        "name": "Aliaksandr Siarohin"
                    },
                    {
                        "name": "Sergey Tulyakov"
                    },
                    {
                        "name": "Jian Ren"
                    }
                ],
                "author_detail": {
                    "name": "Jian Ren"
                },
                "author": "Jian Ren",
                "arxiv_comment": "NeurIPS 2024. Project Page:\n  https://snap-research.github.io/snap_image/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04965v1",
                "updated": "2024-11-07T18:41:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    41,
                    50,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T18:41:50Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    41,
                    50,
                    3,
                    312,
                    0
                ],
                "title": "BitNet a4.8: 4-bit Activations for 1-bit LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitNet a4.8: 4-bit Activations for 1-bit LLMs"
                },
                "summary": "Recent research on the 1-bit Large Language Models (LLMs), such as BitNet\nb1.58, presents a promising direction for reducing the inference cost of LLMs\nwhile maintaining their performance. In this work, we introduce BitNet a4.8,\nenabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid\nquantization and sparsification strategy to mitigate the quantization errors\nintroduced by the outlier channels. Specifically, we utilize 4-bit activations\nfor inputs to the attention and feed-forward network layers, while sparsifying\nintermediate states followed with 8-bit quantization. Extensive experiments\ndemonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58\nwith equivalent training costs, while being faster in inference with enabling\n4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of\nparameters and supports 3-bit KV cache, further enhancing the efficiency of\nlarge-scale LLM deployment and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research on the 1-bit Large Language Models (LLMs), such as BitNet\nb1.58, presents a promising direction for reducing the inference cost of LLMs\nwhile maintaining their performance. In this work, we introduce BitNet a4.8,\nenabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid\nquantization and sparsification strategy to mitigate the quantization errors\nintroduced by the outlier channels. Specifically, we utilize 4-bit activations\nfor inputs to the attention and feed-forward network layers, while sparsifying\nintermediate states followed with 8-bit quantization. Extensive experiments\ndemonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58\nwith equivalent training costs, while being faster in inference with enabling\n4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of\nparameters and supports 3-bit KV cache, further enhancing the efficiency of\nlarge-scale LLM deployment and inference."
                },
                "authors": [
                    {
                        "name": "Hongyu Wang"
                    },
                    {
                        "name": "Shuming Ma"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04962v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04962v1",
                "updated": "2024-11-07T18:39:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    39,
                    4,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T18:39:04Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    39,
                    4,
                    3,
                    312,
                    0
                ],
                "title": "Position Paper On Diagnostic Uncertainty Estimation from Large Language\n  Models: Next-Word Probability Is Not Pre-test Probability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Position Paper On Diagnostic Uncertainty Estimation from Large Language\n  Models: Next-Word Probability Is Not Pre-test Probability"
                },
                "summary": "Large language models (LLMs) are being explored for diagnostic decision\nsupport, yet their ability to estimate pre-test probabilities, vital for\nclinical decision-making, remains limited. This study evaluates two LLMs,\nMistral-7B and Llama3-70B, using structured electronic health record data on\nthree diagnosis tasks. We examined three current methods of extracting LLM\nprobability estimations and revealed their limitations. We aim to highlight the\nneed for improved techniques in LLM confidence estimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are being explored for diagnostic decision\nsupport, yet their ability to estimate pre-test probabilities, vital for\nclinical decision-making, remains limited. This study evaluates two LLMs,\nMistral-7B and Llama3-70B, using structured electronic health record data on\nthree diagnosis tasks. We examined three current methods of extracting LLM\nprobability estimations and revealed their limitations. We aim to highlight the\nneed for improved techniques in LLM confidence estimation."
                },
                "authors": [
                    {
                        "name": "Yanjun Gao"
                    },
                    {
                        "name": "Skatje Myers"
                    },
                    {
                        "name": "Shan Chen"
                    },
                    {
                        "name": "Dmitriy Dligach"
                    },
                    {
                        "name": "Timothy A Miller"
                    },
                    {
                        "name": "Danielle Bitterman"
                    },
                    {
                        "name": "Guanhua Chen"
                    },
                    {
                        "name": "Anoop Mayampurath"
                    },
                    {
                        "name": "Matthew Churpek"
                    },
                    {
                        "name": "Majid Afshar"
                    }
                ],
                "author_detail": {
                    "name": "Majid Afshar"
                },
                "author": "Majid Afshar",
                "arxiv_comment": "Accepted to GenAI4Health Workshop at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04962v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04962v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04957v1",
                "updated": "2024-11-07T18:32:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    32,
                    42,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T18:32:42Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    32,
                    42,
                    3,
                    312,
                    0
                ],
                "title": "Belief propagation for general graphical models with loops",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Belief propagation for general graphical models with loops"
                },
                "summary": "Belief Propagation (BP) decoders for quantum error correcting codes are not\nalways precise. There is a growing interest in the application of tensor\nnetworks to quantum error correction in general and, in particular, in\ndegenerate quantum maximum likelihood decoding and the tensor network decoder.\nWe develop a unified view to make the generalized BP proposal by Kirkley et. al\nexplicit on arbitrary graphical models. We derive BP schemes and provide\ninference equations for BP on loopy tensor networks and, more generally, loopy\ngraphical models. In doing so we introduce a tree-equivalent approach which\nallows us to relate the tensor network BlockBP to a generalized BP for loopy\nnetworks. Moreover, we show that the tensor network message passing approach\nrelies essentially on the same approximation as the method by Kirkley. This\nallows us to make tensor network message passing available for degenerate\nquantum maximum likelihood decoding. Our method and results are key to\nobtaining guidelines regarding how the exchange between complexity and decoding\naccuracy works between BP and tensor network decoders. Finally, we discuss how\nthe tree-equivalent method and the method by Kirkley can justify why message\nscheduling improves the performance of BP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Belief Propagation (BP) decoders for quantum error correcting codes are not\nalways precise. There is a growing interest in the application of tensor\nnetworks to quantum error correction in general and, in particular, in\ndegenerate quantum maximum likelihood decoding and the tensor network decoder.\nWe develop a unified view to make the generalized BP proposal by Kirkley et. al\nexplicit on arbitrary graphical models. We derive BP schemes and provide\ninference equations for BP on loopy tensor networks and, more generally, loopy\ngraphical models. In doing so we introduce a tree-equivalent approach which\nallows us to relate the tensor network BlockBP to a generalized BP for loopy\nnetworks. Moreover, we show that the tensor network message passing approach\nrelies essentially on the same approximation as the method by Kirkley. This\nallows us to make tensor network message passing available for degenerate\nquantum maximum likelihood decoding. Our method and results are key to\nobtaining guidelines regarding how the exchange between complexity and decoding\naccuracy works between BP and tensor network decoders. Finally, we discuss how\nthe tree-equivalent method and the method by Kirkley can justify why message\nscheduling improves the performance of BP."
                },
                "authors": [
                    {
                        "name": "Pedro Hack"
                    },
                    {
                        "name": "Christian B. Mendl"
                    },
                    {
                        "name": "Alexandru Paler"
                    }
                ],
                "author_detail": {
                    "name": "Alexandru Paler"
                },
                "author": "Alexandru Paler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04954v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04954v1",
                "updated": "2024-11-07T18:31:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    31,
                    8,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T18:31:08Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    31,
                    8,
                    3,
                    312,
                    0
                ],
                "title": "CAD-MLLM: Unifying Multimodality-Conditioned CAD Generation With MLLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAD-MLLM: Unifying Multimodality-Conditioned CAD Generation With MLLM"
                },
                "summary": "This paper aims to design a unified Computer-Aided Design (CAD) generation\nsystem that can easily generate CAD models based on the user's inputs in the\nform of textual description, images, point clouds, or even a combination of\nthem. Towards this goal, we introduce the CAD-MLLM, the first system capable of\ngenerating parametric CAD models conditioned on the multimodal input.\nSpecifically, within the CAD-MLLM framework, we leverage the command sequences\nof CAD models and then employ advanced large language models (LLMs) to align\nthe feature space across these diverse multi-modalities data and CAD models'\nvectorized representations. To facilitate the model training, we design a\ncomprehensive data construction and annotation pipeline that equips each CAD\nmodel with corresponding multimodal data. Our resulting dataset, named\nOmni-CAD, is the first multimodal CAD dataset that contains textual\ndescription, multi-view images, points, and command sequence for each CAD\nmodel. It contains approximately 450K instances and their CAD construction\nsequences. To thoroughly evaluate the quality of our generated CAD models, we\ngo beyond current evaluation metrics that focus on reconstruction quality by\nintroducing additional metrics that assess topology quality and surface\nenclosure extent. Extensive experimental results demonstrate that CAD-MLLM\nsignificantly outperforms existing conditional generative methods and remains\nhighly robust to noises and missing points. The project page and more\nvisualizations can be found at: https://cad-mllm.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper aims to design a unified Computer-Aided Design (CAD) generation\nsystem that can easily generate CAD models based on the user's inputs in the\nform of textual description, images, point clouds, or even a combination of\nthem. Towards this goal, we introduce the CAD-MLLM, the first system capable of\ngenerating parametric CAD models conditioned on the multimodal input.\nSpecifically, within the CAD-MLLM framework, we leverage the command sequences\nof CAD models and then employ advanced large language models (LLMs) to align\nthe feature space across these diverse multi-modalities data and CAD models'\nvectorized representations. To facilitate the model training, we design a\ncomprehensive data construction and annotation pipeline that equips each CAD\nmodel with corresponding multimodal data. Our resulting dataset, named\nOmni-CAD, is the first multimodal CAD dataset that contains textual\ndescription, multi-view images, points, and command sequence for each CAD\nmodel. It contains approximately 450K instances and their CAD construction\nsequences. To thoroughly evaluate the quality of our generated CAD models, we\ngo beyond current evaluation metrics that focus on reconstruction quality by\nintroducing additional metrics that assess topology quality and surface\nenclosure extent. Extensive experimental results demonstrate that CAD-MLLM\nsignificantly outperforms existing conditional generative methods and remains\nhighly robust to noises and missing points. The project page and more\nvisualizations can be found at: https://cad-mllm.github.io/"
                },
                "authors": [
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Chenyu Wang"
                    },
                    {
                        "name": "Zibo Zhao"
                    },
                    {
                        "name": "Wen Liu"
                    },
                    {
                        "name": "Yi Ma"
                    },
                    {
                        "name": "Shenghua Gao"
                    }
                ],
                "author_detail": {
                    "name": "Shenghua Gao"
                },
                "author": "Shenghua Gao",
                "arxiv_comment": "Project page: https://cad-mllm.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04954v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02472v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02472v3",
                "updated": "2024-11-07T18:30:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    30,
                    38,
                    3,
                    312,
                    0
                ],
                "published": "2024-10-03T13:25:15Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    13,
                    25,
                    15,
                    3,
                    277,
                    0
                ],
                "title": "Meta-Models: An Architecture for Decoding LLM Behaviors Through\n  Interpreted Embeddings and Natural Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meta-Models: An Architecture for Decoding LLM Behaviors Through\n  Interpreted Embeddings and Natural Language"
                },
                "summary": "As Large Language Models (LLMs) become increasingly integrated into our daily\nlives, the potential harms from deceptive behavior underlie the need for\nfaithfully interpreting their decision-making. While traditional probing\nmethods have shown some effectiveness, they remain best for narrowly scoped\ntasks while more comprehensive explanations are still necessary. To this end,\nwe investigate meta-models-an architecture using a \"meta-model\" that takes\nactivations from an \"input-model\" and answers natural language questions about\nthe input-model's behaviors. We evaluate the meta-model's ability to generalize\nby training them on selected task types and assessing their out-of-distribution\nperformance in deceptive scenarios. Our findings show that meta-models\ngeneralize well to out-of-distribution tasks and point towards opportunities\nfor future research in this area. Our code is available at\nhttps://github.com/acostarelli/meta-models-public .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) become increasingly integrated into our daily\nlives, the potential harms from deceptive behavior underlie the need for\nfaithfully interpreting their decision-making. While traditional probing\nmethods have shown some effectiveness, they remain best for narrowly scoped\ntasks while more comprehensive explanations are still necessary. To this end,\nwe investigate meta-models-an architecture using a \"meta-model\" that takes\nactivations from an \"input-model\" and answers natural language questions about\nthe input-model's behaviors. We evaluate the meta-model's ability to generalize\nby training them on selected task types and assessing their out-of-distribution\nperformance in deceptive scenarios. Our findings show that meta-models\ngeneralize well to out-of-distribution tasks and point towards opportunities\nfor future research in this area. Our code is available at\nhttps://github.com/acostarelli/meta-models-public ."
                },
                "authors": [
                    {
                        "name": "Anthony Costarelli"
                    },
                    {
                        "name": "Mat Allen"
                    },
                    {
                        "name": "Severin Field"
                    }
                ],
                "author_detail": {
                    "name": "Severin Field"
                },
                "author": "Severin Field",
                "arxiv_comment": "11 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02472v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02472v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04944v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04944v1",
                "updated": "2024-11-07T18:21:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    21,
                    17,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T18:21:17Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    21,
                    17,
                    3,
                    312,
                    0
                ],
                "title": "Galaxy Mergers in the Epoch of Reionization II: Major Merger-Triggered\n  Star Formation and AGN Activities at $z = 4.5 - 8.5$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Galaxy Mergers in the Epoch of Reionization II: Major Merger-Triggered\n  Star Formation and AGN Activities at $z = 4.5 - 8.5$"
                },
                "summary": "Galaxy mergers are a key driver of galaxy formation and evolution, including\nthe triggering of AGN and star formation to a still unknown degree. We thus\ninvestigate the impact of galaxy mergers on star formation and AGN activity\nusing a sample of 3,330 galaxies at $z = [4.5, 8.5]$ from eight JWST fields\n(CEERS, JADES GOODS-S, NEP-TDF, NGDEEP, GLASS, El-Gordo, SMACS-0723, and\nMACS-0416), collectively covering an unmasked area of 189 arcmin$^2$. We\nfocuses on star formation rate (SFR) enhancement, AGN fraction, and AGN excess\nin major merger ($\\mu > 1/4$) close-pair samples, defined by $\\Delta z < 0.3$\nand projected separations $r_p < 100$ kpc, compared to non-merger samples. We\nfind that SFR enhancement occurs only at $r_p < 20$ kpc, with values of $0.25\n\\pm 0.10$ dex and $0.26 \\pm 0.11$ dex above the non-merger medians for $z =\n[4.5, 6.5]$ and $z = [6.5, 8.5]$, respectively. No other statistically\nsignificant enhancements in galaxy sSFR or stellar mass are observed at any\nprojected separation or redshift bin. We also compare our observational results\nwith predictions from the SC-SAM simulation and find no evidence of star\nformation enhancement in the simulations at any separation range. Finally, we\nexamine the AGN fraction and AGN excess, finding that the fraction of AGNs in\nAGN-galaxy pairs, relative to the total AGN population, is\n$3.25^{+1.50}_{-1.06}$ times greater than the fraction of galaxy pairs relative\nto the overall galaxy population at the same redshift. We find that nearly all\nAGNs have a companion within 100 kpc and observe an excess AGN fraction in\nclose-pair samples compared to non-merger samples. This excess is found to be\n$1.26 \\pm 0.06$ and $1.34 \\pm 0.06$ for AGNs identified via the inferred BPT\ndiagram and photometric SED selection, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Galaxy mergers are a key driver of galaxy formation and evolution, including\nthe triggering of AGN and star formation to a still unknown degree. We thus\ninvestigate the impact of galaxy mergers on star formation and AGN activity\nusing a sample of 3,330 galaxies at $z = [4.5, 8.5]$ from eight JWST fields\n(CEERS, JADES GOODS-S, NEP-TDF, NGDEEP, GLASS, El-Gordo, SMACS-0723, and\nMACS-0416), collectively covering an unmasked area of 189 arcmin$^2$. We\nfocuses on star formation rate (SFR) enhancement, AGN fraction, and AGN excess\nin major merger ($\\mu > 1/4$) close-pair samples, defined by $\\Delta z < 0.3$\nand projected separations $r_p < 100$ kpc, compared to non-merger samples. We\nfind that SFR enhancement occurs only at $r_p < 20$ kpc, with values of $0.25\n\\pm 0.10$ dex and $0.26 \\pm 0.11$ dex above the non-merger medians for $z =\n[4.5, 6.5]$ and $z = [6.5, 8.5]$, respectively. No other statistically\nsignificant enhancements in galaxy sSFR or stellar mass are observed at any\nprojected separation or redshift bin. We also compare our observational results\nwith predictions from the SC-SAM simulation and find no evidence of star\nformation enhancement in the simulations at any separation range. Finally, we\nexamine the AGN fraction and AGN excess, finding that the fraction of AGNs in\nAGN-galaxy pairs, relative to the total AGN population, is\n$3.25^{+1.50}_{-1.06}$ times greater than the fraction of galaxy pairs relative\nto the overall galaxy population at the same redshift. We find that nearly all\nAGNs have a companion within 100 kpc and observe an excess AGN fraction in\nclose-pair samples compared to non-merger samples. This excess is found to be\n$1.26 \\pm 0.06$ and $1.34 \\pm 0.06$ for AGNs identified via the inferred BPT\ndiagram and photometric SED selection, respectively."
                },
                "authors": [
                    {
                        "name": "Qiao Duan"
                    },
                    {
                        "name": "Qiong Li"
                    },
                    {
                        "name": "Christopher J. Conselice"
                    },
                    {
                        "name": "Thomas Harvey"
                    },
                    {
                        "name": "Duncan Austin"
                    },
                    {
                        "name": "Nathan J. Adams"
                    },
                    {
                        "name": "Leonardo Ferreira"
                    },
                    {
                        "name": "Kenneth J. Duncan"
                    },
                    {
                        "name": "James Trussler"
                    },
                    {
                        "name": "Robert G. Pascalau"
                    },
                    {
                        "name": "Rogier A. Windhorst"
                    },
                    {
                        "name": "Benne W. Holwerda"
                    },
                    {
                        "name": "Thomas J. Broadhurst"
                    },
                    {
                        "name": "Dan Coe"
                    },
                    {
                        "name": "Seth H. Cohen"
                    },
                    {
                        "name": "Xiaojing Du"
                    },
                    {
                        "name": "Simon P. Driver"
                    },
                    {
                        "name": "Brenda Frye"
                    },
                    {
                        "name": "Norman A. Grogin"
                    },
                    {
                        "name": "Nimish P. Hathi"
                    },
                    {
                        "name": "Rolf A. Jansen"
                    },
                    {
                        "name": "Anton M. Koekemoer"
                    },
                    {
                        "name": "Madeline A. Marshall"
                    },
                    {
                        "name": "Mario Nonino"
                    },
                    {
                        "name": "Rafael Ortiz III"
                    },
                    {
                        "name": "Nor Pirzkal"
                    },
                    {
                        "name": "Aaron Robotham"
                    },
                    {
                        "name": "Russell E. Ryan Jr"
                    },
                    {
                        "name": "Jake Summers"
                    },
                    {
                        "name": "Jordan C. J. D'Silva"
                    },
                    {
                        "name": "Christopher N. A. Willmer"
                    },
                    {
                        "name": "Haojing Yan"
                    }
                ],
                "author_detail": {
                    "name": "Haojing Yan"
                },
                "author": "Haojing Yan",
                "arxiv_comment": "17 Pages, 7 Figures, Submitted to MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04944v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04944v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14894v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14894v2",
                "updated": "2024-11-07T18:15:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    15,
                    23,
                    3,
                    312,
                    0
                ],
                "published": "2024-06-21T06:30:16Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    6,
                    30,
                    16,
                    4,
                    173,
                    0
                ],
                "title": "Talking the Talk Does Not Entail Walking the Walk: On the Limits of\n  Large Language Models in Lexical Entailment Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Talking the Talk Does Not Entail Walking the Walk: On the Limits of\n  Large Language Models in Lexical Entailment Recognition"
                },
                "summary": "Verbs form the backbone of language, providing the structure and meaning to\nsentences. Yet, their intricate semantic nuances pose a longstanding challenge.\nUnderstanding verb relations through the concept of lexical entailment is\ncrucial for comprehending sentence meanings and grasping verb dynamics. This\nwork investigates the capabilities of eight Large Language Models in\nrecognizing lexical entailment relations among verbs through differently\ndevised prompting strategies and zero-/few-shot settings over verb pairs from\ntwo lexical databases, namely WordNet and HyperLex. Our findings unveil that\nthe models can tackle the lexical entailment recognition task with moderately\ngood performance, although at varying degree of effectiveness and under\ndifferent conditions. Also, utilizing few-shot prompting can enhance the\nmodels' performance. However, perfectly solving the task arises as an unmet\nchallenge for all examined LLMs, which raises an emergence for further research\ndevelopments on this topic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verbs form the backbone of language, providing the structure and meaning to\nsentences. Yet, their intricate semantic nuances pose a longstanding challenge.\nUnderstanding verb relations through the concept of lexical entailment is\ncrucial for comprehending sentence meanings and grasping verb dynamics. This\nwork investigates the capabilities of eight Large Language Models in\nrecognizing lexical entailment relations among verbs through differently\ndevised prompting strategies and zero-/few-shot settings over verb pairs from\ntwo lexical databases, namely WordNet and HyperLex. Our findings unveil that\nthe models can tackle the lexical entailment recognition task with moderately\ngood performance, although at varying degree of effectiveness and under\ndifferent conditions. Also, utilizing few-shot prompting can enhance the\nmodels' performance. However, perfectly solving the task arises as an unmet\nchallenge for all examined LLMs, which raises an emergence for further research\ndevelopments on this topic."
                },
                "authors": [
                    {
                        "name": "Candida M. Greco"
                    },
                    {
                        "name": "Lucio La Cava"
                    },
                    {
                        "name": "Andrea Tagarelli"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Tagarelli"
                },
                "author": "Andrea Tagarelli",
                "arxiv_comment": "Accepted for publication at The 2024 Conference on Empirical Methods\n  in Natural Language Processing (EMNLP-2024) - Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14894v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14894v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04920v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04920v1",
                "updated": "2024-11-07T17:57:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    17,
                    57,
                    3,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T17:57:03Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    17,
                    57,
                    3,
                    3,
                    312,
                    0
                ],
                "title": "GPTKB: Building Very Large Knowledge Bases from Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPTKB: Building Very Large Knowledge Bases from Language Models"
                },
                "summary": "General-domain knowledge bases (KB), in particular the \"big three\" --\nWikidata, Yago and DBpedia -- are the backbone of many intelligent\napplications. While these three have seen steady development, comprehensive KB\nconstruction at large has seen few fresh attempts. In this work, we propose to\nbuild a large general-domain KB entirely from a large language model (LLM). We\ndemonstrate the feasibility of large-scale KB construction from LLMs, while\nhighlighting specific challenges arising around entity recognition, entity and\nproperty canonicalization, and taxonomy construction. As a prototype, we use\nGPT-4o-mini to construct GPTKB, which contains 105 million triples for more\nthan 2.9 million entities, at a cost 100x less than previous KBC projects. Our\nwork is a landmark for two fields: For NLP, for the first time, it provides\n\\textit{constructive} insights into the knowledge (or beliefs) of LLMs. For the\nSemantic Web, it shows novel ways forward for the long-standing challenge of\ngeneral-domain KB construction. GPTKB is accessible at https://gptkb.org.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General-domain knowledge bases (KB), in particular the \"big three\" --\nWikidata, Yago and DBpedia -- are the backbone of many intelligent\napplications. While these three have seen steady development, comprehensive KB\nconstruction at large has seen few fresh attempts. In this work, we propose to\nbuild a large general-domain KB entirely from a large language model (LLM). We\ndemonstrate the feasibility of large-scale KB construction from LLMs, while\nhighlighting specific challenges arising around entity recognition, entity and\nproperty canonicalization, and taxonomy construction. As a prototype, we use\nGPT-4o-mini to construct GPTKB, which contains 105 million triples for more\nthan 2.9 million entities, at a cost 100x less than previous KBC projects. Our\nwork is a landmark for two fields: For NLP, for the first time, it provides\n\\textit{constructive} insights into the knowledge (or beliefs) of LLMs. For the\nSemantic Web, it shows novel ways forward for the long-standing challenge of\ngeneral-domain KB construction. GPTKB is accessible at https://gptkb.org."
                },
                "authors": [
                    {
                        "name": "Yujia Hu"
                    },
                    {
                        "name": "Shrestha Ghosh"
                    },
                    {
                        "name": "Tuan-Phong Nugyen"
                    },
                    {
                        "name": "Simon Razniewski"
                    }
                ],
                "author_detail": {
                    "name": "Simon Razniewski"
                },
                "author": "Simon Razniewski",
                "arxiv_comment": "11 pages, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04920v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04920v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04914v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04914v1",
                "updated": "2024-11-07T17:53:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    17,
                    53,
                    47,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T17:53:47Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    17,
                    53,
                    47,
                    3,
                    312,
                    0
                ],
                "title": "GASE: Generatively Augmented Sentence Encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GASE: Generatively Augmented Sentence Encoding"
                },
                "summary": "We propose an approach to enhance sentence embeddings by applying generative\ntext models for data augmentation at inference time. Unlike conventional data\naugmentation that utilises synthetic training data, our approach does not\nrequire access to model parameters or the computational resources typically\nrequired for fine-tuning state-of-the-art models. Generatively Augmented\nSentence Encoding uses diverse linguistic synthetic variants of input texts\ngenerated by paraphrasing, summarising, or extracting keywords, followed by\npooling the original and synthetic embeddings. Experimental results on the\nMassive Text Embedding Benchmark for Semantic Textual Similarity (STS)\ndemonstrate performance improvements across a range of embedding models using\ndifferent generative models for augmentation. We find that generative\naugmentation leads to larger performance improvements for embedding models with\nlower baseline performance. These findings suggest that integrating generative\naugmentation at inference time adds semantic diversity and can enhance the\nrobustness and generalizability of sentence embeddings for embedding models.\nOur results show that the degree to which generative augmentation can improve\nSTS performance depends not only on the embedding model but also on the\ndataset. From a broader perspective, the approach allows trading training for\ninference compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose an approach to enhance sentence embeddings by applying generative\ntext models for data augmentation at inference time. Unlike conventional data\naugmentation that utilises synthetic training data, our approach does not\nrequire access to model parameters or the computational resources typically\nrequired for fine-tuning state-of-the-art models. Generatively Augmented\nSentence Encoding uses diverse linguistic synthetic variants of input texts\ngenerated by paraphrasing, summarising, or extracting keywords, followed by\npooling the original and synthetic embeddings. Experimental results on the\nMassive Text Embedding Benchmark for Semantic Textual Similarity (STS)\ndemonstrate performance improvements across a range of embedding models using\ndifferent generative models for augmentation. We find that generative\naugmentation leads to larger performance improvements for embedding models with\nlower baseline performance. These findings suggest that integrating generative\naugmentation at inference time adds semantic diversity and can enhance the\nrobustness and generalizability of sentence embeddings for embedding models.\nOur results show that the degree to which generative augmentation can improve\nSTS performance depends not only on the embedding model but also on the\ndataset. From a broader perspective, the approach allows trading training for\ninference compute."
                },
                "authors": [
                    {
                        "name": "Manuel Frank"
                    },
                    {
                        "name": "Haithem Afli"
                    }
                ],
                "author_detail": {
                    "name": "Haithem Afli"
                },
                "author": "Haithem Afli",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04914v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04914v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04909v1",
                "updated": "2024-11-07T17:48:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    17,
                    48,
                    54,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T17:48:54Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    17,
                    48,
                    54,
                    3,
                    312,
                    0
                ],
                "title": "Doubly robust inference with censoring unbiased transformations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Doubly robust inference with censoring unbiased transformations"
                },
                "summary": "This paper extends doubly robust censoring unbiased transformations to a\nbroad class of censored data structures under the assumption of coarsening at\nrandom and positivity. This includes the classic survival and competing risks\nsetting, but also encompasses multiple events. A doubly robust representation\nfor the conditional bias of the transformed data is derived. This leads to rate\ndouble robustness and oracle efficiency properties for estimating conditional\nexpectations when combined with cross-fitting and linear smoothers. Simulation\nstudies demonstrate favourable performance of the proposed method relative to\nexisting approaches. An application of the methods to a regression\ndiscontinuity design with censored data illustrates its practical utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper extends doubly robust censoring unbiased transformations to a\nbroad class of censored data structures under the assumption of coarsening at\nrandom and positivity. This includes the classic survival and competing risks\nsetting, but also encompasses multiple events. A doubly robust representation\nfor the conditional bias of the transformed data is derived. This leads to rate\ndouble robustness and oracle efficiency properties for estimating conditional\nexpectations when combined with cross-fitting and linear smoothers. Simulation\nstudies demonstrate favourable performance of the proposed method relative to\nexisting approaches. An application of the methods to a regression\ndiscontinuity design with censored data illustrates its practical utility."
                },
                "authors": [
                    {
                        "name": "Oliver Lunding Sandqvist"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Lunding Sandqvist"
                },
                "author": "Oliver Lunding Sandqvist",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04905v1",
                "updated": "2024-11-07T17:47:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    17,
                    47,
                    25,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T17:47:25Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    17,
                    47,
                    25,
                    3,
                    312,
                    0
                ],
                "title": "OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models"
                },
                "summary": "Large language models (LLMs) for code have become indispensable in various\ndomains, including code generation, reasoning tasks and agent systems.While\nopen-access code LLMs are increasingly approaching the performance levels of\nproprietary models, high-quality code LLMs suitable for rigorous scientific\ninvestigation, particularly those with reproducible data processing pipelines\nand transparent training protocols, remain limited. The scarcity is due to\nvarious challenges, including resource constraints, ethical considerations, and\nthe competitive advantages of keeping models advanced. To address the gap, we\nintroduce OpenCoder, a top-tier code LLM that not only achieves performance\ncomparable to leading models but also serves as an ``open cookbook'' for the\nresearch community. Unlike most prior efforts, we release not only model\nweights and inference code, but also the reproducible training data, complete\ndata processing pipeline, rigorous experimental ablation results, and detailed\ntraining protocols for open scientific research. Through this comprehensive\nrelease, we identify the key ingredients for building a top-tier code LLM: (1)\ncode optimized heuristic rules for data cleaning and methods for data\ndeduplication, (2) recall of text corpus related to code and (3) high-quality\nsynthetic data in both annealing and supervised fine-tuning stages. By offering\nthis level of openness, we aim to broaden access to all aspects of a top-tier\ncode LLM, with OpenCoder serving as both a powerful model and an open\nfoundation to accelerate research, and enable reproducible advancements in code\nAI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) for code have become indispensable in various\ndomains, including code generation, reasoning tasks and agent systems.While\nopen-access code LLMs are increasingly approaching the performance levels of\nproprietary models, high-quality code LLMs suitable for rigorous scientific\ninvestigation, particularly those with reproducible data processing pipelines\nand transparent training protocols, remain limited. The scarcity is due to\nvarious challenges, including resource constraints, ethical considerations, and\nthe competitive advantages of keeping models advanced. To address the gap, we\nintroduce OpenCoder, a top-tier code LLM that not only achieves performance\ncomparable to leading models but also serves as an ``open cookbook'' for the\nresearch community. Unlike most prior efforts, we release not only model\nweights and inference code, but also the reproducible training data, complete\ndata processing pipeline, rigorous experimental ablation results, and detailed\ntraining protocols for open scientific research. Through this comprehensive\nrelease, we identify the key ingredients for building a top-tier code LLM: (1)\ncode optimized heuristic rules for data cleaning and methods for data\ndeduplication, (2) recall of text corpus related to code and (3) high-quality\nsynthetic data in both annealing and supervised fine-tuning stages. By offering\nthis level of openness, we aim to broaden access to all aspects of a top-tier\ncode LLM, with OpenCoder serving as both a powerful model and an open\nfoundation to accelerate research, and enable reproducible advancements in code\nAI."
                },
                "authors": [
                    {
                        "name": "Siming Huang"
                    },
                    {
                        "name": "Tianhao Cheng"
                    },
                    {
                        "name": "Jason Klein Liu"
                    },
                    {
                        "name": "Jiaran Hao"
                    },
                    {
                        "name": "Liuyihan Song"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "J. Yang"
                    },
                    {
                        "name": "J. H. Liu"
                    },
                    {
                        "name": "Chenchen Zhang"
                    },
                    {
                        "name": "Linzheng Chai"
                    },
                    {
                        "name": "Ruifeng Yuan"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    },
                    {
                        "name": "Jie Fu"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Yuan Qi"
                    },
                    {
                        "name": "Yinghui Xu"
                    },
                    {
                        "name": "Wei Chu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Chu"
                },
                "author": "Wei Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04890v1",
                "updated": "2024-11-07T17:28:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    17,
                    28,
                    10,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T17:28:10Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    17,
                    28,
                    10,
                    3,
                    312,
                    0
                ],
                "title": "GUI Agents with Foundation Models: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUI Agents with Foundation Models: A Comprehensive Survey"
                },
                "summary": "Recent advances in foundation models, particularly Large Language Models\n(LLMs) and Multimodal Large Language Models (MLLMs), facilitate intelligent\nagents being capable of performing complex tasks. By leveraging the ability of\n(M)LLMs to process and interpret Graphical User Interfaces (GUIs), these agents\ncan autonomously execute user instructions by simulating human-like\ninteractions such as clicking and typing. This survey consolidates recent\nresearch on (M)LLM-based GUI agents, highlighting key innovations in data,\nframeworks, and applications. We begin by discussing representative datasets\nand benchmarks. Next, we summarize a unified framework that captures the\nessential components used in prior research, accompanied by a taxonomy.\nAdditionally, we explore commercial applications of (M)LLM-based GUI agents.\nDrawing from existing work, we identify several key challenges and propose\nfuture research directions. We hope this paper will inspire further\ndevelopments in the field of (M)LLM-based GUI agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in foundation models, particularly Large Language Models\n(LLMs) and Multimodal Large Language Models (MLLMs), facilitate intelligent\nagents being capable of performing complex tasks. By leveraging the ability of\n(M)LLMs to process and interpret Graphical User Interfaces (GUIs), these agents\ncan autonomously execute user instructions by simulating human-like\ninteractions such as clicking and typing. This survey consolidates recent\nresearch on (M)LLM-based GUI agents, highlighting key innovations in data,\nframeworks, and applications. We begin by discussing representative datasets\nand benchmarks. Next, we summarize a unified framework that captures the\nessential components used in prior research, accompanied by a taxonomy.\nAdditionally, we explore commercial applications of (M)LLM-based GUI agents.\nDrawing from existing work, we identify several key challenges and propose\nfuture research directions. We hope this paper will inspire further\ndevelopments in the field of (M)LLM-based GUI agents."
                },
                "authors": [
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Jingxuan Chen"
                    },
                    {
                        "name": "Weinan Gan"
                    },
                    {
                        "name": "Xingshan Zeng"
                    },
                    {
                        "name": "Shuai Yu"
                    },
                    {
                        "name": "Xinlong Hao"
                    },
                    {
                        "name": "Kun Shao"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Ruiming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Ruiming Tang"
                },
                "author": "Ruiming Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04878v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04878v1",
                "updated": "2024-11-07T17:14:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    17,
                    14,
                    56,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T17:14:56Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    17,
                    14,
                    56,
                    3,
                    312,
                    0
                ],
                "title": "Dark energy constraints using gamma-ray burst correlations with DESI\n  2024 data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dark energy constraints using gamma-ray burst correlations with DESI\n  2024 data"
                },
                "summary": "Even though the Dark Energy Spectroscopic Instrument (DESI) mission does not\nexclude a dynamical dark energy evolution, the concordance paradigm, i.e., the\n$\\Lambda$CDM model, remains statistically favored, as it depends on the fewest\nnumber of free parameters. In this respect, high redshift astrophysical\nsources, such as gamma-ray bursts, represent a formidable tool to model the\nform of dark energy, since they may provide a link between early and local\nredshift regimes. Hence, the use of these objects as possible distance\nindicators turns out to be essential to investigate the cosmological puzzle. To\nthis end, we adopt two gamma-ray burst linear correlations, namely the\n$L_p-E_p$ and $L_0-E_p-T$ relations, to test the flat and non-flat\n$\\Lambda$CDM, $\\omega_0$CDM, and $\\omega_0\\omega_1$CDM cosmological models,\ni.e., those directly examined by the DESI collaboration. The inferred\ncorrelation coefficients and cosmological parameters are thus obtained by\nconsidering two independent Monte Carlo Markov chain analyses, the first\nconsidering the whole DESI data set and the second excluding a seemingly\nproblematic data point placed at $z_{eff} = 0.51$. Using model selection\ncriteria, the two above correlations do not show a preference on a precise\ncosmological model although, when the data point at $z_{eff}$ is included, the\nconcordance paradigm appears to be the least favored among the tested\ncosmological models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Even though the Dark Energy Spectroscopic Instrument (DESI) mission does not\nexclude a dynamical dark energy evolution, the concordance paradigm, i.e., the\n$\\Lambda$CDM model, remains statistically favored, as it depends on the fewest\nnumber of free parameters. In this respect, high redshift astrophysical\nsources, such as gamma-ray bursts, represent a formidable tool to model the\nform of dark energy, since they may provide a link between early and local\nredshift regimes. Hence, the use of these objects as possible distance\nindicators turns out to be essential to investigate the cosmological puzzle. To\nthis end, we adopt two gamma-ray burst linear correlations, namely the\n$L_p-E_p$ and $L_0-E_p-T$ relations, to test the flat and non-flat\n$\\Lambda$CDM, $\\omega_0$CDM, and $\\omega_0\\omega_1$CDM cosmological models,\ni.e., those directly examined by the DESI collaboration. The inferred\ncorrelation coefficients and cosmological parameters are thus obtained by\nconsidering two independent Monte Carlo Markov chain analyses, the first\nconsidering the whole DESI data set and the second excluding a seemingly\nproblematic data point placed at $z_{eff} = 0.51$. Using model selection\ncriteria, the two above correlations do not show a preference on a precise\ncosmological model although, when the data point at $z_{eff}$ is included, the\nconcordance paradigm appears to be the least favored among the tested\ncosmological models."
                },
                "authors": [
                    {
                        "name": "Anna Chiara Alfano"
                    },
                    {
                        "name": "Orlando Luongo"
                    },
                    {
                        "name": "Marco Muccino"
                    }
                ],
                "author_detail": {
                    "name": "Marco Muccino"
                },
                "author": "Marco Muccino",
                "arxiv_comment": "14 pages, 4 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04878v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04878v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02397v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02397v2",
                "updated": "2024-11-07T17:06:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    17,
                    6,
                    32,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-04T18:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    59,
                    44,
                    0,
                    309,
                    0
                ],
                "title": "Adaptive Caching for Faster Video Generation with Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Caching for Faster Video Generation with Diffusion Transformers"
                },
                "summary": "Generating temporally-consistent high-fidelity videos can be computationally\nexpensive, especially over longer temporal spans. More-recent Diffusion\nTransformers (DiTs) -- despite making significant headway in this context --\nhave only heightened such challenges as they rely on larger models and heavier\nattention mechanisms, resulting in slower inference speeds. In this paper, we\nintroduce a training-free method to accelerate video DiTs, termed Adaptive\nCaching (AdaCache), which is motivated by the fact that \"not all videos are\ncreated equal\": meaning, some videos require fewer denoising steps to attain a\nreasonable quality than others. Building on this, we not only cache\ncomputations through the diffusion process, but also devise a caching schedule\ntailored to each video generation, maximizing the quality-latency trade-off. We\nfurther introduce a Motion Regularization (MoReg) scheme to utilize video\ninformation within AdaCache, essentially controlling the compute allocation\nbased on motion content. Altogether, our plug-and-play contributions grant\nsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video\ngeneration) without sacrificing the generation quality, across multiple video\nDiT baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating temporally-consistent high-fidelity videos can be computationally\nexpensive, especially over longer temporal spans. More-recent Diffusion\nTransformers (DiTs) -- despite making significant headway in this context --\nhave only heightened such challenges as they rely on larger models and heavier\nattention mechanisms, resulting in slower inference speeds. In this paper, we\nintroduce a training-free method to accelerate video DiTs, termed Adaptive\nCaching (AdaCache), which is motivated by the fact that \"not all videos are\ncreated equal\": meaning, some videos require fewer denoising steps to attain a\nreasonable quality than others. Building on this, we not only cache\ncomputations through the diffusion process, but also devise a caching schedule\ntailored to each video generation, maximizing the quality-latency trade-off. We\nfurther introduce a Motion Regularization (MoReg) scheme to utilize video\ninformation within AdaCache, essentially controlling the compute allocation\nbased on motion content. Altogether, our plug-and-play contributions grant\nsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video\ngeneration) without sacrificing the generation quality, across multiple video\nDiT baselines."
                },
                "authors": [
                    {
                        "name": "Kumara Kahatapitiya"
                    },
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Sen He"
                    },
                    {
                        "name": "Ding Liu"
                    },
                    {
                        "name": "Menglin Jia"
                    },
                    {
                        "name": "Chenyang Zhang"
                    },
                    {
                        "name": "Michael S. Ryoo"
                    },
                    {
                        "name": "Tian Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tian Xie"
                },
                "author": "Tian Xie",
                "arxiv_comment": "Project-page is available at https://adacache-dit.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02397v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13835v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13835v2",
                "updated": "2024-11-07T16:57:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    16,
                    57,
                    2,
                    3,
                    312,
                    0
                ],
                "published": "2024-10-17T17:54:06Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    54,
                    6,
                    3,
                    291,
                    0
                ],
                "title": "Active-Dormant Attention Heads: Mechanistically Demystifying\n  Extreme-Token Phenomena in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active-Dormant Attention Heads: Mechanistically Demystifying\n  Extreme-Token Phenomena in LLMs"
                },
                "summary": "Practitioners have consistently observed three puzzling phenomena in\ntransformer-based large language models (LLMs): attention sinks, value-state\ndrains, and residual-state peaks, collectively referred to as extreme-token\nphenomena. These phenomena are characterized by certain so-called \"sink tokens\"\nreceiving disproportionately high attention weights, exhibiting significantly\nsmaller value states, and having much larger residual-state norms than those of\nother tokens. These extreme tokens give rise to various challenges in LLM\ninference, quantization, and interpretability.\n  We elucidate the mechanisms behind extreme-token phenomena. First, we show\nthat these phenomena arise in very simple architectures -- transformers with\none to three layers -- trained on a toy model, the Bigram-Backcopy (BB) task.\nIn this setting, we identify an active-dormant mechanism, where attention heads\nbecome sinks for specific input domains while remaining non-sinks for others.\nOur theoretical analysis of the training dynamics reveals that these phenomena\nare driven by a mutual reinforcement mechanism. Building on these insights, we\npropose strategies to mitigate extreme-token phenomena during pretraining,\nincluding replacing softmax with ReLU and Adam with SGD. Next, we extend our\nanalysis to pretrained LLMs, including Llama and OLMo, showing that many\nattention heads exhibit a similar active-dormant mechanism as in the BB task,\nand that the mutual reinforcement mechanism also governs the emergence of\nextreme-token phenomena during LLM pretraining. Our results reveal that many of\nthe static and dynamic properties of extreme-token phenomena predicted by the\nBB task align with observations in pretrained LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practitioners have consistently observed three puzzling phenomena in\ntransformer-based large language models (LLMs): attention sinks, value-state\ndrains, and residual-state peaks, collectively referred to as extreme-token\nphenomena. These phenomena are characterized by certain so-called \"sink tokens\"\nreceiving disproportionately high attention weights, exhibiting significantly\nsmaller value states, and having much larger residual-state norms than those of\nother tokens. These extreme tokens give rise to various challenges in LLM\ninference, quantization, and interpretability.\n  We elucidate the mechanisms behind extreme-token phenomena. First, we show\nthat these phenomena arise in very simple architectures -- transformers with\none to three layers -- trained on a toy model, the Bigram-Backcopy (BB) task.\nIn this setting, we identify an active-dormant mechanism, where attention heads\nbecome sinks for specific input domains while remaining non-sinks for others.\nOur theoretical analysis of the training dynamics reveals that these phenomena\nare driven by a mutual reinforcement mechanism. Building on these insights, we\npropose strategies to mitigate extreme-token phenomena during pretraining,\nincluding replacing softmax with ReLU and Adam with SGD. Next, we extend our\nanalysis to pretrained LLMs, including Llama and OLMo, showing that many\nattention heads exhibit a similar active-dormant mechanism as in the BB task,\nand that the mutual reinforcement mechanism also governs the emergence of\nextreme-token phenomena during LLM pretraining. Our results reveal that many of\nthe static and dynamic properties of extreme-token phenomena predicted by the\nBB task align with observations in pretrained LLMs."
                },
                "authors": [
                    {
                        "name": "Tianyu Guo"
                    },
                    {
                        "name": "Druv Pai"
                    },
                    {
                        "name": "Yu Bai"
                    },
                    {
                        "name": "Jiantao Jiao"
                    },
                    {
                        "name": "Michael I. Jordan"
                    },
                    {
                        "name": "Song Mei"
                    }
                ],
                "author_detail": {
                    "name": "Song Mei"
                },
                "author": "Song Mei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13835v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13835v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.12530v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.12530v3",
                "updated": "2024-11-07T16:52:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    16,
                    52,
                    39,
                    3,
                    312,
                    0
                ],
                "published": "2023-11-21T11:21:53Z",
                "published_parsed": [
                    2023,
                    11,
                    21,
                    11,
                    21,
                    53,
                    1,
                    325,
                    0
                ],
                "title": "An efficient likelihood-free Bayesian inference method based on\n  sequential neural posterior estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An efficient likelihood-free Bayesian inference method based on\n  sequential neural posterior estimation"
                },
                "summary": "Sequential neural posterior estimation (SNPE) techniques have been recently\nproposed for dealing with simulation-based models with intractable likelihoods.\nUnlike approximate Bayesian computation, SNPE techniques learn the posterior\nfrom sequential simulation using neural network-based conditional density\nestimators by minimizing a specific loss function. The SNPE method proposed by\nLueckmann et al. (2017) used a calibration kernel to boost the sample weights\naround the observed data, resulting in a concentrated loss function. However,\nthe use of calibration kernels may increase the variances of both the empirical\nloss and its gradient, making the training inefficient. To improve the\nstability of SNPE, this paper proposes to use an adaptive calibration kernel\nand several variance reduction techniques. The proposed method greatly speeds\nup the process of training and provides a better approximation of the posterior\nthan the original SNPE method and some existing competitors as confirmed by\nnumerical experiments. We also manage to demonstrate the superiority of the\nproposed method for a high-dimensional model with real-world dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential neural posterior estimation (SNPE) techniques have been recently\nproposed for dealing with simulation-based models with intractable likelihoods.\nUnlike approximate Bayesian computation, SNPE techniques learn the posterior\nfrom sequential simulation using neural network-based conditional density\nestimators by minimizing a specific loss function. The SNPE method proposed by\nLueckmann et al. (2017) used a calibration kernel to boost the sample weights\naround the observed data, resulting in a concentrated loss function. However,\nthe use of calibration kernels may increase the variances of both the empirical\nloss and its gradient, making the training inefficient. To improve the\nstability of SNPE, this paper proposes to use an adaptive calibration kernel\nand several variance reduction techniques. The proposed method greatly speeds\nup the process of training and provides a better approximation of the posterior\nthan the original SNPE method and some existing competitors as confirmed by\nnumerical experiments. We also manage to demonstrate the superiority of the\nproposed method for a high-dimensional model with real-world dataset."
                },
                "authors": [
                    {
                        "name": "Yifei Xiong"
                    },
                    {
                        "name": "Xiliang Yang"
                    },
                    {
                        "name": "Sanguo Zhang"
                    },
                    {
                        "name": "Zhijian He"
                    }
                ],
                "author_detail": {
                    "name": "Zhijian He"
                },
                "author": "Zhijian He",
                "arxiv_comment": "28 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.12530v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.12530v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.09269v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.09269v2",
                "updated": "2024-11-07T16:43:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    16,
                    43,
                    1,
                    3,
                    312,
                    0
                ],
                "published": "2024-02-14T15:55:30Z",
                "published_parsed": [
                    2024,
                    2,
                    14,
                    15,
                    55,
                    30,
                    2,
                    45,
                    0
                ],
                "title": "Personalized Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Large Language Models"
                },
                "summary": "Large language models (LLMs) have significantly advanced Natural Language\nProcessing (NLP) tasks in recent years. However, their universal nature poses\nlimitations in scenarios requiring personalized responses, such as\nrecommendation systems and chatbots. This paper investigates methods to\npersonalize LLMs, comparing fine-tuning and zero-shot reasoning approaches on\nsubjective tasks. Results demonstrate that personalized fine-tuning improves\nmodel reasoning compared to non-personalized models. Experiments on datasets\nfor emotion recognition and hate speech detection show consistent performance\ngains with personalized methods across different LLM architectures. These\nfindings underscore the importance of personalization for enhancing LLM\ncapabilities in subjective text perception tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have significantly advanced Natural Language\nProcessing (NLP) tasks in recent years. However, their universal nature poses\nlimitations in scenarios requiring personalized responses, such as\nrecommendation systems and chatbots. This paper investigates methods to\npersonalize LLMs, comparing fine-tuning and zero-shot reasoning approaches on\nsubjective tasks. Results demonstrate that personalized fine-tuning improves\nmodel reasoning compared to non-personalized models. Experiments on datasets\nfor emotion recognition and hate speech detection show consistent performance\ngains with personalized methods across different LLM architectures. These\nfindings underscore the importance of personalization for enhancing LLM\ncapabilities in subjective text perception tasks."
                },
                "authors": [
                    {
                        "name": "Stanisaw Woniak"
                    },
                    {
                        "name": "Bartomiej Koptyra"
                    },
                    {
                        "name": "Arkadiusz Janz"
                    },
                    {
                        "name": "Przemysaw Kazienko"
                    },
                    {
                        "name": "Jan Koco"
                    }
                ],
                "author_detail": {
                    "name": "Jan Koco"
                },
                "author": "Jan Koco",
                "arxiv_comment": "Accepted to SENTIRE 2024 (ICDM Workshops):\n  https://sentic.net/sentire2024wozniak.pdf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.09269v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.09269v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04847v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04847v1",
                "updated": "2024-11-07T16:33:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    16,
                    33,
                    48,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T16:33:48Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    16,
                    33,
                    48,
                    3,
                    312,
                    0
                ],
                "title": "Prompt-Guided Internal States for Hallucination Detection of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt-Guided Internal States for Hallucination Detection of Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na variety of tasks in different domains. However, they sometimes generate\nresponses that are logically coherent but factually incorrect or misleading,\nwhich is known as LLM hallucinations. Data-driven supervised methods train\nhallucination detectors by leveraging the internal states of LLMs, but\ndetectors trained on specific domains often struggle to generalize well to\nother domains. In this paper, we aim to enhance the cross-domain performance of\nsupervised detectors with only in-domain data. We propose a novel framework,\nprompt-guided internal states for hallucination detection of LLMs, namely\nPRISM. By utilizing appropriate prompts to guide changes in the structure\nrelated to text truthfulness within the LLM's internal states, we make this\nstructure more salient and consistent across texts from different domains. We\nintegrated our framework with existing hallucination detection methods and\nconducted experiments on datasets from different domains. The experimental\nresults indicate that our framework significantly enhances the cross-domain\ngeneralization of existing hallucination detection methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na variety of tasks in different domains. However, they sometimes generate\nresponses that are logically coherent but factually incorrect or misleading,\nwhich is known as LLM hallucinations. Data-driven supervised methods train\nhallucination detectors by leveraging the internal states of LLMs, but\ndetectors trained on specific domains often struggle to generalize well to\nother domains. In this paper, we aim to enhance the cross-domain performance of\nsupervised detectors with only in-domain data. We propose a novel framework,\nprompt-guided internal states for hallucination detection of LLMs, namely\nPRISM. By utilizing appropriate prompts to guide changes in the structure\nrelated to text truthfulness within the LLM's internal states, we make this\nstructure more salient and consistent across texts from different domains. We\nintegrated our framework with existing hallucination detection methods and\nconducted experiments on datasets from different domains. The experimental\nresults indicate that our framework significantly enhances the cross-domain\ngeneralization of existing hallucination detection methods."
                },
                "authors": [
                    {
                        "name": "Fujie Zhang"
                    },
                    {
                        "name": "Peiqi Yu"
                    },
                    {
                        "name": "Biao Yi"
                    },
                    {
                        "name": "Baolei Zhang"
                    },
                    {
                        "name": "Tong Li"
                    },
                    {
                        "name": "Zheli Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheli Liu"
                },
                "author": "Zheli Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04847v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04847v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00708v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00708v2",
                "updated": "2024-11-07T16:23:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    16,
                    23,
                    53,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-01T16:18:33Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    16,
                    18,
                    33,
                    4,
                    306,
                    0
                ],
                "title": "Simplifying and Characterizing DAGs and Phylogenetic Networks via Least\n  Common Ancestor Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simplifying and Characterizing DAGs and Phylogenetic Networks via Least\n  Common Ancestor Constraints"
                },
                "summary": "Rooted phylogenetic networks, or more generally, directed acyclic graphs\n(DAGs), are widely used to model species or gene relationships that traditional\nrooted trees cannot fully capture, especially in the presence of reticulate\nprocesses or horizontal gene transfers. Such networks or DAGs are typically\ninferred from genomic data of extant taxa, providing only an estimate of the\ntrue evolutionary history. However, these inferred DAGs are often complex and\ndifficult to interpret. In particular, many contain vertices that do not serve\nas least common ancestors (LCAs) for any subset of the underlying genes or\nspecies, thus lacking direct support from the observed data. In contrast, LCA\nvertices represent ancestral states substantiated by the data, offering\nimportant insights into evolutionary relationships among subsets of taxa. To\nreduce unnecessary complexity and eliminate unsupported vertices, we aim to\nsimplify a DAG to retain only LCA vertices while preserving essential\nevolutionary information.\n  In this paper, we characterize $\\mathrm{LCA}$-relevant and\n$\\mathrm{lca}$-relevant DAGs, defined as those in which every vertex serves as\nan LCA (or unique LCA) for some subset of taxa. We introduce methods to\nidentify LCAs in DAGs and efficiently transform any DAG into an\n$\\mathrm{LCA}$-relevant or $\\mathrm{lca}$-relevant one while preserving key\nstructural properties of the original DAG or network. This transformation is\nachieved using a simple operator ``$\\ominus$'' that mimics vertex suppression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rooted phylogenetic networks, or more generally, directed acyclic graphs\n(DAGs), are widely used to model species or gene relationships that traditional\nrooted trees cannot fully capture, especially in the presence of reticulate\nprocesses or horizontal gene transfers. Such networks or DAGs are typically\ninferred from genomic data of extant taxa, providing only an estimate of the\ntrue evolutionary history. However, these inferred DAGs are often complex and\ndifficult to interpret. In particular, many contain vertices that do not serve\nas least common ancestors (LCAs) for any subset of the underlying genes or\nspecies, thus lacking direct support from the observed data. In contrast, LCA\nvertices represent ancestral states substantiated by the data, offering\nimportant insights into evolutionary relationships among subsets of taxa. To\nreduce unnecessary complexity and eliminate unsupported vertices, we aim to\nsimplify a DAG to retain only LCA vertices while preserving essential\nevolutionary information.\n  In this paper, we characterize $\\mathrm{LCA}$-relevant and\n$\\mathrm{lca}$-relevant DAGs, defined as those in which every vertex serves as\nan LCA (or unique LCA) for some subset of taxa. We introduce methods to\nidentify LCAs in DAGs and efficiently transform any DAG into an\n$\\mathrm{LCA}$-relevant or $\\mathrm{lca}$-relevant one while preserving key\nstructural properties of the original DAG or network. This transformation is\nachieved using a simple operator ``$\\ominus$'' that mimics vertex suppression."
                },
                "authors": [
                    {
                        "name": "Anna Lindeberg"
                    },
                    {
                        "name": "Marc Hellmuth"
                    }
                ],
                "author_detail": {
                    "name": "Marc Hellmuth"
                },
                "author": "Marc Hellmuth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00708v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00708v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.PE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05030v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05030v2",
                "updated": "2024-11-07T16:18:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    16,
                    18,
                    26,
                    3,
                    312,
                    0
                ],
                "published": "2024-02-07T16:59:00Z",
                "published_parsed": [
                    2024,
                    2,
                    7,
                    16,
                    59,
                    0,
                    2,
                    38,
                    0
                ],
                "title": "Inference for Two-Stage Extremum Estimators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference for Two-Stage Extremum Estimators"
                },
                "summary": "We present a simulation-based inference approach for two-stage estimators,\nfocusing on extremum estimators in the second stage. We accommodate a broad\nrange of first-stage estimators, including extremum estimators,\nhigh-dimensional estimators, and other types of estimators such as Bayesian\nestimators. The key contribution of our approach lies in its ability to\nestimate the asymptotic distribution of two-stage estimators, even when the\ndistributions of both the first- and second-stage estimators are non-normal and\nwhen the second-stage estimator's bias, scaled by the square root of the sample\nsize, does not vanish asymptotically. This enables reliable inference in\nsituations where standard methods fail. Additionally, we propose a debiased\nestimator, based on the mean of the estimated distribution function, which\nexhibits improved finite sample properties. Unlike resampling methods, our\napproach avoids the need for multiple calculations of the two-stage estimator.\nWe illustrate the effectiveness of our method in an empirical application on\npeer effects in adolescent fast-food consumption, where we address the issue of\nbiased instrumental variable estimates resulting from many weak instruments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a simulation-based inference approach for two-stage estimators,\nfocusing on extremum estimators in the second stage. We accommodate a broad\nrange of first-stage estimators, including extremum estimators,\nhigh-dimensional estimators, and other types of estimators such as Bayesian\nestimators. The key contribution of our approach lies in its ability to\nestimate the asymptotic distribution of two-stage estimators, even when the\ndistributions of both the first- and second-stage estimators are non-normal and\nwhen the second-stage estimator's bias, scaled by the square root of the sample\nsize, does not vanish asymptotically. This enables reliable inference in\nsituations where standard methods fail. Additionally, we propose a debiased\nestimator, based on the mean of the estimated distribution function, which\nexhibits improved finite sample properties. Unlike resampling methods, our\napproach avoids the need for multiple calculations of the two-stage estimator.\nWe illustrate the effectiveness of our method in an empirical application on\npeer effects in adolescent fast-food consumption, where we address the issue of\nbiased instrumental variable estimates resulting from many weak instruments."
                },
                "authors": [
                    {
                        "name": "Aristide Houndetoungan"
                    },
                    {
                        "name": "Abdoul Haki Maoude"
                    }
                ],
                "author_detail": {
                    "name": "Abdoul Haki Maoude"
                },
                "author": "Abdoul Haki Maoude",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.05030v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05030v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00220v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00220v2",
                "updated": "2024-11-07T16:09:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    16,
                    9,
                    14,
                    3,
                    312,
                    0
                ],
                "published": "2024-08-30T19:25:28Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    19,
                    25,
                    28,
                    4,
                    243,
                    0
                ],
                "title": "Learning Latent Space Dynamics with Model-Form Uncertainties: A\n  Stochastic Reduced-Order Modeling Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Latent Space Dynamics with Model-Form Uncertainties: A\n  Stochastic Reduced-Order Modeling Approach"
                },
                "summary": "This paper presents a probabilistic approach to represent and quantify\nmodel-form uncertainties in the reduced-order modeling of complex systems using\noperator inference techniques. Such uncertainties can arise in the selection of\nan appropriate state-space representation, in the projection step that\nunderlies many reduced-order modeling methods, or as a byproduct of\nconsiderations made during training, to name a few. Following previous works in\nthe literature, the proposed method captures these uncertainties by expanding\nthe approximation space through the randomization of the projection matrix.\nThis is achieved by combining Riemannian projection and retraction operators -\nacting on a subset of the Stiefel manifold - with an information-theoretic\nformulation. The efficacy of the approach is assessed on canonical problems in\nfluid mechanics by identifying and quantifying the impact of model-form\nuncertainties on the inferred operators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a probabilistic approach to represent and quantify\nmodel-form uncertainties in the reduced-order modeling of complex systems using\noperator inference techniques. Such uncertainties can arise in the selection of\nan appropriate state-space representation, in the projection step that\nunderlies many reduced-order modeling methods, or as a byproduct of\nconsiderations made during training, to name a few. Following previous works in\nthe literature, the proposed method captures these uncertainties by expanding\nthe approximation space through the randomization of the projection matrix.\nThis is achieved by combining Riemannian projection and retraction operators -\nacting on a subset of the Stiefel manifold - with an information-theoretic\nformulation. The efficacy of the approach is assessed on canonical problems in\nfluid mechanics by identifying and quantifying the impact of model-form\nuncertainties on the inferred operators."
                },
                "authors": [
                    {
                        "name": "Jin Yi Yong"
                    },
                    {
                        "name": "Rudy Geelen"
                    },
                    {
                        "name": "Johann Guilleminot"
                    }
                ],
                "author_detail": {
                    "name": "Johann Guilleminot"
                },
                "author": "Johann Guilleminot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00220v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00220v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04825v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04825v1",
                "updated": "2024-11-07T16:06:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    16,
                    6,
                    0,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T16:06:00Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    16,
                    6,
                    0,
                    3,
                    312,
                    0
                ],
                "title": "VTechAGP: An Academic-to-General-Audience Text Paraphrase Dataset and\n  Benchmark Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VTechAGP: An Academic-to-General-Audience Text Paraphrase Dataset and\n  Benchmark Models"
                },
                "summary": "Existing text simplification or paraphrase datasets mainly focus on\nsentence-level text generation in a general domain. These datasets are\ntypically developed without using domain knowledge. In this paper, we release a\nnovel dataset, VTechAGP, which is the first academic-to-general-audience text\nparaphrase dataset consisting of 4,938 document-level these and dissertation\nacademic and general-audience abstract pairs from 8 colleges authored over 25\nyears. We also propose a novel dynamic soft prompt generative language model,\nDSPT5. For training, we leverage a contrastive-generative loss function to\nlearn the keyword vectors in the dynamic prompt. For inference, we adopt a\ncrowd-sampling decoding strategy at both semantic and structural levels to\nfurther select the best output candidate. We evaluate DSPT5 and various\nstate-of-the-art large language models (LLMs) from multiple perspectives.\nResults demonstrate that the SOTA LLMs does not provide satisfactory outcomes,\nwhile the lightweight DSPT5 can achieve competitive results. To the best of our\nknowledge, we are the first to build a benchmark dataset and solutions for\nacademic-to-general-audience text paraphrase dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing text simplification or paraphrase datasets mainly focus on\nsentence-level text generation in a general domain. These datasets are\ntypically developed without using domain knowledge. In this paper, we release a\nnovel dataset, VTechAGP, which is the first academic-to-general-audience text\nparaphrase dataset consisting of 4,938 document-level these and dissertation\nacademic and general-audience abstract pairs from 8 colleges authored over 25\nyears. We also propose a novel dynamic soft prompt generative language model,\nDSPT5. For training, we leverage a contrastive-generative loss function to\nlearn the keyword vectors in the dynamic prompt. For inference, we adopt a\ncrowd-sampling decoding strategy at both semantic and structural levels to\nfurther select the best output candidate. We evaluate DSPT5 and various\nstate-of-the-art large language models (LLMs) from multiple perspectives.\nResults demonstrate that the SOTA LLMs does not provide satisfactory outcomes,\nwhile the lightweight DSPT5 can achieve competitive results. To the best of our\nknowledge, we are the first to build a benchmark dataset and solutions for\nacademic-to-general-audience text paraphrase dataset."
                },
                "authors": [
                    {
                        "name": "Ming Cheng"
                    },
                    {
                        "name": "Jiaying Gong"
                    },
                    {
                        "name": "Chenhan Yuan"
                    },
                    {
                        "name": "William A. Ingram"
                    },
                    {
                        "name": "Edward Fox"
                    },
                    {
                        "name": "Hoda Eldardiry"
                    }
                ],
                "author_detail": {
                    "name": "Hoda Eldardiry"
                },
                "author": "Hoda Eldardiry",
                "arxiv_comment": "21 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04825v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04825v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16163v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16163v2",
                "updated": "2024-11-07T15:48:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    15,
                    48,
                    11,
                    3,
                    312,
                    0
                ],
                "published": "2024-08-28T22:51:29Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    22,
                    51,
                    29,
                    2,
                    241,
                    0
                ],
                "title": "FRACTURED-SORRY-Bench: Framework for Revealing Attacks in Conversational\n  Turns Undermining Refusal Efficacy and Defenses over SORRY-Bench (Automated\n  Multi-shot Jailbreaks)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FRACTURED-SORRY-Bench: Framework for Revealing Attacks in Conversational\n  Turns Undermining Refusal Efficacy and Defenses over SORRY-Bench (Automated\n  Multi-shot Jailbreaks)"
                },
                "summary": "This paper introduces FRACTURED-SORRY-Bench, a framework for evaluating the\nsafety of Large Language Models (LLMs) against multi-turn conversational\nattacks. Building upon the SORRY-Bench dataset, we propose a simple yet\neffective method for generating adversarial prompts by breaking down harmful\nqueries into seemingly innocuous sub-questions. Our approach achieves a maximum\nincrease of +46.22\\% in Attack Success Rates (ASRs) across GPT-4, GPT-4o,\nGPT-4o-mini, and GPT-3.5-Turbo models compared to baseline methods. We\ndemonstrate that this technique poses a challenge to current LLM safety\nmeasures and highlights the need for more robust defenses against subtle,\nmulti-turn attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces FRACTURED-SORRY-Bench, a framework for evaluating the\nsafety of Large Language Models (LLMs) against multi-turn conversational\nattacks. Building upon the SORRY-Bench dataset, we propose a simple yet\neffective method for generating adversarial prompts by breaking down harmful\nqueries into seemingly innocuous sub-questions. Our approach achieves a maximum\nincrease of +46.22\\% in Attack Success Rates (ASRs) across GPT-4, GPT-4o,\nGPT-4o-mini, and GPT-3.5-Turbo models compared to baseline methods. We\ndemonstrate that this technique poses a challenge to current LLM safety\nmeasures and highlights the need for more robust defenses against subtle,\nmulti-turn attacks."
                },
                "authors": [
                    {
                        "name": "Aman Priyanshu"
                    },
                    {
                        "name": "Supriti Vijay"
                    }
                ],
                "author_detail": {
                    "name": "Supriti Vijay"
                },
                "author": "Supriti Vijay",
                "arxiv_comment": "4 pages, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16163v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16163v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04810v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04810v1",
                "updated": "2024-11-07T15:47:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    15,
                    47,
                    7,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T15:47:07Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    15,
                    47,
                    7,
                    3,
                    312,
                    0
                ],
                "title": "GANESH: Generalizable NeRF for Lensless Imaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GANESH: Generalizable NeRF for Lensless Imaging"
                },
                "summary": "Lensless imaging offers a significant opportunity to develop ultra-compact\ncameras by removing the conventional bulky lens system. However, without a\nfocusing element, the sensor's output is no longer a direct image but a complex\nmultiplexed scene representation. Traditional methods have attempted to address\nthis challenge by employing learnable inversions and refinement models, but\nthese methods are primarily designed for 2D reconstruction and do not\ngeneralize well to 3D reconstruction. We introduce GANESH, a novel framework\ndesigned to enable simultaneous refinement and novel view synthesis from\nmulti-view lensless images. Unlike existing methods that require scene-specific\ntraining, our approach supports on-the-fly inference without retraining on each\nscene. Moreover, our framework allows us to tune our model to specific scenes,\nenhancing the rendering and refinement quality. To facilitate research in this\narea, we also present the first multi-view lensless dataset, LenslessScenes.\nExtensive experiments demonstrate that our method outperforms current\napproaches in reconstruction accuracy and refinement quality. Code and video\nresults are available at https://rakesh-123-cryp.github.io/Rakesh.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lensless imaging offers a significant opportunity to develop ultra-compact\ncameras by removing the conventional bulky lens system. However, without a\nfocusing element, the sensor's output is no longer a direct image but a complex\nmultiplexed scene representation. Traditional methods have attempted to address\nthis challenge by employing learnable inversions and refinement models, but\nthese methods are primarily designed for 2D reconstruction and do not\ngeneralize well to 3D reconstruction. We introduce GANESH, a novel framework\ndesigned to enable simultaneous refinement and novel view synthesis from\nmulti-view lensless images. Unlike existing methods that require scene-specific\ntraining, our approach supports on-the-fly inference without retraining on each\nscene. Moreover, our framework allows us to tune our model to specific scenes,\nenhancing the rendering and refinement quality. To facilitate research in this\narea, we also present the first multi-view lensless dataset, LenslessScenes.\nExtensive experiments demonstrate that our method outperforms current\napproaches in reconstruction accuracy and refinement quality. Code and video\nresults are available at https://rakesh-123-cryp.github.io/Rakesh.github.io/"
                },
                "authors": [
                    {
                        "name": "Rakesh Raj Madavan"
                    },
                    {
                        "name": "Akshat Kaimal"
                    },
                    {
                        "name": "Badhrinarayanan K V"
                    },
                    {
                        "name": "Vinayak Gupta"
                    },
                    {
                        "name": "Rohit Choudhary"
                    },
                    {
                        "name": "Chandrakala Shanmuganathan"
                    },
                    {
                        "name": "Kaushik Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Mitra"
                },
                "author": "Kaushik Mitra",
                "arxiv_journal_ref": "IEEE/CVF Winter Conference on Applications of Computer Vision\n  (WACV) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04810v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04810v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01031v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01031v2",
                "updated": "2024-11-07T15:41:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    15,
                    41,
                    48,
                    3,
                    312,
                    0
                ],
                "published": "2024-10-01T19:45:01Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    19,
                    45,
                    1,
                    1,
                    275,
                    0
                ],
                "title": "Pediatric Wrist Fracture Detection Using Feature Context Excitation\n  Modules in X-ray Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pediatric Wrist Fracture Detection Using Feature Context Excitation\n  Modules in X-ray Images"
                },
                "summary": "Children often suffer wrist trauma in daily life, while they usually need\nradiologists to analyze and interpret X-ray images before surgical treatment by\nsurgeons. The development of deep learning has enabled neural networks to serve\nas computer-assisted diagnosis (CAD) tools to help doctors and experts in\nmedical image diagnostics. Since YOLOv8 model has obtained the satisfactory\nsuccess in object detection tasks, it has been applied to various fracture\ndetection. This work introduces four variants of Feature Contexts\nExcitation-YOLOv8 (FCE-YOLOv8) model, each incorporating a different FCE module\n(i.e., modules of Squeeze-and-Excitation (SE), Global Context (GC),\nGather-Excite (GE), and Gaussian Context Transformer (GCT)) to enhance the\nmodel performance. Experimental results on GRAZPEDWRI-DX dataset demonstrate\nthat our proposed YOLOv8+GC-M3 model improves the mAP@50 value from 65.78% to\n66.32%, outperforming the state-of-the-art (SOTA) model while reducing\ninference time. Furthermore, our proposed YOLOv8+SE-M3 model achieves the\nhighest mAP@50 value of 67.07%, exceeding the SOTA performance. The\nimplementation of this work is available at\nhttps://github.com/RuiyangJu/FCE-YOLOv8.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Children often suffer wrist trauma in daily life, while they usually need\nradiologists to analyze and interpret X-ray images before surgical treatment by\nsurgeons. The development of deep learning has enabled neural networks to serve\nas computer-assisted diagnosis (CAD) tools to help doctors and experts in\nmedical image diagnostics. Since YOLOv8 model has obtained the satisfactory\nsuccess in object detection tasks, it has been applied to various fracture\ndetection. This work introduces four variants of Feature Contexts\nExcitation-YOLOv8 (FCE-YOLOv8) model, each incorporating a different FCE module\n(i.e., modules of Squeeze-and-Excitation (SE), Global Context (GC),\nGather-Excite (GE), and Gaussian Context Transformer (GCT)) to enhance the\nmodel performance. Experimental results on GRAZPEDWRI-DX dataset demonstrate\nthat our proposed YOLOv8+GC-M3 model improves the mAP@50 value from 65.78% to\n66.32%, outperforming the state-of-the-art (SOTA) model while reducing\ninference time. Furthermore, our proposed YOLOv8+SE-M3 model achieves the\nhighest mAP@50 value of 67.07%, exceeding the SOTA performance. The\nimplementation of this work is available at\nhttps://github.com/RuiyangJu/FCE-YOLOv8."
                },
                "authors": [
                    {
                        "name": "Rui-Yang Ju"
                    },
                    {
                        "name": "Chun-Tse Chien"
                    },
                    {
                        "name": "Enkaer Xieerke"
                    },
                    {
                        "name": "Jen-Shiun Chiang"
                    }
                ],
                "author_detail": {
                    "name": "Jen-Shiun Chiang"
                },
                "author": "Jen-Shiun Chiang",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2407.03163",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01031v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01031v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.00867v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.00867v3",
                "updated": "2024-11-07T15:41:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    15,
                    41,
                    38,
                    3,
                    312,
                    0
                ],
                "published": "2024-03-01T03:29:54Z",
                "published_parsed": [
                    2024,
                    3,
                    1,
                    3,
                    29,
                    54,
                    4,
                    61,
                    0
                ],
                "title": "Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by\n  Exploring Refusal Loss Landscapes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by\n  Exploring Refusal Loss Landscapes"
                },
                "summary": "Large Language Models (LLMs) are becoming a prominent generative AI tool,\nwhere the user enters a query and the LLM generates an answer. To reduce harm\nand misuse, efforts have been made to align these LLMs to human values using\nadvanced training techniques such as Reinforcement Learning from Human Feedback\n(RLHF). However, recent studies have highlighted the vulnerability of LLMs to\nadversarial jailbreak attempts aiming at subverting the embedded safety\nguardrails. To address this challenge, this paper defines and investigates the\nRefusal Loss of LLMs and then proposes a method called Gradient Cuff to detect\njailbreak attempts. Gradient Cuff exploits the unique properties observed in\nthe refusal loss landscape, including functional values and its smoothness, to\ndesign an effective two-step detection strategy. Experimental results on two\naligned LLMs (LLaMA-2-7B-Chat and Vicuna-7B-V1.5) and six types of jailbreak\nattacks (GCG, AutoDAN, PAIR, TAP, Base64, and LRL) show that Gradient Cuff can\nsignificantly improve the LLM's rejection capability for malicious jailbreak\nqueries, while maintaining the model's performance for benign user queries by\nadjusting the detection threshold.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are becoming a prominent generative AI tool,\nwhere the user enters a query and the LLM generates an answer. To reduce harm\nand misuse, efforts have been made to align these LLMs to human values using\nadvanced training techniques such as Reinforcement Learning from Human Feedback\n(RLHF). However, recent studies have highlighted the vulnerability of LLMs to\nadversarial jailbreak attempts aiming at subverting the embedded safety\nguardrails. To address this challenge, this paper defines and investigates the\nRefusal Loss of LLMs and then proposes a method called Gradient Cuff to detect\njailbreak attempts. Gradient Cuff exploits the unique properties observed in\nthe refusal loss landscape, including functional values and its smoothness, to\ndesign an effective two-step detection strategy. Experimental results on two\naligned LLMs (LLaMA-2-7B-Chat and Vicuna-7B-V1.5) and six types of jailbreak\nattacks (GCG, AutoDAN, PAIR, TAP, Base64, and LRL) show that Gradient Cuff can\nsignificantly improve the LLM's rejection capability for malicious jailbreak\nqueries, while maintaining the model's performance for benign user queries by\nadjusting the detection threshold."
                },
                "authors": [
                    {
                        "name": "Xiaomeng Hu"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    },
                    {
                        "name": "Tsung-Yi Ho"
                    }
                ],
                "author_detail": {
                    "name": "Tsung-Yi Ho"
                },
                "author": "Tsung-Yi Ho",
                "arxiv_comment": "Accepted by NeurIPS 2024. Project page:\n  https://huggingface.co/spaces/TrustSafeAI/GradientCuff-Jailbreak-Defense",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.00867v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.00867v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04799v1",
                "updated": "2024-11-07T15:38:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    15,
                    38,
                    25,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T15:38:25Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    15,
                    38,
                    25,
                    3,
                    312,
                    0
                ],
                "title": "Kwai-STaR: Transform LLMs into State-Transition Reasoners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kwai-STaR: Transform LLMs into State-Transition Reasoners"
                },
                "summary": "Mathematical reasoning presents a significant challenge to the cognitive\ncapabilities of LLMs. Various methods have been proposed to enhance the\nmathematical ability of LLMs. However, few recognize the value of state\ntransition for LLM reasoning. In this work, we define mathematical\nproblem-solving as a process of transiting from an initial unsolved state to\nthe final resolved state, and propose Kwai-STaR framework, which transforms\nLLMs into State-Transition Reasoners to improve their intuitive reasoning\ncapabilities. Our approach comprises three main steps: (1) Define the state\nspace tailored to the mathematical reasoning. (2) Generate state-transition\ndata based on the state space. (3) Convert original LLMs into State-Transition\nReasoners via a curricular training strategy. Our experiments validate the\neffectiveness of Kwai-STaR in enhancing mathematical reasoning: After training\non the small-scale Kwai-STaR dataset, general LLMs, including Mistral-7B and\nLLaMA-3, achieve considerable performance gain on the GSM8K and GSM-Hard\ndataset. Additionally, the state transition-based design endows Kwai-STaR with\nremarkable training and inference efficiency. Further experiments are underway\nto establish the generality of Kwai-STaR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical reasoning presents a significant challenge to the cognitive\ncapabilities of LLMs. Various methods have been proposed to enhance the\nmathematical ability of LLMs. However, few recognize the value of state\ntransition for LLM reasoning. In this work, we define mathematical\nproblem-solving as a process of transiting from an initial unsolved state to\nthe final resolved state, and propose Kwai-STaR framework, which transforms\nLLMs into State-Transition Reasoners to improve their intuitive reasoning\ncapabilities. Our approach comprises three main steps: (1) Define the state\nspace tailored to the mathematical reasoning. (2) Generate state-transition\ndata based on the state space. (3) Convert original LLMs into State-Transition\nReasoners via a curricular training strategy. Our experiments validate the\neffectiveness of Kwai-STaR in enhancing mathematical reasoning: After training\non the small-scale Kwai-STaR dataset, general LLMs, including Mistral-7B and\nLLaMA-3, achieve considerable performance gain on the GSM8K and GSM-Hard\ndataset. Additionally, the state transition-based design endows Kwai-STaR with\nremarkable training and inference efficiency. Further experiments are underway\nto establish the generality of Kwai-STaR."
                },
                "authors": [
                    {
                        "name": "Xingyu Lu"
                    },
                    {
                        "name": "Yuhang Hu"
                    },
                    {
                        "name": "Changyi Liu"
                    },
                    {
                        "name": "Tianke Zhang"
                    },
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Zhixiang Ding"
                    },
                    {
                        "name": "Shengsheng Qian"
                    },
                    {
                        "name": "Meng Du"
                    },
                    {
                        "name": "Ruiwen Kang"
                    },
                    {
                        "name": "Kaiyu Tang"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Tingting Gao"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Hai-Tao Zheng"
                    },
                    {
                        "name": "Bin Wen"
                    }
                ],
                "author_detail": {
                    "name": "Bin Wen"
                },
                "author": "Bin Wen",
                "arxiv_comment": "6 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04796v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04796v1",
                "updated": "2024-11-07T15:36:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    15,
                    36,
                    49,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T15:36:49Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    15,
                    36,
                    49,
                    3,
                    312,
                    0
                ],
                "title": "MPVO: Motion-Prior based Visual Odometry for PointGoal Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPVO: Motion-Prior based Visual Odometry for PointGoal Navigation"
                },
                "summary": "Visual odometry (VO) is essential for enabling accurate point-goal navigation\nof embodied agents in indoor environments where GPS and compass sensors are\nunreliable and inaccurate. However, traditional VO methods face challenges in\nwide-baseline scenarios, where fast robot motions and low frames per second\n(FPS) during inference hinder their performance, leading to drift and\ncatastrophic failures in point-goal navigation. Recent deep-learned VO methods\nshow robust performance but suffer from sample inefficiency during training;\nhence, they require huge datasets and compute resources. So, we propose a\nrobust and sample-efficient VO pipeline based on motion priors available while\nan agent is navigating an environment. It consists of a training-free\naction-prior based geometric VO module that estimates a coarse relative pose\nwhich is further consumed as a motion prior by a deep-learned VO model, which\nfinally produces a fine relative pose to be used by the navigation policy. This\nstrategy helps our pipeline achieve up to 2x sample efficiency during training\nand demonstrates superior accuracy and robustness in point-goal navigation\ntasks compared to state-of-the-art VO method(s). Realistic indoor environments\nof the Gibson dataset is used in the AI-Habitat simulator to evaluate the\nproposed approach using navigation metrics (like success/SPL) and pose metrics\n(like RPE/ATE). We hope this method further opens a direction of work where\nmotion priors from various sources can be utilized to improve VO estimates and\nachieve better results in embodied navigation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual odometry (VO) is essential for enabling accurate point-goal navigation\nof embodied agents in indoor environments where GPS and compass sensors are\nunreliable and inaccurate. However, traditional VO methods face challenges in\nwide-baseline scenarios, where fast robot motions and low frames per second\n(FPS) during inference hinder their performance, leading to drift and\ncatastrophic failures in point-goal navigation. Recent deep-learned VO methods\nshow robust performance but suffer from sample inefficiency during training;\nhence, they require huge datasets and compute resources. So, we propose a\nrobust and sample-efficient VO pipeline based on motion priors available while\nan agent is navigating an environment. It consists of a training-free\naction-prior based geometric VO module that estimates a coarse relative pose\nwhich is further consumed as a motion prior by a deep-learned VO model, which\nfinally produces a fine relative pose to be used by the navigation policy. This\nstrategy helps our pipeline achieve up to 2x sample efficiency during training\nand demonstrates superior accuracy and robustness in point-goal navigation\ntasks compared to state-of-the-art VO method(s). Realistic indoor environments\nof the Gibson dataset is used in the AI-Habitat simulator to evaluate the\nproposed approach using navigation metrics (like success/SPL) and pose metrics\n(like RPE/ATE). We hope this method further opens a direction of work where\nmotion priors from various sources can be utilized to improve VO estimates and\nachieve better results in embodied navigation tasks."
                },
                "authors": [
                    {
                        "name": "Sayan Paul"
                    },
                    {
                        "name": "Ruddra dev Roychoudhury"
                    },
                    {
                        "name": "Brojeshwar Bhowmick"
                    }
                ],
                "author_detail": {
                    "name": "Brojeshwar Bhowmick"
                },
                "author": "Brojeshwar Bhowmick",
                "arxiv_comment": "Accepted in 50SFM Workshop of the 18th European Conference on\n  Computer Vision (ECCV) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04796v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04796v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04794v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04794v1",
                "updated": "2024-11-07T15:36:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    15,
                    36,
                    5,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T15:36:05Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    15,
                    36,
                    5,
                    3,
                    312,
                    0
                ],
                "title": "AlignXIE: Improving Multilingual Information Extraction by Cross-Lingual\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlignXIE: Improving Multilingual Information Extraction by Cross-Lingual\n  Alignment"
                },
                "summary": "Empirical evidence suggests that LLMs exhibit spontaneous cross-lingual\nalignment. Our findings suggest that although LLMs also demonstrate promising\ncross-lingual alignment in Information Extraction, there remains significant\nimbalance across languages, revealing an underlying deficiency in the IE\nalignment. To address this issue, we propose AlignXIE, a powerful code-based\nLLM that significantly enhances cross-lingual IE alignment through two\nstrategies. Firstly, AlignXIE formulates IE across different languages,\nespecially non-English ones, as code generation tasks, standardizing the\nrepresentation of various schemas using Python classes to ensure consistency of\nthe same ontology in different languages and align the schema. Secondly, it\nincorporates an IE cross-lingual alignment phase through a translated instance\nprediction task proposed in this paper to align the extraction process,\nutilizing ParallelNER, an IE bilingual parallel dataset with 257,190 samples,\ngenerated by our proposed LLM-based automatic pipeline for IE parallel data\nconstruction, with manual annotation to ensure quality. Ultimately, we obtain\nAlignXIE through multilingual IE instruction tuning. Although without training\nin 9 unseen languages, AlignXIE surpasses ChatGPT by $30.17\\%$ and SoTA by\n$20.03\\%$, thereby demonstrating superior cross-lingual IE capabilities.\nComprehensive evaluations on 63 IE benchmarks in Chinese and English under\nvarious settings, demonstrate that AlignXIE significantly enhances\ncross-lingual and multilingual IE through boosting the IE alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empirical evidence suggests that LLMs exhibit spontaneous cross-lingual\nalignment. Our findings suggest that although LLMs also demonstrate promising\ncross-lingual alignment in Information Extraction, there remains significant\nimbalance across languages, revealing an underlying deficiency in the IE\nalignment. To address this issue, we propose AlignXIE, a powerful code-based\nLLM that significantly enhances cross-lingual IE alignment through two\nstrategies. Firstly, AlignXIE formulates IE across different languages,\nespecially non-English ones, as code generation tasks, standardizing the\nrepresentation of various schemas using Python classes to ensure consistency of\nthe same ontology in different languages and align the schema. Secondly, it\nincorporates an IE cross-lingual alignment phase through a translated instance\nprediction task proposed in this paper to align the extraction process,\nutilizing ParallelNER, an IE bilingual parallel dataset with 257,190 samples,\ngenerated by our proposed LLM-based automatic pipeline for IE parallel data\nconstruction, with manual annotation to ensure quality. Ultimately, we obtain\nAlignXIE through multilingual IE instruction tuning. Although without training\nin 9 unseen languages, AlignXIE surpasses ChatGPT by $30.17\\%$ and SoTA by\n$20.03\\%$, thereby demonstrating superior cross-lingual IE capabilities.\nComprehensive evaluations on 63 IE benchmarks in Chinese and English under\nvarious settings, demonstrate that AlignXIE significantly enhances\ncross-lingual and multilingual IE through boosting the IE alignment."
                },
                "authors": [
                    {
                        "name": "Yuxin Zuo"
                    },
                    {
                        "name": "Wenxuan Jiang"
                    },
                    {
                        "name": "Wenxuan Liu"
                    },
                    {
                        "name": "Zixuan Li"
                    },
                    {
                        "name": "Long Bai"
                    },
                    {
                        "name": "Hanbin Wang"
                    },
                    {
                        "name": "Yutao Zeng"
                    },
                    {
                        "name": "Xiaolong Jin"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04794v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04794v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.03955v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.03955v8",
                "updated": "2024-11-07T15:07:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    15,
                    7,
                    22,
                    3,
                    312,
                    0
                ],
                "published": "2024-01-08T15:21:21Z",
                "published_parsed": [
                    2024,
                    1,
                    8,
                    15,
                    21,
                    21,
                    0,
                    8,
                    0
                ],
                "title": "Tiny Time Mixers (TTMs): Fast Pre-trained Models for Enhanced\n  Zero/Few-Shot Forecasting of Multivariate Time Series",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tiny Time Mixers (TTMs): Fast Pre-trained Models for Enhanced\n  Zero/Few-Shot Forecasting of Multivariate Time Series"
                },
                "summary": "Large pre-trained models excel in zero/few-shot learning for language and\nvision tasks but face challenges in multivariate time series (TS) forecasting\ndue to diverse data characteristics. Consequently, recent research efforts have\nfocused on developing pre-trained TS forecasting models. These models, whether\nbuilt from scratch or adapted from large language models (LLMs), excel in\nzero/few-shot forecasting tasks. However, they are limited by slow performance,\nhigh computational demands, and neglect of cross-channel and exogenous\ncorrelations. To address this, we introduce Tiny Time Mixers (TTM), a compact\nmodel (starting from 1M parameters) with effective transfer learning\ncapabilities, trained exclusively on public TS datasets. TTM, based on the\nlight-weight TSMixer architecture, incorporates innovations like adaptive\npatching, diverse resolution sampling, and resolution prefix tuning to handle\npre-training on varied dataset resolutions with minimal model capacity.\nAdditionally, it employs multi-level modeling to capture channel correlations\nand infuse exogenous signals during fine-tuning. TTM outperforms existing\npopular benchmarks in zero/few-shot forecasting by (4-40%), while reducing\ncomputational requirements significantly. Moreover, TTMs are lightweight and\ncan be executed even on CPU-only machines, enhancing usability and fostering\nwider adoption in resource-constrained environments. The model weights for\nreproducibility and research use are available at\nhttps://huggingface.co/ibm/ttm-research-r2/, while enterprise-use weights under\nthe Apache license can be accessed as follows: the initial TTM-Q variant at\nhttps://huggingface.co/ibm-granite/granite-timeseries-ttm-r1, and the latest\nvariants (TTM-B, TTM-E, TTM-A) weights are available at\nhttps://huggingface.co/ibm-granite/granite-timeseries-ttm-r2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large pre-trained models excel in zero/few-shot learning for language and\nvision tasks but face challenges in multivariate time series (TS) forecasting\ndue to diverse data characteristics. Consequently, recent research efforts have\nfocused on developing pre-trained TS forecasting models. These models, whether\nbuilt from scratch or adapted from large language models (LLMs), excel in\nzero/few-shot forecasting tasks. However, they are limited by slow performance,\nhigh computational demands, and neglect of cross-channel and exogenous\ncorrelations. To address this, we introduce Tiny Time Mixers (TTM), a compact\nmodel (starting from 1M parameters) with effective transfer learning\ncapabilities, trained exclusively on public TS datasets. TTM, based on the\nlight-weight TSMixer architecture, incorporates innovations like adaptive\npatching, diverse resolution sampling, and resolution prefix tuning to handle\npre-training on varied dataset resolutions with minimal model capacity.\nAdditionally, it employs multi-level modeling to capture channel correlations\nand infuse exogenous signals during fine-tuning. TTM outperforms existing\npopular benchmarks in zero/few-shot forecasting by (4-40%), while reducing\ncomputational requirements significantly. Moreover, TTMs are lightweight and\ncan be executed even on CPU-only machines, enhancing usability and fostering\nwider adoption in resource-constrained environments. The model weights for\nreproducibility and research use are available at\nhttps://huggingface.co/ibm/ttm-research-r2/, while enterprise-use weights under\nthe Apache license can be accessed as follows: the initial TTM-Q variant at\nhttps://huggingface.co/ibm-granite/granite-timeseries-ttm-r1, and the latest\nvariants (TTM-B, TTM-E, TTM-A) weights are available at\nhttps://huggingface.co/ibm-granite/granite-timeseries-ttm-r2."
                },
                "authors": [
                    {
                        "name": "Vijay Ekambaram"
                    },
                    {
                        "name": "Arindam Jati"
                    },
                    {
                        "name": "Pankaj Dayama"
                    },
                    {
                        "name": "Sumanta Mukherjee"
                    },
                    {
                        "name": "Nam H. Nguyen"
                    },
                    {
                        "name": "Wesley M. Gifford"
                    },
                    {
                        "name": "Chandra Reddy"
                    },
                    {
                        "name": "Jayant Kalagnanam"
                    }
                ],
                "author_detail": {
                    "name": "Jayant Kalagnanam"
                },
                "author": "Jayant Kalagnanam",
                "arxiv_comment": "Accepted at the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.03955v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.03955v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17437v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17437v2",
                "updated": "2024-11-07T15:00:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    15,
                    0,
                    0,
                    3,
                    312,
                    0
                ],
                "published": "2024-08-30T17:41:30Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    17,
                    41,
                    30,
                    4,
                    243,
                    0
                ],
                "title": "SYNTHEVAL: Hybrid Behavioral Testing of NLP Models with Synthetic\n  CheckLists",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SYNTHEVAL: Hybrid Behavioral Testing of NLP Models with Synthetic\n  CheckLists"
                },
                "summary": "Traditional benchmarking in NLP typically involves using static held-out test\nsets. However, this approach often results in an overestimation of performance\nand lacks the ability to offer comprehensive, interpretable, and dynamic\nassessments of NLP models. Recently, works like DynaBench (Kiela et al., 2021)\nand CheckList (Ribeiro et al., 2020) have addressed these limitations through\nbehavioral testing of NLP models with test types generated by a multistep\nhuman-annotated pipeline. Unfortunately, manually creating a variety of test\ntypes requires much human labor, often at prohibitive cost. In this work, we\npropose SYNTHEVAL, a hybrid behavioral testing framework that leverages large\nlanguage models (LLMs) to generate a wide range of test types for a\ncomprehensive evaluation of NLP models. SYNTHEVAL first generates sentences via\nLLMs using controlled generation, and then identifies challenging examples by\ncomparing the predictions made by LLMs with task-specific NLP models. In the\nlast stage, human experts investigate the challenging examples, manually design\ntemplates, and identify the types of failures the taskspecific models\nconsistently exhibit. We apply SYNTHEVAL to two classification tasks, sentiment\nanalysis and toxic language detection, and show that our framework is effective\nin identifying weaknesses of strong models on these tasks. We share our code in\nhttps://github.com/Loreley99/SynthEval_CheckList.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional benchmarking in NLP typically involves using static held-out test\nsets. However, this approach often results in an overestimation of performance\nand lacks the ability to offer comprehensive, interpretable, and dynamic\nassessments of NLP models. Recently, works like DynaBench (Kiela et al., 2021)\nand CheckList (Ribeiro et al., 2020) have addressed these limitations through\nbehavioral testing of NLP models with test types generated by a multistep\nhuman-annotated pipeline. Unfortunately, manually creating a variety of test\ntypes requires much human labor, often at prohibitive cost. In this work, we\npropose SYNTHEVAL, a hybrid behavioral testing framework that leverages large\nlanguage models (LLMs) to generate a wide range of test types for a\ncomprehensive evaluation of NLP models. SYNTHEVAL first generates sentences via\nLLMs using controlled generation, and then identifies challenging examples by\ncomparing the predictions made by LLMs with task-specific NLP models. In the\nlast stage, human experts investigate the challenging examples, manually design\ntemplates, and identify the types of failures the taskspecific models\nconsistently exhibit. We apply SYNTHEVAL to two classification tasks, sentiment\nanalysis and toxic language detection, and show that our framework is effective\nin identifying weaknesses of strong models on these tasks. We share our code in\nhttps://github.com/Loreley99/SynthEval_CheckList."
                },
                "authors": [
                    {
                        "name": "Raoyuan Zhao"
                    },
                    {
                        "name": "Abdullatif Kksal"
                    },
                    {
                        "name": "Yihong Liu"
                    },
                    {
                        "name": "Leonie Weissweiler"
                    },
                    {
                        "name": "Anna Korhonen"
                    },
                    {
                        "name": "Hinrich Schtze"
                    }
                ],
                "author_detail": {
                    "name": "Hinrich Schtze"
                },
                "author": "Hinrich Schtze",
                "arxiv_comment": "EMNLP 2024 - Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17437v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17437v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03883v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03883v2",
                "updated": "2024-11-07T14:57:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    57,
                    14,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-06T12:57:58Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    12,
                    57,
                    58,
                    2,
                    311,
                    0
                ],
                "title": "MEG: Medical Knowledge-Augmented Large Language Models for Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEG: Medical Knowledge-Augmented Large Language Models for Question\n  Answering"
                },
                "summary": "Question answering is a natural language understanding task that involves\nreasoning over both explicit context and unstated, relevant domain knowledge.\nLarge language models (LLMs), which underpin most contemporary question\nanswering systems, struggle to induce how concepts relate in specialized\ndomains such as medicine. Existing medical LLMs are also costly to train. In\nthis work, we present MEG, a parameter-efficient approach for medical\nknowledge-augmented LLMs. MEG uses a lightweight mapping network to integrate\ngraph embeddings into the LLM, enabling it to leverage external knowledge in a\ncost-effective way. We evaluate our method on four popular medical\nmultiple-choice datasets and show that LLMs greatly benefit from the factual\ngrounding provided by knowledge graph embeddings. MEG attains an average of\n+10.2% accuracy over the Mistral-Instruct baseline, and +6.7% over specialized\nmodels like BioMistral. We also show results based on Llama-3. Finally, we show\nthat MEG's performance remains robust to the choice of graph encoder.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question answering is a natural language understanding task that involves\nreasoning over both explicit context and unstated, relevant domain knowledge.\nLarge language models (LLMs), which underpin most contemporary question\nanswering systems, struggle to induce how concepts relate in specialized\ndomains such as medicine. Existing medical LLMs are also costly to train. In\nthis work, we present MEG, a parameter-efficient approach for medical\nknowledge-augmented LLMs. MEG uses a lightweight mapping network to integrate\ngraph embeddings into the LLM, enabling it to leverage external knowledge in a\ncost-effective way. We evaluate our method on four popular medical\nmultiple-choice datasets and show that LLMs greatly benefit from the factual\ngrounding provided by knowledge graph embeddings. MEG attains an average of\n+10.2% accuracy over the Mistral-Instruct baseline, and +6.7% over specialized\nmodels like BioMistral. We also show results based on Llama-3. Finally, we show\nthat MEG's performance remains robust to the choice of graph encoder."
                },
                "authors": [
                    {
                        "name": "Laura Cabello"
                    },
                    {
                        "name": "Carmen Martin-Turrero"
                    },
                    {
                        "name": "Uchenna Akujuobi"
                    },
                    {
                        "name": "Anders Sgaard"
                    },
                    {
                        "name": "Carlos Bobed"
                    }
                ],
                "author_detail": {
                    "name": "Carlos Bobed"
                },
                "author": "Carlos Bobed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03883v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03883v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04759v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04759v1",
                "updated": "2024-11-07T14:56:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    56,
                    11,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T14:56:11Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    56,
                    11,
                    3,
                    312,
                    0
                ],
                "title": "Fast Generation of Weak Lensing Maps with Analytical Point\n  Transformation Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Generation of Weak Lensing Maps with Analytical Point\n  Transformation Functions"
                },
                "summary": "Nonlinear cosmological fields like galaxy density and lensing convergence can\nbe approximately related to Gaussian fields via analytic point transforms. The\nlognormal transform (LN) has been widely used and is a simple example of a\nfunction that relates nonlinear fields to Gaussian fields. We consider more\naccurate General Point-Transformed Gaussian (GPTG) functions for such a mapping\nand apply them to convergence maps. We show that we can create maps that\npreserve the LN's ability to exactly match any desired power spectrum but go\nbeyond LN by significantly improving the accuracy of the probability\ndistribution function (PDF). With the aid of symbolic regression, we find a\nremarkably accurate GPTG function for convergence maps: its higher-order\nmoments, scattering wavelet transform, Minkowski functionals, and peak counts\nmatch those of N-body simulations to the statistical uncertainty expected from\ntomographic lensing maps of the Rubin LSST 10 years survey. Our five-parameter\nfunction performs 2 to 5$\\times$ better than the lognormal. We restrict our\nstudy to scales above about 7 arcmin; baryonic feedback alters the mass\ndistribution on smaller scales. We demonstrate that the GPTG can robustly\nemulate variations in cosmological parameters due to the simplicity of the\nanalytic transform. This opens up several possible applications, such as\nfield-level inference, rapid covariance estimation, and other uses based on the\ngeneration of arbitrarily many maps with laptop-level computation capability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonlinear cosmological fields like galaxy density and lensing convergence can\nbe approximately related to Gaussian fields via analytic point transforms. The\nlognormal transform (LN) has been widely used and is a simple example of a\nfunction that relates nonlinear fields to Gaussian fields. We consider more\naccurate General Point-Transformed Gaussian (GPTG) functions for such a mapping\nand apply them to convergence maps. We show that we can create maps that\npreserve the LN's ability to exactly match any desired power spectrum but go\nbeyond LN by significantly improving the accuracy of the probability\ndistribution function (PDF). With the aid of symbolic regression, we find a\nremarkably accurate GPTG function for convergence maps: its higher-order\nmoments, scattering wavelet transform, Minkowski functionals, and peak counts\nmatch those of N-body simulations to the statistical uncertainty expected from\ntomographic lensing maps of the Rubin LSST 10 years survey. Our five-parameter\nfunction performs 2 to 5$\\times$ better than the lognormal. We restrict our\nstudy to scales above about 7 arcmin; baryonic feedback alters the mass\ndistribution on smaller scales. We demonstrate that the GPTG can robustly\nemulate variations in cosmological parameters due to the simplicity of the\nanalytic transform. This opens up several possible applications, such as\nfield-level inference, rapid covariance estimation, and other uses based on the\ngeneration of arbitrarily many maps with laptop-level computation capability."
                },
                "authors": [
                    {
                        "name": "Kunhao Zhong"
                    },
                    {
                        "name": "Gary Bernstein"
                    },
                    {
                        "name": "Supranta S. Boruah"
                    },
                    {
                        "name": "Bhuvnesh Jain"
                    },
                    {
                        "name": "Sanjit Kobla"
                    }
                ],
                "author_detail": {
                    "name": "Sanjit Kobla"
                },
                "author": "Sanjit Kobla",
                "arxiv_comment": "16 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04759v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04759v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08673v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08673v2",
                "updated": "2024-11-07T14:49:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    49,
                    28,
                    3,
                    312,
                    0
                ],
                "published": "2024-10-11T09:59:21Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    9,
                    59,
                    21,
                    4,
                    285,
                    0
                ],
                "title": "SpikeBottleNet: Spike-Driven Feature Compression Architecture for\n  Edge-Cloud Co-Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpikeBottleNet: Spike-Driven Feature Compression Architecture for\n  Edge-Cloud Co-Inference"
                },
                "summary": "Edge-cloud co-inference enables efficient deep neural network (DNN)\ndeployment by splitting the architecture between an edge device and cloud\nserver, crucial for resource-constraint edge devices. This approach requires\nbalancing on-device computations and communication costs, often achieved\nthrough compressed intermediate feature transmission. Conventional DNN\narchitectures require continuous data processing and floating point\nactivations, leading to considerable energy consumption and increased feature\nsizes, thus raising transmission costs. This challenge motivates exploring\nbinary, event-driven activations using spiking neural networks (SNNs), known\nfor their extreme energy efficiency. In this research, we propose\nSpikeBottleNet, a novel architecture for edge-cloud co-inference systems that\nintegrates a spiking neuron model to significantly reduce energy consumption on\nedge devices. A key innovation of our study is an intermediate feature\ncompression technique tailored for SNNs for efficient feature transmission.\nThis technique leverages a split computing approach to strategically place\nencoder-decoder bottleneck units within complex deep architectures like ResNet\nand MobileNet. Experimental results demonstrate that SpikeBottleNet achieves up\nto 256x bit compression in the final convolutional layer of ResNet, with\nminimal accuracy loss (0.16%). Additionally, our approach enhances edge device\nenergy efficiency by up to 144x compared to the baseline BottleNet, making it\nideal for resource-limited edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge-cloud co-inference enables efficient deep neural network (DNN)\ndeployment by splitting the architecture between an edge device and cloud\nserver, crucial for resource-constraint edge devices. This approach requires\nbalancing on-device computations and communication costs, often achieved\nthrough compressed intermediate feature transmission. Conventional DNN\narchitectures require continuous data processing and floating point\nactivations, leading to considerable energy consumption and increased feature\nsizes, thus raising transmission costs. This challenge motivates exploring\nbinary, event-driven activations using spiking neural networks (SNNs), known\nfor their extreme energy efficiency. In this research, we propose\nSpikeBottleNet, a novel architecture for edge-cloud co-inference systems that\nintegrates a spiking neuron model to significantly reduce energy consumption on\nedge devices. A key innovation of our study is an intermediate feature\ncompression technique tailored for SNNs for efficient feature transmission.\nThis technique leverages a split computing approach to strategically place\nencoder-decoder bottleneck units within complex deep architectures like ResNet\nand MobileNet. Experimental results demonstrate that SpikeBottleNet achieves up\nto 256x bit compression in the final convolutional layer of ResNet, with\nminimal accuracy loss (0.16%). Additionally, our approach enhances edge device\nenergy efficiency by up to 144x compared to the baseline BottleNet, making it\nideal for resource-limited edge devices."
                },
                "authors": [
                    {
                        "name": "Maruf Hassan"
                    },
                    {
                        "name": "Steven Davy"
                    }
                ],
                "author_detail": {
                    "name": "Steven Davy"
                },
                "author": "Steven Davy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08673v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08673v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14331v2",
                "updated": "2024-11-07T14:48:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    48,
                    18,
                    3,
                    312,
                    0
                ],
                "published": "2024-10-18T09:43:30Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    9,
                    43,
                    30,
                    4,
                    292,
                    0
                ],
                "title": "ChartifyText: Automated Chart Generation from Data-Involved Texts via\n  LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChartifyText: Automated Chart Generation from Data-Involved Texts via\n  LLM"
                },
                "summary": "Text documents with numerical values involved are widely used in various\napplications such as scientific research, economy, public health and\njournalism. However, it is difficult for readers to quickly interpret such\ndata-involved texts and gain deep insights. To fill this research gap, this\nwork aims to automatically generate charts to accurately convey the underlying\ndata and ideas to readers, which is essentially a challenging task. The\nchallenges originate from text ambiguities, intrinsic sparsity and uncertainty\nof data in text documents, and subjective sentiment differences. Specifically,\nwe propose ChartifyText, a novel fully-automated approach that leverages Large\nLanguage Models (LLMs) to convert complex data-involved texts to expressive\ncharts. It consists of two major modules: tabular data inference and expressive\nchart generation. The tabular data inference module employs systematic prompt\nengineering to guide the LLM (e.g., GPT-4) to infer table data, where data\nranges, uncertainties, missing data values and corresponding subjective\nsentiments are explicitly considered. The expressive chart generation module\naugments standard charts with intuitive visual encodings and concise texts to\naccurately convey the underlying data and insights. We extensively evaluate the\neffectiveness of ChartifyText on real-world data-involved text documents\nthrough case studies, in-depth interviews with three visualization experts, and\na carefully-designed user study with 15 participants. The results demonstrate\nthe usefulness and effectiveness of ChartifyText in helping readers efficiently\nand effectively make sense of data-involved texts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text documents with numerical values involved are widely used in various\napplications such as scientific research, economy, public health and\njournalism. However, it is difficult for readers to quickly interpret such\ndata-involved texts and gain deep insights. To fill this research gap, this\nwork aims to automatically generate charts to accurately convey the underlying\ndata and ideas to readers, which is essentially a challenging task. The\nchallenges originate from text ambiguities, intrinsic sparsity and uncertainty\nof data in text documents, and subjective sentiment differences. Specifically,\nwe propose ChartifyText, a novel fully-automated approach that leverages Large\nLanguage Models (LLMs) to convert complex data-involved texts to expressive\ncharts. It consists of two major modules: tabular data inference and expressive\nchart generation. The tabular data inference module employs systematic prompt\nengineering to guide the LLM (e.g., GPT-4) to infer table data, where data\nranges, uncertainties, missing data values and corresponding subjective\nsentiments are explicitly considered. The expressive chart generation module\naugments standard charts with intuitive visual encodings and concise texts to\naccurately convey the underlying data and insights. We extensively evaluate the\neffectiveness of ChartifyText on real-world data-involved text documents\nthrough case studies, in-depth interviews with three visualization experts, and\na carefully-designed user study with 15 participants. The results demonstrate\nthe usefulness and effectiveness of ChartifyText in helping readers efficiently\nand effectively make sense of data-involved texts."
                },
                "authors": [
                    {
                        "name": "Songheng Zhang"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Toby Jia-Jun Li"
                    },
                    {
                        "name": "Qiaomu Shen"
                    },
                    {
                        "name": "Yixin Cao"
                    },
                    {
                        "name": "Yong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yong Wang"
                },
                "author": "Yong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04732v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04732v1",
                "updated": "2024-11-07T14:12:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    12,
                    0,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T14:12:00Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    12,
                    0,
                    3,
                    312,
                    0
                ],
                "title": "Convolutional Differentiable Logic Gate Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Convolutional Differentiable Logic Gate Networks"
                },
                "summary": "With the increasing inference cost of machine learning models, there is a\ngrowing interest in models with fast and efficient inference. Recently, an\napproach for learning logic gate networks directly via a differentiable\nrelaxation was proposed. Logic gate networks are faster than conventional\nneural network approaches because their inference only requires logic gate\noperators such as NAND, OR, and XOR, which are the underlying building blocks\nof current hardware and can be efficiently executed. We build on this idea,\nextending it by deep logic gate tree convolutions, logical OR pooling, and\nresidual initializations. This allows scaling logic gate networks up by over\none order of magnitude and utilizing the paradigm of convolution. On CIFAR-10,\nwe achieve an accuracy of 86.29% using only 61 million logic gates, which\nimproves over the SOTA while being 29x smaller.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing inference cost of machine learning models, there is a\ngrowing interest in models with fast and efficient inference. Recently, an\napproach for learning logic gate networks directly via a differentiable\nrelaxation was proposed. Logic gate networks are faster than conventional\nneural network approaches because their inference only requires logic gate\noperators such as NAND, OR, and XOR, which are the underlying building blocks\nof current hardware and can be efficiently executed. We build on this idea,\nextending it by deep logic gate tree convolutions, logical OR pooling, and\nresidual initializations. This allows scaling logic gate networks up by over\none order of magnitude and utilizing the paradigm of convolution. On CIFAR-10,\nwe achieve an accuracy of 86.29% using only 61 million logic gates, which\nimproves over the SOTA while being 29x smaller."
                },
                "authors": [
                    {
                        "name": "Felix Petersen"
                    },
                    {
                        "name": "Hilde Kuehne"
                    },
                    {
                        "name": "Christian Borgelt"
                    },
                    {
                        "name": "Julian Welzel"
                    },
                    {
                        "name": "Stefano Ermon"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Ermon"
                },
                "author": "Stefano Ermon",
                "arxiv_comment": "Published at NeurIPS 2024 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04732v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04732v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04728v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04728v1",
                "updated": "2024-11-07T14:08:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    8,
                    35,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T14:08:35Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    8,
                    35,
                    3,
                    312,
                    0
                ],
                "title": "Neuromorphic Wireless Split Computing with Multi-Level Spikes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuromorphic Wireless Split Computing with Multi-Level Spikes"
                },
                "summary": "Inspired by biological processes, neuromorphic computing utilizes spiking\nneural networks (SNNs) to perform inference tasks, offering significant\nefficiency gains for workloads involving sequential data. Recent advances in\nhardware and software have demonstrated that embedding a few bits of payload in\neach spike exchanged between the spiking neurons can further enhance inference\naccuracy. In a split computing architecture, where the SNN is divided across\ntwo separate devices, the device storing the first layers must share\ninformation about the spikes generated by the local output neurons with the\nother device. Consequently, the advantages of multi-level spikes must be\nbalanced against the challenges of transmitting additional bits between the two\ndevices.\n  This paper addresses these challenges by investigating a wireless\nneuromorphic split computing architecture employing multi-level SNNs. For this\nsystem, we present the design of digital and analog modulation schemes\noptimized for an orthogonal frequency division multiplexing (OFDM) radio\ninterface. Simulation and experimental results using software-defined radios\nprovide insights into the performance gains of multi-level SNN models and the\noptimal payload size as a function of the quality of the connection between a\ntransmitter and receiver.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inspired by biological processes, neuromorphic computing utilizes spiking\nneural networks (SNNs) to perform inference tasks, offering significant\nefficiency gains for workloads involving sequential data. Recent advances in\nhardware and software have demonstrated that embedding a few bits of payload in\neach spike exchanged between the spiking neurons can further enhance inference\naccuracy. In a split computing architecture, where the SNN is divided across\ntwo separate devices, the device storing the first layers must share\ninformation about the spikes generated by the local output neurons with the\nother device. Consequently, the advantages of multi-level spikes must be\nbalanced against the challenges of transmitting additional bits between the two\ndevices.\n  This paper addresses these challenges by investigating a wireless\nneuromorphic split computing architecture employing multi-level SNNs. For this\nsystem, we present the design of digital and analog modulation schemes\noptimized for an orthogonal frequency division multiplexing (OFDM) radio\ninterface. Simulation and experimental results using software-defined radios\nprovide insights into the performance gains of multi-level SNN models and the\noptimal payload size as a function of the quality of the connection between a\ntransmitter and receiver."
                },
                "authors": [
                    {
                        "name": "Dengyu Wu"
                    },
                    {
                        "name": "Jiechen Chen"
                    },
                    {
                        "name": "Bipin Rajendran"
                    },
                    {
                        "name": "H. Vincent Poor"
                    },
                    {
                        "name": "Osvaldo Simeone"
                    }
                ],
                "author_detail": {
                    "name": "Osvaldo Simeone"
                },
                "author": "Osvaldo Simeone",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04728v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04728v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17113v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17113v3",
                "updated": "2024-11-07T13:55:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    13,
                    55,
                    50,
                    3,
                    312,
                    0
                ],
                "published": "2024-09-25T17:27:02Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    27,
                    2,
                    2,
                    269,
                    0
                ],
                "title": "Characterizing stable regions in the residual stream of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing stable regions in the residual stream of LLMs"
                },
                "summary": "We identify stable regions in the residual stream of Transformers, where the\nmodel's output remains insensitive to small activation changes, but exhibits\nhigh sensitivity at region boundaries. These regions emerge during training and\nbecome more defined as training progresses or model size increases. The regions\nappear to be much larger than previously studied polytopes. Our analysis\nsuggests that these stable regions align with semantic distinctions, where\nsimilar prompts cluster within regions, and activations from the same region\nlead to similar next token predictions. This work provides a promising research\ndirection for understanding the complexity of neural networks, shedding light\non training dynamics, and advancing interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We identify stable regions in the residual stream of Transformers, where the\nmodel's output remains insensitive to small activation changes, but exhibits\nhigh sensitivity at region boundaries. These regions emerge during training and\nbecome more defined as training progresses or model size increases. The regions\nappear to be much larger than previously studied polytopes. Our analysis\nsuggests that these stable regions align with semantic distinctions, where\nsimilar prompts cluster within regions, and activations from the same region\nlead to similar next token predictions. This work provides a promising research\ndirection for understanding the complexity of neural networks, shedding light\non training dynamics, and advancing interpretability."
                },
                "authors": [
                    {
                        "name": "Jett Janiak"
                    },
                    {
                        "name": "Jacek Karwowski"
                    },
                    {
                        "name": "Chatrik Singh Mangat"
                    },
                    {
                        "name": "Giorgi Giglemiani"
                    },
                    {
                        "name": "Nora Petrova"
                    },
                    {
                        "name": "Stefan Heimersheim"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Heimersheim"
                },
                "author": "Stefan Heimersheim",
                "arxiv_comment": "Published at Scientific Methods for Understanding Deep Learning\n  (SciForDL) workshop at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17113v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17113v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04708v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04708v1",
                "updated": "2024-11-07T13:45:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    13,
                    45,
                    26,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T13:45:26Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    13,
                    45,
                    26,
                    3,
                    312,
                    0
                ],
                "title": "Exploring Hierarchical Molecular Graph Representation in Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Hierarchical Molecular Graph Representation in Multimodal LLMs"
                },
                "summary": "Following the milestones in large language models (LLMs) and multimodal\nmodels, we have seen a surge in applying LLMs to biochemical tasks. Leveraging\ngraph features and molecular text representations, LLMs can tackle various\ntasks, such as predicting chemical reaction outcomes and describing molecular\nproperties. However, most current work overlooks the multi-level nature of\ngraph features. The impact of different feature levels on LLMs and the\nimportance of each level remain unexplored, and it is possible that different\nchemistry tasks require different feature levels. In this work, we first\ninvestigate the effect of feature granularity by fusing GNN-generated feature\ntokens, discovering that even reducing all tokens to a single token does not\nsignificantly impact performance. We then explore the effect of various feature\nlevels on performance, finding that both the quality of LLM-generated molecules\nand performance on different tasks benefit from different feature levels. We\nconclude with two key insights: (1) current molecular Multimodal LLMs(MLLMs)\nlack a comprehensive understanding of graph features, and (2) static processing\nis not sufficient for hierarchical graph feature. Our code will be publicly\navailable soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Following the milestones in large language models (LLMs) and multimodal\nmodels, we have seen a surge in applying LLMs to biochemical tasks. Leveraging\ngraph features and molecular text representations, LLMs can tackle various\ntasks, such as predicting chemical reaction outcomes and describing molecular\nproperties. However, most current work overlooks the multi-level nature of\ngraph features. The impact of different feature levels on LLMs and the\nimportance of each level remain unexplored, and it is possible that different\nchemistry tasks require different feature levels. In this work, we first\ninvestigate the effect of feature granularity by fusing GNN-generated feature\ntokens, discovering that even reducing all tokens to a single token does not\nsignificantly impact performance. We then explore the effect of various feature\nlevels on performance, finding that both the quality of LLM-generated molecules\nand performance on different tasks benefit from different feature levels. We\nconclude with two key insights: (1) current molecular Multimodal LLMs(MLLMs)\nlack a comprehensive understanding of graph features, and (2) static processing\nis not sufficient for hierarchical graph feature. Our code will be publicly\navailable soon."
                },
                "authors": [
                    {
                        "name": "Chengxin Hu"
                    },
                    {
                        "name": "Hao Li"
                    }
                ],
                "author_detail": {
                    "name": "Hao Li"
                },
                "author": "Hao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04708v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04708v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04704v1",
                "updated": "2024-11-07T13:39:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    13,
                    39,
                    14,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T13:39:14Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    13,
                    39,
                    14,
                    3,
                    312,
                    0
                ],
                "title": "Distinguishing LLM-generated from Human-written Code by Contrastive\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distinguishing LLM-generated from Human-written Code by Contrastive\n  Learning"
                },
                "summary": "Large language models (LLMs), such as ChatGPT released by OpenAI, have\nattracted significant attention from both industry and academia due to their\ndemonstrated ability to generate high-quality content for various tasks.\nDespite the impressive capabilities of LLMs, there are growing concerns\nregarding their potential risks in various fields, such as news, education, and\nsoftware engineering. Recently, several commercial and open-source\nLLM-generated content detectors have been proposed, which, however, are\nprimarily designed for detecting natural language content without considering\nthe specific characteristics of program code. This paper aims to fill this gap\nby proposing a novel ChatGPT-generated code detector, CodeGPTSensor, based on a\ncontrastive learning framework and a semantic encoder built with UniXcoder. To\nassess the effectiveness of CodeGPTSensor on differentiating ChatGPT-generated\ncode from human-written code, we first curate a large-scale Human and Machine\ncomparison Corpus (HMCorp), which includes 550K pairs of human-written and\nChatGPT-generated code (i.e., 288K Python code pairs and 222K Java code pairs).\nBased on the HMCorp dataset, our qualitative and quantitative analysis of the\ncharacteristics of ChatGPT-generated code reveals the challenge and opportunity\nof distinguishing ChatGPT-generated code from human-written code with their\nrepresentative features. Our experimental results indicate that CodeGPTSensor\ncan effectively identify ChatGPT-generated code, outperforming all selected\nbaselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), such as ChatGPT released by OpenAI, have\nattracted significant attention from both industry and academia due to their\ndemonstrated ability to generate high-quality content for various tasks.\nDespite the impressive capabilities of LLMs, there are growing concerns\nregarding their potential risks in various fields, such as news, education, and\nsoftware engineering. Recently, several commercial and open-source\nLLM-generated content detectors have been proposed, which, however, are\nprimarily designed for detecting natural language content without considering\nthe specific characteristics of program code. This paper aims to fill this gap\nby proposing a novel ChatGPT-generated code detector, CodeGPTSensor, based on a\ncontrastive learning framework and a semantic encoder built with UniXcoder. To\nassess the effectiveness of CodeGPTSensor on differentiating ChatGPT-generated\ncode from human-written code, we first curate a large-scale Human and Machine\ncomparison Corpus (HMCorp), which includes 550K pairs of human-written and\nChatGPT-generated code (i.e., 288K Python code pairs and 222K Java code pairs).\nBased on the HMCorp dataset, our qualitative and quantitative analysis of the\ncharacteristics of ChatGPT-generated code reveals the challenge and opportunity\nof distinguishing ChatGPT-generated code from human-written code with their\nrepresentative features. Our experimental results indicate that CodeGPTSensor\ncan effectively identify ChatGPT-generated code, outperforming all selected\nbaselines."
                },
                "authors": [
                    {
                        "name": "Xiaodan Xu"
                    },
                    {
                        "name": "Chao Ni"
                    },
                    {
                        "name": "Xinrong Guo"
                    },
                    {
                        "name": "Shaoxuan Liu"
                    },
                    {
                        "name": "Xiaoya Wang"
                    },
                    {
                        "name": "Kui Liu"
                    },
                    {
                        "name": "Xiaohu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohu Yang"
                },
                "author": "Xiaohu Yang",
                "arxiv_comment": "30 pages, 6 figures, Accepted by TOSEM'24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04691v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04691v1",
                "updated": "2024-11-07T13:23:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    13,
                    23,
                    57,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T13:23:57Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    13,
                    23,
                    57,
                    3,
                    312,
                    0
                ],
                "title": "AWARE Narrator and the Utilization of Large Language Models to Extract\n  Behavioral Insights from Smartphone Sensing Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AWARE Narrator and the Utilization of Large Language Models to Extract\n  Behavioral Insights from Smartphone Sensing Data"
                },
                "summary": "Smartphones, equipped with an array of sensors, have become valuable tools\nfor personal sensing. Particularly in digital health, smartphones facilitate\nthe tracking of health-related behaviors and contexts, contributing\nsignificantly to digital phenotyping, a process where data from digital\ninteractions is analyzed to infer behaviors and assess mental health.\nTraditional methods process raw sensor data into information features for\nstatistical and machine learning analyses. In this paper, we introduce a novel\napproach that systematically converts smartphone-collected data into\nstructured, chronological narratives. The AWARE Narrator translates\nquantitative smartphone sensing data into English language descriptions,\nforming comprehensive narratives of an individual's activities. We apply the\nframework to the data collected from university students over a week,\ndemonstrating the potential of utilizing the narratives to summarize individual\nbehavior, and analyzing psychological states by leveraging large language\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smartphones, equipped with an array of sensors, have become valuable tools\nfor personal sensing. Particularly in digital health, smartphones facilitate\nthe tracking of health-related behaviors and contexts, contributing\nsignificantly to digital phenotyping, a process where data from digital\ninteractions is analyzed to infer behaviors and assess mental health.\nTraditional methods process raw sensor data into information features for\nstatistical and machine learning analyses. In this paper, we introduce a novel\napproach that systematically converts smartphone-collected data into\nstructured, chronological narratives. The AWARE Narrator translates\nquantitative smartphone sensing data into English language descriptions,\nforming comprehensive narratives of an individual's activities. We apply the\nframework to the data collected from university students over a week,\ndemonstrating the potential of utilizing the narratives to summarize individual\nbehavior, and analyzing psychological states by leveraging large language\nmodels."
                },
                "authors": [
                    {
                        "name": "Tianyi Zhang"
                    },
                    {
                        "name": "Miu Kojima"
                    },
                    {
                        "name": "Simon D'Alfonso"
                    }
                ],
                "author_detail": {
                    "name": "Simon D'Alfonso"
                },
                "author": "Simon D'Alfonso",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04691v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04691v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04682v1",
                "updated": "2024-11-07T13:13:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    13,
                    13,
                    23,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T13:13:23Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    13,
                    13,
                    23,
                    3,
                    312,
                    0
                ],
                "title": "DNN-based 3D Cloud Retrieval for Variable Solar Illumination and\n  Multiview Spaceborne Imaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DNN-based 3D Cloud Retrieval for Variable Solar Illumination and\n  Multiview Spaceborne Imaging"
                },
                "summary": "Climate studies often rely on remotely sensed images to retrieve\ntwo-dimensional maps of cloud properties. To advance volumetric analysis, we\nfocus on recovering the three-dimensional (3D) heterogeneous extinction\ncoefficient field of shallow clouds using multiview remote sensing data.\nClimate research requires large-scale worldwide statistics. To enable scalable\ndata processing, previous deep neural networks (DNNs) can infer at spaceborne\nremote sensing downlink rates. However, prior methods are limited to a fixed\nsolar illumination direction. In this work, we introduce the first scalable\nDNN-based system for 3D cloud retrieval that accommodates varying camera poses\nand solar directions. By integrating multiview cloud intensity images with\ncamera poses and solar direction data, we achieve greater flexibility in\nrecovery. Training of the DNN is performed by a novel two-stage scheme to\naddress the high number of degrees of freedom in this problem. Our approach\nshows substantial improvements over previous state-of-the-art, particularly in\nhandling variations in the sun's zenith angle.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Climate studies often rely on remotely sensed images to retrieve\ntwo-dimensional maps of cloud properties. To advance volumetric analysis, we\nfocus on recovering the three-dimensional (3D) heterogeneous extinction\ncoefficient field of shallow clouds using multiview remote sensing data.\nClimate research requires large-scale worldwide statistics. To enable scalable\ndata processing, previous deep neural networks (DNNs) can infer at spaceborne\nremote sensing downlink rates. However, prior methods are limited to a fixed\nsolar illumination direction. In this work, we introduce the first scalable\nDNN-based system for 3D cloud retrieval that accommodates varying camera poses\nand solar directions. By integrating multiview cloud intensity images with\ncamera poses and solar direction data, we achieve greater flexibility in\nrecovery. Training of the DNN is performed by a novel two-stage scheme to\naddress the high number of degrees of freedom in this problem. Our approach\nshows substantial improvements over previous state-of-the-art, particularly in\nhandling variations in the sun's zenith angle."
                },
                "authors": [
                    {
                        "name": "Tamar Klein"
                    },
                    {
                        "name": "Tom Aizenberg"
                    },
                    {
                        "name": "Roi Ronen"
                    }
                ],
                "author_detail": {
                    "name": "Roi Ronen"
                },
                "author": "Roi Ronen",
                "arxiv_comment": "4 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04679v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04679v1",
                "updated": "2024-11-07T13:08:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    13,
                    8,
                    4,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T13:08:04Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    13,
                    8,
                    4,
                    3,
                    312,
                    0
                ],
                "title": "CaPo: Cooperative Plan Optimization for Efficient Embodied Multi-Agent\n  Cooperation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaPo: Cooperative Plan Optimization for Efficient Embodied Multi-Agent\n  Cooperation"
                },
                "summary": "In this work, we address the cooperation problem among large language model\n(LLM) based embodied agents, where agents must cooperate to achieve a common\ngoal. Previous methods often execute actions extemporaneously and incoherently,\nwithout long-term strategic and cooperative planning, leading to redundant\nsteps, failures, and even serious repercussions in complex tasks like\nsearch-and-rescue missions where discussion and cooperative plan are crucial.\nTo solve this issue, we propose Cooperative Plan Optimization (CaPo) to enhance\nthe cooperation efficiency of LLM-based embodied agents. Inspired by human\ncooperation schemes, CaPo improves cooperation efficiency with two phases: 1)\nmeta-plan generation, and 2) progress-adaptive meta-plan and execution. In the\nfirst phase, all agents analyze the task, discuss, and cooperatively create a\nmeta-plan that decomposes the task into subtasks with detailed steps, ensuring\na long-term strategic and coherent plan for efficient coordination. In the\nsecond phase, agents execute tasks according to the meta-plan and dynamically\nadjust it based on their latest progress (e.g., discovering a target object)\nthrough multi-turn discussions. This progress-based adaptation eliminates\nredundant actions, improving the overall cooperation efficiency of agents.\nExperimental results on the ThreeDworld Multi-Agent Transport and Communicative\nWatch-And-Help tasks demonstrate that CaPo achieves much higher task completion\nrate and efficiency compared with state-of-the-arts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we address the cooperation problem among large language model\n(LLM) based embodied agents, where agents must cooperate to achieve a common\ngoal. Previous methods often execute actions extemporaneously and incoherently,\nwithout long-term strategic and cooperative planning, leading to redundant\nsteps, failures, and even serious repercussions in complex tasks like\nsearch-and-rescue missions where discussion and cooperative plan are crucial.\nTo solve this issue, we propose Cooperative Plan Optimization (CaPo) to enhance\nthe cooperation efficiency of LLM-based embodied agents. Inspired by human\ncooperation schemes, CaPo improves cooperation efficiency with two phases: 1)\nmeta-plan generation, and 2) progress-adaptive meta-plan and execution. In the\nfirst phase, all agents analyze the task, discuss, and cooperatively create a\nmeta-plan that decomposes the task into subtasks with detailed steps, ensuring\na long-term strategic and coherent plan for efficient coordination. In the\nsecond phase, agents execute tasks according to the meta-plan and dynamically\nadjust it based on their latest progress (e.g., discovering a target object)\nthrough multi-turn discussions. This progress-based adaptation eliminates\nredundant actions, improving the overall cooperation efficiency of agents.\nExperimental results on the ThreeDworld Multi-Agent Transport and Communicative\nWatch-And-Help tasks demonstrate that CaPo achieves much higher task completion\nrate and efficiency compared with state-of-the-arts."
                },
                "authors": [
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Pan Zhou"
                    },
                    {
                        "name": "Yingjun Du"
                    },
                    {
                        "name": "Ah-Hwee Tan"
                    },
                    {
                        "name": "Cees G. M. Snoek"
                    },
                    {
                        "name": "Jan-Jakob Sonke"
                    },
                    {
                        "name": "Efstratios Gavves"
                    }
                ],
                "author_detail": {
                    "name": "Efstratios Gavves"
                },
                "author": "Efstratios Gavves",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04679v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04677v1",
                "updated": "2024-11-07T13:03:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    13,
                    3,
                    21,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T13:03:21Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    13,
                    3,
                    21,
                    3,
                    312,
                    0
                ],
                "title": "Lightning IR: Straightforward Fine-tuning and Inference of\n  Transformer-based Language Models for Information Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightning IR: Straightforward Fine-tuning and Inference of\n  Transformer-based Language Models for Information Retrieval"
                },
                "summary": "A wide range of transformer-based language models have been proposed for\ninformation retrieval tasks. However, fine-tuning and inference of these models\nis often complex and requires substantial engineering effort. This paper\nintroduces Lightning IR, a PyTorch Lightning-based framework for fine-tuning\nand inference of transformer-based language models for information retrieval.\nLightning IR provides a modular and extensible architecture that supports all\nstages of an information retrieval pipeline: from fine-tuning and indexing to\nsearching and re-ranking. It is designed to be straightforward to use,\nscalable, and reproducible. Lightning IR is available as open-source:\nhttps://github.com/webis-de/lightning-ir.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A wide range of transformer-based language models have been proposed for\ninformation retrieval tasks. However, fine-tuning and inference of these models\nis often complex and requires substantial engineering effort. This paper\nintroduces Lightning IR, a PyTorch Lightning-based framework for fine-tuning\nand inference of transformer-based language models for information retrieval.\nLightning IR provides a modular and extensible architecture that supports all\nstages of an information retrieval pipeline: from fine-tuning and indexing to\nsearching and re-ranking. It is designed to be straightforward to use,\nscalable, and reproducible. Lightning IR is available as open-source:\nhttps://github.com/webis-de/lightning-ir."
                },
                "authors": [
                    {
                        "name": "Ferdinand Schlatt"
                    },
                    {
                        "name": "Maik Frbe"
                    },
                    {
                        "name": "Matthias Hagen"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Hagen"
                },
                "author": "Matthias Hagen",
                "arxiv_comment": "Accepted as a demo at WSDM'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04304v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04304v5",
                "updated": "2024-11-07T12:59:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    12,
                    59,
                    39,
                    3,
                    312,
                    0
                ],
                "published": "2024-05-07T13:27:52Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    13,
                    27,
                    52,
                    1,
                    128,
                    0
                ],
                "title": "Dynamic Speculation Lookahead Accelerates Speculative Decoding of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Speculation Lookahead Accelerates Speculative Decoding of Large\n  Language Models"
                },
                "summary": "Speculative decoding is commonly used for reducing the inference latency of\nlarge language models. Its effectiveness depends highly on the speculation\nlookahead (SL)-the number of tokens generated by the draft model at each\niteration. In this work we show that the common practice of using the same SL\nfor all iterations (static SL) is suboptimal. We introduce DISCO (DynamIc\nSpeCulation lookahead Optimization), a novel method for dynamically selecting\nthe SL. Our experiments with four datasets show that DISCO reaches an average\nspeedup of 10% compared to the best static SL baseline, while generating the\nexact same text.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is commonly used for reducing the inference latency of\nlarge language models. Its effectiveness depends highly on the speculation\nlookahead (SL)-the number of tokens generated by the draft model at each\niteration. In this work we show that the common practice of using the same SL\nfor all iterations (static SL) is suboptimal. We introduce DISCO (DynamIc\nSpeCulation lookahead Optimization), a novel method for dynamically selecting\nthe SL. Our experiments with four datasets show that DISCO reaches an average\nspeedup of 10% compared to the best static SL baseline, while generating the\nexact same text."
                },
                "authors": [
                    {
                        "name": "Jonathan Mamou"
                    },
                    {
                        "name": "Oren Pereg"
                    },
                    {
                        "name": "Daniel Korat"
                    },
                    {
                        "name": "Moshe Berchansky"
                    },
                    {
                        "name": "Nadav Timor"
                    },
                    {
                        "name": "Moshe Wasserblat"
                    },
                    {
                        "name": "Roy Schwartz"
                    }
                ],
                "author_detail": {
                    "name": "Roy Schwartz"
                },
                "author": "Roy Schwartz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04304v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04304v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04671v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04671v1",
                "updated": "2024-11-07T12:55:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    12,
                    55,
                    17,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T12:55:17Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    12,
                    55,
                    17,
                    3,
                    312,
                    0
                ],
                "title": "CUIfy the XR: An Open-Source Package to Embed LLM-powered Conversational\n  Agents in XR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CUIfy the XR: An Open-Source Package to Embed LLM-powered Conversational\n  Agents in XR"
                },
                "summary": "Recent developments in computer graphics, machine learning, and sensor\ntechnologies enable numerous opportunities for extended reality (XR) setups for\neveryday life, from skills training to entertainment. With large corporations\noffering consumer-grade head-mounted displays (HMDs) in an affordable way, it\nis likely that XR will become pervasive, and HMDs will develop as personal\ndevices like smartphones and tablets. However, having intelligent spaces and\nnaturalistic interactions in XR is as important as technological advances so\nthat users grow their engagement in virtual and augmented spaces. To this end,\nlarge language model (LLM)--powered non-player characters (NPCs) with\nspeech-to-text (STT) and text-to-speech (TTS) models bring significant\nadvantages over conventional or pre-scripted NPCs for facilitating more natural\nconversational user interfaces (CUIs) in XR. In this paper, we provide the\ncommunity with an open-source, customizable, extensible, and privacy-aware\nUnity package, CUIfy, that facilitates speech-based NPC-user interaction with\nvarious LLMs, STT, and TTS models. Our package also supports multiple\nLLM-powered NPCs per environment and minimizes the latency between different\ncomputational models through streaming to achieve usable interactions between\nusers and NPCs. We publish our source code in the following repository:\nhttps://gitlab.lrz.de/hctl/cuify",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent developments in computer graphics, machine learning, and sensor\ntechnologies enable numerous opportunities for extended reality (XR) setups for\neveryday life, from skills training to entertainment. With large corporations\noffering consumer-grade head-mounted displays (HMDs) in an affordable way, it\nis likely that XR will become pervasive, and HMDs will develop as personal\ndevices like smartphones and tablets. However, having intelligent spaces and\nnaturalistic interactions in XR is as important as technological advances so\nthat users grow their engagement in virtual and augmented spaces. To this end,\nlarge language model (LLM)--powered non-player characters (NPCs) with\nspeech-to-text (STT) and text-to-speech (TTS) models bring significant\nadvantages over conventional or pre-scripted NPCs for facilitating more natural\nconversational user interfaces (CUIs) in XR. In this paper, we provide the\ncommunity with an open-source, customizable, extensible, and privacy-aware\nUnity package, CUIfy, that facilitates speech-based NPC-user interaction with\nvarious LLMs, STT, and TTS models. Our package also supports multiple\nLLM-powered NPCs per environment and minimizes the latency between different\ncomputational models through streaming to achieve usable interactions between\nusers and NPCs. We publish our source code in the following repository:\nhttps://gitlab.lrz.de/hctl/cuify"
                },
                "authors": [
                    {
                        "name": "Kadir Burak Buldu"
                    },
                    {
                        "name": "Sleyman zdel"
                    },
                    {
                        "name": "Ka Hei Carrie Lau"
                    },
                    {
                        "name": "Mengdi Wang"
                    },
                    {
                        "name": "Daniel Saad"
                    },
                    {
                        "name": "Sofie Schnborn"
                    },
                    {
                        "name": "Auxane Boch"
                    },
                    {
                        "name": "Enkelejda Kasneci"
                    },
                    {
                        "name": "Efe Bozkir"
                    }
                ],
                "author_detail": {
                    "name": "Efe Bozkir"
                },
                "author": "Efe Bozkir",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04671v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04671v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04667v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04667v1",
                "updated": "2024-11-07T12:53:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    12,
                    53,
                    33,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T12:53:33Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    12,
                    53,
                    33,
                    3,
                    312,
                    0
                ],
                "title": "HypoNet Nankai: Rapid hypocenter determination tool for the Nankai\n  Trough subduction zone using physics-informed neural networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HypoNet Nankai: Rapid hypocenter determination tool for the Nankai\n  Trough subduction zone using physics-informed neural networks"
                },
                "summary": "Accurate hypocenter determination in the Nankai Trough subduction zone is\nessential for hazard assessment and advancing our understanding of seismic\nactivity in the region. A handy hypocenter determination tool incorporating a\nrealistic 3D velocity structure, accessible to the scientific community, is\nbeneficial. In this study, we developed HypoNet Nankai, a rapid hypocenter\ndetermination tool based on a physics-informed neural network (PINN) emulator\n(surrogate model) for travel time calculations. This tool leverages a PINN\ntrained to predict P-wave travel times between arbitrary underground sources\nand surface receivers with a realistic 3D P-wave velocity structure model of\nthe Nankai Trough subduction zone that incorporates marine seismic survey data.\nThe PINN embeds physical laws, namely, the Eikonal equation, directly into the\nloss function of training and circumvents the need for labeled training data.\nTo address the training challenges posed by small-scale features in the\nvelocity model, we employed a simple domain decomposition approach and Fourier\nfeature embedding. Once trained, the PINN immediately infers the P-wave travel\ntime, enabling rapid hypocenter determination. The data size required to store\nNNs for travel time calculations is significantly smaller than those of\nconventional travel-time tables. HypoNet Nankai provides high flexibility for\naddition of new observation points. We verified HypoNet Nankai by comparing its\nperformance with a widely used grid-based numerical method for forward travel\ntime calculations and synthetic hypocenter determination. In both tests,\nHypoNet Nankai provided results consistent with those for the conventional\nmethod. HypoNet Nankai offers a rapid, accurate, and easy-to-use hypocenter\ndetermination method for the Nankai Trough subduction zone, with greater data\nefficiency and extendibility compared to conventional approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate hypocenter determination in the Nankai Trough subduction zone is\nessential for hazard assessment and advancing our understanding of seismic\nactivity in the region. A handy hypocenter determination tool incorporating a\nrealistic 3D velocity structure, accessible to the scientific community, is\nbeneficial. In this study, we developed HypoNet Nankai, a rapid hypocenter\ndetermination tool based on a physics-informed neural network (PINN) emulator\n(surrogate model) for travel time calculations. This tool leverages a PINN\ntrained to predict P-wave travel times between arbitrary underground sources\nand surface receivers with a realistic 3D P-wave velocity structure model of\nthe Nankai Trough subduction zone that incorporates marine seismic survey data.\nThe PINN embeds physical laws, namely, the Eikonal equation, directly into the\nloss function of training and circumvents the need for labeled training data.\nTo address the training challenges posed by small-scale features in the\nvelocity model, we employed a simple domain decomposition approach and Fourier\nfeature embedding. Once trained, the PINN immediately infers the P-wave travel\ntime, enabling rapid hypocenter determination. The data size required to store\nNNs for travel time calculations is significantly smaller than those of\nconventional travel-time tables. HypoNet Nankai provides high flexibility for\naddition of new observation points. We verified HypoNet Nankai by comparing its\nperformance with a widely used grid-based numerical method for forward travel\ntime calculations and synthetic hypocenter determination. In both tests,\nHypoNet Nankai provided results consistent with those for the conventional\nmethod. HypoNet Nankai offers a rapid, accurate, and easy-to-use hypocenter\ndetermination method for the Nankai Trough subduction zone, with greater data\nefficiency and extendibility compared to conventional approaches."
                },
                "authors": [
                    {
                        "name": "Ryoichiro Agata"
                    },
                    {
                        "name": "Satoru Baba"
                    },
                    {
                        "name": "Ayako Nakanishi"
                    },
                    {
                        "name": "Yasuyuki Nakamura"
                    }
                ],
                "author_detail": {
                    "name": "Yasuyuki Nakamura"
                },
                "author": "Yasuyuki Nakamura",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04667v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04667v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04664v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04664v1",
                "updated": "2024-11-07T12:49:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    12,
                    49,
                    55,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T12:49:55Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    12,
                    49,
                    55,
                    3,
                    312,
                    0
                ],
                "title": "Tracking and Decoding Rydberg Leakage Error with MBQC",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tracking and Decoding Rydberg Leakage Error with MBQC"
                },
                "summary": "Neutral atom array has emerged as a promising platform for quantum\ncomputation owing to its high-fidelity two-qubit gate, arbitrary connectivity\nand overwhelming scalability. Nevertheless, fault-tolerant quantum computing on\nthe neutral atom platform requires consideration of the types of errors that\nneutral atoms are prone to. One typical and major error is leakage error from\nRydberg state when implementing multi-qubit gate. Such leakage error is harmful\nby propagating multiple pauli errors in quantum circuit. Researchers have\nproposed erasure conversion protocol, which utilizes fast leakage detection to\nconvert leakage error to benign erasure error. This method has a favorable\nerror distance d, but is limited to certain atom species. Here, we propose a\nnew method to deal with such leakage error in measurement-based quantum\ncomputation (MBQC), to which we refer as \"Leakage Tracking\". We remove the\ndemand for mid-circuit leakage detection but infer the probabilities and\nlocations of pauli errors through gate sequence and final leakage detection. We\nshow that this method has an error distance de = d and reaches a high threshold\n1.7% per CZ gate for pure leakage error and perfect final leakage detection. In\npresence of atom loss and other pauli errors, we show the advantage in error\ndistance over erasure conversion when the ratio of leakage error is close to\none.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neutral atom array has emerged as a promising platform for quantum\ncomputation owing to its high-fidelity two-qubit gate, arbitrary connectivity\nand overwhelming scalability. Nevertheless, fault-tolerant quantum computing on\nthe neutral atom platform requires consideration of the types of errors that\nneutral atoms are prone to. One typical and major error is leakage error from\nRydberg state when implementing multi-qubit gate. Such leakage error is harmful\nby propagating multiple pauli errors in quantum circuit. Researchers have\nproposed erasure conversion protocol, which utilizes fast leakage detection to\nconvert leakage error to benign erasure error. This method has a favorable\nerror distance d, but is limited to certain atom species. Here, we propose a\nnew method to deal with such leakage error in measurement-based quantum\ncomputation (MBQC), to which we refer as \"Leakage Tracking\". We remove the\ndemand for mid-circuit leakage detection but infer the probabilities and\nlocations of pauli errors through gate sequence and final leakage detection. We\nshow that this method has an error distance de = d and reaches a high threshold\n1.7% per CZ gate for pure leakage error and perfect final leakage detection. In\npresence of atom loss and other pauli errors, we show the advantage in error\ndistance over erasure conversion when the ratio of leakage error is close to\none."
                },
                "authors": [
                    {
                        "name": "Cheng-Cheng Yu"
                    },
                    {
                        "name": "Zi-Han Chen"
                    },
                    {
                        "name": "Yu-Hao Deng"
                    },
                    {
                        "name": "Ming-Cheng Chen"
                    },
                    {
                        "name": "Chao-Yang Lu"
                    },
                    {
                        "name": "Jian-Wei Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jian-Wei Pan"
                },
                "author": "Jian-Wei Pan",
                "arxiv_comment": "11 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04664v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04664v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04663v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04663v1",
                "updated": "2024-11-07T12:48:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    12,
                    48,
                    39,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T12:48:39Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    12,
                    48,
                    39,
                    3,
                    312,
                    0
                ],
                "title": "Explainable Search and Discovery of Visual Cultural Heritage Collections\n  with Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable Search and Discovery of Visual Cultural Heritage Collections\n  with Multimodal Large Language Models"
                },
                "summary": "Many cultural institutions have made large digitized visual collections\navailable online, often under permissible re-use licences. Creating interfaces\nfor exploring and searching these collections is difficult, particularly in the\nabsence of granular metadata. In this paper, we introduce a method for using\nstate-of-the-art multimodal large language models (LLMs) to enable an\nopen-ended, explainable search and discovery interface for visual collections.\nWe show how our approach can create novel clustering and recommendation systems\nthat avoid common pitfalls of methods based directly on visual embeddings. Of\nparticular interest is the ability to offer concrete textual explanations of\neach recommendation without the need to preselect the features of interest.\nTogether, these features can create a digital interface that is more open-ended\nand flexible while also being better suited to addressing privacy and ethical\nconcerns. Through a case study using a collection of documentary photographs,\nwe provide several metrics showing the efficacy and possibilities of our\napproach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many cultural institutions have made large digitized visual collections\navailable online, often under permissible re-use licences. Creating interfaces\nfor exploring and searching these collections is difficult, particularly in the\nabsence of granular metadata. In this paper, we introduce a method for using\nstate-of-the-art multimodal large language models (LLMs) to enable an\nopen-ended, explainable search and discovery interface for visual collections.\nWe show how our approach can create novel clustering and recommendation systems\nthat avoid common pitfalls of methods based directly on visual embeddings. Of\nparticular interest is the ability to offer concrete textual explanations of\neach recommendation without the need to preselect the features of interest.\nTogether, these features can create a digital interface that is more open-ended\nand flexible while also being better suited to addressing privacy and ethical\nconcerns. Through a case study using a collection of documentary photographs,\nwe provide several metrics showing the efficacy and possibilities of our\napproach."
                },
                "authors": [
                    {
                        "name": "Taylor Arnold"
                    },
                    {
                        "name": "Lauren Tilton"
                    }
                ],
                "author_detail": {
                    "name": "Lauren Tilton"
                },
                "author": "Lauren Tilton",
                "arxiv_comment": "16 pages, CHR 2024: Computational Humanities Research Conference,\n  December 4 - 6, 2024, Aarhus University, Denmark",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04663v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04663v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14125v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14125v3",
                "updated": "2024-11-07T12:07:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    12,
                    7,
                    7,
                    3,
                    312,
                    0
                ],
                "published": "2024-05-23T02:57:42Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    2,
                    57,
                    42,
                    3,
                    144,
                    0
                ],
                "title": "ALI-Agent: Assessing LLMs' Alignment with Human Values via Agent-based\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALI-Agent: Assessing LLMs' Alignment with Human Values via Agent-based\n  Evaluation"
                },
                "summary": "Large Language Models (LLMs) can elicit unintended and even harmful content\nwhen misaligned with human values, posing severe risks to users and society. To\nmitigate these risks, current evaluation benchmarks predominantly employ\nexpert-designed contextual scenarios to assess how well LLMs align with human\nvalues. However, the labor-intensive nature of these benchmarks limits their\ntest scope, hindering their ability to generalize to the extensive variety of\nopen-world use cases and identify rare but crucial long-tail risks.\nAdditionally, these static tests fail to adapt to the rapid evolution of LLMs,\nmaking it hard to evaluate timely alignment issues. To address these\nchallenges, we propose ALI-Agent, an evaluation framework that leverages the\nautonomous abilities of LLM-powered agents to conduct in-depth and adaptive\nalignment assessments. ALI-Agent operates through two principal stages:\nEmulation and Refinement. During the Emulation stage, ALI-Agent automates the\ngeneration of realistic test scenarios. In the Refinement stage, it iteratively\nrefines the scenarios to probe long-tail risks. Specifically, ALI-Agent\nincorporates a memory module to guide test scenario generation, a tool-using\nmodule to reduce human labor in tasks such as evaluating feedback from target\nLLMs, and an action module to refine tests. Extensive experiments across three\naspects of human values--stereotypes, morality, and legality--demonstrate that\nALI-Agent, as a general evaluation framework, effectively identifies model\nmisalignment. Systematic analysis also validates that the generated test\nscenarios represent meaningful use cases, as well as integrate enhanced\nmeasures to probe long-tail risks. Our code is available at\nhttps://github.com/SophieZheng998/ALI-Agent.git",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can elicit unintended and even harmful content\nwhen misaligned with human values, posing severe risks to users and society. To\nmitigate these risks, current evaluation benchmarks predominantly employ\nexpert-designed contextual scenarios to assess how well LLMs align with human\nvalues. However, the labor-intensive nature of these benchmarks limits their\ntest scope, hindering their ability to generalize to the extensive variety of\nopen-world use cases and identify rare but crucial long-tail risks.\nAdditionally, these static tests fail to adapt to the rapid evolution of LLMs,\nmaking it hard to evaluate timely alignment issues. To address these\nchallenges, we propose ALI-Agent, an evaluation framework that leverages the\nautonomous abilities of LLM-powered agents to conduct in-depth and adaptive\nalignment assessments. ALI-Agent operates through two principal stages:\nEmulation and Refinement. During the Emulation stage, ALI-Agent automates the\ngeneration of realistic test scenarios. In the Refinement stage, it iteratively\nrefines the scenarios to probe long-tail risks. Specifically, ALI-Agent\nincorporates a memory module to guide test scenario generation, a tool-using\nmodule to reduce human labor in tasks such as evaluating feedback from target\nLLMs, and an action module to refine tests. Extensive experiments across three\naspects of human values--stereotypes, morality, and legality--demonstrate that\nALI-Agent, as a general evaluation framework, effectively identifies model\nmisalignment. Systematic analysis also validates that the generated test\nscenarios represent meaningful use cases, as well as integrate enhanced\nmeasures to probe long-tail risks. Our code is available at\nhttps://github.com/SophieZheng998/ALI-Agent.git"
                },
                "authors": [
                    {
                        "name": "Jingnan Zheng"
                    },
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "An Zhang"
                    },
                    {
                        "name": "Tai D. Nguyen"
                    },
                    {
                        "name": "Jun Sun"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "arxiv_journal_ref": "2024 Neurips",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14125v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14125v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04644v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04644v1",
                "updated": "2024-11-07T12:01:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    12,
                    1,
                    36,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T12:01:36Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    12,
                    1,
                    36,
                    3,
                    312,
                    0
                ],
                "title": "wav2sleep: A Unified Multi-Modal Approach to Sleep Stage Classification\n  from Physiological Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "wav2sleep: A Unified Multi-Modal Approach to Sleep Stage Classification\n  from Physiological Signals"
                },
                "summary": "Accurate classification of sleep stages from less obtrusive sensor\nmeasurements such as the electrocardiogram (ECG) or photoplethysmogram (PPG)\ncould enable important applications in sleep medicine. Existing approaches to\nthis problem have typically used deep learning models designed and trained to\noperate on one or more specific input signals. However, the datasets used to\ndevelop these models often do not contain the same sets of input signals. Some\nsignals, particularly PPG, are much less prevalent than others, and this has\npreviously been addressed with techniques such as transfer learning.\nAdditionally, only training on one or more fixed modalities precludes\ncross-modal information transfer from other sources, which has proved valuable\nin other problem domains. To address this, we introduce wav2sleep, a unified\nmodel designed to operate on variable sets of input signals during training and\ninference. After jointly training on over 10,000 overnight recordings from six\npublicly available polysomnography datasets, including SHHS and MESA, wav2sleep\noutperforms existing sleep stage classification models across test-time input\ncombinations including ECG, PPG, and respiratory signals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate classification of sleep stages from less obtrusive sensor\nmeasurements such as the electrocardiogram (ECG) or photoplethysmogram (PPG)\ncould enable important applications in sleep medicine. Existing approaches to\nthis problem have typically used deep learning models designed and trained to\noperate on one or more specific input signals. However, the datasets used to\ndevelop these models often do not contain the same sets of input signals. Some\nsignals, particularly PPG, are much less prevalent than others, and this has\npreviously been addressed with techniques such as transfer learning.\nAdditionally, only training on one or more fixed modalities precludes\ncross-modal information transfer from other sources, which has proved valuable\nin other problem domains. To address this, we introduce wav2sleep, a unified\nmodel designed to operate on variable sets of input signals during training and\ninference. After jointly training on over 10,000 overnight recordings from six\npublicly available polysomnography datasets, including SHHS and MESA, wav2sleep\noutperforms existing sleep stage classification models across test-time input\ncombinations including ECG, PPG, and respiratory signals."
                },
                "authors": [
                    {
                        "name": "Jonathan F. Carter"
                    },
                    {
                        "name": "Lionel Tarassenko"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Tarassenko"
                },
                "author": "Lionel Tarassenko",
                "arxiv_comment": "Accepted to Machine Learning for Health (ML4H) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04644v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04644v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04642v1",
                "updated": "2024-11-07T11:54:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    11,
                    54,
                    1,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T11:54:01Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    11,
                    54,
                    1,
                    3,
                    312,
                    0
                ],
                "title": "TAP-VL: Text Layout-Aware Pre-training for Enriched Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TAP-VL: Text Layout-Aware Pre-training for Enriched Vision-Language\n  Models"
                },
                "summary": "Vision-Language (VL) models have garnered considerable research interest;\nhowever, they still face challenges in effectively handling text within images.\nTo address this limitation, researchers have developed two approaches. The\nfirst method involves utilizing external Optical Character Recognition (OCR)\ntools to extract textual information from images, which is then prepended to\nother textual inputs. The second strategy focuses on employing extremely\nhigh-resolution images to improve text recognition capabilities. In this paper,\nwe focus on enhancing the first strategy by introducing a novel method, named\nTAP-VL, which treats OCR information as a distinct modality and seamlessly\nintegrates it into any VL model. TAP-VL employs a lightweight transformer-based\nOCR module to receive OCR with layout information, compressing it into a short\nfixed-length sequence for input into the LLM. Initially, we conduct\nmodel-agnostic pretraining of the OCR module on unlabeled documents, followed\nby its integration into any VL architecture through brief fine-tuning.\nExtensive experiments demonstrate consistent performance improvements when\napplying TAP-VL to top-performing VL models, across scene-text and\ndocument-based VL benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language (VL) models have garnered considerable research interest;\nhowever, they still face challenges in effectively handling text within images.\nTo address this limitation, researchers have developed two approaches. The\nfirst method involves utilizing external Optical Character Recognition (OCR)\ntools to extract textual information from images, which is then prepended to\nother textual inputs. The second strategy focuses on employing extremely\nhigh-resolution images to improve text recognition capabilities. In this paper,\nwe focus on enhancing the first strategy by introducing a novel method, named\nTAP-VL, which treats OCR information as a distinct modality and seamlessly\nintegrates it into any VL model. TAP-VL employs a lightweight transformer-based\nOCR module to receive OCR with layout information, compressing it into a short\nfixed-length sequence for input into the LLM. Initially, we conduct\nmodel-agnostic pretraining of the OCR module on unlabeled documents, followed\nby its integration into any VL architecture through brief fine-tuning.\nExtensive experiments demonstrate consistent performance improvements when\napplying TAP-VL to top-performing VL models, across scene-text and\ndocument-based VL benchmarks."
                },
                "authors": [
                    {
                        "name": "Jonathan Fhima"
                    },
                    {
                        "name": "Elad Ben Avraham"
                    },
                    {
                        "name": "Oren Nuriel"
                    },
                    {
                        "name": "Yair Kittenplon"
                    },
                    {
                        "name": "Roy Ganz"
                    },
                    {
                        "name": "Aviad Aberdam"
                    },
                    {
                        "name": "Ron Litman"
                    }
                ],
                "author_detail": {
                    "name": "Ron Litman"
                },
                "author": "Ron Litman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04637v1",
                "updated": "2024-11-07T11:51:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    11,
                    51,
                    14,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T11:51:14Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    11,
                    51,
                    14,
                    3,
                    312,
                    0
                ],
                "title": "Hands-On Tutorial: Labeling with LLM and Human-in-the-Loop",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hands-On Tutorial: Labeling with LLM and Human-in-the-Loop"
                },
                "summary": "Training and deploying machine learning models relies on a large amount of\nhuman-annotated data. As human labeling becomes increasingly expensive and\ntime-consuming, recent research has developed multiple strategies to speed up\nannotation and reduce costs and human workload: generating synthetic training\ndata, active learning, and hybrid labeling. This tutorial is oriented toward\npractical applications: we will present the basics of each strategy, highlight\ntheir benefits and limitations, and discuss in detail real-life case studies.\nAdditionally, we will walk through best practices for managing human annotators\nand controlling the quality of the final dataset. The tutorial includes a\nhands-on workshop, where attendees will be guided in implementing a hybrid\nannotation setup. This tutorial is designed for NLP practitioners from both\nresearch and industry backgrounds who are involved in or interested in\noptimizing data labeling projects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training and deploying machine learning models relies on a large amount of\nhuman-annotated data. As human labeling becomes increasingly expensive and\ntime-consuming, recent research has developed multiple strategies to speed up\nannotation and reduce costs and human workload: generating synthetic training\ndata, active learning, and hybrid labeling. This tutorial is oriented toward\npractical applications: we will present the basics of each strategy, highlight\ntheir benefits and limitations, and discuss in detail real-life case studies.\nAdditionally, we will walk through best practices for managing human annotators\nand controlling the quality of the final dataset. The tutorial includes a\nhands-on workshop, where attendees will be guided in implementing a hybrid\nannotation setup. This tutorial is designed for NLP practitioners from both\nresearch and industry backgrounds who are involved in or interested in\noptimizing data labeling projects."
                },
                "authors": [
                    {
                        "name": "Ekaterina Artemova"
                    },
                    {
                        "name": "Akim Tsvigun"
                    },
                    {
                        "name": "Dominik Schlechtweg"
                    },
                    {
                        "name": "Natalia Fedorova"
                    },
                    {
                        "name": "Sergei Tilga"
                    },
                    {
                        "name": "Boris Obmoroshev"
                    }
                ],
                "author_detail": {
                    "name": "Boris Obmoroshev"
                },
                "author": "Boris Obmoroshev",
                "arxiv_comment": "To be presented at COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15052v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15052v3",
                "updated": "2024-11-07T11:43:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    11,
                    43,
                    4,
                    3,
                    312,
                    0
                ],
                "published": "2024-10-19T09:49:12Z",
                "published_parsed": [
                    2024,
                    10,
                    19,
                    9,
                    49,
                    12,
                    5,
                    293,
                    0
                ],
                "title": "Mining Glitch Tokens in Large Language Models via Gradient-based\n  Discrete Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mining Glitch Tokens in Large Language Models via Gradient-based\n  Discrete Optimization"
                },
                "summary": "Glitch tokens in Large Language Models (LLMs) can trigger unpredictable\nbehaviors, threatening model reliability and safety. Existing detection methods\noften depend on predefined patterns, limiting their adaptability across diverse\nLLM architectures. We propose GlitchMiner, a gradient-based discrete\noptimization framework that efficiently identifies glitch tokens by leveraging\nentropy to quantify prediction uncertainty and a local search strategy for\nexploring the token space. Experiments across multiple LLM architectures show\nthat GlitchMiner outperforms existing methods in both detection accuracy and\nadaptability, achieving over 10% average efficiency improvement. GlitchMiner\nenhances vulnerability assessment in LLMs, contributing to more robust and\nreliable applications. Code is available at\nhttps://github.com/wooozihui/GlitchMiner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Glitch tokens in Large Language Models (LLMs) can trigger unpredictable\nbehaviors, threatening model reliability and safety. Existing detection methods\noften depend on predefined patterns, limiting their adaptability across diverse\nLLM architectures. We propose GlitchMiner, a gradient-based discrete\noptimization framework that efficiently identifies glitch tokens by leveraging\nentropy to quantify prediction uncertainty and a local search strategy for\nexploring the token space. Experiments across multiple LLM architectures show\nthat GlitchMiner outperforms existing methods in both detection accuracy and\nadaptability, achieving over 10% average efficiency improvement. GlitchMiner\nenhances vulnerability assessment in LLMs, contributing to more robust and\nreliable applications. Code is available at\nhttps://github.com/wooozihui/GlitchMiner."
                },
                "authors": [
                    {
                        "name": "Zihui Wu"
                    },
                    {
                        "name": "Haichang Gao"
                    },
                    {
                        "name": "Ping Wang"
                    },
                    {
                        "name": "Shudong Zhang"
                    },
                    {
                        "name": "Zhaoxiang Liu"
                    },
                    {
                        "name": "Shiguo Lian"
                    }
                ],
                "author_detail": {
                    "name": "Shiguo Lian"
                },
                "author": "Shiguo Lian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15052v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15052v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07712v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07712v3",
                "updated": "2024-11-07T11:26:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    11,
                    26,
                    39,
                    3,
                    312,
                    0
                ],
                "published": "2024-07-10T14:44:25Z",
                "published_parsed": [
                    2024,
                    7,
                    10,
                    14,
                    44,
                    25,
                    2,
                    192,
                    0
                ],
                "title": "Deep-Graph-Sprints: Accelerated Representation Learning in\n  Continuous-Time Dynamic Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep-Graph-Sprints: Accelerated Representation Learning in\n  Continuous-Time Dynamic Graphs"
                },
                "summary": "Continuous-time dynamic graphs (CTDGs) are essential for modeling\ninterconnected, evolving systems. Traditional methods for extracting knowledge\nfrom these graphs often depend on feature engineering or deep learning. Feature\nengineering is limited by the manual and time-intensive nature of crafting\nfeatures, while deep learning approaches suffer from high inference latency,\nmaking them impractical for real-time applications. This paper introduces\nDeep-Graph-Sprints (DGS), a novel deep learning architecture designed for\nefficient representation learning on CTDGs with low-latency inference\nrequirements. We benchmark DGS against state-of-the-art (SOTA) feature\nengineering and graph neural network methods using five diverse datasets. The\nresults indicate that DGS achieves competitive performance while inference\nspeed improves between 4x and 12x compared to other deep learning approaches on\nour benchmark datasets. Our method effectively bridges the gap between deep\nrepresentation learning and low-latency application requirements for CTDGs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous-time dynamic graphs (CTDGs) are essential for modeling\ninterconnected, evolving systems. Traditional methods for extracting knowledge\nfrom these graphs often depend on feature engineering or deep learning. Feature\nengineering is limited by the manual and time-intensive nature of crafting\nfeatures, while deep learning approaches suffer from high inference latency,\nmaking them impractical for real-time applications. This paper introduces\nDeep-Graph-Sprints (DGS), a novel deep learning architecture designed for\nefficient representation learning on CTDGs with low-latency inference\nrequirements. We benchmark DGS against state-of-the-art (SOTA) feature\nengineering and graph neural network methods using five diverse datasets. The\nresults indicate that DGS achieves competitive performance while inference\nspeed improves between 4x and 12x compared to other deep learning approaches on\nour benchmark datasets. Our method effectively bridges the gap between deep\nrepresentation learning and low-latency application requirements for CTDGs."
                },
                "authors": [
                    {
                        "name": "Ahmad Naser Eddin"
                    },
                    {
                        "name": "Jacopo Bono"
                    },
                    {
                        "name": "David Aparcio"
                    },
                    {
                        "name": "Hugo Ferreira"
                    },
                    {
                        "name": "Pedro Ribeiro"
                    },
                    {
                        "name": "Pedro Bizarro"
                    }
                ],
                "author_detail": {
                    "name": "Pedro Bizarro"
                },
                "author": "Pedro Bizarro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07712v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07712v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04623v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04623v1",
                "updated": "2024-11-07T11:14:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    11,
                    14,
                    19,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T11:14:19Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    11,
                    14,
                    19,
                    3,
                    312,
                    0
                ],
                "title": "Unveiling VVV/WISE Mira variables on the far side of the Galactic disk:\n  Distances, kinematics and a new extinction law",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling VVV/WISE Mira variables on the far side of the Galactic disk:\n  Distances, kinematics and a new extinction law"
                },
                "summary": "The structure and kinematics of the Milky Way disk are largely inferred from\nthe solar vicinity. To gain a comprehensive understanding, it is essential to\nfind reliable tracers in less-explored regions like the bulge and the far side\nof the disk. Mira variables, which are well-studied and bright standard\ncandles, offer an excellent opportunity to trace intermediate and old\npopulations in these complex regions. We aim to isolate a clean sample of Miras\nin the Vista Variables in the V\\'ia L\\'actea survey using Gaussian process\nalgorithms. This sample will be used to study intermediate and old age\npopulations in the Galactic bulge and far disk. Near- and mid-infrared\ntime-series photometry were processed using Gaussian Process algorithms to\nidentify Mira variables and model their light curves. We calibrated selection\ncriteria with a visually inspected sample to create a high-purity sample of\nMiras, integrating multi-band photometry and kinematic data from proper\nmotions. We present a catalog of 3602 Mira variables. By analyzing photometry,\nwe classify them by O-rich or C-rich surface chemistry and derive\nselective-to-total extinction ratios of $A_{K_{s}}/E(J - K_{s}) = 0.471 \\pm\n0.01$ and $A_{K_{s}}/E(H - K_{s}) = 1.320 \\pm 0.020$. Using the Mira period-age\nrelation, we find evidence supporting the inside-out formation of the Milky Way\ndisk. The distribution of proper motions and distances aligns with the Galactic\nrotation curve and disk kinematics. We extend the rotation curve up to R$_{\\rm\nGC} \\sim 17 \\ \\rm{kpc}$ and find no strong evidence of the nuclear stellar disk\nin our Mira sample. This study constitutes the largest catalog of variable\nstars on the far side of the Galactic disk to date.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The structure and kinematics of the Milky Way disk are largely inferred from\nthe solar vicinity. To gain a comprehensive understanding, it is essential to\nfind reliable tracers in less-explored regions like the bulge and the far side\nof the disk. Mira variables, which are well-studied and bright standard\ncandles, offer an excellent opportunity to trace intermediate and old\npopulations in these complex regions. We aim to isolate a clean sample of Miras\nin the Vista Variables in the V\\'ia L\\'actea survey using Gaussian process\nalgorithms. This sample will be used to study intermediate and old age\npopulations in the Galactic bulge and far disk. Near- and mid-infrared\ntime-series photometry were processed using Gaussian Process algorithms to\nidentify Mira variables and model their light curves. We calibrated selection\ncriteria with a visually inspected sample to create a high-purity sample of\nMiras, integrating multi-band photometry and kinematic data from proper\nmotions. We present a catalog of 3602 Mira variables. By analyzing photometry,\nwe classify them by O-rich or C-rich surface chemistry and derive\nselective-to-total extinction ratios of $A_{K_{s}}/E(J - K_{s}) = 0.471 \\pm\n0.01$ and $A_{K_{s}}/E(H - K_{s}) = 1.320 \\pm 0.020$. Using the Mira period-age\nrelation, we find evidence supporting the inside-out formation of the Milky Way\ndisk. The distribution of proper motions and distances aligns with the Galactic\nrotation curve and disk kinematics. We extend the rotation curve up to R$_{\\rm\nGC} \\sim 17 \\ \\rm{kpc}$ and find no strong evidence of the nuclear stellar disk\nin our Mira sample. This study constitutes the largest catalog of variable\nstars on the far side of the Galactic disk to date."
                },
                "authors": [
                    {
                        "name": "Rogelio Albarracn"
                    },
                    {
                        "name": "M. Zoccali"
                    },
                    {
                        "name": "J. Olivares Carvajal"
                    },
                    {
                        "name": ". Rojas-Arriagada"
                    },
                    {
                        "name": "J. H. Minniti"
                    },
                    {
                        "name": "M. Catelan"
                    },
                    {
                        "name": "M. De Leo"
                    },
                    {
                        "name": "F. Gran"
                    },
                    {
                        "name": "R. Contreras Ramos"
                    },
                    {
                        "name": ". Valenzuela Navarro"
                    },
                    {
                        "name": "C. Salvo-Guajardo"
                    }
                ],
                "author_detail": {
                    "name": "C. Salvo-Guajardo"
                },
                "author": "C. Salvo-Guajardo",
                "arxiv_comment": "20 pages, 19 figures, Accepted in A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04623v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04623v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02099v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02099v2",
                "updated": "2024-11-07T10:53:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    10,
                    53,
                    14,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-04T14:08:26Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    14,
                    8,
                    26,
                    0,
                    309,
                    0
                ],
                "title": "Differentially Private Integrated Decision Gradients (IDG-DP) for\n  Radar-based Human Activity Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentially Private Integrated Decision Gradients (IDG-DP) for\n  Radar-based Human Activity Recognition"
                },
                "summary": "Human motion analysis offers significant potential for healthcare monitoring\nand early detection of diseases. The advent of radar-based sensing systems has\ncaptured the spotlight for they are able to operate without physical contact\nand they can integrate with pre-existing Wi-Fi networks. They are also seen as\nless privacy-invasive compared to camera-based systems. However, recent\nresearch has shown high accuracy in recognizing subjects or gender from radar\ngait patterns, raising privacy concerns. This study addresses these issues by\ninvestigating privacy vulnerabilities in radar-based Human Activity Recognition\n(HAR) systems and proposing a novel method for privacy preservation using\nDifferential Privacy (DP) driven by attributions derived with Integrated\nDecision Gradient (IDG) algorithm. We investigate Black-box Membership\nInference Attack (MIA) Models in HAR settings across various levels of\nattacker-accessible information. We extensively evaluated the effectiveness of\nthe proposed IDG-DP method by designing a CNN-based HAR model and rigorously\nassessing its resilience against MIAs. Experimental results demonstrate the\npotential of IDG-DP in mitigating privacy attacks while maintaining utility\nacross all settings, particularly excelling against label-only and shadow model\nblack-box MIA attacks. This work represents a crucial step towards balancing\nthe need for effective radar-based HAR with robust privacy protection in\nhealthcare environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human motion analysis offers significant potential for healthcare monitoring\nand early detection of diseases. The advent of radar-based sensing systems has\ncaptured the spotlight for they are able to operate without physical contact\nand they can integrate with pre-existing Wi-Fi networks. They are also seen as\nless privacy-invasive compared to camera-based systems. However, recent\nresearch has shown high accuracy in recognizing subjects or gender from radar\ngait patterns, raising privacy concerns. This study addresses these issues by\ninvestigating privacy vulnerabilities in radar-based Human Activity Recognition\n(HAR) systems and proposing a novel method for privacy preservation using\nDifferential Privacy (DP) driven by attributions derived with Integrated\nDecision Gradient (IDG) algorithm. We investigate Black-box Membership\nInference Attack (MIA) Models in HAR settings across various levels of\nattacker-accessible information. We extensively evaluated the effectiveness of\nthe proposed IDG-DP method by designing a CNN-based HAR model and rigorously\nassessing its resilience against MIAs. Experimental results demonstrate the\npotential of IDG-DP in mitigating privacy attacks while maintaining utility\nacross all settings, particularly excelling against label-only and shadow model\nblack-box MIA attacks. This work represents a crucial step towards balancing\nthe need for effective radar-based HAR with robust privacy protection in\nhealthcare environments."
                },
                "authors": [
                    {
                        "name": "Idris Zakariyya"
                    },
                    {
                        "name": "Linda Tran"
                    },
                    {
                        "name": "Kaushik Bhargav Sivangi"
                    },
                    {
                        "name": "Paul Henderson"
                    },
                    {
                        "name": "Fani Deligianni"
                    }
                ],
                "author_detail": {
                    "name": "Fani Deligianni"
                },
                "author": "Fani Deligianni",
                "arxiv_comment": "Accepted at WACV 2025. 12 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02099v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02099v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05779v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05779v2",
                "updated": "2024-11-07T10:44:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    10,
                    44,
                    59,
                    3,
                    312,
                    0
                ],
                "published": "2024-10-08T08:00:12Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    8,
                    0,
                    12,
                    1,
                    282,
                    0
                ],
                "title": "LightRAG: Simple and Fast Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightRAG: Simple and Fast Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems enhance large language models\n(LLMs) by integrating external knowledge sources, enabling more accurate and\ncontextually relevant responses tailored to user needs. However, existing RAG\nsystems have significant limitations, including reliance on flat data\nrepresentations and inadequate contextual awareness, which can lead to\nfragmented answers that fail to capture complex inter-dependencies. To address\nthese challenges, we propose LightRAG, which incorporates graph structures into\ntext indexing and retrieval processes. This innovative framework employs a\ndual-level retrieval system that enhances comprehensive information retrieval\nfrom both low-level and high-level knowledge discovery. Additionally, the\nintegration of graph structures with vector representations facilitates\nefficient retrieval of related entities and their relationships, significantly\nimproving response times while maintaining contextual relevance. This\ncapability is further enhanced by an incremental update algorithm that ensures\nthe timely integration of new data, allowing the system to remain effective and\nresponsive in rapidly changing data environments. Extensive experimental\nvalidation demonstrates considerable improvements in retrieval accuracy and\nefficiency compared to existing approaches. We have made our LightRAG\nopen-source and available at the link: https://github.com/HKUDS/LightRAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems enhance large language models\n(LLMs) by integrating external knowledge sources, enabling more accurate and\ncontextually relevant responses tailored to user needs. However, existing RAG\nsystems have significant limitations, including reliance on flat data\nrepresentations and inadequate contextual awareness, which can lead to\nfragmented answers that fail to capture complex inter-dependencies. To address\nthese challenges, we propose LightRAG, which incorporates graph structures into\ntext indexing and retrieval processes. This innovative framework employs a\ndual-level retrieval system that enhances comprehensive information retrieval\nfrom both low-level and high-level knowledge discovery. Additionally, the\nintegration of graph structures with vector representations facilitates\nefficient retrieval of related entities and their relationships, significantly\nimproving response times while maintaining contextual relevance. This\ncapability is further enhanced by an incremental update algorithm that ensures\nthe timely integration of new data, allowing the system to remain effective and\nresponsive in rapidly changing data environments. Extensive experimental\nvalidation demonstrates considerable improvements in retrieval accuracy and\nefficiency compared to existing approaches. We have made our LightRAG\nopen-source and available at the link: https://github.com/HKUDS/LightRAG."
                },
                "authors": [
                    {
                        "name": "Zirui Guo"
                    },
                    {
                        "name": "Lianghao Xia"
                    },
                    {
                        "name": "Yanhua Yu"
                    },
                    {
                        "name": "Tu Ao"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05779v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04602v1",
                "updated": "2024-11-07T10:31:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    10,
                    31,
                    31,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T10:31:31Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    10,
                    31,
                    31,
                    3,
                    312,
                    0
                ],
                "title": "Self-Calibrated Listwise Reranking with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Calibrated Listwise Reranking with Large Language Models"
                },
                "summary": "Large language models (LLMs), with advanced linguistic capabilities, have\nbeen employed in reranking tasks through a sequence-to-sequence approach. In\nthis paradigm, multiple passages are reranked in a listwise manner and a\ntextual reranked permutation is generated. However, due to the limited context\nwindow of LLMs, this reranking paradigm requires a sliding window strategy to\niteratively handle larger candidate sets. This not only increases computational\ncosts but also restricts the LLM from fully capturing all the comparison\ninformation for all candidates. To address these challenges, we propose a novel\nself-calibrated listwise reranking method, which aims to leverage LLMs to\nproduce global relevance scores for ranking. To achieve it, we first propose\nthe relevance-aware listwise reranking framework, which incorporates explicit\nlist-view relevance scores to improve reranking efficiency and enable global\ncomparison across the entire candidate set. Second, to ensure the comparability\nof the computed scores, we propose self-calibrated training that uses\npoint-view relevance assessments generated internally by the LLM itself to\ncalibrate the list-view relevance assessments. Extensive experiments and\ncomprehensive analysis on the BEIR benchmark and TREC Deep Learning Tracks\ndemonstrate the effectiveness and efficiency of our proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), with advanced linguistic capabilities, have\nbeen employed in reranking tasks through a sequence-to-sequence approach. In\nthis paradigm, multiple passages are reranked in a listwise manner and a\ntextual reranked permutation is generated. However, due to the limited context\nwindow of LLMs, this reranking paradigm requires a sliding window strategy to\niteratively handle larger candidate sets. This not only increases computational\ncosts but also restricts the LLM from fully capturing all the comparison\ninformation for all candidates. To address these challenges, we propose a novel\nself-calibrated listwise reranking method, which aims to leverage LLMs to\nproduce global relevance scores for ranking. To achieve it, we first propose\nthe relevance-aware listwise reranking framework, which incorporates explicit\nlist-view relevance scores to improve reranking efficiency and enable global\ncomparison across the entire candidate set. Second, to ensure the comparability\nof the computed scores, we propose self-calibrated training that uses\npoint-view relevance assessments generated internally by the LLM itself to\ncalibrate the list-view relevance assessments. Extensive experiments and\ncomprehensive analysis on the BEIR benchmark and TREC Deep Learning Tracks\ndemonstrate the effectiveness and efficiency of our proposed method."
                },
                "authors": [
                    {
                        "name": "Ruiyang Ren"
                    },
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Kun Zhou"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04620v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04620v4",
                "updated": "2024-11-07T10:16:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    10,
                    16,
                    23,
                    3,
                    312,
                    0
                ],
                "published": "2024-02-07T07:07:02Z",
                "published_parsed": [
                    2024,
                    2,
                    7,
                    7,
                    7,
                    2,
                    2,
                    38,
                    0
                ],
                "title": "CataractBot: An LLM-Powered Expert-in-the-Loop Chatbot for Cataract\n  Patients",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CataractBot: An LLM-Powered Expert-in-the-Loop Chatbot for Cataract\n  Patients"
                },
                "summary": "The healthcare landscape is evolving, with patients seeking reliable\ninformation about their health conditions and available treatment options.\nDespite the abundance of information sources, the digital age overwhelms\nindividuals with excess, often inaccurate information. Patients primarily trust\nmedical professionals, highlighting the need for expert-endorsed health\ninformation. However, increased patient loads on experts has led to reduced\ncommunication time, impacting information sharing. To address this gap, we\ndeveloped CataractBot, an experts-in-the-loop chatbot powered by LLMs, in\ncollaboration with an eye hospital in India. CataractBot answers cataract\nsurgery related questions instantly by querying a curated knowledge base and\nprovides expert-verified responses asynchronously. It has multimodal and\nmultilingual capabilities. In an in-the-wild deployment study with 55\nparticipants, CataractBot proved valuable, providing anytime accessibility,\nsaving time, accommodating diverse literacy levels, alleviating power\ndifferences, and adding a privacy layer between patients and doctors. Users\nreported that their trust in the system was established through expert\nverification. Broadly, our results could inform future work on designing\nexpert-mediated LLM bots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The healthcare landscape is evolving, with patients seeking reliable\ninformation about their health conditions and available treatment options.\nDespite the abundance of information sources, the digital age overwhelms\nindividuals with excess, often inaccurate information. Patients primarily trust\nmedical professionals, highlighting the need for expert-endorsed health\ninformation. However, increased patient loads on experts has led to reduced\ncommunication time, impacting information sharing. To address this gap, we\ndeveloped CataractBot, an experts-in-the-loop chatbot powered by LLMs, in\ncollaboration with an eye hospital in India. CataractBot answers cataract\nsurgery related questions instantly by querying a curated knowledge base and\nprovides expert-verified responses asynchronously. It has multimodal and\nmultilingual capabilities. In an in-the-wild deployment study with 55\nparticipants, CataractBot proved valuable, providing anytime accessibility,\nsaving time, accommodating diverse literacy levels, alleviating power\ndifferences, and adding a privacy layer between patients and doctors. Users\nreported that their trust in the system was established through expert\nverification. Broadly, our results could inform future work on designing\nexpert-mediated LLM bots."
                },
                "authors": [
                    {
                        "name": "Pragnya Ramjee"
                    },
                    {
                        "name": "Bhuvan Sachdeva"
                    },
                    {
                        "name": "Satvik Golechha"
                    },
                    {
                        "name": "Shreyas Kulkarni"
                    },
                    {
                        "name": "Geeta Fulari"
                    },
                    {
                        "name": "Kaushik Murali"
                    },
                    {
                        "name": "Mohit Jain"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Jain"
                },
                "author": "Mohit Jain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04620v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04620v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04586v1",
                "updated": "2024-11-07T10:15:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    10,
                    15,
                    25,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T10:15:25Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    10,
                    15,
                    25,
                    3,
                    312,
                    0
                ],
                "title": "On the Inherent Robustness of One-Stage Object Detection against\n  Out-of-Distribution Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Inherent Robustness of One-Stage Object Detection against\n  Out-of-Distribution Data"
                },
                "summary": "Robustness is a fundamental aspect for developing safe and trustworthy\nmodels, particularly when they are deployed in the open world. In this work we\nanalyze the inherent capability of one-stage object detectors to robustly\noperate in the presence of out-of-distribution (OoD) data. Specifically, we\npropose a novel detection algorithm for detecting unknown objects in image\ndata, which leverages the features extracted by the model from each sample.\nDifferently from other recent approaches in the literature, our proposal does\nnot require retraining the object detector, thereby allowing for the use of\npretrained models. Our proposed OoD detector exploits the application of\nsupervised dimensionality reduction techniques to mitigate the effects of the\ncurse of dimensionality on the features extracted by the model. Furthermore, it\nutilizes high-resolution feature maps to identify potential unknown objects in\nan unsupervised fashion. Our experiments analyze the Pareto trade-off between\nthe performance detecting known and unknown objects resulting from different\nalgorithmic configurations and inference confidence thresholds. We also compare\nthe performance of our proposed algorithm to that of logits-based post-hoc OoD\nmethods, as well as possible fusion strategies. Finally, we discuss on the\ncompetitiveness of all tested methods against state-of-the-art OoD approaches\nfor object detection models over the recently published Unknown Object\nDetection benchmark. The obtained results verify that the performance of\navant-garde post-hoc OoD detectors can be further improved when combined with\nour proposed algorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustness is a fundamental aspect for developing safe and trustworthy\nmodels, particularly when they are deployed in the open world. In this work we\nanalyze the inherent capability of one-stage object detectors to robustly\noperate in the presence of out-of-distribution (OoD) data. Specifically, we\npropose a novel detection algorithm for detecting unknown objects in image\ndata, which leverages the features extracted by the model from each sample.\nDifferently from other recent approaches in the literature, our proposal does\nnot require retraining the object detector, thereby allowing for the use of\npretrained models. Our proposed OoD detector exploits the application of\nsupervised dimensionality reduction techniques to mitigate the effects of the\ncurse of dimensionality on the features extracted by the model. Furthermore, it\nutilizes high-resolution feature maps to identify potential unknown objects in\nan unsupervised fashion. Our experiments analyze the Pareto trade-off between\nthe performance detecting known and unknown objects resulting from different\nalgorithmic configurations and inference confidence thresholds. We also compare\nthe performance of our proposed algorithm to that of logits-based post-hoc OoD\nmethods, as well as possible fusion strategies. Finally, we discuss on the\ncompetitiveness of all tested methods against state-of-the-art OoD approaches\nfor object detection models over the recently published Unknown Object\nDetection benchmark. The obtained results verify that the performance of\navant-garde post-hoc OoD detectors can be further improved when combined with\nour proposed algorithm."
                },
                "authors": [
                    {
                        "name": "Aitor Martinez-Seras"
                    },
                    {
                        "name": "Javier Del Ser"
                    },
                    {
                        "name": "Alain Andres"
                    },
                    {
                        "name": "Pablo Garcia-Bringas"
                    }
                ],
                "author_detail": {
                    "name": "Pablo Garcia-Bringas"
                },
                "author": "Pablo Garcia-Bringas",
                "arxiv_comment": "12 figures, 4 tables, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T45, 68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04576v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04576v1",
                "updated": "2024-11-07T09:58:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    9,
                    58,
                    20,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T09:58:20Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    9,
                    58,
                    20,
                    3,
                    312,
                    0
                ],
                "title": "\"I Always Felt that Something Was Wrong.\": Understanding Compliance\n  Risks and Mitigation Strategies when Professionals Use Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"I Always Felt that Something Was Wrong.\": Understanding Compliance\n  Risks and Mitigation Strategies when Professionals Use Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have been increasingly adopted by professionals\nfor work tasks. However, using LLMs also introduces compliance risks relating\nto privacy, ethics, and regulations. This study investigated the compliance\nrisks professionals perceive with LLM use and their risk mitigation strategies.\nSemi-structured interviews were conducted with 24 law, healthcare, and academia\nprofessionals. Results showed that the main compliance concerns centered around\npotential exposure to sensitive customer/patient information through LLMs. To\naddress risks, professionals reported proactively inputting distorted data to\npreserve privacy. However, full compliance proved challenging, given the\ncomplex interactions between user inputs, LLM behaviors, and regulations. This\nresearch provides valuable insights into designing LLMs with built-in privacy\nand risk controls to support professionals' evaluation and adoption of emerging\nAI technologies while meeting compliance obligations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been increasingly adopted by professionals\nfor work tasks. However, using LLMs also introduces compliance risks relating\nto privacy, ethics, and regulations. This study investigated the compliance\nrisks professionals perceive with LLM use and their risk mitigation strategies.\nSemi-structured interviews were conducted with 24 law, healthcare, and academia\nprofessionals. Results showed that the main compliance concerns centered around\npotential exposure to sensitive customer/patient information through LLMs. To\naddress risks, professionals reported proactively inputting distorted data to\npreserve privacy. However, full compliance proved challenging, given the\ncomplex interactions between user inputs, LLM behaviors, and regulations. This\nresearch provides valuable insights into designing LLMs with built-in privacy\nand risk controls to support professionals' evaluation and adoption of emerging\nAI technologies while meeting compliance obligations."
                },
                "authors": [
                    {
                        "name": "Siying Hu"
                    },
                    {
                        "name": "Piaohong Wang"
                    },
                    {
                        "name": "Yaxing Yao"
                    },
                    {
                        "name": "Zhicong Lu"
                    }
                ],
                "author_detail": {
                    "name": "Zhicong Lu"
                },
                "author": "Zhicong Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04576v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04576v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03627v2",
                "updated": "2024-11-07T09:36:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    9,
                    36,
                    36,
                    3,
                    312,
                    0
                ],
                "published": "2024-09-05T15:39:11Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    15,
                    39,
                    11,
                    3,
                    249,
                    0
                ],
                "title": "Fewer supermassive binary black holes in pulsar timing array\n  observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fewer supermassive binary black holes in pulsar timing array\n  observations"
                },
                "summary": "We find the inferred properties of the putative gravitational wave background\nin the second data release of the European Pulsar Timing Array (EPTA) to be in\nbetter agreement with theoretical expectations under the improved noise model.\nIn particular, if the gravitational wave background is from supermassive black\nhole binaries, the observed gravitational wave emission is the dominant source\nof the binary energy loss, with no evidence of environmental effects, eccentric\norbits, or an overabundance of sources. Our improved noise model addresses the\nmisspecification of noise priors which impacts the inference of temporal\ncorrelations of the background. We also find evidence for epoch-correlated\ntemporally-uncorrelated (white) noise which has a stronger impact on the\nmeasurement in the presence of Hellings-Downs correlations of the background.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We find the inferred properties of the putative gravitational wave background\nin the second data release of the European Pulsar Timing Array (EPTA) to be in\nbetter agreement with theoretical expectations under the improved noise model.\nIn particular, if the gravitational wave background is from supermassive black\nhole binaries, the observed gravitational wave emission is the dominant source\nof the binary energy loss, with no evidence of environmental effects, eccentric\norbits, or an overabundance of sources. Our improved noise model addresses the\nmisspecification of noise priors which impacts the inference of temporal\ncorrelations of the background. We also find evidence for epoch-correlated\ntemporally-uncorrelated (white) noise which has a stronger impact on the\nmeasurement in the presence of Hellings-Downs correlations of the background."
                },
                "authors": [
                    {
                        "name": "Boris Goncharov"
                    },
                    {
                        "name": "Shubhit Sardana"
                    },
                    {
                        "name": "A. Sesana"
                    },
                    {
                        "name": "S. M. Tomson"
                    },
                    {
                        "name": "J. Antoniadis"
                    },
                    {
                        "name": "A. Chalumeau"
                    },
                    {
                        "name": "D. Champion"
                    },
                    {
                        "name": "S. Chen"
                    },
                    {
                        "name": "E. F. Keane"
                    },
                    {
                        "name": "K. Liu"
                    },
                    {
                        "name": "G. Shaifullah"
                    },
                    {
                        "name": "L. Speri"
                    },
                    {
                        "name": "S. Valtolina"
                    }
                ],
                "author_detail": {
                    "name": "S. Valtolina"
                },
                "author": "S. Valtolina",
                "arxiv_comment": "6 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12096v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12096v3",
                "updated": "2024-11-07T09:29:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    9,
                    29,
                    32,
                    3,
                    312,
                    0
                ],
                "published": "2024-04-18T11:29:23Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    11,
                    29,
                    23,
                    3,
                    109,
                    0
                ],
                "title": "LongEmbed: Extending Embedding Models for Long Context Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongEmbed: Extending Embedding Models for Long Context Retrieval"
                },
                "summary": "Embedding models play a pivot role in modern NLP applications such as IR and\nRAG. While the context limit of LLMs has been pushed beyond 1 million tokens,\nembedding models are still confined to a narrow context window not exceeding 8k\ntokens, refrained from application scenarios requiring long inputs such as\nlegal contracts. This paper explores context window extension of existing\nembedding models, pushing the limit to 32k without requiring additional\ntraining. First, we examine the performance of current embedding models for\nlong context retrieval on our newly constructed LongEmbed benchmark. LongEmbed\ncomprises two synthetic tasks and four carefully chosen real-world tasks,\nfeaturing documents of varying length and dispersed target information.\nBenchmarking results underscore huge room for improvement in these models.\nBased on this, comprehensive experiments show that training-free context window\nextension strategies like position interpolation can effectively extend the\ncontext window of existing embedding models by several folds, regardless of\ntheir original context being 512 or beyond 4k. Furthermore, for models\nemploying absolute position encoding (APE), we show the possibility of further\nfine-tuning to harvest notable performance gains while strictly preserving\noriginal behavior for short inputs. For models using rotary position embedding\n(RoPE), significant enhancements are observed when employing RoPE-specific\nmethods, such as NTK and SelfExtend, indicating RoPE's superiority over APE for\ncontext window extension. To facilitate future research, we release E5-Base-4k\nand E5-RoPE-Base, along with the LongEmbed benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedding models play a pivot role in modern NLP applications such as IR and\nRAG. While the context limit of LLMs has been pushed beyond 1 million tokens,\nembedding models are still confined to a narrow context window not exceeding 8k\ntokens, refrained from application scenarios requiring long inputs such as\nlegal contracts. This paper explores context window extension of existing\nembedding models, pushing the limit to 32k without requiring additional\ntraining. First, we examine the performance of current embedding models for\nlong context retrieval on our newly constructed LongEmbed benchmark. LongEmbed\ncomprises two synthetic tasks and four carefully chosen real-world tasks,\nfeaturing documents of varying length and dispersed target information.\nBenchmarking results underscore huge room for improvement in these models.\nBased on this, comprehensive experiments show that training-free context window\nextension strategies like position interpolation can effectively extend the\ncontext window of existing embedding models by several folds, regardless of\ntheir original context being 512 or beyond 4k. Furthermore, for models\nemploying absolute position encoding (APE), we show the possibility of further\nfine-tuning to harvest notable performance gains while strictly preserving\noriginal behavior for short inputs. For models using rotary position embedding\n(RoPE), significant enhancements are observed when employing RoPE-specific\nmethods, such as NTK and SelfExtend, indicating RoPE's superiority over APE for\ncontext window extension. To facilitate future research, we release E5-Base-4k\nand E5-RoPE-Base, along with the LongEmbed benchmark."
                },
                "authors": [
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Nan Yang"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Wenhao Wu"
                    },
                    {
                        "name": "Furu Wei"
                    },
                    {
                        "name": "Sujian Li"
                    }
                ],
                "author_detail": {
                    "name": "Sujian Li"
                },
                "author": "Sujian Li",
                "arxiv_comment": "EMNLP 2024 Camera Ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12096v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12096v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04556v1",
                "updated": "2024-11-07T09:27:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    9,
                    27,
                    42,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T09:27:42Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    9,
                    27,
                    42,
                    3,
                    312,
                    0
                ],
                "title": "Uncertainty Prediction Neural Network (UpNet): Embedding Artificial\n  Neural Network in Bayesian Inversion Framework to Quantify the Uncertainty of\n  Remote Sensing Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty Prediction Neural Network (UpNet): Embedding Artificial\n  Neural Network in Bayesian Inversion Framework to Quantify the Uncertainty of\n  Remote Sensing Retrieval"
                },
                "summary": "For the retrieval of large-scale vegetation biophysical parameters, the\ninversion of radiative transfer models (RTMs) is the most commonly used\napproach. In recent years, Artificial Neural Network (ANN)-based methods have\nbecome the mainstream for inverting RTMs due to their high accuracy and\ncomputational efficiency. It has been widely used in the retrieval of\nbiophysical variables (BV). However, due to the lack of the Bayesian inversion\ntheory interpretation, it faces challenges in quantifying the retrieval\nuncertainty, a crucial metric for product quality validation and downstream\napplications such as data assimilation or ecosystem carbon cycling modeling.\nThis study proved that the ANN trained with squared loss outputs the posterior\nmean, providing a rigorous foundation for its uncertainty quantification,\nregularization, and incorporation of prior information. A Bayesian theoretical\nframework was subsequently proposed for ANN-based methods. Using this\nframework, we derived a new algorithm called Uncertainty Prediction Neural\nNetwork (UpNet), which enables the simultaneous training of two ANNs to\nretrieve BV and provide retrieval uncertainty. To validate our method, we\ncompared UpNet with the standard Bayesian inference method, i.e., Markov Chain\nMonte Carlo (MCMC), in the inversion of a widely used RTM called ProSAIL for\nretrieving BVs and estimating uncertainty. The results demonstrated that the\nBVs retrieved and the uncertainties estimated by UpNet were highly consistent\nwith those from MCMC, achieving over a million-fold acceleration. These results\nindicated that UpNet has significant potential for fast retrieval and\nuncertainty quantification of BVs or other parameters with medium and\nhigh-resolution remote sensing data. Our Python implementation is available at:\nhttps://github.com/Dash-RSer/UpNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For the retrieval of large-scale vegetation biophysical parameters, the\ninversion of radiative transfer models (RTMs) is the most commonly used\napproach. In recent years, Artificial Neural Network (ANN)-based methods have\nbecome the mainstream for inverting RTMs due to their high accuracy and\ncomputational efficiency. It has been widely used in the retrieval of\nbiophysical variables (BV). However, due to the lack of the Bayesian inversion\ntheory interpretation, it faces challenges in quantifying the retrieval\nuncertainty, a crucial metric for product quality validation and downstream\napplications such as data assimilation or ecosystem carbon cycling modeling.\nThis study proved that the ANN trained with squared loss outputs the posterior\nmean, providing a rigorous foundation for its uncertainty quantification,\nregularization, and incorporation of prior information. A Bayesian theoretical\nframework was subsequently proposed for ANN-based methods. Using this\nframework, we derived a new algorithm called Uncertainty Prediction Neural\nNetwork (UpNet), which enables the simultaneous training of two ANNs to\nretrieve BV and provide retrieval uncertainty. To validate our method, we\ncompared UpNet with the standard Bayesian inference method, i.e., Markov Chain\nMonte Carlo (MCMC), in the inversion of a widely used RTM called ProSAIL for\nretrieving BVs and estimating uncertainty. The results demonstrated that the\nBVs retrieved and the uncertainties estimated by UpNet were highly consistent\nwith those from MCMC, achieving over a million-fold acceleration. These results\nindicated that UpNet has significant potential for fast retrieval and\nuncertainty quantification of BVs or other parameters with medium and\nhigh-resolution remote sensing data. Our Python implementation is available at:\nhttps://github.com/Dash-RSer/UpNet."
                },
                "authors": [
                    {
                        "name": "Dasheng Fan"
                    },
                    {
                        "name": "Xihan Mu"
                    },
                    {
                        "name": "Yongkang Lai"
                    },
                    {
                        "name": "Donghui Xie"
                    },
                    {
                        "name": "Guangjian Yan"
                    }
                ],
                "author_detail": {
                    "name": "Guangjian Yan"
                },
                "author": "Guangjian Yan",
                "arxiv_comment": "24 pages, f figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02623v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02623v2",
                "updated": "2024-11-07T09:25:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    9,
                    25,
                    28,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-04T21:31:04Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    21,
                    31,
                    4,
                    0,
                    309,
                    0
                ],
                "title": "Learning to Assist Humans without Inferring Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Assist Humans without Inferring Rewards"
                },
                "summary": "Assistive agents should make humans' lives easier. Classically, such\nassistance is studied through the lens of inverse reinforcement learning, where\nan assistive agent (e.g., a chatbot, a robot) infers a human's intention and\nthen selects actions to help the human reach that goal. This approach requires\ninferring intentions, which can be difficult in high-dimensional settings. We\nbuild upon prior work that studies assistance through the lens of empowerment:\nan assistive agent aims to maximize the influence of the human's actions such\nthat they exert a greater control over the environmental outcomes and can solve\ntasks in fewer steps. We lift the major limitation of prior work in this\narea--scalability to high-dimensional settings--with contrastive successor\nrepresentations. We formally prove that these representations estimate a\nsimilar notion of empowerment to that studied by prior work and provide a\nready-made mechanism for optimizing it. Empirically, our proposed method\noutperforms prior methods on synthetic benchmarks, and scales to Overcooked, a\ncooperative game setting. Theoretically, our work connects ideas from\ninformation theory, neuroscience, and reinforcement learning, and charts a path\nfor representations to play a critical role in solving assistive problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assistive agents should make humans' lives easier. Classically, such\nassistance is studied through the lens of inverse reinforcement learning, where\nan assistive agent (e.g., a chatbot, a robot) infers a human's intention and\nthen selects actions to help the human reach that goal. This approach requires\ninferring intentions, which can be difficult in high-dimensional settings. We\nbuild upon prior work that studies assistance through the lens of empowerment:\nan assistive agent aims to maximize the influence of the human's actions such\nthat they exert a greater control over the environmental outcomes and can solve\ntasks in fewer steps. We lift the major limitation of prior work in this\narea--scalability to high-dimensional settings--with contrastive successor\nrepresentations. We formally prove that these representations estimate a\nsimilar notion of empowerment to that studied by prior work and provide a\nready-made mechanism for optimizing it. Empirically, our proposed method\noutperforms prior methods on synthetic benchmarks, and scales to Overcooked, a\ncooperative game setting. Theoretically, our work connects ideas from\ninformation theory, neuroscience, and reinforcement learning, and charts a path\nfor representations to play a critical role in solving assistive problems."
                },
                "authors": [
                    {
                        "name": "Vivek Myers"
                    },
                    {
                        "name": "Evan Ellis"
                    },
                    {
                        "name": "Sergey Levine"
                    },
                    {
                        "name": "Benjamin Eysenbach"
                    },
                    {
                        "name": "Anca Dragan"
                    }
                ],
                "author_detail": {
                    "name": "Anca Dragan"
                },
                "author": "Anca Dragan",
                "arxiv_comment": "Conference on Neural Information Processing Systems (NeurIPS), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02623v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02623v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04539v1",
                "updated": "2024-11-07T08:54:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    8,
                    54,
                    46,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T08:54:46Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    8,
                    54,
                    46,
                    3,
                    312,
                    0
                ],
                "title": "Best Practices for Distilling Large Language Models into BERT for Web\n  Search Ranking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Best Practices for Distilling Large Language Models into BERT for Web\n  Search Ranking"
                },
                "summary": "Recent studies have highlighted the significant potential of Large Language\nModels (LLMs) as zero-shot relevance rankers. These methods predominantly\nutilize prompt learning to assess the relevance between queries and documents\nby generating a ranked list of potential documents. Despite their promise, the\nsubstantial costs associated with LLMs pose a significant challenge for their\ndirect implementation in commercial search systems. To overcome this barrier\nand fully exploit the capabilities of LLMs for text ranking, we explore\ntechniques to transfer the ranking expertise of LLMs to a more compact model\nsimilar to BERT, using a ranking loss to enable the deployment of less\nresource-intensive models. Specifically, we enhance the training of LLMs\nthrough Continued Pre-Training, taking the query as input and the clicked title\nand summary as output. We then proceed with supervised fine-tuning of the LLM\nusing a rank loss, assigning the final token as a representative of the entire\nsentence. Given the inherent characteristics of autoregressive language models,\nonly the final token </s> can encapsulate all preceding tokens. Additionally,\nwe introduce a hybrid point-wise and margin MSE loss to transfer the ranking\nknowledge from LLMs to smaller models like BERT. This method creates a viable\nsolution for environments with strict resource constraints. Both offline and\nonline evaluations have confirmed the efficacy of our approach, and our model\nhas been successfully integrated into a commercial web search engine as of\nFebruary 2024.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have highlighted the significant potential of Large Language\nModels (LLMs) as zero-shot relevance rankers. These methods predominantly\nutilize prompt learning to assess the relevance between queries and documents\nby generating a ranked list of potential documents. Despite their promise, the\nsubstantial costs associated with LLMs pose a significant challenge for their\ndirect implementation in commercial search systems. To overcome this barrier\nand fully exploit the capabilities of LLMs for text ranking, we explore\ntechniques to transfer the ranking expertise of LLMs to a more compact model\nsimilar to BERT, using a ranking loss to enable the deployment of less\nresource-intensive models. Specifically, we enhance the training of LLMs\nthrough Continued Pre-Training, taking the query as input and the clicked title\nand summary as output. We then proceed with supervised fine-tuning of the LLM\nusing a rank loss, assigning the final token as a representative of the entire\nsentence. Given the inherent characteristics of autoregressive language models,\nonly the final token </s> can encapsulate all preceding tokens. Additionally,\nwe introduce a hybrid point-wise and margin MSE loss to transfer the ranking\nknowledge from LLMs to smaller models like BERT. This method creates a viable\nsolution for environments with strict resource constraints. Both offline and\nonline evaluations have confirmed the efficacy of our approach, and our model\nhas been successfully integrated into a commercial web search engine as of\nFebruary 2024."
                },
                "authors": [
                    {
                        "name": "Dezhi Ye"
                    },
                    {
                        "name": "Junwei Hu"
                    },
                    {
                        "name": "Jiabin Fan"
                    },
                    {
                        "name": "Bowen Tian"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Haijin Liang"
                    },
                    {
                        "name": "Jin Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jin Ma"
                },
                "author": "Jin Ma",
                "arxiv_comment": "Arxiv Version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04535v1",
                "updated": "2024-11-07T08:48:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    8,
                    48,
                    33,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T08:48:33Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    8,
                    48,
                    33,
                    3,
                    312,
                    0
                ],
                "title": "Meta-Reasoning Improves Tool Use in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meta-Reasoning Improves Tool Use in Large Language Models"
                },
                "summary": "External tools help large language models (LLMs) succeed at tasks where they\nwould otherwise typically fail. In existing frameworks, LLMs learn tool use\neither by in-context demonstrations or via full model fine-tuning on annotated\ndata. As these approaches do not easily scale, a recent trend is to abandon\nthem in favor of lightweight, parameter-efficient tuning paradigms. These\nmethods allow quickly alternating between the frozen LLM and its specialised\nfine-tuned version, by switching on or off a handful of additional custom\nparameters. Hence, we postulate that the generalization ability of the frozen\nmodel can be leveraged to improve tool selection. We present Tool selECTion via\nmeta-reasONing (TECTON), a two-phase system that first reasons over a task\nusing a custom fine-tuned LM head and outputs candidate tools. Then, with the\ncustom head disabled, it meta-reasons (i.e., it reasons over the previous\nreasoning process) to make a final choice. We show that TECTON results in\nsubstantial gains - both in-distribution and out-of-distribution - on a range\nof math reasoning datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "External tools help large language models (LLMs) succeed at tasks where they\nwould otherwise typically fail. In existing frameworks, LLMs learn tool use\neither by in-context demonstrations or via full model fine-tuning on annotated\ndata. As these approaches do not easily scale, a recent trend is to abandon\nthem in favor of lightweight, parameter-efficient tuning paradigms. These\nmethods allow quickly alternating between the frozen LLM and its specialised\nfine-tuned version, by switching on or off a handful of additional custom\nparameters. Hence, we postulate that the generalization ability of the frozen\nmodel can be leveraged to improve tool selection. We present Tool selECTion via\nmeta-reasONing (TECTON), a two-phase system that first reasons over a task\nusing a custom fine-tuned LM head and outputs candidate tools. Then, with the\ncustom head disabled, it meta-reasons (i.e., it reasons over the previous\nreasoning process) to make a final choice. We show that TECTON results in\nsubstantial gains - both in-distribution and out-of-distribution - on a range\nof math reasoning datasets."
                },
                "authors": [
                    {
                        "name": "Lisa Alazraki"
                    },
                    {
                        "name": "Marek Rei"
                    }
                ],
                "author_detail": {
                    "name": "Marek Rei"
                },
                "author": "Marek Rei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17382v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17382v2",
                "updated": "2024-11-07T08:34:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    8,
                    34,
                    27,
                    3,
                    312,
                    0
                ],
                "published": "2024-05-27T17:38:33Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    17,
                    38,
                    33,
                    0,
                    148,
                    0
                ],
                "title": "ReMoDetect: Reward Models Recognize Aligned LLM's Generations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReMoDetect: Reward Models Recognize Aligned LLM's Generations"
                },
                "summary": "The remarkable capabilities and easy accessibility of large language models\n(LLMs) have significantly increased societal risks (e.g., fake news\ngeneration), necessitating the development of LLM-generated text (LGT)\ndetection methods for safe usage. However, detecting LGTs is challenging due to\nthe vast number of LLMs, making it impractical to account for each LLM\nindividually; hence, it is crucial to identify the common characteristics\nshared by these models. In this paper, we draw attention to a common feature of\nrecent powerful LLMs, namely the alignment training, i.e., training LLMs to\ngenerate human-preferable texts. Our key finding is that as these aligned LLMs\nare trained to maximize the human preferences, they generate texts with higher\nestimated preferences even than human-written texts; thus, such texts are\neasily detected by using the reward model (i.e., an LLM trained to model human\npreference distribution). Based on this finding, we propose two training\nschemes to further improve the detection ability of the reward model, namely\n(i) continual preference fine-tuning to make the reward model prefer aligned\nLGTs even further and (ii) reward modeling of Human/LLM mixed texts (a\nrephrased texts from human-written texts using aligned LLMs), which serves as a\nmedian preference text corpus between LGTs and human-written texts to learn the\ndecision boundary better. We provide an extensive evaluation by considering six\ntext domains across twelve aligned LLMs, where our method demonstrates\nstate-of-the-art results. Code is available at\nhttps://github.com/hyunseoklee-ai/ReMoDetect.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable capabilities and easy accessibility of large language models\n(LLMs) have significantly increased societal risks (e.g., fake news\ngeneration), necessitating the development of LLM-generated text (LGT)\ndetection methods for safe usage. However, detecting LGTs is challenging due to\nthe vast number of LLMs, making it impractical to account for each LLM\nindividually; hence, it is crucial to identify the common characteristics\nshared by these models. In this paper, we draw attention to a common feature of\nrecent powerful LLMs, namely the alignment training, i.e., training LLMs to\ngenerate human-preferable texts. Our key finding is that as these aligned LLMs\nare trained to maximize the human preferences, they generate texts with higher\nestimated preferences even than human-written texts; thus, such texts are\neasily detected by using the reward model (i.e., an LLM trained to model human\npreference distribution). Based on this finding, we propose two training\nschemes to further improve the detection ability of the reward model, namely\n(i) continual preference fine-tuning to make the reward model prefer aligned\nLGTs even further and (ii) reward modeling of Human/LLM mixed texts (a\nrephrased texts from human-written texts using aligned LLMs), which serves as a\nmedian preference text corpus between LGTs and human-written texts to learn the\ndecision boundary better. We provide an extensive evaluation by considering six\ntext domains across twelve aligned LLMs, where our method demonstrates\nstate-of-the-art results. Code is available at\nhttps://github.com/hyunseoklee-ai/ReMoDetect."
                },
                "authors": [
                    {
                        "name": "Hyunseok Lee"
                    },
                    {
                        "name": "Jihoon Tack"
                    },
                    {
                        "name": "Jinwoo Shin"
                    }
                ],
                "author_detail": {
                    "name": "Jinwoo Shin"
                },
                "author": "Jinwoo Shin",
                "arxiv_comment": "Published as a conference proceeding for NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17382v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17382v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04525v1",
                "updated": "2024-11-07T08:31:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    8,
                    31,
                    1,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T08:31:01Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    8,
                    31,
                    1,
                    3,
                    312,
                    0
                ],
                "title": "GenJoin: Conditional Generative Plan-to-Plan Query Optimizer that Learns\n  from Subplan Hints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenJoin: Conditional Generative Plan-to-Plan Query Optimizer that Learns\n  from Subplan Hints"
                },
                "summary": "Query optimization has become a research area where classical algorithms are\nbeing challenged by machine learning algorithms. At the same time, recent\ntrends in learned query optimizers have shown that it is prudent to take\nadvantage of decades of database research and augment classical query\noptimizers by shrinking the plan search space through different types of hints\n(e.g. by specifying the join type, scan type or the order of joins) rather than\ncompletely replacing the classical query optimizer with machine learning\nmodels. It is especially relevant for cases when classical optimizers cannot\nfully enumerate all logical and physical plans and, as an alternative, need to\nrely on less robust approaches like genetic algorithms. However, even\nsymbiotically learned query optimizers are hampered by the need for vast\namounts of training data, slow plan generation during inference and unstable\nresults across various workload conditions. In this paper, we present GenJoin -\na novel learned query optimizer that considers the query optimization problem\nas a generative task and is capable of learning from a random set of subplan\nhints to produce query plans that outperform the classical optimizer. GenJoin\nis the first learned query optimizer that significantly and consistently\noutperforms PostgreSQL as well as state-of-the-art methods on two well-known\nreal-world benchmarks across a variety of workloads using rigorous machine\nlearning evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query optimization has become a research area where classical algorithms are\nbeing challenged by machine learning algorithms. At the same time, recent\ntrends in learned query optimizers have shown that it is prudent to take\nadvantage of decades of database research and augment classical query\noptimizers by shrinking the plan search space through different types of hints\n(e.g. by specifying the join type, scan type or the order of joins) rather than\ncompletely replacing the classical query optimizer with machine learning\nmodels. It is especially relevant for cases when classical optimizers cannot\nfully enumerate all logical and physical plans and, as an alternative, need to\nrely on less robust approaches like genetic algorithms. However, even\nsymbiotically learned query optimizers are hampered by the need for vast\namounts of training data, slow plan generation during inference and unstable\nresults across various workload conditions. In this paper, we present GenJoin -\na novel learned query optimizer that considers the query optimization problem\nas a generative task and is capable of learning from a random set of subplan\nhints to produce query plans that outperform the classical optimizer. GenJoin\nis the first learned query optimizer that significantly and consistently\noutperforms PostgreSQL as well as state-of-the-art methods on two well-known\nreal-world benchmarks across a variety of workloads using rigorous machine\nlearning evaluations."
                },
                "authors": [
                    {
                        "name": "Pavel Sulimov"
                    },
                    {
                        "name": "Claude Lehmann"
                    },
                    {
                        "name": "Kurt Stockinger"
                    }
                ],
                "author_detail": {
                    "name": "Kurt Stockinger"
                },
                "author": "Kurt Stockinger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.13149v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.13149v2",
                "updated": "2024-11-07T08:20:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    8,
                    20,
                    46,
                    3,
                    312,
                    0
                ],
                "published": "2023-08-25T03:05:33Z",
                "published_parsed": [
                    2023,
                    8,
                    25,
                    3,
                    5,
                    33,
                    4,
                    237,
                    0
                ],
                "title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for\n  Scientific Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for\n  Scientific Research"
                },
                "summary": "Recently, there has been growing interest in using Large Language Models\n(LLMs) for scientific research. Numerous benchmarks have been proposed to\nevaluate the ability of LLMs for scientific research. However, current\nbenchmarks are mostly based on pre-collected objective questions. This design\nsuffers from data leakage problem and lacks the evaluation of subjective Q/A\nability. In this paper, we propose SciEval, a comprehensive and\nmulti-disciplinary evaluation benchmark to address these issues. Based on\nBloom's taxonomy, SciEval covers four dimensions to systematically evaluate\nscientific research ability. In particular, we design a \"dynamic\" subset based\non scientific principles to prevent evaluation from potential data leakage.\nBoth objective and subjective questions are included in SciEval. These\ncharacteristics make SciEval a more effective benchmark for scientific research\nability evaluation of LLMs. Comprehensive experiments on most advanced LLMs\nshow that, although GPT-4 achieves SOTA performance compared to other LLMs,\nthere is still substantial room for improvement, especially for dynamic\nquestions. The codes and data are publicly available on\nhttps://github.com/OpenDFM/SciEval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, there has been growing interest in using Large Language Models\n(LLMs) for scientific research. Numerous benchmarks have been proposed to\nevaluate the ability of LLMs for scientific research. However, current\nbenchmarks are mostly based on pre-collected objective questions. This design\nsuffers from data leakage problem and lacks the evaluation of subjective Q/A\nability. In this paper, we propose SciEval, a comprehensive and\nmulti-disciplinary evaluation benchmark to address these issues. Based on\nBloom's taxonomy, SciEval covers four dimensions to systematically evaluate\nscientific research ability. In particular, we design a \"dynamic\" subset based\non scientific principles to prevent evaluation from potential data leakage.\nBoth objective and subjective questions are included in SciEval. These\ncharacteristics make SciEval a more effective benchmark for scientific research\nability evaluation of LLMs. Comprehensive experiments on most advanced LLMs\nshow that, although GPT-4 achieves SOTA performance compared to other LLMs,\nthere is still substantial room for improvement, especially for dynamic\nquestions. The codes and data are publicly available on\nhttps://github.com/OpenDFM/SciEval."
                },
                "authors": [
                    {
                        "name": "Liangtai Sun"
                    },
                    {
                        "name": "Yang Han"
                    },
                    {
                        "name": "Zihan Zhao"
                    },
                    {
                        "name": "Da Ma"
                    },
                    {
                        "name": "Zhennan Shen"
                    },
                    {
                        "name": "Baocai Chen"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "12 pages, 17 figures, 12 tables. Accepted by AAAI 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.13149v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.13149v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04513v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04513v1",
                "updated": "2024-11-07T08:12:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    8,
                    12,
                    55,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T08:12:55Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    8,
                    12,
                    55,
                    3,
                    312,
                    0
                ],
                "title": "Fast, Accurate and Perturbative Forward Modeling of Galaxy Clustering\n  Part II: Redshift Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast, Accurate and Perturbative Forward Modeling of Galaxy Clustering\n  Part II: Redshift Space"
                },
                "summary": "Forward modeling the galaxy density within the Effective Field Theory of\nLarge Scale Structure (EFT of LSS) enables field-level analyses that are robust\nto theoretical uncertainties. At the same time, they can maximize the\nconstraining power from galaxy clustering on the scales amenable to\nperturbation theory. In order to apply the method to galaxy surveys, the\nforward model must account for the full observational complexity of the data.\nIn this context, a major challenge is the inclusion of redshift space\ndistortions (RSDs) from the peculiar motion of galaxies. Here, we present\nimprovements in the efficiency and accuracy of the RSD modeling in the\nperturbative LEFTfield forward model. We perform a detailed quantification of\nthe perturbative and numerical error for the prediction of momentum, velocity\nand the redshift-space matter density. Further, we test the recovery of\ncosmological parameters at the field level, namely the growth rate $f$, from\nsimulated halos in redshift space. For a rigorous test and to scan through a\nwide range of analysis choices, we fix the linear (initial) density field to\nthe known ground truth but marginalize over all unknown bias coefficients and\nnoise amplitudes. With a third-order model for gravity and bias, our results\nyield $<1\\,\\%$ statistical and $<1.5\\,\\%$ systematic error. The computational\ncost of the redshift-space forward model is only $\\sim 1.5$ times of the rest\nframe equivalent, enabling future field-level inference that simultaneously\ntargets cosmological parameters and the initial matter distribution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forward modeling the galaxy density within the Effective Field Theory of\nLarge Scale Structure (EFT of LSS) enables field-level analyses that are robust\nto theoretical uncertainties. At the same time, they can maximize the\nconstraining power from galaxy clustering on the scales amenable to\nperturbation theory. In order to apply the method to galaxy surveys, the\nforward model must account for the full observational complexity of the data.\nIn this context, a major challenge is the inclusion of redshift space\ndistortions (RSDs) from the peculiar motion of galaxies. Here, we present\nimprovements in the efficiency and accuracy of the RSD modeling in the\nperturbative LEFTfield forward model. We perform a detailed quantification of\nthe perturbative and numerical error for the prediction of momentum, velocity\nand the redshift-space matter density. Further, we test the recovery of\ncosmological parameters at the field level, namely the growth rate $f$, from\nsimulated halos in redshift space. For a rigorous test and to scan through a\nwide range of analysis choices, we fix the linear (initial) density field to\nthe known ground truth but marginalize over all unknown bias coefficients and\nnoise amplitudes. With a third-order model for gravity and bias, our results\nyield $<1\\,\\%$ statistical and $<1.5\\,\\%$ systematic error. The computational\ncost of the redshift-space forward model is only $\\sim 1.5$ times of the rest\nframe equivalent, enabling future field-level inference that simultaneously\ntargets cosmological parameters and the initial matter distribution."
                },
                "authors": [
                    {
                        "name": "Julia Stadler"
                    },
                    {
                        "name": "Fabian Schmidt"
                    },
                    {
                        "name": "Martin Reinecke"
                    },
                    {
                        "name": "Matteo Esposito"
                    }
                ],
                "author_detail": {
                    "name": "Matteo Esposito"
                },
                "author": "Matteo Esposito",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04513v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04513v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04509v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04509v1",
                "updated": "2024-11-07T08:02:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    8,
                    2,
                    58,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T08:02:58Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    8,
                    2,
                    58,
                    3,
                    312,
                    0
                ],
                "title": "FedDP: Privacy-preserving method based on federated learning for\n  histopathology image segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedDP: Privacy-preserving method based on federated learning for\n  histopathology image segmentation"
                },
                "summary": "Hematoxylin and Eosin (H&E) staining of whole slide images (WSIs) is\nconsidered the gold standard for pathologists and medical practitioners for\ntumor diagnosis, surgical planning, and post-operative assessment. With the\nrapid advancement of deep learning technologies, the development of numerous\nmodels based on convolutional neural networks and transformer-based models has\nbeen applied to the precise segmentation of WSIs. However, due to privacy\nregulations and the need to protect patient confidentiality, centralized\nstorage and processing of image data are impractical. Training a centralized\nmodel directly is challenging to implement in medical settings due to these\nprivacy concerns.This paper addresses the dispersed nature and privacy\nsensitivity of medical image data by employing a federated learning framework,\nallowing medical institutions to collaboratively learn while protecting patient\nprivacy. Additionally, to address the issue of original data reconstruction\nthrough gradient inversion during the federated learning training process,\ndifferential privacy introduces noise into the model updates, preventing\nattackers from inferring the contributions of individual samples, thereby\nprotecting the privacy of the training data.Experimental results show that the\nproposed method, FedDP, minimally impacts model accuracy while effectively\nsafeguarding the privacy of cancer pathology image data, with only a slight\ndecrease in Dice, Jaccard, and Acc indices by 0.55%, 0.63%, and 0.42%,\nrespectively. This approach facilitates cross-institutional collaboration and\nknowledge sharing while protecting sensitive data privacy, providing a viable\nsolution for further research and application in the medical field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hematoxylin and Eosin (H&E) staining of whole slide images (WSIs) is\nconsidered the gold standard for pathologists and medical practitioners for\ntumor diagnosis, surgical planning, and post-operative assessment. With the\nrapid advancement of deep learning technologies, the development of numerous\nmodels based on convolutional neural networks and transformer-based models has\nbeen applied to the precise segmentation of WSIs. However, due to privacy\nregulations and the need to protect patient confidentiality, centralized\nstorage and processing of image data are impractical. Training a centralized\nmodel directly is challenging to implement in medical settings due to these\nprivacy concerns.This paper addresses the dispersed nature and privacy\nsensitivity of medical image data by employing a federated learning framework,\nallowing medical institutions to collaboratively learn while protecting patient\nprivacy. Additionally, to address the issue of original data reconstruction\nthrough gradient inversion during the federated learning training process,\ndifferential privacy introduces noise into the model updates, preventing\nattackers from inferring the contributions of individual samples, thereby\nprotecting the privacy of the training data.Experimental results show that the\nproposed method, FedDP, minimally impacts model accuracy while effectively\nsafeguarding the privacy of cancer pathology image data, with only a slight\ndecrease in Dice, Jaccard, and Acc indices by 0.55%, 0.63%, and 0.42%,\nrespectively. This approach facilitates cross-institutional collaboration and\nknowledge sharing while protecting sensitive data privacy, providing a viable\nsolution for further research and application in the medical field."
                },
                "authors": [
                    {
                        "name": "Liangrui Pan"
                    },
                    {
                        "name": "Mao Huang"
                    },
                    {
                        "name": "Lian Wang"
                    },
                    {
                        "name": "Pinle Qin"
                    },
                    {
                        "name": "Shaoliang Peng"
                    }
                ],
                "author_detail": {
                    "name": "Shaoliang Peng"
                },
                "author": "Shaoliang Peng",
                "arxiv_comment": "Accepted in BIBM2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04509v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04509v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04496v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04496v1",
                "updated": "2024-11-07T07:46:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    7,
                    46,
                    6,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T07:46:06Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    7,
                    46,
                    6,
                    3,
                    312,
                    0
                ],
                "title": "Thanos: Enhancing Conversational Agents with Skill-of-Mind-Infused Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thanos: Enhancing Conversational Agents with Skill-of-Mind-Infused Large\n  Language Model"
                },
                "summary": "To increase social bonding with interlocutors, humans naturally acquire the\nability to respond appropriately in a given situation by considering which\nconversational skill is most suitable for the response - a process we call\nskill-of-mind. For large language model (LLM)-based conversational agents,\nplanning appropriate conversational skills, as humans do, is challenging due to\nthe complexity of social dialogue, especially in interactive scenarios. To\naddress this, we propose a skill-of-mind-annotated conversation dataset, named\nMultifaceted Skill-of-Mind, which includes multi-turn and multifaceted\nconversational skills across various interactive scenarios (e.g., long-term,\ncounseling, task-oriented), grounded in diverse social contexts (e.g.,\ndemographics, persona, rules of thumb). This dataset consists of roughly 100K\nconversations. Using this dataset, we introduce a new family of\nskill-of-mind-infused LLMs, named Thanos, with model sizes of 1B, 3B, and 8B\nparameters. With extensive experiments, these models successfully demonstrate\nthe skill-of-mind process and exhibit strong generalizability in inferring\nmultifaceted skills across a variety of domains. Moreover, we show that Thanos\nsignificantly enhances the quality of responses generated by LLM-based\nconversational agents and promotes prosocial behavior in human evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To increase social bonding with interlocutors, humans naturally acquire the\nability to respond appropriately in a given situation by considering which\nconversational skill is most suitable for the response - a process we call\nskill-of-mind. For large language model (LLM)-based conversational agents,\nplanning appropriate conversational skills, as humans do, is challenging due to\nthe complexity of social dialogue, especially in interactive scenarios. To\naddress this, we propose a skill-of-mind-annotated conversation dataset, named\nMultifaceted Skill-of-Mind, which includes multi-turn and multifaceted\nconversational skills across various interactive scenarios (e.g., long-term,\ncounseling, task-oriented), grounded in diverse social contexts (e.g.,\ndemographics, persona, rules of thumb). This dataset consists of roughly 100K\nconversations. Using this dataset, we introduce a new family of\nskill-of-mind-infused LLMs, named Thanos, with model sizes of 1B, 3B, and 8B\nparameters. With extensive experiments, these models successfully demonstrate\nthe skill-of-mind process and exhibit strong generalizability in inferring\nmultifaceted skills across a variety of domains. Moreover, we show that Thanos\nsignificantly enhances the quality of responses generated by LLM-based\nconversational agents and promotes prosocial behavior in human evaluations."
                },
                "authors": [
                    {
                        "name": "Young-Jun Lee"
                    },
                    {
                        "name": "Dokyong Lee"
                    },
                    {
                        "name": "Junyoung Youn"
                    },
                    {
                        "name": "Kyeongjin Oh"
                    },
                    {
                        "name": "Ho-Jin Choi"
                    }
                ],
                "author_detail": {
                    "name": "Ho-Jin Choi"
                },
                "author": "Ho-Jin Choi",
                "arxiv_comment": "Code: https://github.com/passing2961/Thanos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04496v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04496v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14979v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14979v3",
                "updated": "2024-11-07T07:25:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    7,
                    25,
                    4,
                    3,
                    312,
                    0
                ],
                "published": "2024-10-19T05:01:56Z",
                "published_parsed": [
                    2024,
                    10,
                    19,
                    5,
                    1,
                    56,
                    5,
                    293,
                    0
                ],
                "title": "Do Large Language Models Truly Grasp Mathematics? An Empirical\n  Exploration From A Psychological Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Truly Grasp Mathematics? An Empirical\n  Exploration From A Psychological Perspective"
                },
                "summary": "Despite their proficiency in math tasks, the mechanisms underlying LLMs'\nmathematical reasoning abilities remain a subject of debate. Recent studies\nsuggest that chain-of-thought (CoT) prompts can bolster mathematical reasoning\nby encouraging LLMs to employ human-like logical reasoning (System 2), enabling\nthem to excel on the Cognitive Reflection Test (CRT). To assess whether LLMs\ngenuinely possess System 2-like logical reasoning, we introduced targeted\nmodifications to CRT problems. Our findings reveal that, despite the use of CoT\nprompts, mainstream LLMs, including the latest o1-preview model, continue to\nexhibit a significant error rate. Further analysis indicates that they\npredominantly rely on System 1-like intuitive reasoning and pattern matching\nderived from training data, rather than demonstrating mastery of mathematical\nthinking. This discovery challenges the prevailing notion that LLMs possess\ngenuine logical reasoning abilities and that CoT can enhance them.\nConsequently, this work may temper overly optimistic projections regarding\nLLMs' advancement toward artificial general intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their proficiency in math tasks, the mechanisms underlying LLMs'\nmathematical reasoning abilities remain a subject of debate. Recent studies\nsuggest that chain-of-thought (CoT) prompts can bolster mathematical reasoning\nby encouraging LLMs to employ human-like logical reasoning (System 2), enabling\nthem to excel on the Cognitive Reflection Test (CRT). To assess whether LLMs\ngenuinely possess System 2-like logical reasoning, we introduced targeted\nmodifications to CRT problems. Our findings reveal that, despite the use of CoT\nprompts, mainstream LLMs, including the latest o1-preview model, continue to\nexhibit a significant error rate. Further analysis indicates that they\npredominantly rely on System 1-like intuitive reasoning and pattern matching\nderived from training data, rather than demonstrating mastery of mathematical\nthinking. This discovery challenges the prevailing notion that LLMs possess\ngenuine logical reasoning abilities and that CoT can enhance them.\nConsequently, this work may temper overly optimistic projections regarding\nLLMs' advancement toward artificial general intelligence."
                },
                "authors": [
                    {
                        "name": "Wei Xie"
                    },
                    {
                        "name": "Shuoyoucheng Ma"
                    },
                    {
                        "name": "Zhenhua Wang"
                    },
                    {
                        "name": "Enze Wang"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Xiaobing Sun"
                    },
                    {
                        "name": "Baosheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Baosheng Wang"
                },
                "author": "Baosheng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14979v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14979v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04476v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04476v1",
                "updated": "2024-11-07T07:07:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    7,
                    7,
                    34,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T07:07:34Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    7,
                    7,
                    34,
                    3,
                    312,
                    0
                ],
                "title": "LLM-R: A Framework for Domain-Adaptive Maintenance Scheme Generation\n  Combining Hierarchical Agents and RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-R: A Framework for Domain-Adaptive Maintenance Scheme Generation\n  Combining Hierarchical Agents and RAG"
                },
                "summary": "The increasing use of smart devices has emphasized the critical role of\nmaintenance in production activities. Interactive Electronic Technical Manuals\n(IETMs) are vital tools that support the maintenance of smart equipment.\nHowever, traditional IETMs face challenges such as transitioning from Graphical\nUser Interfaces (GUIs) to natural Language User Interfaces (LUIs) and managing\ncomplex logical relationships. Additionally, they must meet the current demands\nfor higher intelligence. This paper proposes a Maintenance Scheme Generation\nMethod based on Large Language Models (LLM-R). The proposed method includes\nseveral key innovations: We propose the Low Rank Adaptation-Knowledge Retention\n(LORA-KR) loss technology to proportionally adjust mixed maintenance data for\nfine-tuning the LLM. This method prevents knowledge conflicts caused by mixed\ndata, improving the model's adaptability and reasoning ability in specific\nmaintenance domains, Besides, Hierarchical Task-Based Agent and\nInstruction-level Retrieval-Augmented Generation (RAG) technologies are adopted\nto optimize the generation steps and mitigate the phenomenon of hallucination\ncaused by the model's Inability to access contextual information. This\nenhancement improves the model's flexibility and accuracy in handling known or\nunknown maintenance objects and maintenance scheme scenarios. To validate the\nproposed method's effectiveness in maintenance tasks, a maintenance scheme\ndataset was constructed using objects from different fields. The experimental\nresults show that the accuracy of the maintenance schemes generated by the\nproposed method reached 91.59%, indicating which improvement enhances the\nintelligence of maintenance schemes and introduces novel technical approaches\nfor equipment maintenance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing use of smart devices has emphasized the critical role of\nmaintenance in production activities. Interactive Electronic Technical Manuals\n(IETMs) are vital tools that support the maintenance of smart equipment.\nHowever, traditional IETMs face challenges such as transitioning from Graphical\nUser Interfaces (GUIs) to natural Language User Interfaces (LUIs) and managing\ncomplex logical relationships. Additionally, they must meet the current demands\nfor higher intelligence. This paper proposes a Maintenance Scheme Generation\nMethod based on Large Language Models (LLM-R). The proposed method includes\nseveral key innovations: We propose the Low Rank Adaptation-Knowledge Retention\n(LORA-KR) loss technology to proportionally adjust mixed maintenance data for\nfine-tuning the LLM. This method prevents knowledge conflicts caused by mixed\ndata, improving the model's adaptability and reasoning ability in specific\nmaintenance domains, Besides, Hierarchical Task-Based Agent and\nInstruction-level Retrieval-Augmented Generation (RAG) technologies are adopted\nto optimize the generation steps and mitigate the phenomenon of hallucination\ncaused by the model's Inability to access contextual information. This\nenhancement improves the model's flexibility and accuracy in handling known or\nunknown maintenance objects and maintenance scheme scenarios. To validate the\nproposed method's effectiveness in maintenance tasks, a maintenance scheme\ndataset was constructed using objects from different fields. The experimental\nresults show that the accuracy of the maintenance schemes generated by the\nproposed method reached 91.59%, indicating which improvement enhances the\nintelligence of maintenance schemes and introduces novel technical approaches\nfor equipment maintenance."
                },
                "authors": [
                    {
                        "name": "Laifa Tao"
                    },
                    {
                        "name": "Qixuan Huang"
                    },
                    {
                        "name": "Xianjun Wu"
                    },
                    {
                        "name": "Weiwei Zhang"
                    },
                    {
                        "name": "Yunlong Wu"
                    },
                    {
                        "name": "Bin Li"
                    },
                    {
                        "name": "Chen Lu"
                    },
                    {
                        "name": "Xingshuo Hai"
                    }
                ],
                "author_detail": {
                    "name": "Xingshuo Hai"
                },
                "author": "Xingshuo Hai",
                "arxiv_comment": "30 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04476v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04476v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04475v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04475v1",
                "updated": "2024-11-07T07:03:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    7,
                    3,
                    40,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T07:03:40Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    7,
                    3,
                    40,
                    3,
                    312,
                    0
                ],
                "title": "Deep Learning Models for UAV-Assisted Bridge Inspection: A YOLO\n  Benchmark Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning Models for UAV-Assisted Bridge Inspection: A YOLO\n  Benchmark Analysis"
                },
                "summary": "Visual inspections of bridges are critical to ensure their safety and\nidentify potential failures early. This inspection process can be rapidly and\naccurately automated by using unmanned aerial vehicles (UAVs) integrated with\ndeep learning models. However, choosing an appropriate model that is\nlightweight enough to integrate into the UAV and fulfills the strict\nrequirements for inference time and accuracy is challenging. Therefore, our\nwork contributes to the advancement of this model selection process by\nconducting a benchmark of 23 models belonging to the four newest YOLO variants\n(YOLOv5, YOLOv6, YOLOv7, YOLOv8) on COCO-Bridge-2021+, a dataset for bridge\ndetails detection. Through comprehensive benchmarking, we identify YOLOv8n,\nYOLOv7tiny, YOLOv6m, and YOLOv6m6 as the models offering an optimal balance\nbetween accuracy and processing speed, with mAP@50 scores of 0.803, 0.837,\n0.853, and 0.872, and inference times of 5.3ms, 7.5ms, 14.06ms, and 39.33ms,\nrespectively. Our findings accelerate the model selection process for UAVs,\nenabling more efficient and reliable bridge inspections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual inspections of bridges are critical to ensure their safety and\nidentify potential failures early. This inspection process can be rapidly and\naccurately automated by using unmanned aerial vehicles (UAVs) integrated with\ndeep learning models. However, choosing an appropriate model that is\nlightweight enough to integrate into the UAV and fulfills the strict\nrequirements for inference time and accuracy is challenging. Therefore, our\nwork contributes to the advancement of this model selection process by\nconducting a benchmark of 23 models belonging to the four newest YOLO variants\n(YOLOv5, YOLOv6, YOLOv7, YOLOv8) on COCO-Bridge-2021+, a dataset for bridge\ndetails detection. Through comprehensive benchmarking, we identify YOLOv8n,\nYOLOv7tiny, YOLOv6m, and YOLOv6m6 as the models offering an optimal balance\nbetween accuracy and processing speed, with mAP@50 scores of 0.803, 0.837,\n0.853, and 0.872, and inference times of 5.3ms, 7.5ms, 14.06ms, and 39.33ms,\nrespectively. Our findings accelerate the model selection process for UAVs,\nenabling more efficient and reliable bridge inspections."
                },
                "authors": [
                    {
                        "name": "Trong-Nhan Phan"
                    },
                    {
                        "name": "Hoang-Hai Nguyen"
                    },
                    {
                        "name": "Thi-Thu-Hien Ha"
                    },
                    {
                        "name": "Huy-Tan Thai"
                    },
                    {
                        "name": "Kim-Hung Le"
                    }
                ],
                "author_detail": {
                    "name": "Kim-Hung Le"
                },
                "author": "Kim-Hung Le",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04475v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11709v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11709v4",
                "updated": "2024-11-07T07:00:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    7,
                    0,
                    14,
                    3,
                    312,
                    0
                ],
                "published": "2024-06-17T16:28:21Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    16,
                    28,
                    21,
                    0,
                    169,
                    0
                ],
                "title": "Instruct, Not Assist: LLM-based Multi-Turn Planning and Hierarchical\n  Questioning for Socratic Code Debugging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruct, Not Assist: LLM-based Multi-Turn Planning and Hierarchical\n  Questioning for Socratic Code Debugging"
                },
                "summary": "Socratic questioning is an effective teaching strategy, encouraging critical\nthinking and problem-solving. The conversational capabilities of large language\nmodels (LLMs) show great potential for providing scalable, real-time student\nguidance. However, current LLMs often give away solutions directly, making them\nineffective instructors. We tackle this issue in the code debugging domain with\nTreeInstruct, an Instructor agent guided by a novel state space-based planning\nalgorithm. TreeInstruct asks probing questions to help students independently\nidentify and resolve errors. It estimates a student's conceptual and\nsyntactical knowledge to dynamically construct a question tree based on their\nresponses and current knowledge state, effectively addressing both independent\nand dependent mistakes concurrently in a multi-turn interaction setting. In\naddition to using an existing single-bug debugging benchmark, we construct a\nmore challenging multi-bug dataset of 150 coding problems, incorrect solutions,\nand bug fixes -- all carefully constructed and annotated by experts. Extensive\nevaluation shows TreeInstruct's state-of-the-art performance on both datasets,\nproving it to be a more effective instructor than baselines. Furthermore, a\nreal-world case study with five students of varying skill levels further\ndemonstrates TreeInstruct's ability to guide students to debug their code\nefficiently with minimal turns and highly Socratic questioning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Socratic questioning is an effective teaching strategy, encouraging critical\nthinking and problem-solving. The conversational capabilities of large language\nmodels (LLMs) show great potential for providing scalable, real-time student\nguidance. However, current LLMs often give away solutions directly, making them\nineffective instructors. We tackle this issue in the code debugging domain with\nTreeInstruct, an Instructor agent guided by a novel state space-based planning\nalgorithm. TreeInstruct asks probing questions to help students independently\nidentify and resolve errors. It estimates a student's conceptual and\nsyntactical knowledge to dynamically construct a question tree based on their\nresponses and current knowledge state, effectively addressing both independent\nand dependent mistakes concurrently in a multi-turn interaction setting. In\naddition to using an existing single-bug debugging benchmark, we construct a\nmore challenging multi-bug dataset of 150 coding problems, incorrect solutions,\nand bug fixes -- all carefully constructed and annotated by experts. Extensive\nevaluation shows TreeInstruct's state-of-the-art performance on both datasets,\nproving it to be a more effective instructor than baselines. Furthermore, a\nreal-world case study with five students of varying skill levels further\ndemonstrates TreeInstruct's ability to guide students to debug their code\nefficiently with minimal turns and highly Socratic questioning."
                },
                "authors": [
                    {
                        "name": "Priyanka Kargupta"
                    },
                    {
                        "name": "Ishika Agarwal"
                    },
                    {
                        "name": "Dilek Hakkani-Tur"
                    },
                    {
                        "name": "Jiawei Han"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Han"
                },
                "author": "Jiawei Han",
                "arxiv_comment": "Code available at: https://github.com/agarwalishika/TreeInstruct\n  Accepted at EMNLP'24 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11709v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11709v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.18897v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18897v3",
                "updated": "2024-11-07T06:55:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    6,
                    55,
                    46,
                    3,
                    312,
                    0
                ],
                "published": "2024-02-29T06:51:52Z",
                "published_parsed": [
                    2024,
                    2,
                    29,
                    6,
                    51,
                    52,
                    3,
                    60,
                    0
                ],
                "title": "Contact-Implicit Model Predictive Control for Dexterous In-hand\n  Manipulation: A Long-Horizon and Robust Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contact-Implicit Model Predictive Control for Dexterous In-hand\n  Manipulation: A Long-Horizon and Robust Approach"
                },
                "summary": "Dexterous in-hand manipulation is an essential skill of production and life.\nHowever, the highly stiff and mutable nature of contacts limits real-time\ncontact detection and inference, degrading the performance of model-based\nmethods. Inspired by recent advances in contact-rich locomotion and\nmanipulation, this paper proposes a novel model-based approach to control\ndexterous in-hand manipulation and overcome the current limitations. The\nproposed approach has an attractive feature, which allows the robot to robustly\nperform long-horizon in-hand manipulation without predefined contact sequences\nor separate planning procedures. Specifically, we design a high-level\ncontact-implicit model predictive controller to generate real-time contact\nplans executed by the low-level tracking controller. Compared to other\nmodel-based methods, such a long-horizon feature enables replanning and robust\nexecution of contact-rich motions to achieve large displacements in-hand\nmanipulation more efficiently; Compared to existing learning-based methods, the\nproposed approach achieves dexterity and also generalizes to different objects\nwithout any pre-training. Detailed simulations and ablation studies demonstrate\nthe efficiency and effectiveness of our method. It runs at 20Hz on the\n23-degree-of-freedom, long-horizon, in-hand object rotation task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dexterous in-hand manipulation is an essential skill of production and life.\nHowever, the highly stiff and mutable nature of contacts limits real-time\ncontact detection and inference, degrading the performance of model-based\nmethods. Inspired by recent advances in contact-rich locomotion and\nmanipulation, this paper proposes a novel model-based approach to control\ndexterous in-hand manipulation and overcome the current limitations. The\nproposed approach has an attractive feature, which allows the robot to robustly\nperform long-horizon in-hand manipulation without predefined contact sequences\nor separate planning procedures. Specifically, we design a high-level\ncontact-implicit model predictive controller to generate real-time contact\nplans executed by the low-level tracking controller. Compared to other\nmodel-based methods, such a long-horizon feature enables replanning and robust\nexecution of contact-rich motions to achieve large displacements in-hand\nmanipulation more efficiently; Compared to existing learning-based methods, the\nproposed approach achieves dexterity and also generalizes to different objects\nwithout any pre-training. Detailed simulations and ablation studies demonstrate\nthe efficiency and effectiveness of our method. It runs at 20Hz on the\n23-degree-of-freedom, long-horizon, in-hand object rotation task."
                },
                "authors": [
                    {
                        "name": "Yongpeng Jiang"
                    },
                    {
                        "name": "Mingrui Yu"
                    },
                    {
                        "name": "Xinghao Zhu"
                    },
                    {
                        "name": "Masayoshi Tomizuka"
                    },
                    {
                        "name": "Xiang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Li"
                },
                "author": "Xiang Li",
                "arxiv_comment": "7 pages, 8 figures, accepted by IROS2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.18897v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18897v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04070v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04070v5",
                "updated": "2024-11-07T06:21:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    6,
                    21,
                    14,
                    3,
                    312,
                    0
                ],
                "published": "2024-10-05T08:00:55Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    8,
                    0,
                    55,
                    5,
                    279,
                    0
                ],
                "title": "PAD: Personalized Alignment of LLMs at Decoding-Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAD: Personalized Alignment of LLMs at Decoding-Time"
                },
                "summary": "Aligning with personalized preferences, which vary significantly across\ncultural, educational, and political differences, poses a significant challenge\ndue to the computational costs and data demands of traditional alignment\nmethods. In response, this paper presents Personalized Alignment at\nDecoding-time (PAD), a novel framework designed to align LLM outputs with\ndiverse personalized preferences during the inference phase, eliminating the\nneed for additional training. By introducing a unique personalized reward\nmodeling strategy, this framework decouples the text generation process from\npersonalized preferences, facilitating the generation of generalizable\ntoken-level personalized rewards. The PAD algorithm leverages these rewards to\nguide the decoding process, dynamically tailoring the base model's predictions\nto personalized preferences. Extensive experimental results demonstrate that\nPAD not only outperforms existing training-based alignment methods in terms of\naligning with diverse preferences but also shows significant generalizability\nto preferences unseen during training and scalability across different base\nmodels. This work advances the capability of LLMs to meet user needs in\nreal-time applications, presenting a substantial step forward in personalized\nLLM alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning with personalized preferences, which vary significantly across\ncultural, educational, and political differences, poses a significant challenge\ndue to the computational costs and data demands of traditional alignment\nmethods. In response, this paper presents Personalized Alignment at\nDecoding-time (PAD), a novel framework designed to align LLM outputs with\ndiverse personalized preferences during the inference phase, eliminating the\nneed for additional training. By introducing a unique personalized reward\nmodeling strategy, this framework decouples the text generation process from\npersonalized preferences, facilitating the generation of generalizable\ntoken-level personalized rewards. The PAD algorithm leverages these rewards to\nguide the decoding process, dynamically tailoring the base model's predictions\nto personalized preferences. Extensive experimental results demonstrate that\nPAD not only outperforms existing training-based alignment methods in terms of\naligning with diverse preferences but also shows significant generalizability\nto preferences unseen during training and scalability across different base\nmodels. This work advances the capability of LLMs to meet user needs in\nreal-time applications, presenting a substantial step forward in personalized\nLLM alignment."
                },
                "authors": [
                    {
                        "name": "Ruizhe Chen"
                    },
                    {
                        "name": "Xiaotian Zhang"
                    },
                    {
                        "name": "Meng Luo"
                    },
                    {
                        "name": "Wenhao Chai"
                    },
                    {
                        "name": "Zuozhu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zuozhu Liu"
                },
                "author": "Zuozhu Liu",
                "arxiv_comment": "This paper presents Personalized Alignment at Decoding-time (PAD), a\n  novel framework designed to align LLM outputs with diverse personalized\n  preferences during the inference phase",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04070v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04070v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01442v2",
                "updated": "2024-11-07T05:54:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    5,
                    54,
                    7,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-03T05:43:55Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    5,
                    43,
                    55,
                    6,
                    308,
                    0
                ],
                "title": "Online Relational Inference for Evolving Multi-agent Interacting Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Relational Inference for Evolving Multi-agent Interacting Systems"
                },
                "summary": "We introduce a novel framework, Online Relational Inference (ORI), designed\nto efficiently identify hidden interaction graphs in evolving multi-agent\ninteracting systems using streaming data. Unlike traditional offline methods\nthat rely on a fixed training set, ORI employs online backpropagation, updating\nthe model with each new data point, thereby allowing it to adapt to changing\nenvironments in real-time. A key innovation is the use of an adjacency matrix\nas a trainable parameter, optimized through a new adaptive learning rate\ntechnique called AdaRelation, which adjusts based on the historical sensitivity\nof the decoder to changes in the interaction graph. Additionally, a data\naugmentation method named Trajectory Mirror (TM) is introduced to improve\ngeneralization by exposing the model to varied trajectory patterns.\nExperimental results on both synthetic datasets and real-world data (CMU MoCap\nfor human motion) demonstrate that ORI significantly improves the accuracy and\nadaptability of relational inference in dynamic settings compared to existing\nmethods. This approach is model-agnostic, enabling seamless integration with\nvarious neural relational inference (NRI) architectures, and offers a robust\nsolution for real-time applications in complex, evolving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel framework, Online Relational Inference (ORI), designed\nto efficiently identify hidden interaction graphs in evolving multi-agent\ninteracting systems using streaming data. Unlike traditional offline methods\nthat rely on a fixed training set, ORI employs online backpropagation, updating\nthe model with each new data point, thereby allowing it to adapt to changing\nenvironments in real-time. A key innovation is the use of an adjacency matrix\nas a trainable parameter, optimized through a new adaptive learning rate\ntechnique called AdaRelation, which adjusts based on the historical sensitivity\nof the decoder to changes in the interaction graph. Additionally, a data\naugmentation method named Trajectory Mirror (TM) is introduced to improve\ngeneralization by exposing the model to varied trajectory patterns.\nExperimental results on both synthetic datasets and real-world data (CMU MoCap\nfor human motion) demonstrate that ORI significantly improves the accuracy and\nadaptability of relational inference in dynamic settings compared to existing\nmethods. This approach is model-agnostic, enabling seamless integration with\nvarious neural relational inference (NRI) architectures, and offers a robust\nsolution for real-time applications in complex, evolving systems."
                },
                "authors": [
                    {
                        "name": "Beomseok Kang"
                    },
                    {
                        "name": "Priyabrata Saha"
                    },
                    {
                        "name": "Sudarshan Sharma"
                    },
                    {
                        "name": "Biswadeep Chakraborty"
                    },
                    {
                        "name": "Saibal Mukhopadhyay"
                    }
                ],
                "author_detail": {
                    "name": "Saibal Mukhopadhyay"
                },
                "author": "Saibal Mukhopadhyay",
                "arxiv_comment": "Accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04448v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04448v1",
                "updated": "2024-11-07T05:43:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    5,
                    43,
                    50,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T05:43:50Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    5,
                    43,
                    50,
                    3,
                    312,
                    0
                ],
                "title": "Gradient Localization Improves Lifelong Pretraining of Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradient Localization Improves Lifelong Pretraining of Language Models"
                },
                "summary": "Large Language Models (LLMs) trained on web-scale text corpora have been\nshown to capture world knowledge in their parameters. However, the mechanism by\nwhich language models store different types of knowledge is poorly understood.\nIn this work, we examine two types of knowledge relating to temporally\nsensitive entities and demonstrate that each type is localized to different\nsets of parameters within the LLMs. We hypothesize that the lack of\nconsideration of the locality of knowledge in existing continual learning\nmethods contributes to both: the failed uptake of new information, and\ncatastrophic forgetting of previously learned information. We observe that\nsequences containing references to updated and newly mentioned entities exhibit\nlarger gradient norms in a subset of layers. We demonstrate that targeting\nparameter updates to these relevant layers can improve the performance of\ncontinually pretraining on language containing temporal drift.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) trained on web-scale text corpora have been\nshown to capture world knowledge in their parameters. However, the mechanism by\nwhich language models store different types of knowledge is poorly understood.\nIn this work, we examine two types of knowledge relating to temporally\nsensitive entities and demonstrate that each type is localized to different\nsets of parameters within the LLMs. We hypothesize that the lack of\nconsideration of the locality of knowledge in existing continual learning\nmethods contributes to both: the failed uptake of new information, and\ncatastrophic forgetting of previously learned information. We observe that\nsequences containing references to updated and newly mentioned entities exhibit\nlarger gradient norms in a subset of layers. We demonstrate that targeting\nparameter updates to these relevant layers can improve the performance of\ncontinually pretraining on language containing temporal drift."
                },
                "authors": [
                    {
                        "name": "Jared Fernandez"
                    },
                    {
                        "name": "Yonatan Bisk"
                    },
                    {
                        "name": "Emma Strubell"
                    }
                ],
                "author_detail": {
                    "name": "Emma Strubell"
                },
                "author": "Emma Strubell",
                "arxiv_comment": "EMNLP Findings 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04448v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04448v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18412v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18412v2",
                "updated": "2024-11-07T05:38:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    5,
                    38,
                    31,
                    3,
                    312,
                    0
                ],
                "published": "2024-09-27T03:00:29Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    3,
                    0,
                    29,
                    4,
                    271,
                    0
                ],
                "title": "SciDFM: A Large Language Model with Mixture-of-Experts for Science",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SciDFM: A Large Language Model with Mixture-of-Experts for Science"
                },
                "summary": "Recently, there has been a significant upsurge of interest in leveraging\nlarge language models (LLMs) to assist scientific discovery. However, most LLMs\nonly focus on general science, while they lack domain-specific knowledge, such\nas chemical molecules and amino acid sequences. To bridge these gaps, we\nintroduce SciDFM, a mixture-of-experts LLM, which is trained from scratch and\nis able to conduct college-level scientific reasoning and understand molecules\nand amino acid sequences. We collect a large-scale training corpus containing\nnumerous scientific papers and books from different disciplines as well as data\nfrom domain-specific databases. We further fine-tune the pre-trained model on\nlots of instruction data to improve performances on downstream benchmarks. From\nexperiment results, we show that SciDFM achieves strong performance on general\nscientific benchmarks such as SciEval and SciQ, and it reaches a SOTA\nperformance on domain-specific benchmarks among models of similar size. We\nfurther analyze the expert layers and show that the results of expert selection\nvary with data from different disciplines. To benefit the broader research\ncommunity, we open-source SciDFM at\nhttps://huggingface.co/OpenDFM/SciDFM-MoE-A5.6B-v1.0.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, there has been a significant upsurge of interest in leveraging\nlarge language models (LLMs) to assist scientific discovery. However, most LLMs\nonly focus on general science, while they lack domain-specific knowledge, such\nas chemical molecules and amino acid sequences. To bridge these gaps, we\nintroduce SciDFM, a mixture-of-experts LLM, which is trained from scratch and\nis able to conduct college-level scientific reasoning and understand molecules\nand amino acid sequences. We collect a large-scale training corpus containing\nnumerous scientific papers and books from different disciplines as well as data\nfrom domain-specific databases. We further fine-tune the pre-trained model on\nlots of instruction data to improve performances on downstream benchmarks. From\nexperiment results, we show that SciDFM achieves strong performance on general\nscientific benchmarks such as SciEval and SciQ, and it reaches a SOTA\nperformance on domain-specific benchmarks among models of similar size. We\nfurther analyze the expert layers and show that the results of expert selection\nvary with data from different disciplines. To benefit the broader research\ncommunity, we open-source SciDFM at\nhttps://huggingface.co/OpenDFM/SciDFM-MoE-A5.6B-v1.0."
                },
                "authors": [
                    {
                        "name": "Liangtai Sun"
                    },
                    {
                        "name": "Danyu Luo"
                    },
                    {
                        "name": "Da Ma"
                    },
                    {
                        "name": "Zihan Zhao"
                    },
                    {
                        "name": "Baocai Chen"
                    },
                    {
                        "name": "Zhennan Shen"
                    },
                    {
                        "name": "Su Zhu"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Xin Chen"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "12 pages, 1 figure, 9 tables. Technical Report, accepted by NeurIPS\n  2024 Workshop FM4Science",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18412v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18412v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04444v1",
                "updated": "2024-11-07T05:35:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    5,
                    35,
                    55,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T05:35:55Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    5,
                    35,
                    55,
                    3,
                    312,
                    0
                ],
                "title": "An Empirical Study on the Potential of LLMs in Automated Software\n  Refactoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study on the Potential of LLMs in Automated Software\n  Refactoring"
                },
                "summary": "Recent advances in large language models (LLMs), make it potentially feasible\nto automatically refactor source code with LLMs. However, it remains unclear\nhow well LLMs perform compared to human experts in conducting refactorings\nautomatically and accurately. To fill this gap, in this paper, we conduct an\nempirical study to investigate the potential of LLMs in automated software\nrefactoring, focusing on the identification of refactoring opportunities and\nthe recommendation of refactoring solutions. We first construct a high-quality\nrefactoring dataset comprising 180 real-world refactorings from 20 projects,\nand conduct the empirical study on the dataset. With the to-be-refactored Java\ndocuments as input, ChatGPT and Gemini identified only 28 and 7 respectively\nout of the 180 refactoring opportunities. However, explaining the expected\nrefactoring subcategories and narrowing the search space in the prompts\nsubstantially increased the success rate of ChatGPT from 15.6% to 86.7%.\nConcerning the recommendation of refactoring solutions, ChatGPT recommended 176\nrefactoring solutions for the 180 refactorings, and 63.6% of the recommended\nsolutions were comparable to (even better than) those constructed by human\nexperts. However, 13 out of the 176 solutions suggested by ChatGPT and 9 out of\nthe 137 solutions suggested by Gemini were unsafe in that they either changed\nthe functionality of the source code or introduced syntax errors, which\nindicate the risk of LLM-based refactoring. To this end, we propose a\ndetect-and-reapply tactic, called RefactoringMirror, to avoid such unsafe\nrefactorings. By reapplying the identified refactorings to the original code\nusing thoroughly tested refactoring engines, we can effectively mitigate the\nrisks associated with LLM-based automated refactoring while still leveraging\nLLM's intelligence to obtain valuable refactoring recommendations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs), make it potentially feasible\nto automatically refactor source code with LLMs. However, it remains unclear\nhow well LLMs perform compared to human experts in conducting refactorings\nautomatically and accurately. To fill this gap, in this paper, we conduct an\nempirical study to investigate the potential of LLMs in automated software\nrefactoring, focusing on the identification of refactoring opportunities and\nthe recommendation of refactoring solutions. We first construct a high-quality\nrefactoring dataset comprising 180 real-world refactorings from 20 projects,\nand conduct the empirical study on the dataset. With the to-be-refactored Java\ndocuments as input, ChatGPT and Gemini identified only 28 and 7 respectively\nout of the 180 refactoring opportunities. However, explaining the expected\nrefactoring subcategories and narrowing the search space in the prompts\nsubstantially increased the success rate of ChatGPT from 15.6% to 86.7%.\nConcerning the recommendation of refactoring solutions, ChatGPT recommended 176\nrefactoring solutions for the 180 refactorings, and 63.6% of the recommended\nsolutions were comparable to (even better than) those constructed by human\nexperts. However, 13 out of the 176 solutions suggested by ChatGPT and 9 out of\nthe 137 solutions suggested by Gemini were unsafe in that they either changed\nthe functionality of the source code or introduced syntax errors, which\nindicate the risk of LLM-based refactoring. To this end, we propose a\ndetect-and-reapply tactic, called RefactoringMirror, to avoid such unsafe\nrefactorings. By reapplying the identified refactorings to the original code\nusing thoroughly tested refactoring engines, we can effectively mitigate the\nrisks associated with LLM-based automated refactoring while still leveraging\nLLM's intelligence to obtain valuable refactoring recommendations."
                },
                "authors": [
                    {
                        "name": "Bo Liu"
                    },
                    {
                        "name": "Yanjie Jiang"
                    },
                    {
                        "name": "Yuxia Zhang"
                    },
                    {
                        "name": "Nan Niu"
                    },
                    {
                        "name": "Guangjie Li"
                    },
                    {
                        "name": "Hui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hui Liu"
                },
                "author": "Hui Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04440v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04440v1",
                "updated": "2024-11-07T05:23:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    5,
                    23,
                    31,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T05:23:31Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    5,
                    23,
                    31,
                    3,
                    312,
                    0
                ],
                "title": "AutoProteinEngine: A Large Language Model Driven Agent Framework for\n  Multimodal AutoML in Protein Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoProteinEngine: A Large Language Model Driven Agent Framework for\n  Multimodal AutoML in Protein Engineering"
                },
                "summary": "Protein engineering is important for biomedical applications, but\nconventional approaches are often inefficient and resource-intensive. While\ndeep learning (DL) models have shown promise, their training or implementation\ninto protein engineering remains challenging for biologists without specialized\ncomputational expertise. To address this gap, we propose AutoProteinEngine\n(AutoPE), an agent framework that leverages large language models (LLMs) for\nmultimodal automated machine learning (AutoML) for protein engineering. AutoPE\ninnovatively allows biologists without DL backgrounds to interact with DL\nmodels using natural language, lowering the entry barrier for protein\nengineering tasks. Our AutoPE uniquely integrates LLMs with AutoML to handle\nmodel selection for both protein sequence and graph modalities, automatic\nhyperparameter optimization, and automated data retrieval from protein\ndatabases. We evaluated AutoPE through two real-world protein engineering\ntasks, demonstrating substantial performance improvements compared to\ntraditional zero-shot and manual fine-tuning approaches. By bridging the gap\nbetween DL and biologists' domain expertise, AutoPE empowers researchers to\nleverage DL without extensive programming knowledge. Our code is available at\nhttps://github.com/tsynbio/AutoPE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protein engineering is important for biomedical applications, but\nconventional approaches are often inefficient and resource-intensive. While\ndeep learning (DL) models have shown promise, their training or implementation\ninto protein engineering remains challenging for biologists without specialized\ncomputational expertise. To address this gap, we propose AutoProteinEngine\n(AutoPE), an agent framework that leverages large language models (LLMs) for\nmultimodal automated machine learning (AutoML) for protein engineering. AutoPE\ninnovatively allows biologists without DL backgrounds to interact with DL\nmodels using natural language, lowering the entry barrier for protein\nengineering tasks. Our AutoPE uniquely integrates LLMs with AutoML to handle\nmodel selection for both protein sequence and graph modalities, automatic\nhyperparameter optimization, and automated data retrieval from protein\ndatabases. We evaluated AutoPE through two real-world protein engineering\ntasks, demonstrating substantial performance improvements compared to\ntraditional zero-shot and manual fine-tuning approaches. By bridging the gap\nbetween DL and biologists' domain expertise, AutoPE empowers researchers to\nleverage DL without extensive programming knowledge. Our code is available at\nhttps://github.com/tsynbio/AutoPE."
                },
                "authors": [
                    {
                        "name": "Yungeng Liu"
                    },
                    {
                        "name": "Zan Chen"
                    },
                    {
                        "name": "Yu Guang Wang"
                    },
                    {
                        "name": "Yiqing Shen"
                    }
                ],
                "author_detail": {
                    "name": "Yiqing Shen"
                },
                "author": "Yiqing Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04440v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04440v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18032v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18032v2",
                "updated": "2024-11-07T05:10:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    5,
                    10,
                    20,
                    3,
                    312,
                    0
                ],
                "published": "2024-10-23T17:02:59Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    2,
                    59,
                    2,
                    297,
                    0
                ],
                "title": "GraphTeam: Facilitating Large Language Model-based Graph Analysis via\n  Multi-Agent Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphTeam: Facilitating Large Language Model-based Graph Analysis via\n  Multi-Agent Collaboration"
                },
                "summary": "Graphs are widely used for modeling relational data in real-world scenarios,\nsuch as social networks and urban computing. Existing LLM-based graph analysis\napproaches either integrate graph neural networks (GNNs) for specific machine\nlearning tasks, limiting their transferability, or rely solely on LLMs'\ninternal reasoning ability, resulting in suboptimal performance. To address\nthese limitations, we take advantage of recent advances in LLM-based agents,\nwhich have shown capabilities of utilizing external knowledge or tools for\nproblem solving. By simulating human problem-solving strategies such as analogy\nand collaboration, we propose a multi-agent system based on LLMs named\nGraphTeam, for graph analysis. GraphTeam consists of five LLM-based agents from\nthree modules, and the agents with different specialities can collaborate with\neach other to address complex problems. Specifically, (1) input-output\nnormalization module: the question agent extracts and refines four key\narguments from the original question, facilitating the problem understanding,\nand the answer agent organizes the results to meet the output requirement; (2)\nexternal knowledge retrieval module: we first build a knowledge base consisting\nof relevant documentation and experience information, and then the search agent\nretrieves the most relevant entries for each question. (3) problem-solving\nmodule: given the retrieved information from search agent, the coding agent\nuses established algorithms via programming to generate solutions, and in case\nthe coding agent does not work, the reasoning agent will directly compute the\nresults without programming. Extensive experiments on six graph analysis\nbenchmarks demonstrate that GraphTeam achieves state-of-the-art performance\nwith an average 25.85% improvement over the best baseline in terms of accuracy.\nThe code and data are available at https://github.com/BUPT-GAMMA/GraphTeam.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphs are widely used for modeling relational data in real-world scenarios,\nsuch as social networks and urban computing. Existing LLM-based graph analysis\napproaches either integrate graph neural networks (GNNs) for specific machine\nlearning tasks, limiting their transferability, or rely solely on LLMs'\ninternal reasoning ability, resulting in suboptimal performance. To address\nthese limitations, we take advantage of recent advances in LLM-based agents,\nwhich have shown capabilities of utilizing external knowledge or tools for\nproblem solving. By simulating human problem-solving strategies such as analogy\nand collaboration, we propose a multi-agent system based on LLMs named\nGraphTeam, for graph analysis. GraphTeam consists of five LLM-based agents from\nthree modules, and the agents with different specialities can collaborate with\neach other to address complex problems. Specifically, (1) input-output\nnormalization module: the question agent extracts and refines four key\narguments from the original question, facilitating the problem understanding,\nand the answer agent organizes the results to meet the output requirement; (2)\nexternal knowledge retrieval module: we first build a knowledge base consisting\nof relevant documentation and experience information, and then the search agent\nretrieves the most relevant entries for each question. (3) problem-solving\nmodule: given the retrieved information from search agent, the coding agent\nuses established algorithms via programming to generate solutions, and in case\nthe coding agent does not work, the reasoning agent will directly compute the\nresults without programming. Extensive experiments on six graph analysis\nbenchmarks demonstrate that GraphTeam achieves state-of-the-art performance\nwith an average 25.85% improvement over the best baseline in terms of accuracy.\nThe code and data are available at https://github.com/BUPT-GAMMA/GraphTeam."
                },
                "authors": [
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Qizhi Chu"
                    },
                    {
                        "name": "Yubin Chen"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Yaoqi Liu"
                    },
                    {
                        "name": "Zekai Yu"
                    },
                    {
                        "name": "Weize Chen"
                    },
                    {
                        "name": "Chen Qian"
                    },
                    {
                        "name": "Chuan Shi"
                    },
                    {
                        "name": "Cheng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Cheng Yang"
                },
                "author": "Cheng Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18032v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18032v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.02633v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.02633v4",
                "updated": "2024-11-07T05:07:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    5,
                    7,
                    0,
                    3,
                    312,
                    0
                ],
                "published": "2024-03-05T03:55:55Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    3,
                    55,
                    55,
                    1,
                    65,
                    0
                ],
                "title": "Spatially Non-Stationary XL-MIMO Channel Estimation: A Three-Layer\n  Generalized Approximate Message Passing Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatially Non-Stationary XL-MIMO Channel Estimation: A Three-Layer\n  Generalized Approximate Message Passing Method"
                },
                "summary": "In this paper, channel estimation problem for extremely large-scale\nmulti-input multi-output (XL-MIMO) systems is investigated with the\nconsiderations of the spherical wavefront effect and the spatially\nnon-stationary (SnS) property. Due to the diversities of SnS characteristics\namong different propagation paths, the concurrent channel estimation of\nmultiple paths becomes intractable. To address this challenge, we propose a\ntwo-phase channel estimation scheme. In the first phase, the angles of\ndeparture (AoDs) on the user side are estimated, and a carefully designed pilot\ntransmission scheme enables the decomposition of the received signal from\ndifferent paths. In the second phase, the subchannel estimation corresponding\nto different paths is formulated as a three-layer Bayesian inference problem.\nSpecifically, the first layer captures block sparsity in the angular domain,\nthe second layer promotes SnS property in the antenna domain, and the third\nlayer decouples the subchannels from the observed signals. To efficiently\nfacilitate Bayesian inference, we propose a novel three-layer generalized\napproximate message passing (TL-GAMP) algorithm based on structured variational\nmassage passing and belief propagation rules. Simulation results validate the\nconvergence and effectiveness of the proposed algorithm, showcasing its\nrobustness to different channel scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, channel estimation problem for extremely large-scale\nmulti-input multi-output (XL-MIMO) systems is investigated with the\nconsiderations of the spherical wavefront effect and the spatially\nnon-stationary (SnS) property. Due to the diversities of SnS characteristics\namong different propagation paths, the concurrent channel estimation of\nmultiple paths becomes intractable. To address this challenge, we propose a\ntwo-phase channel estimation scheme. In the first phase, the angles of\ndeparture (AoDs) on the user side are estimated, and a carefully designed pilot\ntransmission scheme enables the decomposition of the received signal from\ndifferent paths. In the second phase, the subchannel estimation corresponding\nto different paths is formulated as a three-layer Bayesian inference problem.\nSpecifically, the first layer captures block sparsity in the angular domain,\nthe second layer promotes SnS property in the antenna domain, and the third\nlayer decouples the subchannels from the observed signals. To efficiently\nfacilitate Bayesian inference, we propose a novel three-layer generalized\napproximate message passing (TL-GAMP) algorithm based on structured variational\nmassage passing and belief propagation rules. Simulation results validate the\nconvergence and effectiveness of the proposed algorithm, showcasing its\nrobustness to different channel scenarios."
                },
                "authors": [
                    {
                        "name": "Anzheng Tang"
                    },
                    {
                        "name": "Jun-Bo Wang"
                    },
                    {
                        "name": "Yijin Pan"
                    },
                    {
                        "name": "Wence Zhang"
                    },
                    {
                        "name": "Yijian Chen"
                    },
                    {
                        "name": "Hongkang Yu"
                    },
                    {
                        "name": "Rodrigo C. de Lamare"
                    }
                ],
                "author_detail": {
                    "name": "Rodrigo C. de Lamare"
                },
                "author": "Rodrigo C. de Lamare",
                "arxiv_comment": "A revised manuscript has been submitted to the IEEE journal for\n  possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.02633v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.02633v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04427v1",
                "updated": "2024-11-07T04:38:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    4,
                    38,
                    58,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T04:38:58Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    4,
                    38,
                    58,
                    3,
                    312,
                    0
                ],
                "title": "One fish, two fish, but not the whole sea: Alignment reduces language\n  models' conceptual diversity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One fish, two fish, but not the whole sea: Alignment reduces language\n  models' conceptual diversity"
                },
                "summary": "Researchers in social science and psychology have recently proposed using\nlarge language models (LLMs) as replacements for humans in behavioral research.\nIn addition to arguments about whether LLMs accurately capture population-level\npatterns, this has raised questions about whether LLMs capture human-like\nconceptual diversity. Separately, it is debated whether post-training alignment\n(RLHF or RLAIF) affects models' internal diversity. Inspired by human studies,\nwe use a new way of measuring the conceptual diversity of\nsynthetically-generated LLM \"populations\" by relating the internal variability\nof simulated individuals to the population-level variability. We use this\napproach to evaluate non-aligned and aligned LLMs on two domains with rich\nhuman behavioral data. While no model reaches human-like diversity, aligned\nmodels generally display less diversity than their instruction fine-tuned\ncounterparts. Our findings highlight potential trade-offs between increasing\nmodels' value alignment and decreasing the diversity of their conceptual\nrepresentations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Researchers in social science and psychology have recently proposed using\nlarge language models (LLMs) as replacements for humans in behavioral research.\nIn addition to arguments about whether LLMs accurately capture population-level\npatterns, this has raised questions about whether LLMs capture human-like\nconceptual diversity. Separately, it is debated whether post-training alignment\n(RLHF or RLAIF) affects models' internal diversity. Inspired by human studies,\nwe use a new way of measuring the conceptual diversity of\nsynthetically-generated LLM \"populations\" by relating the internal variability\nof simulated individuals to the population-level variability. We use this\napproach to evaluate non-aligned and aligned LLMs on two domains with rich\nhuman behavioral data. While no model reaches human-like diversity, aligned\nmodels generally display less diversity than their instruction fine-tuned\ncounterparts. Our findings highlight potential trade-offs between increasing\nmodels' value alignment and decreasing the diversity of their conceptual\nrepresentations."
                },
                "authors": [
                    {
                        "name": "Sonia K. Murthy"
                    },
                    {
                        "name": "Tomer Ullman"
                    },
                    {
                        "name": "Jennifer Hu"
                    }
                ],
                "author_detail": {
                    "name": "Jennifer Hu"
                },
                "author": "Jennifer Hu",
                "arxiv_comment": "17 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04425v1",
                "updated": "2024-11-07T04:38:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    4,
                    38,
                    29,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T04:38:29Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    4,
                    38,
                    29,
                    3,
                    312,
                    0
                ],
                "title": "DELIFT: Data Efficient Language model Instruction Fine Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DELIFT: Data Efficient Language model Instruction Fine Tuning"
                },
                "summary": "Fine-tuning large language models (LLMs) is essential for enhancing their\nperformance on specific tasks but is often resource-intensive due to redundant\nor uninformative data. To address this inefficiency, we introduce DELIFT (Data\nEfficient Language model Instruction Fine-Tuning), a novel algorithm that\nsystematically optimizes data selection across the three key stages of\nfine-tuning: (1) instruction tuning, (2) task-specific fine-tuning (e.g.,\nreasoning, question-answering), and (3) continual fine-tuning (e.g.,\nincorporating new data versions). Unlike existing methods that focus on\nsingle-stage optimization or rely on computationally intensive gradient\ncalculations, DELIFT operates efficiently across all stages. Central to our\napproach is a pairwise utility metric that quantifies how beneficial a data\nsample is for improving the model's responses to other samples, effectively\nmeasuring the informational value relative to the model's current capabilities.\nBy leveraging different submodular functions applied to this metric, DELIFT\nselects diverse and optimal subsets that are useful across all stages of\nfine-tuning. Experiments across various tasks and model scales demonstrate that\nDELIFT can reduce the fine-tuning data size by up to 70% without compromising\nperformance, offering significant computational savings and outperforming\nexisting methods in both efficiency and efficacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) is essential for enhancing their\nperformance on specific tasks but is often resource-intensive due to redundant\nor uninformative data. To address this inefficiency, we introduce DELIFT (Data\nEfficient Language model Instruction Fine-Tuning), a novel algorithm that\nsystematically optimizes data selection across the three key stages of\nfine-tuning: (1) instruction tuning, (2) task-specific fine-tuning (e.g.,\nreasoning, question-answering), and (3) continual fine-tuning (e.g.,\nincorporating new data versions). Unlike existing methods that focus on\nsingle-stage optimization or rely on computationally intensive gradient\ncalculations, DELIFT operates efficiently across all stages. Central to our\napproach is a pairwise utility metric that quantifies how beneficial a data\nsample is for improving the model's responses to other samples, effectively\nmeasuring the informational value relative to the model's current capabilities.\nBy leveraging different submodular functions applied to this metric, DELIFT\nselects diverse and optimal subsets that are useful across all stages of\nfine-tuning. Experiments across various tasks and model scales demonstrate that\nDELIFT can reduce the fine-tuning data size by up to 70% without compromising\nperformance, offering significant computational savings and outperforming\nexisting methods in both efficiency and efficacy."
                },
                "authors": [
                    {
                        "name": "Ishika Agarwal"
                    },
                    {
                        "name": "Krishna Killamsetty"
                    },
                    {
                        "name": "Lucian Popa"
                    },
                    {
                        "name": "Marina Danilevksy"
                    }
                ],
                "author_detail": {
                    "name": "Marina Danilevksy"
                },
                "author": "Marina Danilevksy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.05007v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05007v1",
                "updated": "2024-11-07T18:59:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    59,
                    58,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T18:59:58Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    59,
                    58,
                    3,
                    312,
                    0
                ],
                "title": "SVDQunat: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SVDQunat: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion\n  Models"
                },
                "summary": "Diffusion models have been proven highly effective at generating high-quality\nimages. However, as these models grow larger, they require significantly more\nmemory and suffer from higher latency, posing substantial challenges for\ndeployment. In this work, we aim to accelerate diffusion models by quantizing\ntheir weights and activations to 4 bits. At such an aggressive level, both\nweights and activations are highly sensitive, where conventional post-training\nquantization methods for large language models like smoothing become\ninsufficient. To overcome this limitation, we propose SVDQuant, a new 4-bit\nquantization paradigm. Different from smoothing which redistributes outliers\nbetween weights and activations, our approach absorbs these outliers using a\nlow-rank branch. We first consolidate the outliers by shifting them from\nactivations to weights, then employ a high-precision low-rank branch to take in\nthe weight outliers with Singular Value Decomposition (SVD). This process eases\nthe quantization on both sides. However, na\\\"{\\i}vely running the low-rank\nbranch independently incurs significant overhead due to extra data movement of\nactivations, negating the quantization speedup. To address this, we co-design\nan inference engine Nunchaku that fuses the kernels of the low-rank branch into\nthose of the low-bit branch to cut off redundant memory access. It can also\nseamlessly support off-the-shelf low-rank adapters (LoRAs) without the need for\nre-quantization. Extensive experiments on SDXL, PixArt-$\\Sigma$, and FLUX.1\nvalidate the effectiveness of SVDQuant in preserving image quality. We reduce\nthe memory usage for the 12B FLUX.1 models by 3.5$\\times$, achieving\n3.0$\\times$ speedup over the 4-bit weight-only quantized baseline on the 16GB\nlaptop 4090 GPU, paving the way for more interactive applications on PCs. Our\nquantization library and inference engine are open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have been proven highly effective at generating high-quality\nimages. However, as these models grow larger, they require significantly more\nmemory and suffer from higher latency, posing substantial challenges for\ndeployment. In this work, we aim to accelerate diffusion models by quantizing\ntheir weights and activations to 4 bits. At such an aggressive level, both\nweights and activations are highly sensitive, where conventional post-training\nquantization methods for large language models like smoothing become\ninsufficient. To overcome this limitation, we propose SVDQuant, a new 4-bit\nquantization paradigm. Different from smoothing which redistributes outliers\nbetween weights and activations, our approach absorbs these outliers using a\nlow-rank branch. We first consolidate the outliers by shifting them from\nactivations to weights, then employ a high-precision low-rank branch to take in\nthe weight outliers with Singular Value Decomposition (SVD). This process eases\nthe quantization on both sides. However, na\\\"{\\i}vely running the low-rank\nbranch independently incurs significant overhead due to extra data movement of\nactivations, negating the quantization speedup. To address this, we co-design\nan inference engine Nunchaku that fuses the kernels of the low-rank branch into\nthose of the low-bit branch to cut off redundant memory access. It can also\nseamlessly support off-the-shelf low-rank adapters (LoRAs) without the need for\nre-quantization. Extensive experiments on SDXL, PixArt-$\\Sigma$, and FLUX.1\nvalidate the effectiveness of SVDQuant in preserving image quality. We reduce\nthe memory usage for the 12B FLUX.1 models by 3.5$\\times$, achieving\n3.0$\\times$ speedup over the 4-bit weight-only quantized baseline on the 16GB\nlaptop 4090 GPU, paving the way for more interactive applications on PCs. Our\nquantization library and inference engine are open-sourced."
                },
                "authors": [
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Zhekai Zhang"
                    },
                    {
                        "name": "Tianle Cai"
                    },
                    {
                        "name": "Xiuyu Li"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Enze Xie"
                    },
                    {
                        "name": "Chenlin Meng"
                    },
                    {
                        "name": "Jun-Yan Zhu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Quantization Library: https://github.com/mit-han-lab/deepcompressor\n  Inference Engine: https://github.com/mit-han-lab/nunchaku Website:\n  https://hanlab.mit.edu/projects/svdquant Demo: https://svdquant.mit.edu Blog:\n  https://hanlab.mit.edu/blog/svdquant",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05007v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.00922v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.00922v3",
                "updated": "2024-11-07T18:59:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    59,
                    30,
                    3,
                    312,
                    0
                ],
                "published": "2024-06-03T01:32:52Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    1,
                    32,
                    52,
                    0,
                    155,
                    0
                ],
                "title": "MediQ: Question-Asking LLMs and a Benchmark for Reliable Interactive\n  Clinical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MediQ: Question-Asking LLMs and a Benchmark for Reliable Interactive\n  Clinical Reasoning"
                },
                "summary": "Users typically engage with LLMs interactively, yet most existing benchmarks\nevaluate them in a static, single-turn format, posing reliability concerns in\ninteractive scenarios. We identify a key obstacle towards reliability: LLMs are\ntrained to answer any question, even with incomplete context or insufficient\nknowledge. In this paper, we propose to change the static paradigm to an\ninteractive one, develop systems that proactively ask questions to gather more\ninformation and respond reliably, and introduce an benchmark - MediQ - to\nevaluate question-asking ability in LLMs. MediQ simulates clinical interactions\nconsisting of a Patient System and an adaptive Expert System; with potentially\nincomplete initial information, the Expert refrains from making diagnostic\ndecisions when unconfident, and instead elicits missing details via follow-up\nquestions. We provide a pipeline to convert single-turn medical benchmarks into\nan interactive format. Our results show that directly prompting\nstate-of-the-art LLMs to ask questions degrades performance, indicating that\nadapting LLMs to proactive information-seeking settings is nontrivial. We\nexperiment with abstention strategies to better estimate model confidence and\ndecide when to ask questions, improving diagnostic accuracy by 22.3%; however,\nperformance still lags compared to an (unrealistic in practice) upper bound\nwith complete information upfront. Further analyses show improved interactive\nperformance with filtering irrelevant contexts and reformatting conversations.\nOverall, we introduce a novel problem towards LLM reliability, an interactive\nMediQ benchmark and a novel question-asking system, and highlight directions to\nextend LLMs' information-seeking abilities in critical domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Users typically engage with LLMs interactively, yet most existing benchmarks\nevaluate them in a static, single-turn format, posing reliability concerns in\ninteractive scenarios. We identify a key obstacle towards reliability: LLMs are\ntrained to answer any question, even with incomplete context or insufficient\nknowledge. In this paper, we propose to change the static paradigm to an\ninteractive one, develop systems that proactively ask questions to gather more\ninformation and respond reliably, and introduce an benchmark - MediQ - to\nevaluate question-asking ability in LLMs. MediQ simulates clinical interactions\nconsisting of a Patient System and an adaptive Expert System; with potentially\nincomplete initial information, the Expert refrains from making diagnostic\ndecisions when unconfident, and instead elicits missing details via follow-up\nquestions. We provide a pipeline to convert single-turn medical benchmarks into\nan interactive format. Our results show that directly prompting\nstate-of-the-art LLMs to ask questions degrades performance, indicating that\nadapting LLMs to proactive information-seeking settings is nontrivial. We\nexperiment with abstention strategies to better estimate model confidence and\ndecide when to ask questions, improving diagnostic accuracy by 22.3%; however,\nperformance still lags compared to an (unrealistic in practice) upper bound\nwith complete information upfront. Further analyses show improved interactive\nperformance with filtering irrelevant contexts and reformatting conversations.\nOverall, we introduce a novel problem towards LLM reliability, an interactive\nMediQ benchmark and a novel question-asking system, and highlight directions to\nextend LLMs' information-seeking abilities in critical domains."
                },
                "authors": [
                    {
                        "name": "Shuyue Stella Li"
                    },
                    {
                        "name": "Vidhisha Balachandran"
                    },
                    {
                        "name": "Shangbin Feng"
                    },
                    {
                        "name": "Jonathan S. Ilgen"
                    },
                    {
                        "name": "Emma Pierson"
                    },
                    {
                        "name": "Pang Wei Koh"
                    },
                    {
                        "name": "Yulia Tsvetkov"
                    }
                ],
                "author_detail": {
                    "name": "Yulia Tsvetkov"
                },
                "author": "Yulia Tsvetkov",
                "arxiv_comment": "29 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.00922v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.00922v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04999v1",
                "updated": "2024-11-07T18:59:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    59,
                    27,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T18:59:27Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    59,
                    27,
                    3,
                    312,
                    0
                ],
                "title": "DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile\n  Manipulation"
                },
                "summary": "Significant progress has been made in open-vocabulary mobile manipulation,\nwhere the goal is for a robot to perform tasks in any environment given a\nnatural language description. However, most current systems assume a static\nenvironment, which limits the system's applicability in real-world scenarios\nwhere environments frequently change due to human intervention or the robot's\nown actions. In this work, we present DynaMem, a new approach to open-world\nmobile manipulation that uses a dynamic spatio-semantic memory to represent a\nrobot's environment. DynaMem constructs a 3D data structure to maintain a\ndynamic memory of point clouds, and answers open-vocabulary object localization\nqueries using multimodal LLMs or open-vocabulary features generated by\nstate-of-the-art vision-language models. Powered by DynaMem, our robots can\nexplore novel environments, search for objects not found in memory, and\ncontinuously update the memory as objects move, appear, or disappear in the\nscene. We run extensive experiments on the Stretch SE3 robots in three real and\nnine offline scenes, and achieve an average pick-and-drop success rate of 70%\non non-stationary objects, which is more than a 2x improvement over\nstate-of-the-art static systems. Our code as well as our experiment and\ndeployment videos are open sourced and can be found on our project website:\nhttps://dynamem.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Significant progress has been made in open-vocabulary mobile manipulation,\nwhere the goal is for a robot to perform tasks in any environment given a\nnatural language description. However, most current systems assume a static\nenvironment, which limits the system's applicability in real-world scenarios\nwhere environments frequently change due to human intervention or the robot's\nown actions. In this work, we present DynaMem, a new approach to open-world\nmobile manipulation that uses a dynamic spatio-semantic memory to represent a\nrobot's environment. DynaMem constructs a 3D data structure to maintain a\ndynamic memory of point clouds, and answers open-vocabulary object localization\nqueries using multimodal LLMs or open-vocabulary features generated by\nstate-of-the-art vision-language models. Powered by DynaMem, our robots can\nexplore novel environments, search for objects not found in memory, and\ncontinuously update the memory as objects move, appear, or disappear in the\nscene. We run extensive experiments on the Stretch SE3 robots in three real and\nnine offline scenes, and achieve an average pick-and-drop success rate of 70%\non non-stationary objects, which is more than a 2x improvement over\nstate-of-the-art static systems. Our code as well as our experiment and\ndeployment videos are open sourced and can be found on our project website:\nhttps://dynamem.github.io/"
                },
                "authors": [
                    {
                        "name": "Peiqi Liu"
                    },
                    {
                        "name": "Zhanqiu Guo"
                    },
                    {
                        "name": "Mohit Warke"
                    },
                    {
                        "name": "Soumith Chintala"
                    },
                    {
                        "name": "Chris Paxton"
                    },
                    {
                        "name": "Nur Muhammad Mahi Shafiullah"
                    },
                    {
                        "name": "Lerrel Pinto"
                    }
                ],
                "author_detail": {
                    "name": "Lerrel Pinto"
                },
                "author": "Lerrel Pinto",
                "arxiv_comment": "Website: https://dynamem.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05000v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05000v1",
                "updated": "2024-11-07T18:59:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    59,
                    27,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T18:59:27Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    59,
                    27,
                    3,
                    312,
                    0
                ],
                "title": "Needle Threading: Can LLMs Follow Threads through Near-Million-Scale\n  Haystacks?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Needle Threading: Can LLMs Follow Threads through Near-Million-Scale\n  Haystacks?"
                },
                "summary": "As the context limits of Large Language Models (LLMs) increase, the range of\npossible applications and downstream functions broadens. In many real-world\ntasks, decisions depend on details scattered across collections of often\ndisparate documents containing mostly irrelevant information. Long-context LLMs\nappear well-suited to this form of complex information retrieval and reasoning,\nwhich has traditionally proven costly and time-consuming. However, although the\ndevelopment of longer context models has seen rapid gains in recent years, our\nunderstanding of how effectively LLMs use their context has not kept pace. To\naddress this, we conduct a set of retrieval experiments designed to evaluate\nthe capabilities of 17 leading LLMs, such as their ability to follow threads of\ninformation through the context window. Strikingly, we find that many models\nare remarkably threadsafe: capable of simultaneously following multiple threads\nwithout significant loss in performance. Still, for many models, we find the\neffective context limit is significantly shorter than the supported context\nlength, with accuracy decreasing as the context window grows. Our study also\nhighlights the important point that token counts from different tokenizers\nshould not be directly compared -- they often correspond to substantially\ndifferent numbers of written characters. We release our code and long-context\nexperimental data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the context limits of Large Language Models (LLMs) increase, the range of\npossible applications and downstream functions broadens. In many real-world\ntasks, decisions depend on details scattered across collections of often\ndisparate documents containing mostly irrelevant information. Long-context LLMs\nappear well-suited to this form of complex information retrieval and reasoning,\nwhich has traditionally proven costly and time-consuming. However, although the\ndevelopment of longer context models has seen rapid gains in recent years, our\nunderstanding of how effectively LLMs use their context has not kept pace. To\naddress this, we conduct a set of retrieval experiments designed to evaluate\nthe capabilities of 17 leading LLMs, such as their ability to follow threads of\ninformation through the context window. Strikingly, we find that many models\nare remarkably threadsafe: capable of simultaneously following multiple threads\nwithout significant loss in performance. Still, for many models, we find the\neffective context limit is significantly shorter than the supported context\nlength, with accuracy decreasing as the context window grows. Our study also\nhighlights the important point that token counts from different tokenizers\nshould not be directly compared -- they often correspond to substantially\ndifferent numbers of written characters. We release our code and long-context\nexperimental data."
                },
                "authors": [
                    {
                        "name": "Jonathan Roberts"
                    },
                    {
                        "name": "Kai Han"
                    },
                    {
                        "name": "Samuel Albanie"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Albanie"
                },
                "author": "Samuel Albanie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05000v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05000v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04997v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04997v1",
                "updated": "2024-11-07T18:59:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    59,
                    16,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T18:59:16Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    59,
                    16,
                    3,
                    312,
                    0
                ],
                "title": "LLM2CLIP: Powerful Language Model Unlock Richer Visual Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM2CLIP: Powerful Language Model Unlock Richer Visual Representation"
                },
                "summary": "CLIP is one of the most important multimodal foundational models today. What\npowers CLIP's capabilities? The rich supervision signals provided by natural\nlanguage, the carrier of human knowledge, shape a powerful cross-modal\nrepresentation space. However, with the rapid advancements in large language\nmodels LLMs like GPT-4 and LLaMA, the boundaries of language comprehension and\ngeneration are continually being pushed. This raises an intriguing question:\ncan the capabilities of LLMs be harnessed to further improve multimodal\nrepresentation learning? The potential benefits of incorporating LLMs into CLIP\nare clear. LLMs' strong textual understanding can fundamentally improve CLIP's\nability to handle image captions, drastically enhancing its ability to process\nlong and complex texts, a well-known limitation of vanilla CLIP. Moreover, LLMs\nare trained on a vast corpus of text, possessing open-world knowledge. This\nallows them to expand on caption information during training, increasing the\nefficiency of the learning process. In this paper, we propose LLM2CLIP, a novel\napproach that embraces the power of LLMs to unlock CLIP's potential. By\nfine-tuning the LLM in the caption space with contrastive learning, we extract\nits textual capabilities into the output embeddings, significantly improving\nthe output layer's textual discriminability. We then design an efficient\ntraining process where the fine-tuned LLM acts as a powerful teacher for CLIP's\nvisual encoder. Thanks to the LLM's presence, we can now incorporate longer and\nmore complex captions without being restricted by vanilla CLIP's text encoder's\ncontext window and ability limitations. Our experiments demonstrate that this\napproach brings substantial improvements in cross-modal tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLIP is one of the most important multimodal foundational models today. What\npowers CLIP's capabilities? The rich supervision signals provided by natural\nlanguage, the carrier of human knowledge, shape a powerful cross-modal\nrepresentation space. However, with the rapid advancements in large language\nmodels LLMs like GPT-4 and LLaMA, the boundaries of language comprehension and\ngeneration are continually being pushed. This raises an intriguing question:\ncan the capabilities of LLMs be harnessed to further improve multimodal\nrepresentation learning? The potential benefits of incorporating LLMs into CLIP\nare clear. LLMs' strong textual understanding can fundamentally improve CLIP's\nability to handle image captions, drastically enhancing its ability to process\nlong and complex texts, a well-known limitation of vanilla CLIP. Moreover, LLMs\nare trained on a vast corpus of text, possessing open-world knowledge. This\nallows them to expand on caption information during training, increasing the\nefficiency of the learning process. In this paper, we propose LLM2CLIP, a novel\napproach that embraces the power of LLMs to unlock CLIP's potential. By\nfine-tuning the LLM in the caption space with contrastive learning, we extract\nits textual capabilities into the output embeddings, significantly improving\nthe output layer's textual discriminability. We then design an efficient\ntraining process where the fine-tuned LLM acts as a powerful teacher for CLIP's\nvisual encoder. Thanks to the LLM's presence, we can now incorporate longer and\nmore complex captions without being restricted by vanilla CLIP's text encoder's\ncontext window and ability limitations. Our experiments demonstrate that this\napproach brings substantial improvements in cross-modal tasks."
                },
                "authors": [
                    {
                        "name": "Weiquan Huang"
                    },
                    {
                        "name": "Aoqi Wu"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Liang Hu"
                    },
                    {
                        "name": "Qi Dai"
                    },
                    {
                        "name": "Xiyang Dai"
                    },
                    {
                        "name": "Dongdong Chen"
                    },
                    {
                        "name": "Chong Luo"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04997v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04997v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04996v1",
                "updated": "2024-11-07T18:59:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    59,
                    6,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T18:59:06Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    59,
                    6,
                    3,
                    312,
                    0
                ],
                "title": "Mixture-of-Transformers: A Sparse and Scalable Architecture for\n  Multi-Modal Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Transformers: A Sparse and Scalable Architecture for\n  Multi-Modal Foundation Models"
                },
                "summary": "The development of large language models (LLMs) has expanded to multi-modal\nsystems capable of processing text, images, and speech within a unified\nframework. Training these models demands significantly larger datasets and\ncomputational resources compared to text-only LLMs. To address the scaling\nchallenges, we introduce Mixture-of-Transformers (MoT), a sparse multi-modal\ntransformer architecture that significantly reduces pretraining computational\ncosts. MoT decouples non-embedding parameters of the model by modality --\nincluding feed-forward networks, attention matrices, and layer normalization --\nenabling modality-specific processing with global self-attention over the full\ninput sequence. We evaluate MoT across multiple settings and model scales. In\nthe Chameleon 7B setting (autoregressive text-and-image generation), MoT\nmatches the dense baseline's performance using only 55.8\\% of the FLOPs. When\nextended to include speech, MoT reaches speech performance comparable to the\ndense baseline with only 37.2\\% of the FLOPs. In the Transfusion setting, where\ntext and image are trained with different objectives, a 7B MoT model matches\nthe image modality performance of the dense baseline with one third of the\nFLOPs, and a 760M MoT model outperforms a 1.4B dense baseline across key image\ngeneration metrics. System profiling further highlights MoT's practical\nbenefits, achieving dense baseline image quality in 47.2\\% of the wall-clock\ntime and text quality in 75.6\\% of the wall-clock time (measured on AWS\np4de.24xlarge instances with NVIDIA A100 GPUs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of large language models (LLMs) has expanded to multi-modal\nsystems capable of processing text, images, and speech within a unified\nframework. Training these models demands significantly larger datasets and\ncomputational resources compared to text-only LLMs. To address the scaling\nchallenges, we introduce Mixture-of-Transformers (MoT), a sparse multi-modal\ntransformer architecture that significantly reduces pretraining computational\ncosts. MoT decouples non-embedding parameters of the model by modality --\nincluding feed-forward networks, attention matrices, and layer normalization --\nenabling modality-specific processing with global self-attention over the full\ninput sequence. We evaluate MoT across multiple settings and model scales. In\nthe Chameleon 7B setting (autoregressive text-and-image generation), MoT\nmatches the dense baseline's performance using only 55.8\\% of the FLOPs. When\nextended to include speech, MoT reaches speech performance comparable to the\ndense baseline with only 37.2\\% of the FLOPs. In the Transfusion setting, where\ntext and image are trained with different objectives, a 7B MoT model matches\nthe image modality performance of the dense baseline with one third of the\nFLOPs, and a 760M MoT model outperforms a 1.4B dense baseline across key image\ngeneration metrics. System profiling further highlights MoT's practical\nbenefits, achieving dense baseline image quality in 47.2\\% of the wall-clock\ntime and text quality in 75.6\\% of the wall-clock time (measured on AWS\np4de.24xlarge instances with NVIDIA A100 GPUs)."
                },
                "authors": [
                    {
                        "name": "Weixin Liang"
                    },
                    {
                        "name": "Lili Yu"
                    },
                    {
                        "name": "Liang Luo"
                    },
                    {
                        "name": "Srinivasan Iyer"
                    },
                    {
                        "name": "Ning Dong"
                    },
                    {
                        "name": "Chunting Zhou"
                    },
                    {
                        "name": "Gargi Ghosh"
                    },
                    {
                        "name": "Mike Lewis"
                    },
                    {
                        "name": "Wen-tau Yih"
                    },
                    {
                        "name": "Luke Zettlemoyer"
                    },
                    {
                        "name": "Xi Victoria Lin"
                    }
                ],
                "author_detail": {
                    "name": "Xi Victoria Lin"
                },
                "author": "Xi Victoria Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21723v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21723v2",
                "updated": "2024-11-07T18:57:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    57,
                    27,
                    3,
                    312,
                    0
                ],
                "published": "2024-10-29T04:22:28Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    4,
                    22,
                    28,
                    1,
                    303,
                    0
                ],
                "title": "Fine-tuning Large Language Models for DGA and DNS Exfiltration Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning Large Language Models for DGA and DNS Exfiltration Detection"
                },
                "summary": "Domain Generation Algorithms (DGAs) are malicious techniques used by malware\nto dynamically generate seemingly random domain names for communication with\nCommand & Control (C&C) servers. Due to the fast and simple generation of DGA\ndomains, detection methods must be highly efficient and precise to be\neffective. Large Language Models (LLMs) have demonstrated their proficiency in\nreal-time detection tasks, making them ideal candidates for detecting DGAs. Our\nwork validates the effectiveness of fine-tuned LLMs for detecting DGAs and DNS\nexfiltration attacks. We developed LLM models and conducted comprehensive\nevaluation using a diverse dataset comprising 59 distinct real-world DGA\nmalware families and normal domain data. Our LLM model significantly\noutperformed traditional natural language processing techniques, especially in\ndetecting unknown DGAs. We also evaluated its performance on DNS exfiltration\ndatasets, demonstrating its effectiveness in enhancing cybersecurity measures.\nTo the best of our knowledge, this is the first work that empirically applies\nLLMs for DGA and DNS exfiltration detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain Generation Algorithms (DGAs) are malicious techniques used by malware\nto dynamically generate seemingly random domain names for communication with\nCommand & Control (C&C) servers. Due to the fast and simple generation of DGA\ndomains, detection methods must be highly efficient and precise to be\neffective. Large Language Models (LLMs) have demonstrated their proficiency in\nreal-time detection tasks, making them ideal candidates for detecting DGAs. Our\nwork validates the effectiveness of fine-tuned LLMs for detecting DGAs and DNS\nexfiltration attacks. We developed LLM models and conducted comprehensive\nevaluation using a diverse dataset comprising 59 distinct real-world DGA\nmalware families and normal domain data. Our LLM model significantly\noutperformed traditional natural language processing techniques, especially in\ndetecting unknown DGAs. We also evaluated its performance on DNS exfiltration\ndatasets, demonstrating its effectiveness in enhancing cybersecurity measures.\nTo the best of our knowledge, this is the first work that empirically applies\nLLMs for DGA and DNS exfiltration detection."
                },
                "authors": [
                    {
                        "name": "Md Abu Sayed"
                    },
                    {
                        "name": "Asif Rahman"
                    },
                    {
                        "name": "Christopher Kiekintveld"
                    },
                    {
                        "name": "Sebastian Garcia"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Garcia"
                },
                "author": "Sebastian Garcia",
                "arxiv_comment": "Accepted in Proceedings of the Workshop at AI for Cyber Threat\n  Intelligence (WAITI), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21723v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21723v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04991v1",
                "updated": "2024-11-07T18:57:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    57,
                    3,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T18:57:03Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    57,
                    3,
                    3,
                    312,
                    0
                ],
                "title": "Rethinking Bradley-Terry Models in Preference-Based Reward Modeling:\n  Foundations, Theory, and Alternatives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Bradley-Terry Models in Preference-Based Reward Modeling:\n  Foundations, Theory, and Alternatives"
                },
                "summary": "The Bradley-Terry (BT) model is a common and successful practice in reward\nmodeling for Large Language Model (LLM) alignment. However, it remains unclear\nwhy this model -- originally developed for multi-player stochastic game\nmatching -- can be adopted to convert pairwise response comparisons to reward\nvalues and make predictions. Especially given the fact that only a limited\nnumber of prompt-response pairs are sparsely compared with others. In this\npaper, we first revisit the foundations of using BT models in reward modeling,\nand establish the convergence rate of BT reward models based on deep neural\nnetworks using embeddings, providing a theoretical foundation for their use.\nDespite theoretically sound, we argue that the BT model is not a necessary\nchoice from the perspective of downstream optimization. This is because a\nreward model only needs to preserve the correct ranking predictions through a\nmonotonic transformation of the true reward. We highlight the critical concept\nof order consistency in reward modeling and demonstrate that the BT model\npossesses this property. Consequently, we propose a simple and straightforward\nupper-bound algorithm, compatible with off-the-shelf binary classifiers, as an\nalternative order-consistent reward modeling objective. To offer practical\ninsights, we empirically evaluate the performance of these different reward\nmodeling approaches across more than 12,000 experimental setups, using $6$ base\nLLMs, $2$ datasets, and diverse annotation designs that vary in quantity,\nquality, and pairing choices in preference annotations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bradley-Terry (BT) model is a common and successful practice in reward\nmodeling for Large Language Model (LLM) alignment. However, it remains unclear\nwhy this model -- originally developed for multi-player stochastic game\nmatching -- can be adopted to convert pairwise response comparisons to reward\nvalues and make predictions. Especially given the fact that only a limited\nnumber of prompt-response pairs are sparsely compared with others. In this\npaper, we first revisit the foundations of using BT models in reward modeling,\nand establish the convergence rate of BT reward models based on deep neural\nnetworks using embeddings, providing a theoretical foundation for their use.\nDespite theoretically sound, we argue that the BT model is not a necessary\nchoice from the perspective of downstream optimization. This is because a\nreward model only needs to preserve the correct ranking predictions through a\nmonotonic transformation of the true reward. We highlight the critical concept\nof order consistency in reward modeling and demonstrate that the BT model\npossesses this property. Consequently, we propose a simple and straightforward\nupper-bound algorithm, compatible with off-the-shelf binary classifiers, as an\nalternative order-consistent reward modeling objective. To offer practical\ninsights, we empirically evaluate the performance of these different reward\nmodeling approaches across more than 12,000 experimental setups, using $6$ base\nLLMs, $2$ datasets, and diverse annotation designs that vary in quantity,\nquality, and pairing choices in preference annotations."
                },
                "authors": [
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Yunyi Shen"
                    },
                    {
                        "name": "Jean-Francois Ton"
                    }
                ],
                "author_detail": {
                    "name": "Jean-Francois Ton"
                },
                "author": "Jean-Francois Ton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04981v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04981v1",
                "updated": "2024-11-07T18:54:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    54,
                    31,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T18:54:31Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    54,
                    31,
                    3,
                    312,
                    0
                ],
                "title": "Enhancing Reverse Engineering: Investigating and Benchmarking Large\n  Language Models for Vulnerability Analysis in Decompiled Binaries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Reverse Engineering: Investigating and Benchmarking Large\n  Language Models for Vulnerability Analysis in Decompiled Binaries"
                },
                "summary": "Security experts reverse engineer (decompile) binary code to identify\ncritical security vulnerabilities. The limited access to source code in vital\nsystems - such as firmware, drivers, and proprietary software used in Critical\nInfrastructures (CI) - makes this analysis even more crucial on the binary\nlevel. Even with available source code, a semantic gap persists after\ncompilation between the source and the binary code executed by the processor.\nThis gap may hinder the detection of vulnerabilities in source code. That being\nsaid, current research on Large Language Models (LLMs) overlooks the\nsignificance of decompiled binaries in this area by focusing solely on source\ncode. In this work, we are the first to empirically uncover the substantial\nsemantic limitations of state-of-the-art LLMs when it comes to analyzing\nvulnerabilities in decompiled binaries, largely due to the absence of relevant\ndatasets. To bridge the gap, we introduce DeBinVul, a novel decompiled binary\ncode vulnerability dataset. Our dataset is multi-architecture and\nmulti-optimization, focusing on C/C++ due to their wide usage in CI and\nassociation with numerous vulnerabilities. Specifically, we curate 150,872\nsamples of vulnerable and non-vulnerable decompiled binary code for the task of\n(i) identifying; (ii) classifying; (iii) describing vulnerabilities; and (iv)\nrecovering function names in the domain of decompiled binaries. Subsequently,\nwe fine-tune state-of-the-art LLMs using DeBinVul and report on a performance\nincrease of 19%, 24%, and 21% in the capabilities of CodeLlama, Llama3, and\nCodeGen2 respectively, in detecting binary code vulnerabilities. Additionally,\nusing DeBinVul, we report a high performance of 80-90% on the vulnerability\nclassification task. Furthermore, we report improved performance in function\nname recovery and vulnerability description tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Security experts reverse engineer (decompile) binary code to identify\ncritical security vulnerabilities. The limited access to source code in vital\nsystems - such as firmware, drivers, and proprietary software used in Critical\nInfrastructures (CI) - makes this analysis even more crucial on the binary\nlevel. Even with available source code, a semantic gap persists after\ncompilation between the source and the binary code executed by the processor.\nThis gap may hinder the detection of vulnerabilities in source code. That being\nsaid, current research on Large Language Models (LLMs) overlooks the\nsignificance of decompiled binaries in this area by focusing solely on source\ncode. In this work, we are the first to empirically uncover the substantial\nsemantic limitations of state-of-the-art LLMs when it comes to analyzing\nvulnerabilities in decompiled binaries, largely due to the absence of relevant\ndatasets. To bridge the gap, we introduce DeBinVul, a novel decompiled binary\ncode vulnerability dataset. Our dataset is multi-architecture and\nmulti-optimization, focusing on C/C++ due to their wide usage in CI and\nassociation with numerous vulnerabilities. Specifically, we curate 150,872\nsamples of vulnerable and non-vulnerable decompiled binary code for the task of\n(i) identifying; (ii) classifying; (iii) describing vulnerabilities; and (iv)\nrecovering function names in the domain of decompiled binaries. Subsequently,\nwe fine-tune state-of-the-art LLMs using DeBinVul and report on a performance\nincrease of 19%, 24%, and 21% in the capabilities of CodeLlama, Llama3, and\nCodeGen2 respectively, in detecting binary code vulnerabilities. Additionally,\nusing DeBinVul, we report a high performance of 80-90% on the vulnerability\nclassification task. Furthermore, we report improved performance in function\nname recovery and vulnerability description tasks."
                },
                "authors": [
                    {
                        "name": "Dylan Manuel"
                    },
                    {
                        "name": "Nafis Tanveer Islam"
                    },
                    {
                        "name": "Joseph Khoury"
                    },
                    {
                        "name": "Ana Nunez"
                    },
                    {
                        "name": "Elias Bou-Harb"
                    },
                    {
                        "name": "Peyman Najafirad"
                    }
                ],
                "author_detail": {
                    "name": "Peyman Najafirad"
                },
                "author": "Peyman Najafirad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04981v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04981v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04975v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04975v1",
                "updated": "2024-11-07T18:49:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    49,
                    33,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T18:49:33Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    49,
                    33,
                    3,
                    312,
                    0
                ],
                "title": "SuffixDecoding: A Model-Free Approach to Speeding Up Large Language\n  Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuffixDecoding: A Model-Free Approach to Speeding Up Large Language\n  Model Inference"
                },
                "summary": "We present SuffixDecoding, a novel model-free approach to accelerating large\nlanguage model (LLM) inference through speculative decoding. Unlike existing\nmethods that rely on draft models or specialized decoding heads, SuffixDecoding\nleverages suffix trees built from previously generated outputs to efficiently\npredict candidate token sequences. Our approach enables flexible\ntree-structured speculation without the overhead of maintaining and\norchestrating additional models. SuffixDecoding builds and dynamically updates\nsuffix trees to capture patterns in the generated text, using them to construct\nspeculation trees through a principled scoring mechanism based on empirical\ntoken frequencies. SuffixDecoding requires only CPU memory which is plentiful\nand underutilized on typical LLM serving nodes. We demonstrate that\nSuffixDecoding achieves competitive speedups compared to model-based approaches\nacross diverse workloads including open-domain chat, code generation, and\ntext-to-SQL tasks. For open-ended chat and code generation tasks,\nSuffixDecoding achieves up to $1.4\\times$ higher output throughput than\nSpecInfer and up to $1.1\\times$ lower time-per-token (TPOT) latency. For a\nproprietary multi-LLM text-to-SQL application, SuffixDecoding achieves up to\n$2.9\\times$ higher output throughput and $3\\times$ lower latency than\nspeculative decoding. Our evaluation shows that SuffixDecoding maintains high\nacceptance rates even with small reference corpora of 256 examples, while\ncontinuing to improve performance as more historical outputs are incorporated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present SuffixDecoding, a novel model-free approach to accelerating large\nlanguage model (LLM) inference through speculative decoding. Unlike existing\nmethods that rely on draft models or specialized decoding heads, SuffixDecoding\nleverages suffix trees built from previously generated outputs to efficiently\npredict candidate token sequences. Our approach enables flexible\ntree-structured speculation without the overhead of maintaining and\norchestrating additional models. SuffixDecoding builds and dynamically updates\nsuffix trees to capture patterns in the generated text, using them to construct\nspeculation trees through a principled scoring mechanism based on empirical\ntoken frequencies. SuffixDecoding requires only CPU memory which is plentiful\nand underutilized on typical LLM serving nodes. We demonstrate that\nSuffixDecoding achieves competitive speedups compared to model-based approaches\nacross diverse workloads including open-domain chat, code generation, and\ntext-to-SQL tasks. For open-ended chat and code generation tasks,\nSuffixDecoding achieves up to $1.4\\times$ higher output throughput than\nSpecInfer and up to $1.1\\times$ lower time-per-token (TPOT) latency. For a\nproprietary multi-LLM text-to-SQL application, SuffixDecoding achieves up to\n$2.9\\times$ higher output throughput and $3\\times$ lower latency than\nspeculative decoding. Our evaluation shows that SuffixDecoding maintains high\nacceptance rates even with small reference corpora of 256 examples, while\ncontinuing to improve performance as more historical outputs are incorporated."
                },
                "authors": [
                    {
                        "name": "Gabriele Oliaro"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Daniel Campos"
                    },
                    {
                        "name": "Aurick Qiao"
                    }
                ],
                "author_detail": {
                    "name": "Aurick Qiao"
                },
                "author": "Aurick Qiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04975v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04965v1",
                "updated": "2024-11-07T18:41:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    41,
                    50,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T18:41:50Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    41,
                    50,
                    3,
                    312,
                    0
                ],
                "title": "BitNet a4.8: 4-bit Activations for 1-bit LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitNet a4.8: 4-bit Activations for 1-bit LLMs"
                },
                "summary": "Recent research on the 1-bit Large Language Models (LLMs), such as BitNet\nb1.58, presents a promising direction for reducing the inference cost of LLMs\nwhile maintaining their performance. In this work, we introduce BitNet a4.8,\nenabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid\nquantization and sparsification strategy to mitigate the quantization errors\nintroduced by the outlier channels. Specifically, we utilize 4-bit activations\nfor inputs to the attention and feed-forward network layers, while sparsifying\nintermediate states followed with 8-bit quantization. Extensive experiments\ndemonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58\nwith equivalent training costs, while being faster in inference with enabling\n4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of\nparameters and supports 3-bit KV cache, further enhancing the efficiency of\nlarge-scale LLM deployment and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research on the 1-bit Large Language Models (LLMs), such as BitNet\nb1.58, presents a promising direction for reducing the inference cost of LLMs\nwhile maintaining their performance. In this work, we introduce BitNet a4.8,\nenabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid\nquantization and sparsification strategy to mitigate the quantization errors\nintroduced by the outlier channels. Specifically, we utilize 4-bit activations\nfor inputs to the attention and feed-forward network layers, while sparsifying\nintermediate states followed with 8-bit quantization. Extensive experiments\ndemonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58\nwith equivalent training costs, while being faster in inference with enabling\n4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of\nparameters and supports 3-bit KV cache, further enhancing the efficiency of\nlarge-scale LLM deployment and inference."
                },
                "authors": [
                    {
                        "name": "Hongyu Wang"
                    },
                    {
                        "name": "Shuming Ma"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04962v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04962v1",
                "updated": "2024-11-07T18:39:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    39,
                    4,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T18:39:04Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    39,
                    4,
                    3,
                    312,
                    0
                ],
                "title": "Position Paper On Diagnostic Uncertainty Estimation from Large Language\n  Models: Next-Word Probability Is Not Pre-test Probability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Position Paper On Diagnostic Uncertainty Estimation from Large Language\n  Models: Next-Word Probability Is Not Pre-test Probability"
                },
                "summary": "Large language models (LLMs) are being explored for diagnostic decision\nsupport, yet their ability to estimate pre-test probabilities, vital for\nclinical decision-making, remains limited. This study evaluates two LLMs,\nMistral-7B and Llama3-70B, using structured electronic health record data on\nthree diagnosis tasks. We examined three current methods of extracting LLM\nprobability estimations and revealed their limitations. We aim to highlight the\nneed for improved techniques in LLM confidence estimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are being explored for diagnostic decision\nsupport, yet their ability to estimate pre-test probabilities, vital for\nclinical decision-making, remains limited. This study evaluates two LLMs,\nMistral-7B and Llama3-70B, using structured electronic health record data on\nthree diagnosis tasks. We examined three current methods of extracting LLM\nprobability estimations and revealed their limitations. We aim to highlight the\nneed for improved techniques in LLM confidence estimation."
                },
                "authors": [
                    {
                        "name": "Yanjun Gao"
                    },
                    {
                        "name": "Skatje Myers"
                    },
                    {
                        "name": "Shan Chen"
                    },
                    {
                        "name": "Dmitriy Dligach"
                    },
                    {
                        "name": "Timothy A Miller"
                    },
                    {
                        "name": "Danielle Bitterman"
                    },
                    {
                        "name": "Guanhua Chen"
                    },
                    {
                        "name": "Anoop Mayampurath"
                    },
                    {
                        "name": "Matthew Churpek"
                    },
                    {
                        "name": "Majid Afshar"
                    }
                ],
                "author_detail": {
                    "name": "Majid Afshar"
                },
                "author": "Majid Afshar",
                "arxiv_comment": "Accepted to GenAI4Health Workshop at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04962v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04962v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04954v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04954v1",
                "updated": "2024-11-07T18:31:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    31,
                    8,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T18:31:08Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    31,
                    8,
                    3,
                    312,
                    0
                ],
                "title": "CAD-MLLM: Unifying Multimodality-Conditioned CAD Generation With MLLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAD-MLLM: Unifying Multimodality-Conditioned CAD Generation With MLLM"
                },
                "summary": "This paper aims to design a unified Computer-Aided Design (CAD) generation\nsystem that can easily generate CAD models based on the user's inputs in the\nform of textual description, images, point clouds, or even a combination of\nthem. Towards this goal, we introduce the CAD-MLLM, the first system capable of\ngenerating parametric CAD models conditioned on the multimodal input.\nSpecifically, within the CAD-MLLM framework, we leverage the command sequences\nof CAD models and then employ advanced large language models (LLMs) to align\nthe feature space across these diverse multi-modalities data and CAD models'\nvectorized representations. To facilitate the model training, we design a\ncomprehensive data construction and annotation pipeline that equips each CAD\nmodel with corresponding multimodal data. Our resulting dataset, named\nOmni-CAD, is the first multimodal CAD dataset that contains textual\ndescription, multi-view images, points, and command sequence for each CAD\nmodel. It contains approximately 450K instances and their CAD construction\nsequences. To thoroughly evaluate the quality of our generated CAD models, we\ngo beyond current evaluation metrics that focus on reconstruction quality by\nintroducing additional metrics that assess topology quality and surface\nenclosure extent. Extensive experimental results demonstrate that CAD-MLLM\nsignificantly outperforms existing conditional generative methods and remains\nhighly robust to noises and missing points. The project page and more\nvisualizations can be found at: https://cad-mllm.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper aims to design a unified Computer-Aided Design (CAD) generation\nsystem that can easily generate CAD models based on the user's inputs in the\nform of textual description, images, point clouds, or even a combination of\nthem. Towards this goal, we introduce the CAD-MLLM, the first system capable of\ngenerating parametric CAD models conditioned on the multimodal input.\nSpecifically, within the CAD-MLLM framework, we leverage the command sequences\nof CAD models and then employ advanced large language models (LLMs) to align\nthe feature space across these diverse multi-modalities data and CAD models'\nvectorized representations. To facilitate the model training, we design a\ncomprehensive data construction and annotation pipeline that equips each CAD\nmodel with corresponding multimodal data. Our resulting dataset, named\nOmni-CAD, is the first multimodal CAD dataset that contains textual\ndescription, multi-view images, points, and command sequence for each CAD\nmodel. It contains approximately 450K instances and their CAD construction\nsequences. To thoroughly evaluate the quality of our generated CAD models, we\ngo beyond current evaluation metrics that focus on reconstruction quality by\nintroducing additional metrics that assess topology quality and surface\nenclosure extent. Extensive experimental results demonstrate that CAD-MLLM\nsignificantly outperforms existing conditional generative methods and remains\nhighly robust to noises and missing points. The project page and more\nvisualizations can be found at: https://cad-mllm.github.io/"
                },
                "authors": [
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Chenyu Wang"
                    },
                    {
                        "name": "Zibo Zhao"
                    },
                    {
                        "name": "Wen Liu"
                    },
                    {
                        "name": "Yi Ma"
                    },
                    {
                        "name": "Shenghua Gao"
                    }
                ],
                "author_detail": {
                    "name": "Shenghua Gao"
                },
                "author": "Shenghua Gao",
                "arxiv_comment": "Project page: https://cad-mllm.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04954v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02472v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02472v3",
                "updated": "2024-11-07T18:30:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    30,
                    38,
                    3,
                    312,
                    0
                ],
                "published": "2024-10-03T13:25:15Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    13,
                    25,
                    15,
                    3,
                    277,
                    0
                ],
                "title": "Meta-Models: An Architecture for Decoding LLM Behaviors Through\n  Interpreted Embeddings and Natural Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meta-Models: An Architecture for Decoding LLM Behaviors Through\n  Interpreted Embeddings and Natural Language"
                },
                "summary": "As Large Language Models (LLMs) become increasingly integrated into our daily\nlives, the potential harms from deceptive behavior underlie the need for\nfaithfully interpreting their decision-making. While traditional probing\nmethods have shown some effectiveness, they remain best for narrowly scoped\ntasks while more comprehensive explanations are still necessary. To this end,\nwe investigate meta-models-an architecture using a \"meta-model\" that takes\nactivations from an \"input-model\" and answers natural language questions about\nthe input-model's behaviors. We evaluate the meta-model's ability to generalize\nby training them on selected task types and assessing their out-of-distribution\nperformance in deceptive scenarios. Our findings show that meta-models\ngeneralize well to out-of-distribution tasks and point towards opportunities\nfor future research in this area. Our code is available at\nhttps://github.com/acostarelli/meta-models-public .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) become increasingly integrated into our daily\nlives, the potential harms from deceptive behavior underlie the need for\nfaithfully interpreting their decision-making. While traditional probing\nmethods have shown some effectiveness, they remain best for narrowly scoped\ntasks while more comprehensive explanations are still necessary. To this end,\nwe investigate meta-models-an architecture using a \"meta-model\" that takes\nactivations from an \"input-model\" and answers natural language questions about\nthe input-model's behaviors. We evaluate the meta-model's ability to generalize\nby training them on selected task types and assessing their out-of-distribution\nperformance in deceptive scenarios. Our findings show that meta-models\ngeneralize well to out-of-distribution tasks and point towards opportunities\nfor future research in this area. Our code is available at\nhttps://github.com/acostarelli/meta-models-public ."
                },
                "authors": [
                    {
                        "name": "Anthony Costarelli"
                    },
                    {
                        "name": "Mat Allen"
                    },
                    {
                        "name": "Severin Field"
                    }
                ],
                "author_detail": {
                    "name": "Severin Field"
                },
                "author": "Severin Field",
                "arxiv_comment": "11 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02472v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02472v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14894v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14894v2",
                "updated": "2024-11-07T18:15:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    15,
                    23,
                    3,
                    312,
                    0
                ],
                "published": "2024-06-21T06:30:16Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    6,
                    30,
                    16,
                    4,
                    173,
                    0
                ],
                "title": "Talking the Talk Does Not Entail Walking the Walk: On the Limits of\n  Large Language Models in Lexical Entailment Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Talking the Talk Does Not Entail Walking the Walk: On the Limits of\n  Large Language Models in Lexical Entailment Recognition"
                },
                "summary": "Verbs form the backbone of language, providing the structure and meaning to\nsentences. Yet, their intricate semantic nuances pose a longstanding challenge.\nUnderstanding verb relations through the concept of lexical entailment is\ncrucial for comprehending sentence meanings and grasping verb dynamics. This\nwork investigates the capabilities of eight Large Language Models in\nrecognizing lexical entailment relations among verbs through differently\ndevised prompting strategies and zero-/few-shot settings over verb pairs from\ntwo lexical databases, namely WordNet and HyperLex. Our findings unveil that\nthe models can tackle the lexical entailment recognition task with moderately\ngood performance, although at varying degree of effectiveness and under\ndifferent conditions. Also, utilizing few-shot prompting can enhance the\nmodels' performance. However, perfectly solving the task arises as an unmet\nchallenge for all examined LLMs, which raises an emergence for further research\ndevelopments on this topic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verbs form the backbone of language, providing the structure and meaning to\nsentences. Yet, their intricate semantic nuances pose a longstanding challenge.\nUnderstanding verb relations through the concept of lexical entailment is\ncrucial for comprehending sentence meanings and grasping verb dynamics. This\nwork investigates the capabilities of eight Large Language Models in\nrecognizing lexical entailment relations among verbs through differently\ndevised prompting strategies and zero-/few-shot settings over verb pairs from\ntwo lexical databases, namely WordNet and HyperLex. Our findings unveil that\nthe models can tackle the lexical entailment recognition task with moderately\ngood performance, although at varying degree of effectiveness and under\ndifferent conditions. Also, utilizing few-shot prompting can enhance the\nmodels' performance. However, perfectly solving the task arises as an unmet\nchallenge for all examined LLMs, which raises an emergence for further research\ndevelopments on this topic."
                },
                "authors": [
                    {
                        "name": "Candida M. Greco"
                    },
                    {
                        "name": "Lucio La Cava"
                    },
                    {
                        "name": "Andrea Tagarelli"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Tagarelli"
                },
                "author": "Andrea Tagarelli",
                "arxiv_comment": "Accepted for publication at The 2024 Conference on Empirical Methods\n  in Natural Language Processing (EMNLP-2024) - Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14894v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14894v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04920v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04920v1",
                "updated": "2024-11-07T17:57:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    17,
                    57,
                    3,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T17:57:03Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    17,
                    57,
                    3,
                    3,
                    312,
                    0
                ],
                "title": "GPTKB: Building Very Large Knowledge Bases from Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPTKB: Building Very Large Knowledge Bases from Language Models"
                },
                "summary": "General-domain knowledge bases (KB), in particular the \"big three\" --\nWikidata, Yago and DBpedia -- are the backbone of many intelligent\napplications. While these three have seen steady development, comprehensive KB\nconstruction at large has seen few fresh attempts. In this work, we propose to\nbuild a large general-domain KB entirely from a large language model (LLM). We\ndemonstrate the feasibility of large-scale KB construction from LLMs, while\nhighlighting specific challenges arising around entity recognition, entity and\nproperty canonicalization, and taxonomy construction. As a prototype, we use\nGPT-4o-mini to construct GPTKB, which contains 105 million triples for more\nthan 2.9 million entities, at a cost 100x less than previous KBC projects. Our\nwork is a landmark for two fields: For NLP, for the first time, it provides\n\\textit{constructive} insights into the knowledge (or beliefs) of LLMs. For the\nSemantic Web, it shows novel ways forward for the long-standing challenge of\ngeneral-domain KB construction. GPTKB is accessible at https://gptkb.org.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General-domain knowledge bases (KB), in particular the \"big three\" --\nWikidata, Yago and DBpedia -- are the backbone of many intelligent\napplications. While these three have seen steady development, comprehensive KB\nconstruction at large has seen few fresh attempts. In this work, we propose to\nbuild a large general-domain KB entirely from a large language model (LLM). We\ndemonstrate the feasibility of large-scale KB construction from LLMs, while\nhighlighting specific challenges arising around entity recognition, entity and\nproperty canonicalization, and taxonomy construction. As a prototype, we use\nGPT-4o-mini to construct GPTKB, which contains 105 million triples for more\nthan 2.9 million entities, at a cost 100x less than previous KBC projects. Our\nwork is a landmark for two fields: For NLP, for the first time, it provides\n\\textit{constructive} insights into the knowledge (or beliefs) of LLMs. For the\nSemantic Web, it shows novel ways forward for the long-standing challenge of\ngeneral-domain KB construction. GPTKB is accessible at https://gptkb.org."
                },
                "authors": [
                    {
                        "name": "Yujia Hu"
                    },
                    {
                        "name": "Shrestha Ghosh"
                    },
                    {
                        "name": "Tuan-Phong Nugyen"
                    },
                    {
                        "name": "Simon Razniewski"
                    }
                ],
                "author_detail": {
                    "name": "Simon Razniewski"
                },
                "author": "Simon Razniewski",
                "arxiv_comment": "11 pages, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04920v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04920v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04905v1",
                "updated": "2024-11-07T17:47:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    17,
                    47,
                    25,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T17:47:25Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    17,
                    47,
                    25,
                    3,
                    312,
                    0
                ],
                "title": "OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models"
                },
                "summary": "Large language models (LLMs) for code have become indispensable in various\ndomains, including code generation, reasoning tasks and agent systems.While\nopen-access code LLMs are increasingly approaching the performance levels of\nproprietary models, high-quality code LLMs suitable for rigorous scientific\ninvestigation, particularly those with reproducible data processing pipelines\nand transparent training protocols, remain limited. The scarcity is due to\nvarious challenges, including resource constraints, ethical considerations, and\nthe competitive advantages of keeping models advanced. To address the gap, we\nintroduce OpenCoder, a top-tier code LLM that not only achieves performance\ncomparable to leading models but also serves as an ``open cookbook'' for the\nresearch community. Unlike most prior efforts, we release not only model\nweights and inference code, but also the reproducible training data, complete\ndata processing pipeline, rigorous experimental ablation results, and detailed\ntraining protocols for open scientific research. Through this comprehensive\nrelease, we identify the key ingredients for building a top-tier code LLM: (1)\ncode optimized heuristic rules for data cleaning and methods for data\ndeduplication, (2) recall of text corpus related to code and (3) high-quality\nsynthetic data in both annealing and supervised fine-tuning stages. By offering\nthis level of openness, we aim to broaden access to all aspects of a top-tier\ncode LLM, with OpenCoder serving as both a powerful model and an open\nfoundation to accelerate research, and enable reproducible advancements in code\nAI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) for code have become indispensable in various\ndomains, including code generation, reasoning tasks and agent systems.While\nopen-access code LLMs are increasingly approaching the performance levels of\nproprietary models, high-quality code LLMs suitable for rigorous scientific\ninvestigation, particularly those with reproducible data processing pipelines\nand transparent training protocols, remain limited. The scarcity is due to\nvarious challenges, including resource constraints, ethical considerations, and\nthe competitive advantages of keeping models advanced. To address the gap, we\nintroduce OpenCoder, a top-tier code LLM that not only achieves performance\ncomparable to leading models but also serves as an ``open cookbook'' for the\nresearch community. Unlike most prior efforts, we release not only model\nweights and inference code, but also the reproducible training data, complete\ndata processing pipeline, rigorous experimental ablation results, and detailed\ntraining protocols for open scientific research. Through this comprehensive\nrelease, we identify the key ingredients for building a top-tier code LLM: (1)\ncode optimized heuristic rules for data cleaning and methods for data\ndeduplication, (2) recall of text corpus related to code and (3) high-quality\nsynthetic data in both annealing and supervised fine-tuning stages. By offering\nthis level of openness, we aim to broaden access to all aspects of a top-tier\ncode LLM, with OpenCoder serving as both a powerful model and an open\nfoundation to accelerate research, and enable reproducible advancements in code\nAI."
                },
                "authors": [
                    {
                        "name": "Siming Huang"
                    },
                    {
                        "name": "Tianhao Cheng"
                    },
                    {
                        "name": "Jason Klein Liu"
                    },
                    {
                        "name": "Jiaran Hao"
                    },
                    {
                        "name": "Liuyihan Song"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "J. Yang"
                    },
                    {
                        "name": "J. H. Liu"
                    },
                    {
                        "name": "Chenchen Zhang"
                    },
                    {
                        "name": "Linzheng Chai"
                    },
                    {
                        "name": "Ruifeng Yuan"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    },
                    {
                        "name": "Jie Fu"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Yuan Qi"
                    },
                    {
                        "name": "Yinghui Xu"
                    },
                    {
                        "name": "Wei Chu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Chu"
                },
                "author": "Wei Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06754v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06754v3",
                "updated": "2024-11-07T17:46:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    17,
                    46,
                    23,
                    3,
                    312,
                    0
                ],
                "published": "2024-09-10T16:05:02Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    16,
                    5,
                    2,
                    1,
                    254,
                    0
                ],
                "title": "Scaling Law Hypothesis for Multimodal Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Law Hypothesis for Multimodal Model"
                },
                "summary": "We propose a scaling law hypothesis for multimodal models processing text,\naudio, images, and video within a shared token and embedding space. Our\nframework predicts model performance based on modality-specific compression and\ntokenization efficiency, extending established scaling laws from text-based\ndecoder models to mixed-modality systems. We explore whether leveraging more\ntraining data in multiple modalities can reduce the size of the multimodal\nmodel, enabling efficient deployment on resource-constrained devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a scaling law hypothesis for multimodal models processing text,\naudio, images, and video within a shared token and embedding space. Our\nframework predicts model performance based on modality-specific compression and\ntokenization efficiency, extending established scaling laws from text-based\ndecoder models to mixed-modality systems. We explore whether leveraging more\ntraining data in multiple modalities can reduce the size of the multimodal\nmodel, enabling efficient deployment on resource-constrained devices."
                },
                "authors": [
                    {
                        "name": "Qingyun Sun"
                    },
                    {
                        "name": "Zhen Guo"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Guo"
                },
                "author": "Zhen Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06754v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06754v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04890v1",
                "updated": "2024-11-07T17:28:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    17,
                    28,
                    10,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T17:28:10Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    17,
                    28,
                    10,
                    3,
                    312,
                    0
                ],
                "title": "GUI Agents with Foundation Models: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUI Agents with Foundation Models: A Comprehensive Survey"
                },
                "summary": "Recent advances in foundation models, particularly Large Language Models\n(LLMs) and Multimodal Large Language Models (MLLMs), facilitate intelligent\nagents being capable of performing complex tasks. By leveraging the ability of\n(M)LLMs to process and interpret Graphical User Interfaces (GUIs), these agents\ncan autonomously execute user instructions by simulating human-like\ninteractions such as clicking and typing. This survey consolidates recent\nresearch on (M)LLM-based GUI agents, highlighting key innovations in data,\nframeworks, and applications. We begin by discussing representative datasets\nand benchmarks. Next, we summarize a unified framework that captures the\nessential components used in prior research, accompanied by a taxonomy.\nAdditionally, we explore commercial applications of (M)LLM-based GUI agents.\nDrawing from existing work, we identify several key challenges and propose\nfuture research directions. We hope this paper will inspire further\ndevelopments in the field of (M)LLM-based GUI agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in foundation models, particularly Large Language Models\n(LLMs) and Multimodal Large Language Models (MLLMs), facilitate intelligent\nagents being capable of performing complex tasks. By leveraging the ability of\n(M)LLMs to process and interpret Graphical User Interfaces (GUIs), these agents\ncan autonomously execute user instructions by simulating human-like\ninteractions such as clicking and typing. This survey consolidates recent\nresearch on (M)LLM-based GUI agents, highlighting key innovations in data,\nframeworks, and applications. We begin by discussing representative datasets\nand benchmarks. Next, we summarize a unified framework that captures the\nessential components used in prior research, accompanied by a taxonomy.\nAdditionally, we explore commercial applications of (M)LLM-based GUI agents.\nDrawing from existing work, we identify several key challenges and propose\nfuture research directions. We hope this paper will inspire further\ndevelopments in the field of (M)LLM-based GUI agents."
                },
                "authors": [
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Jingxuan Chen"
                    },
                    {
                        "name": "Weinan Gan"
                    },
                    {
                        "name": "Xingshan Zeng"
                    },
                    {
                        "name": "Shuai Yu"
                    },
                    {
                        "name": "Xinlong Hao"
                    },
                    {
                        "name": "Kun Shao"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Ruiming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Ruiming Tang"
                },
                "author": "Ruiming Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04867v1",
                "updated": "2024-11-07T16:59:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    16,
                    59,
                    32,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T16:59:32Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    16,
                    59,
                    32,
                    3,
                    312,
                    0
                ],
                "title": "Think Smart, Act SMARL! Analyzing Probabilistic Logic Driven Safety in\n  Multi-Agent Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think Smart, Act SMARL! Analyzing Probabilistic Logic Driven Safety in\n  Multi-Agent Reinforcement Learning"
                },
                "summary": "An important challenge for enabling the deployment of reinforcement learning\n(RL) algorithms in the real world is safety. This has resulted in the recent\nresearch field of Safe RL, which aims to learn optimal policies that are safe.\nOne successful approach in that direction is probabilistic logic shields (PLS),\na model-based Safe RL technique that uses formal specifications based on\nprobabilistic logic programming, constraining an agent's policy to comply with\nthose specifications in a probabilistic sense. However, safety is inherently a\nmulti-agent concept, since real-world environments often involve multiple\nagents interacting simultaneously, leading to a complex system which is hard to\ncontrol. Moreover, safe multi-agent RL (Safe MARL) is still underexplored. In\norder to address this gap, in this paper we ($i$) introduce Shielded MARL\n(SMARL) by extending PLS to MARL -- in particular, we introduce Probabilistic\nLogic Temporal Difference Learning (PLTD) to enable shielded independent\nQ-learning (SIQL), and introduce shielded independent PPO (SIPPO) using\nprobabilistic logic policy gradients; ($ii$) show its positive effect and use\nas an equilibrium selection mechanism in various game-theoretic environments\nincluding two-player simultaneous games, extensive-form games, stochastic\ngames, and some grid-world extensions in terms of safety, cooperation, and\nalignment with normative behaviors; and ($iii$) look into the asymmetric case\nwhere only one agent is shielded, and show that the shielded agent has a\nsignificant influence on the unshielded one, providing further evidence of\nSMARL's ability to enhance safety and cooperation in diverse multi-agent\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An important challenge for enabling the deployment of reinforcement learning\n(RL) algorithms in the real world is safety. This has resulted in the recent\nresearch field of Safe RL, which aims to learn optimal policies that are safe.\nOne successful approach in that direction is probabilistic logic shields (PLS),\na model-based Safe RL technique that uses formal specifications based on\nprobabilistic logic programming, constraining an agent's policy to comply with\nthose specifications in a probabilistic sense. However, safety is inherently a\nmulti-agent concept, since real-world environments often involve multiple\nagents interacting simultaneously, leading to a complex system which is hard to\ncontrol. Moreover, safe multi-agent RL (Safe MARL) is still underexplored. In\norder to address this gap, in this paper we ($i$) introduce Shielded MARL\n(SMARL) by extending PLS to MARL -- in particular, we introduce Probabilistic\nLogic Temporal Difference Learning (PLTD) to enable shielded independent\nQ-learning (SIQL), and introduce shielded independent PPO (SIPPO) using\nprobabilistic logic policy gradients; ($ii$) show its positive effect and use\nas an equilibrium selection mechanism in various game-theoretic environments\nincluding two-player simultaneous games, extensive-form games, stochastic\ngames, and some grid-world extensions in terms of safety, cooperation, and\nalignment with normative behaviors; and ($iii$) look into the asymmetric case\nwhere only one agent is shielded, and show that the shielded agent has a\nsignificant influence on the unshielded one, providing further evidence of\nSMARL's ability to enhance safety and cooperation in diverse multi-agent\nenvironments."
                },
                "authors": [
                    {
                        "name": "Satchit Chatterji"
                    },
                    {
                        "name": "Erman Acar"
                    }
                ],
                "author_detail": {
                    "name": "Erman Acar"
                },
                "author": "Erman Acar",
                "arxiv_comment": "19 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13835v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13835v2",
                "updated": "2024-11-07T16:57:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    16,
                    57,
                    2,
                    3,
                    312,
                    0
                ],
                "published": "2024-10-17T17:54:06Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    54,
                    6,
                    3,
                    291,
                    0
                ],
                "title": "Active-Dormant Attention Heads: Mechanistically Demystifying\n  Extreme-Token Phenomena in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active-Dormant Attention Heads: Mechanistically Demystifying\n  Extreme-Token Phenomena in LLMs"
                },
                "summary": "Practitioners have consistently observed three puzzling phenomena in\ntransformer-based large language models (LLMs): attention sinks, value-state\ndrains, and residual-state peaks, collectively referred to as extreme-token\nphenomena. These phenomena are characterized by certain so-called \"sink tokens\"\nreceiving disproportionately high attention weights, exhibiting significantly\nsmaller value states, and having much larger residual-state norms than those of\nother tokens. These extreme tokens give rise to various challenges in LLM\ninference, quantization, and interpretability.\n  We elucidate the mechanisms behind extreme-token phenomena. First, we show\nthat these phenomena arise in very simple architectures -- transformers with\none to three layers -- trained on a toy model, the Bigram-Backcopy (BB) task.\nIn this setting, we identify an active-dormant mechanism, where attention heads\nbecome sinks for specific input domains while remaining non-sinks for others.\nOur theoretical analysis of the training dynamics reveals that these phenomena\nare driven by a mutual reinforcement mechanism. Building on these insights, we\npropose strategies to mitigate extreme-token phenomena during pretraining,\nincluding replacing softmax with ReLU and Adam with SGD. Next, we extend our\nanalysis to pretrained LLMs, including Llama and OLMo, showing that many\nattention heads exhibit a similar active-dormant mechanism as in the BB task,\nand that the mutual reinforcement mechanism also governs the emergence of\nextreme-token phenomena during LLM pretraining. Our results reveal that many of\nthe static and dynamic properties of extreme-token phenomena predicted by the\nBB task align with observations in pretrained LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practitioners have consistently observed three puzzling phenomena in\ntransformer-based large language models (LLMs): attention sinks, value-state\ndrains, and residual-state peaks, collectively referred to as extreme-token\nphenomena. These phenomena are characterized by certain so-called \"sink tokens\"\nreceiving disproportionately high attention weights, exhibiting significantly\nsmaller value states, and having much larger residual-state norms than those of\nother tokens. These extreme tokens give rise to various challenges in LLM\ninference, quantization, and interpretability.\n  We elucidate the mechanisms behind extreme-token phenomena. First, we show\nthat these phenomena arise in very simple architectures -- transformers with\none to three layers -- trained on a toy model, the Bigram-Backcopy (BB) task.\nIn this setting, we identify an active-dormant mechanism, where attention heads\nbecome sinks for specific input domains while remaining non-sinks for others.\nOur theoretical analysis of the training dynamics reveals that these phenomena\nare driven by a mutual reinforcement mechanism. Building on these insights, we\npropose strategies to mitigate extreme-token phenomena during pretraining,\nincluding replacing softmax with ReLU and Adam with SGD. Next, we extend our\nanalysis to pretrained LLMs, including Llama and OLMo, showing that many\nattention heads exhibit a similar active-dormant mechanism as in the BB task,\nand that the mutual reinforcement mechanism also governs the emergence of\nextreme-token phenomena during LLM pretraining. Our results reveal that many of\nthe static and dynamic properties of extreme-token phenomena predicted by the\nBB task align with observations in pretrained LLMs."
                },
                "authors": [
                    {
                        "name": "Tianyu Guo"
                    },
                    {
                        "name": "Druv Pai"
                    },
                    {
                        "name": "Yu Bai"
                    },
                    {
                        "name": "Jiantao Jiao"
                    },
                    {
                        "name": "Michael I. Jordan"
                    },
                    {
                        "name": "Song Mei"
                    }
                ],
                "author_detail": {
                    "name": "Song Mei"
                },
                "author": "Song Mei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13835v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13835v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.09269v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.09269v2",
                "updated": "2024-11-07T16:43:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    16,
                    43,
                    1,
                    3,
                    312,
                    0
                ],
                "published": "2024-02-14T15:55:30Z",
                "published_parsed": [
                    2024,
                    2,
                    14,
                    15,
                    55,
                    30,
                    2,
                    45,
                    0
                ],
                "title": "Personalized Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Large Language Models"
                },
                "summary": "Large language models (LLMs) have significantly advanced Natural Language\nProcessing (NLP) tasks in recent years. However, their universal nature poses\nlimitations in scenarios requiring personalized responses, such as\nrecommendation systems and chatbots. This paper investigates methods to\npersonalize LLMs, comparing fine-tuning and zero-shot reasoning approaches on\nsubjective tasks. Results demonstrate that personalized fine-tuning improves\nmodel reasoning compared to non-personalized models. Experiments on datasets\nfor emotion recognition and hate speech detection show consistent performance\ngains with personalized methods across different LLM architectures. These\nfindings underscore the importance of personalization for enhancing LLM\ncapabilities in subjective text perception tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have significantly advanced Natural Language\nProcessing (NLP) tasks in recent years. However, their universal nature poses\nlimitations in scenarios requiring personalized responses, such as\nrecommendation systems and chatbots. This paper investigates methods to\npersonalize LLMs, comparing fine-tuning and zero-shot reasoning approaches on\nsubjective tasks. Results demonstrate that personalized fine-tuning improves\nmodel reasoning compared to non-personalized models. Experiments on datasets\nfor emotion recognition and hate speech detection show consistent performance\ngains with personalized methods across different LLM architectures. These\nfindings underscore the importance of personalization for enhancing LLM\ncapabilities in subjective text perception tasks."
                },
                "authors": [
                    {
                        "name": "Stanisaw Woniak"
                    },
                    {
                        "name": "Bartomiej Koptyra"
                    },
                    {
                        "name": "Arkadiusz Janz"
                    },
                    {
                        "name": "Przemysaw Kazienko"
                    },
                    {
                        "name": "Jan Koco"
                    }
                ],
                "author_detail": {
                    "name": "Jan Koco"
                },
                "author": "Jan Koco",
                "arxiv_comment": "Accepted to SENTIRE 2024 (ICDM Workshops):\n  https://sentic.net/sentire2024wozniak.pdf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.09269v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.09269v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04847v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04847v1",
                "updated": "2024-11-07T16:33:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    16,
                    33,
                    48,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T16:33:48Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    16,
                    33,
                    48,
                    3,
                    312,
                    0
                ],
                "title": "Prompt-Guided Internal States for Hallucination Detection of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt-Guided Internal States for Hallucination Detection of Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na variety of tasks in different domains. However, they sometimes generate\nresponses that are logically coherent but factually incorrect or misleading,\nwhich is known as LLM hallucinations. Data-driven supervised methods train\nhallucination detectors by leveraging the internal states of LLMs, but\ndetectors trained on specific domains often struggle to generalize well to\nother domains. In this paper, we aim to enhance the cross-domain performance of\nsupervised detectors with only in-domain data. We propose a novel framework,\nprompt-guided internal states for hallucination detection of LLMs, namely\nPRISM. By utilizing appropriate prompts to guide changes in the structure\nrelated to text truthfulness within the LLM's internal states, we make this\nstructure more salient and consistent across texts from different domains. We\nintegrated our framework with existing hallucination detection methods and\nconducted experiments on datasets from different domains. The experimental\nresults indicate that our framework significantly enhances the cross-domain\ngeneralization of existing hallucination detection methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na variety of tasks in different domains. However, they sometimes generate\nresponses that are logically coherent but factually incorrect or misleading,\nwhich is known as LLM hallucinations. Data-driven supervised methods train\nhallucination detectors by leveraging the internal states of LLMs, but\ndetectors trained on specific domains often struggle to generalize well to\nother domains. In this paper, we aim to enhance the cross-domain performance of\nsupervised detectors with only in-domain data. We propose a novel framework,\nprompt-guided internal states for hallucination detection of LLMs, namely\nPRISM. By utilizing appropriate prompts to guide changes in the structure\nrelated to text truthfulness within the LLM's internal states, we make this\nstructure more salient and consistent across texts from different domains. We\nintegrated our framework with existing hallucination detection methods and\nconducted experiments on datasets from different domains. The experimental\nresults indicate that our framework significantly enhances the cross-domain\ngeneralization of existing hallucination detection methods."
                },
                "authors": [
                    {
                        "name": "Fujie Zhang"
                    },
                    {
                        "name": "Peiqi Yu"
                    },
                    {
                        "name": "Biao Yi"
                    },
                    {
                        "name": "Baolei Zhang"
                    },
                    {
                        "name": "Tong Li"
                    },
                    {
                        "name": "Zheli Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheli Liu"
                },
                "author": "Zheli Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04847v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04847v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04825v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04825v1",
                "updated": "2024-11-07T16:06:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    16,
                    6,
                    0,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T16:06:00Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    16,
                    6,
                    0,
                    3,
                    312,
                    0
                ],
                "title": "VTechAGP: An Academic-to-General-Audience Text Paraphrase Dataset and\n  Benchmark Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VTechAGP: An Academic-to-General-Audience Text Paraphrase Dataset and\n  Benchmark Models"
                },
                "summary": "Existing text simplification or paraphrase datasets mainly focus on\nsentence-level text generation in a general domain. These datasets are\ntypically developed without using domain knowledge. In this paper, we release a\nnovel dataset, VTechAGP, which is the first academic-to-general-audience text\nparaphrase dataset consisting of 4,938 document-level these and dissertation\nacademic and general-audience abstract pairs from 8 colleges authored over 25\nyears. We also propose a novel dynamic soft prompt generative language model,\nDSPT5. For training, we leverage a contrastive-generative loss function to\nlearn the keyword vectors in the dynamic prompt. For inference, we adopt a\ncrowd-sampling decoding strategy at both semantic and structural levels to\nfurther select the best output candidate. We evaluate DSPT5 and various\nstate-of-the-art large language models (LLMs) from multiple perspectives.\nResults demonstrate that the SOTA LLMs does not provide satisfactory outcomes,\nwhile the lightweight DSPT5 can achieve competitive results. To the best of our\nknowledge, we are the first to build a benchmark dataset and solutions for\nacademic-to-general-audience text paraphrase dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing text simplification or paraphrase datasets mainly focus on\nsentence-level text generation in a general domain. These datasets are\ntypically developed without using domain knowledge. In this paper, we release a\nnovel dataset, VTechAGP, which is the first academic-to-general-audience text\nparaphrase dataset consisting of 4,938 document-level these and dissertation\nacademic and general-audience abstract pairs from 8 colleges authored over 25\nyears. We also propose a novel dynamic soft prompt generative language model,\nDSPT5. For training, we leverage a contrastive-generative loss function to\nlearn the keyword vectors in the dynamic prompt. For inference, we adopt a\ncrowd-sampling decoding strategy at both semantic and structural levels to\nfurther select the best output candidate. We evaluate DSPT5 and various\nstate-of-the-art large language models (LLMs) from multiple perspectives.\nResults demonstrate that the SOTA LLMs does not provide satisfactory outcomes,\nwhile the lightweight DSPT5 can achieve competitive results. To the best of our\nknowledge, we are the first to build a benchmark dataset and solutions for\nacademic-to-general-audience text paraphrase dataset."
                },
                "authors": [
                    {
                        "name": "Ming Cheng"
                    },
                    {
                        "name": "Jiaying Gong"
                    },
                    {
                        "name": "Chenhan Yuan"
                    },
                    {
                        "name": "William A. Ingram"
                    },
                    {
                        "name": "Edward Fox"
                    },
                    {
                        "name": "Hoda Eldardiry"
                    }
                ],
                "author_detail": {
                    "name": "Hoda Eldardiry"
                },
                "author": "Hoda Eldardiry",
                "arxiv_comment": "21 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04825v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04825v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16163v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16163v2",
                "updated": "2024-11-07T15:48:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    15,
                    48,
                    11,
                    3,
                    312,
                    0
                ],
                "published": "2024-08-28T22:51:29Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    22,
                    51,
                    29,
                    2,
                    241,
                    0
                ],
                "title": "FRACTURED-SORRY-Bench: Framework for Revealing Attacks in Conversational\n  Turns Undermining Refusal Efficacy and Defenses over SORRY-Bench (Automated\n  Multi-shot Jailbreaks)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FRACTURED-SORRY-Bench: Framework for Revealing Attacks in Conversational\n  Turns Undermining Refusal Efficacy and Defenses over SORRY-Bench (Automated\n  Multi-shot Jailbreaks)"
                },
                "summary": "This paper introduces FRACTURED-SORRY-Bench, a framework for evaluating the\nsafety of Large Language Models (LLMs) against multi-turn conversational\nattacks. Building upon the SORRY-Bench dataset, we propose a simple yet\neffective method for generating adversarial prompts by breaking down harmful\nqueries into seemingly innocuous sub-questions. Our approach achieves a maximum\nincrease of +46.22\\% in Attack Success Rates (ASRs) across GPT-4, GPT-4o,\nGPT-4o-mini, and GPT-3.5-Turbo models compared to baseline methods. We\ndemonstrate that this technique poses a challenge to current LLM safety\nmeasures and highlights the need for more robust defenses against subtle,\nmulti-turn attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces FRACTURED-SORRY-Bench, a framework for evaluating the\nsafety of Large Language Models (LLMs) against multi-turn conversational\nattacks. Building upon the SORRY-Bench dataset, we propose a simple yet\neffective method for generating adversarial prompts by breaking down harmful\nqueries into seemingly innocuous sub-questions. Our approach achieves a maximum\nincrease of +46.22\\% in Attack Success Rates (ASRs) across GPT-4, GPT-4o,\nGPT-4o-mini, and GPT-3.5-Turbo models compared to baseline methods. We\ndemonstrate that this technique poses a challenge to current LLM safety\nmeasures and highlights the need for more robust defenses against subtle,\nmulti-turn attacks."
                },
                "authors": [
                    {
                        "name": "Aman Priyanshu"
                    },
                    {
                        "name": "Supriti Vijay"
                    }
                ],
                "author_detail": {
                    "name": "Supriti Vijay"
                },
                "author": "Supriti Vijay",
                "arxiv_comment": "4 pages, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16163v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16163v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.00867v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.00867v3",
                "updated": "2024-11-07T15:41:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    15,
                    41,
                    38,
                    3,
                    312,
                    0
                ],
                "published": "2024-03-01T03:29:54Z",
                "published_parsed": [
                    2024,
                    3,
                    1,
                    3,
                    29,
                    54,
                    4,
                    61,
                    0
                ],
                "title": "Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by\n  Exploring Refusal Loss Landscapes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by\n  Exploring Refusal Loss Landscapes"
                },
                "summary": "Large Language Models (LLMs) are becoming a prominent generative AI tool,\nwhere the user enters a query and the LLM generates an answer. To reduce harm\nand misuse, efforts have been made to align these LLMs to human values using\nadvanced training techniques such as Reinforcement Learning from Human Feedback\n(RLHF). However, recent studies have highlighted the vulnerability of LLMs to\nadversarial jailbreak attempts aiming at subverting the embedded safety\nguardrails. To address this challenge, this paper defines and investigates the\nRefusal Loss of LLMs and then proposes a method called Gradient Cuff to detect\njailbreak attempts. Gradient Cuff exploits the unique properties observed in\nthe refusal loss landscape, including functional values and its smoothness, to\ndesign an effective two-step detection strategy. Experimental results on two\naligned LLMs (LLaMA-2-7B-Chat and Vicuna-7B-V1.5) and six types of jailbreak\nattacks (GCG, AutoDAN, PAIR, TAP, Base64, and LRL) show that Gradient Cuff can\nsignificantly improve the LLM's rejection capability for malicious jailbreak\nqueries, while maintaining the model's performance for benign user queries by\nadjusting the detection threshold.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are becoming a prominent generative AI tool,\nwhere the user enters a query and the LLM generates an answer. To reduce harm\nand misuse, efforts have been made to align these LLMs to human values using\nadvanced training techniques such as Reinforcement Learning from Human Feedback\n(RLHF). However, recent studies have highlighted the vulnerability of LLMs to\nadversarial jailbreak attempts aiming at subverting the embedded safety\nguardrails. To address this challenge, this paper defines and investigates the\nRefusal Loss of LLMs and then proposes a method called Gradient Cuff to detect\njailbreak attempts. Gradient Cuff exploits the unique properties observed in\nthe refusal loss landscape, including functional values and its smoothness, to\ndesign an effective two-step detection strategy. Experimental results on two\naligned LLMs (LLaMA-2-7B-Chat and Vicuna-7B-V1.5) and six types of jailbreak\nattacks (GCG, AutoDAN, PAIR, TAP, Base64, and LRL) show that Gradient Cuff can\nsignificantly improve the LLM's rejection capability for malicious jailbreak\nqueries, while maintaining the model's performance for benign user queries by\nadjusting the detection threshold."
                },
                "authors": [
                    {
                        "name": "Xiaomeng Hu"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    },
                    {
                        "name": "Tsung-Yi Ho"
                    }
                ],
                "author_detail": {
                    "name": "Tsung-Yi Ho"
                },
                "author": "Tsung-Yi Ho",
                "arxiv_comment": "Accepted by NeurIPS 2024. Project page:\n  https://huggingface.co/spaces/TrustSafeAI/GradientCuff-Jailbreak-Defense",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.00867v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.00867v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.13040v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.13040v2",
                "updated": "2024-11-07T15:40:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    15,
                    40,
                    9,
                    3,
                    312,
                    0
                ],
                "published": "2023-10-19T17:59:12Z",
                "published_parsed": [
                    2023,
                    10,
                    19,
                    17,
                    59,
                    12,
                    3,
                    292,
                    0
                ],
                "title": "Interpreting CLIP: Insights on the Robustness to ImageNet Distribution\n  Shifts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpreting CLIP: Insights on the Robustness to ImageNet Distribution\n  Shifts"
                },
                "summary": "What distinguishes robust models from non-robust ones? While for ImageNet\ndistribution shifts it has been shown that such differences in robustness can\nbe traced back predominantly to differences in training data, so far it is not\nknown what that translates to in terms of what the model has learned. In this\nwork, we bridge this gap by probing the representation spaces of 16 robust\nzero-shot CLIP vision encoders with various backbones (ResNets and ViTs) and\npretraining sets (OpenAI, LAION-400M, LAION-2B, YFCC15M, CC12M and {DataComp}),\nand comparing them to the representation spaces of less robust models with\nidentical backbones, but different (pre)training sets or objectives (CLIP\npretraining on ImageNet-Captions, and supervised training or finetuning on\nImageNet).Through this analysis, we generate three novel insights. Firstly, we\ndetect the presence of outlier features in robust zero-shot CLIP vision\nencoders, which to the best of our knowledge is the first time these are\nobserved in non-language and non-transformer models. Secondly, we find the\nexistence of outlier features to be an indication of ImageNet shift robustness\nin models, since we only find them in robust models in our analysis. Lastly, we\nalso investigate the number of unique encoded concepts in the representation\nspace and find zero-shot CLIP models to encode a higher number of unique\nconcepts in their representation space. However, we do not find this to be an\nindicator of ImageNet shift robustness and hypothesize that it is rather\nrelated to the language supervision. Since the presence of outlier features can\nbe detected without access to any data from shifted datasets, we believe that\nthey could be a useful tool for practitioners to get a feeling for the\ndistribution shift robustness of a pretrained model during deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What distinguishes robust models from non-robust ones? While for ImageNet\ndistribution shifts it has been shown that such differences in robustness can\nbe traced back predominantly to differences in training data, so far it is not\nknown what that translates to in terms of what the model has learned. In this\nwork, we bridge this gap by probing the representation spaces of 16 robust\nzero-shot CLIP vision encoders with various backbones (ResNets and ViTs) and\npretraining sets (OpenAI, LAION-400M, LAION-2B, YFCC15M, CC12M and {DataComp}),\nand comparing them to the representation spaces of less robust models with\nidentical backbones, but different (pre)training sets or objectives (CLIP\npretraining on ImageNet-Captions, and supervised training or finetuning on\nImageNet).Through this analysis, we generate three novel insights. Firstly, we\ndetect the presence of outlier features in robust zero-shot CLIP vision\nencoders, which to the best of our knowledge is the first time these are\nobserved in non-language and non-transformer models. Secondly, we find the\nexistence of outlier features to be an indication of ImageNet shift robustness\nin models, since we only find them in robust models in our analysis. Lastly, we\nalso investigate the number of unique encoded concepts in the representation\nspace and find zero-shot CLIP models to encode a higher number of unique\nconcepts in their representation space. However, we do not find this to be an\nindicator of ImageNet shift robustness and hypothesize that it is rather\nrelated to the language supervision. Since the presence of outlier features can\nbe detected without access to any data from shifted datasets, we believe that\nthey could be a useful tool for practitioners to get a feeling for the\ndistribution shift robustness of a pretrained model during deployment."
                },
                "authors": [
                    {
                        "name": "Jonathan Crabb"
                    },
                    {
                        "name": "Pau Rodrguez"
                    },
                    {
                        "name": "Vaishaal Shankar"
                    },
                    {
                        "name": "Luca Zappella"
                    },
                    {
                        "name": "Arno Blaas"
                    }
                ],
                "author_detail": {
                    "name": "Arno Blaas"
                },
                "author": "Arno Blaas",
                "arxiv_comment": "Published in TMLR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.13040v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.13040v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04799v1",
                "updated": "2024-11-07T15:38:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    15,
                    38,
                    25,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T15:38:25Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    15,
                    38,
                    25,
                    3,
                    312,
                    0
                ],
                "title": "Kwai-STaR: Transform LLMs into State-Transition Reasoners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kwai-STaR: Transform LLMs into State-Transition Reasoners"
                },
                "summary": "Mathematical reasoning presents a significant challenge to the cognitive\ncapabilities of LLMs. Various methods have been proposed to enhance the\nmathematical ability of LLMs. However, few recognize the value of state\ntransition for LLM reasoning. In this work, we define mathematical\nproblem-solving as a process of transiting from an initial unsolved state to\nthe final resolved state, and propose Kwai-STaR framework, which transforms\nLLMs into State-Transition Reasoners to improve their intuitive reasoning\ncapabilities. Our approach comprises three main steps: (1) Define the state\nspace tailored to the mathematical reasoning. (2) Generate state-transition\ndata based on the state space. (3) Convert original LLMs into State-Transition\nReasoners via a curricular training strategy. Our experiments validate the\neffectiveness of Kwai-STaR in enhancing mathematical reasoning: After training\non the small-scale Kwai-STaR dataset, general LLMs, including Mistral-7B and\nLLaMA-3, achieve considerable performance gain on the GSM8K and GSM-Hard\ndataset. Additionally, the state transition-based design endows Kwai-STaR with\nremarkable training and inference efficiency. Further experiments are underway\nto establish the generality of Kwai-STaR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical reasoning presents a significant challenge to the cognitive\ncapabilities of LLMs. Various methods have been proposed to enhance the\nmathematical ability of LLMs. However, few recognize the value of state\ntransition for LLM reasoning. In this work, we define mathematical\nproblem-solving as a process of transiting from an initial unsolved state to\nthe final resolved state, and propose Kwai-STaR framework, which transforms\nLLMs into State-Transition Reasoners to improve their intuitive reasoning\ncapabilities. Our approach comprises three main steps: (1) Define the state\nspace tailored to the mathematical reasoning. (2) Generate state-transition\ndata based on the state space. (3) Convert original LLMs into State-Transition\nReasoners via a curricular training strategy. Our experiments validate the\neffectiveness of Kwai-STaR in enhancing mathematical reasoning: After training\non the small-scale Kwai-STaR dataset, general LLMs, including Mistral-7B and\nLLaMA-3, achieve considerable performance gain on the GSM8K and GSM-Hard\ndataset. Additionally, the state transition-based design endows Kwai-STaR with\nremarkable training and inference efficiency. Further experiments are underway\nto establish the generality of Kwai-STaR."
                },
                "authors": [
                    {
                        "name": "Xingyu Lu"
                    },
                    {
                        "name": "Yuhang Hu"
                    },
                    {
                        "name": "Changyi Liu"
                    },
                    {
                        "name": "Tianke Zhang"
                    },
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Zhixiang Ding"
                    },
                    {
                        "name": "Shengsheng Qian"
                    },
                    {
                        "name": "Meng Du"
                    },
                    {
                        "name": "Ruiwen Kang"
                    },
                    {
                        "name": "Kaiyu Tang"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Tingting Gao"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Hai-Tao Zheng"
                    },
                    {
                        "name": "Bin Wen"
                    }
                ],
                "author_detail": {
                    "name": "Bin Wen"
                },
                "author": "Bin Wen",
                "arxiv_comment": "6 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04797v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04797v1",
                "updated": "2024-11-07T15:37:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    15,
                    37,
                    8,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T15:37:08Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    15,
                    37,
                    8,
                    3,
                    312,
                    0
                ],
                "title": "Development of a Service Robot for Hospital Environments in\n  Rehabilitation Medicine with LiDAR Based Simultaneous Localization and\n  Mapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development of a Service Robot for Hospital Environments in\n  Rehabilitation Medicine with LiDAR Based Simultaneous Localization and\n  Mapping"
                },
                "summary": "This paper presents the development and evaluation of a medical service robot\nequipped with 3D LiDAR and advanced localization capabilities for use in\nhospital environments. The robot employs LiDAR-based Simultaneous Localization\nand Mapping SLAM to navigate autonomously and interact effectively within\ncomplex and dynamic healthcare settings. A comparative analysis with\nestablished 3D SLAM technology in Autoware version 1.14.0, under a Linux ROS\nframework, provided a benchmark for evaluating our system performance. The\nadaptation of Normal Distribution Transform NDT Matching to indoor navigation\nallowed for precise real-time mapping and enhanced obstacle avoidance\ncapabilities. Empirical validation was conducted through manual maneuvers in\nvarious environments, supplemented by ROS simulations to test the system\nresponse to simulated challenges. The findings demonstrate that the robot\nintegration of 3D LiDAR and NDT Matching significantly improves navigation\naccuracy and operational reliability in a healthcare context. This study\nhighlights the robot ability to perform essential tasks with high efficiency\nand identifies potential areas for further improvement, particularly in sensor\nperformance under diverse environmental conditions. The successful deployment\nof this technology in a hospital setting illustrates its potential to support\nmedical staff and contribute to patient care, suggesting a promising direction\nfor future research and development in healthcare robotics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the development and evaluation of a medical service robot\nequipped with 3D LiDAR and advanced localization capabilities for use in\nhospital environments. The robot employs LiDAR-based Simultaneous Localization\nand Mapping SLAM to navigate autonomously and interact effectively within\ncomplex and dynamic healthcare settings. A comparative analysis with\nestablished 3D SLAM technology in Autoware version 1.14.0, under a Linux ROS\nframework, provided a benchmark for evaluating our system performance. The\nadaptation of Normal Distribution Transform NDT Matching to indoor navigation\nallowed for precise real-time mapping and enhanced obstacle avoidance\ncapabilities. Empirical validation was conducted through manual maneuvers in\nvarious environments, supplemented by ROS simulations to test the system\nresponse to simulated challenges. The findings demonstrate that the robot\nintegration of 3D LiDAR and NDT Matching significantly improves navigation\naccuracy and operational reliability in a healthcare context. This study\nhighlights the robot ability to perform essential tasks with high efficiency\nand identifies potential areas for further improvement, particularly in sensor\nperformance under diverse environmental conditions. The successful deployment\nof this technology in a hospital setting illustrates its potential to support\nmedical staff and contribute to patient care, suggesting a promising direction\nfor future research and development in healthcare robotics."
                },
                "authors": [
                    {
                        "name": "Sayat Ibrayev"
                    },
                    {
                        "name": "Arman Ibrayeva"
                    },
                    {
                        "name": "Bekzat Amanov"
                    },
                    {
                        "name": "Serik Tolenov"
                    }
                ],
                "author_detail": {
                    "name": "Serik Tolenov"
                },
                "author": "Serik Tolenov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04797v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04797v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04794v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04794v1",
                "updated": "2024-11-07T15:36:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    15,
                    36,
                    5,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T15:36:05Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    15,
                    36,
                    5,
                    3,
                    312,
                    0
                ],
                "title": "AlignXIE: Improving Multilingual Information Extraction by Cross-Lingual\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlignXIE: Improving Multilingual Information Extraction by Cross-Lingual\n  Alignment"
                },
                "summary": "Empirical evidence suggests that LLMs exhibit spontaneous cross-lingual\nalignment. Our findings suggest that although LLMs also demonstrate promising\ncross-lingual alignment in Information Extraction, there remains significant\nimbalance across languages, revealing an underlying deficiency in the IE\nalignment. To address this issue, we propose AlignXIE, a powerful code-based\nLLM that significantly enhances cross-lingual IE alignment through two\nstrategies. Firstly, AlignXIE formulates IE across different languages,\nespecially non-English ones, as code generation tasks, standardizing the\nrepresentation of various schemas using Python classes to ensure consistency of\nthe same ontology in different languages and align the schema. Secondly, it\nincorporates an IE cross-lingual alignment phase through a translated instance\nprediction task proposed in this paper to align the extraction process,\nutilizing ParallelNER, an IE bilingual parallel dataset with 257,190 samples,\ngenerated by our proposed LLM-based automatic pipeline for IE parallel data\nconstruction, with manual annotation to ensure quality. Ultimately, we obtain\nAlignXIE through multilingual IE instruction tuning. Although without training\nin 9 unseen languages, AlignXIE surpasses ChatGPT by $30.17\\%$ and SoTA by\n$20.03\\%$, thereby demonstrating superior cross-lingual IE capabilities.\nComprehensive evaluations on 63 IE benchmarks in Chinese and English under\nvarious settings, demonstrate that AlignXIE significantly enhances\ncross-lingual and multilingual IE through boosting the IE alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empirical evidence suggests that LLMs exhibit spontaneous cross-lingual\nalignment. Our findings suggest that although LLMs also demonstrate promising\ncross-lingual alignment in Information Extraction, there remains significant\nimbalance across languages, revealing an underlying deficiency in the IE\nalignment. To address this issue, we propose AlignXIE, a powerful code-based\nLLM that significantly enhances cross-lingual IE alignment through two\nstrategies. Firstly, AlignXIE formulates IE across different languages,\nespecially non-English ones, as code generation tasks, standardizing the\nrepresentation of various schemas using Python classes to ensure consistency of\nthe same ontology in different languages and align the schema. Secondly, it\nincorporates an IE cross-lingual alignment phase through a translated instance\nprediction task proposed in this paper to align the extraction process,\nutilizing ParallelNER, an IE bilingual parallel dataset with 257,190 samples,\ngenerated by our proposed LLM-based automatic pipeline for IE parallel data\nconstruction, with manual annotation to ensure quality. Ultimately, we obtain\nAlignXIE through multilingual IE instruction tuning. Although without training\nin 9 unseen languages, AlignXIE surpasses ChatGPT by $30.17\\%$ and SoTA by\n$20.03\\%$, thereby demonstrating superior cross-lingual IE capabilities.\nComprehensive evaluations on 63 IE benchmarks in Chinese and English under\nvarious settings, demonstrate that AlignXIE significantly enhances\ncross-lingual and multilingual IE through boosting the IE alignment."
                },
                "authors": [
                    {
                        "name": "Yuxin Zuo"
                    },
                    {
                        "name": "Wenxuan Jiang"
                    },
                    {
                        "name": "Wenxuan Liu"
                    },
                    {
                        "name": "Zixuan Li"
                    },
                    {
                        "name": "Long Bai"
                    },
                    {
                        "name": "Hanbin Wang"
                    },
                    {
                        "name": "Yutao Zeng"
                    },
                    {
                        "name": "Xiaolong Jin"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04794v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04794v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.03955v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.03955v8",
                "updated": "2024-11-07T15:07:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    15,
                    7,
                    22,
                    3,
                    312,
                    0
                ],
                "published": "2024-01-08T15:21:21Z",
                "published_parsed": [
                    2024,
                    1,
                    8,
                    15,
                    21,
                    21,
                    0,
                    8,
                    0
                ],
                "title": "Tiny Time Mixers (TTMs): Fast Pre-trained Models for Enhanced\n  Zero/Few-Shot Forecasting of Multivariate Time Series",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tiny Time Mixers (TTMs): Fast Pre-trained Models for Enhanced\n  Zero/Few-Shot Forecasting of Multivariate Time Series"
                },
                "summary": "Large pre-trained models excel in zero/few-shot learning for language and\nvision tasks but face challenges in multivariate time series (TS) forecasting\ndue to diverse data characteristics. Consequently, recent research efforts have\nfocused on developing pre-trained TS forecasting models. These models, whether\nbuilt from scratch or adapted from large language models (LLMs), excel in\nzero/few-shot forecasting tasks. However, they are limited by slow performance,\nhigh computational demands, and neglect of cross-channel and exogenous\ncorrelations. To address this, we introduce Tiny Time Mixers (TTM), a compact\nmodel (starting from 1M parameters) with effective transfer learning\ncapabilities, trained exclusively on public TS datasets. TTM, based on the\nlight-weight TSMixer architecture, incorporates innovations like adaptive\npatching, diverse resolution sampling, and resolution prefix tuning to handle\npre-training on varied dataset resolutions with minimal model capacity.\nAdditionally, it employs multi-level modeling to capture channel correlations\nand infuse exogenous signals during fine-tuning. TTM outperforms existing\npopular benchmarks in zero/few-shot forecasting by (4-40%), while reducing\ncomputational requirements significantly. Moreover, TTMs are lightweight and\ncan be executed even on CPU-only machines, enhancing usability and fostering\nwider adoption in resource-constrained environments. The model weights for\nreproducibility and research use are available at\nhttps://huggingface.co/ibm/ttm-research-r2/, while enterprise-use weights under\nthe Apache license can be accessed as follows: the initial TTM-Q variant at\nhttps://huggingface.co/ibm-granite/granite-timeseries-ttm-r1, and the latest\nvariants (TTM-B, TTM-E, TTM-A) weights are available at\nhttps://huggingface.co/ibm-granite/granite-timeseries-ttm-r2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large pre-trained models excel in zero/few-shot learning for language and\nvision tasks but face challenges in multivariate time series (TS) forecasting\ndue to diverse data characteristics. Consequently, recent research efforts have\nfocused on developing pre-trained TS forecasting models. These models, whether\nbuilt from scratch or adapted from large language models (LLMs), excel in\nzero/few-shot forecasting tasks. However, they are limited by slow performance,\nhigh computational demands, and neglect of cross-channel and exogenous\ncorrelations. To address this, we introduce Tiny Time Mixers (TTM), a compact\nmodel (starting from 1M parameters) with effective transfer learning\ncapabilities, trained exclusively on public TS datasets. TTM, based on the\nlight-weight TSMixer architecture, incorporates innovations like adaptive\npatching, diverse resolution sampling, and resolution prefix tuning to handle\npre-training on varied dataset resolutions with minimal model capacity.\nAdditionally, it employs multi-level modeling to capture channel correlations\nand infuse exogenous signals during fine-tuning. TTM outperforms existing\npopular benchmarks in zero/few-shot forecasting by (4-40%), while reducing\ncomputational requirements significantly. Moreover, TTMs are lightweight and\ncan be executed even on CPU-only machines, enhancing usability and fostering\nwider adoption in resource-constrained environments. The model weights for\nreproducibility and research use are available at\nhttps://huggingface.co/ibm/ttm-research-r2/, while enterprise-use weights under\nthe Apache license can be accessed as follows: the initial TTM-Q variant at\nhttps://huggingface.co/ibm-granite/granite-timeseries-ttm-r1, and the latest\nvariants (TTM-B, TTM-E, TTM-A) weights are available at\nhttps://huggingface.co/ibm-granite/granite-timeseries-ttm-r2."
                },
                "authors": [
                    {
                        "name": "Vijay Ekambaram"
                    },
                    {
                        "name": "Arindam Jati"
                    },
                    {
                        "name": "Pankaj Dayama"
                    },
                    {
                        "name": "Sumanta Mukherjee"
                    },
                    {
                        "name": "Nam H. Nguyen"
                    },
                    {
                        "name": "Wesley M. Gifford"
                    },
                    {
                        "name": "Chandra Reddy"
                    },
                    {
                        "name": "Jayant Kalagnanam"
                    }
                ],
                "author_detail": {
                    "name": "Jayant Kalagnanam"
                },
                "author": "Jayant Kalagnanam",
                "arxiv_comment": "Accepted at the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.03955v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.03955v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17437v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17437v2",
                "updated": "2024-11-07T15:00:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    15,
                    0,
                    0,
                    3,
                    312,
                    0
                ],
                "published": "2024-08-30T17:41:30Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    17,
                    41,
                    30,
                    4,
                    243,
                    0
                ],
                "title": "SYNTHEVAL: Hybrid Behavioral Testing of NLP Models with Synthetic\n  CheckLists",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SYNTHEVAL: Hybrid Behavioral Testing of NLP Models with Synthetic\n  CheckLists"
                },
                "summary": "Traditional benchmarking in NLP typically involves using static held-out test\nsets. However, this approach often results in an overestimation of performance\nand lacks the ability to offer comprehensive, interpretable, and dynamic\nassessments of NLP models. Recently, works like DynaBench (Kiela et al., 2021)\nand CheckList (Ribeiro et al., 2020) have addressed these limitations through\nbehavioral testing of NLP models with test types generated by a multistep\nhuman-annotated pipeline. Unfortunately, manually creating a variety of test\ntypes requires much human labor, often at prohibitive cost. In this work, we\npropose SYNTHEVAL, a hybrid behavioral testing framework that leverages large\nlanguage models (LLMs) to generate a wide range of test types for a\ncomprehensive evaluation of NLP models. SYNTHEVAL first generates sentences via\nLLMs using controlled generation, and then identifies challenging examples by\ncomparing the predictions made by LLMs with task-specific NLP models. In the\nlast stage, human experts investigate the challenging examples, manually design\ntemplates, and identify the types of failures the taskspecific models\nconsistently exhibit. We apply SYNTHEVAL to two classification tasks, sentiment\nanalysis and toxic language detection, and show that our framework is effective\nin identifying weaknesses of strong models on these tasks. We share our code in\nhttps://github.com/Loreley99/SynthEval_CheckList.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional benchmarking in NLP typically involves using static held-out test\nsets. However, this approach often results in an overestimation of performance\nand lacks the ability to offer comprehensive, interpretable, and dynamic\nassessments of NLP models. Recently, works like DynaBench (Kiela et al., 2021)\nand CheckList (Ribeiro et al., 2020) have addressed these limitations through\nbehavioral testing of NLP models with test types generated by a multistep\nhuman-annotated pipeline. Unfortunately, manually creating a variety of test\ntypes requires much human labor, often at prohibitive cost. In this work, we\npropose SYNTHEVAL, a hybrid behavioral testing framework that leverages large\nlanguage models (LLMs) to generate a wide range of test types for a\ncomprehensive evaluation of NLP models. SYNTHEVAL first generates sentences via\nLLMs using controlled generation, and then identifies challenging examples by\ncomparing the predictions made by LLMs with task-specific NLP models. In the\nlast stage, human experts investigate the challenging examples, manually design\ntemplates, and identify the types of failures the taskspecific models\nconsistently exhibit. We apply SYNTHEVAL to two classification tasks, sentiment\nanalysis and toxic language detection, and show that our framework is effective\nin identifying weaknesses of strong models on these tasks. We share our code in\nhttps://github.com/Loreley99/SynthEval_CheckList."
                },
                "authors": [
                    {
                        "name": "Raoyuan Zhao"
                    },
                    {
                        "name": "Abdullatif Kksal"
                    },
                    {
                        "name": "Yihong Liu"
                    },
                    {
                        "name": "Leonie Weissweiler"
                    },
                    {
                        "name": "Anna Korhonen"
                    },
                    {
                        "name": "Hinrich Schtze"
                    }
                ],
                "author_detail": {
                    "name": "Hinrich Schtze"
                },
                "author": "Hinrich Schtze",
                "arxiv_comment": "EMNLP 2024 - Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17437v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17437v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04760v1",
                "updated": "2024-11-07T14:58:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    58,
                    51,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T14:58:51Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    58,
                    51,
                    3,
                    312,
                    0
                ],
                "title": "Zero-Shot Temporal Resolution Domain Adaptation for Spiking Neural\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Temporal Resolution Domain Adaptation for Spiking Neural\n  Networks"
                },
                "summary": "Spiking Neural Networks (SNNs) are biologically-inspired deep neural networks\nthat efficiently extract temporal information while offering promising gains in\nterms of energy efficiency and latency when deployed on neuromorphic devices.\nHowever, SNN model parameters are sensitive to temporal resolution, leading to\nsignificant performance drops when the temporal resolution of target data at\nthe edge is not the same with that of the pre-deployment source data used for\ntraining, especially when fine-tuning is not possible at the edge. To address\nthis challenge, we propose three novel domain adaptation methods for adapting\nneuron parameters to account for the change in time resolution without\nre-training on target time-resolution. The proposed methods are based on a\nmapping between neuron dynamics in SNNs and State Space Models (SSMs); and are\napplicable to general neuron models. We evaluate the proposed methods under\nspatio-temporal data tasks, namely the audio keyword spotting datasets SHD and\nMSWC as well as the image classification NMINST dataset. Our methods provide an\nalternative to - and in majority of the cases significantly outperform - the\nexisting reference method that simply scales the time constant. Moreover, our\nresults show that high accuracy on high temporal resolution data can be\nobtained by time efficient training on lower temporal resolution data and model\nadaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Neural Networks (SNNs) are biologically-inspired deep neural networks\nthat efficiently extract temporal information while offering promising gains in\nterms of energy efficiency and latency when deployed on neuromorphic devices.\nHowever, SNN model parameters are sensitive to temporal resolution, leading to\nsignificant performance drops when the temporal resolution of target data at\nthe edge is not the same with that of the pre-deployment source data used for\ntraining, especially when fine-tuning is not possible at the edge. To address\nthis challenge, we propose three novel domain adaptation methods for adapting\nneuron parameters to account for the change in time resolution without\nre-training on target time-resolution. The proposed methods are based on a\nmapping between neuron dynamics in SNNs and State Space Models (SSMs); and are\napplicable to general neuron models. We evaluate the proposed methods under\nspatio-temporal data tasks, namely the audio keyword spotting datasets SHD and\nMSWC as well as the image classification NMINST dataset. Our methods provide an\nalternative to - and in majority of the cases significantly outperform - the\nexisting reference method that simply scales the time constant. Moreover, our\nresults show that high accuracy on high temporal resolution data can be\nobtained by time efficient training on lower temporal resolution data and model\nadaptation."
                },
                "authors": [
                    {
                        "name": "Sanja Karilanova"
                    },
                    {
                        "name": "Maxime Fabre"
                    },
                    {
                        "name": "Emre Neftci"
                    },
                    {
                        "name": "Aya zelikkale"
                    }
                ],
                "author_detail": {
                    "name": "Aya zelikkale"
                },
                "author": "Aya zelikkale",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03883v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03883v2",
                "updated": "2024-11-07T14:57:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    57,
                    14,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-06T12:57:58Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    12,
                    57,
                    58,
                    2,
                    311,
                    0
                ],
                "title": "MEG: Medical Knowledge-Augmented Large Language Models for Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEG: Medical Knowledge-Augmented Large Language Models for Question\n  Answering"
                },
                "summary": "Question answering is a natural language understanding task that involves\nreasoning over both explicit context and unstated, relevant domain knowledge.\nLarge language models (LLMs), which underpin most contemporary question\nanswering systems, struggle to induce how concepts relate in specialized\ndomains such as medicine. Existing medical LLMs are also costly to train. In\nthis work, we present MEG, a parameter-efficient approach for medical\nknowledge-augmented LLMs. MEG uses a lightweight mapping network to integrate\ngraph embeddings into the LLM, enabling it to leverage external knowledge in a\ncost-effective way. We evaluate our method on four popular medical\nmultiple-choice datasets and show that LLMs greatly benefit from the factual\ngrounding provided by knowledge graph embeddings. MEG attains an average of\n+10.2% accuracy over the Mistral-Instruct baseline, and +6.7% over specialized\nmodels like BioMistral. We also show results based on Llama-3. Finally, we show\nthat MEG's performance remains robust to the choice of graph encoder.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question answering is a natural language understanding task that involves\nreasoning over both explicit context and unstated, relevant domain knowledge.\nLarge language models (LLMs), which underpin most contemporary question\nanswering systems, struggle to induce how concepts relate in specialized\ndomains such as medicine. Existing medical LLMs are also costly to train. In\nthis work, we present MEG, a parameter-efficient approach for medical\nknowledge-augmented LLMs. MEG uses a lightweight mapping network to integrate\ngraph embeddings into the LLM, enabling it to leverage external knowledge in a\ncost-effective way. We evaluate our method on four popular medical\nmultiple-choice datasets and show that LLMs greatly benefit from the factual\ngrounding provided by knowledge graph embeddings. MEG attains an average of\n+10.2% accuracy over the Mistral-Instruct baseline, and +6.7% over specialized\nmodels like BioMistral. We also show results based on Llama-3. Finally, we show\nthat MEG's performance remains robust to the choice of graph encoder."
                },
                "authors": [
                    {
                        "name": "Laura Cabello"
                    },
                    {
                        "name": "Carmen Martin-Turrero"
                    },
                    {
                        "name": "Uchenna Akujuobi"
                    },
                    {
                        "name": "Anders Sgaard"
                    },
                    {
                        "name": "Carlos Bobed"
                    }
                ],
                "author_detail": {
                    "name": "Carlos Bobed"
                },
                "author": "Carlos Bobed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03883v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03883v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08673v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08673v2",
                "updated": "2024-11-07T14:49:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    49,
                    28,
                    3,
                    312,
                    0
                ],
                "published": "2024-10-11T09:59:21Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    9,
                    59,
                    21,
                    4,
                    285,
                    0
                ],
                "title": "SpikeBottleNet: Spike-Driven Feature Compression Architecture for\n  Edge-Cloud Co-Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpikeBottleNet: Spike-Driven Feature Compression Architecture for\n  Edge-Cloud Co-Inference"
                },
                "summary": "Edge-cloud co-inference enables efficient deep neural network (DNN)\ndeployment by splitting the architecture between an edge device and cloud\nserver, crucial for resource-constraint edge devices. This approach requires\nbalancing on-device computations and communication costs, often achieved\nthrough compressed intermediate feature transmission. Conventional DNN\narchitectures require continuous data processing and floating point\nactivations, leading to considerable energy consumption and increased feature\nsizes, thus raising transmission costs. This challenge motivates exploring\nbinary, event-driven activations using spiking neural networks (SNNs), known\nfor their extreme energy efficiency. In this research, we propose\nSpikeBottleNet, a novel architecture for edge-cloud co-inference systems that\nintegrates a spiking neuron model to significantly reduce energy consumption on\nedge devices. A key innovation of our study is an intermediate feature\ncompression technique tailored for SNNs for efficient feature transmission.\nThis technique leverages a split computing approach to strategically place\nencoder-decoder bottleneck units within complex deep architectures like ResNet\nand MobileNet. Experimental results demonstrate that SpikeBottleNet achieves up\nto 256x bit compression in the final convolutional layer of ResNet, with\nminimal accuracy loss (0.16%). Additionally, our approach enhances edge device\nenergy efficiency by up to 144x compared to the baseline BottleNet, making it\nideal for resource-limited edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge-cloud co-inference enables efficient deep neural network (DNN)\ndeployment by splitting the architecture between an edge device and cloud\nserver, crucial for resource-constraint edge devices. This approach requires\nbalancing on-device computations and communication costs, often achieved\nthrough compressed intermediate feature transmission. Conventional DNN\narchitectures require continuous data processing and floating point\nactivations, leading to considerable energy consumption and increased feature\nsizes, thus raising transmission costs. This challenge motivates exploring\nbinary, event-driven activations using spiking neural networks (SNNs), known\nfor their extreme energy efficiency. In this research, we propose\nSpikeBottleNet, a novel architecture for edge-cloud co-inference systems that\nintegrates a spiking neuron model to significantly reduce energy consumption on\nedge devices. A key innovation of our study is an intermediate feature\ncompression technique tailored for SNNs for efficient feature transmission.\nThis technique leverages a split computing approach to strategically place\nencoder-decoder bottleneck units within complex deep architectures like ResNet\nand MobileNet. Experimental results demonstrate that SpikeBottleNet achieves up\nto 256x bit compression in the final convolutional layer of ResNet, with\nminimal accuracy loss (0.16%). Additionally, our approach enhances edge device\nenergy efficiency by up to 144x compared to the baseline BottleNet, making it\nideal for resource-limited edge devices."
                },
                "authors": [
                    {
                        "name": "Maruf Hassan"
                    },
                    {
                        "name": "Steven Davy"
                    }
                ],
                "author_detail": {
                    "name": "Steven Davy"
                },
                "author": "Steven Davy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08673v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08673v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14331v2",
                "updated": "2024-11-07T14:48:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    48,
                    18,
                    3,
                    312,
                    0
                ],
                "published": "2024-10-18T09:43:30Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    9,
                    43,
                    30,
                    4,
                    292,
                    0
                ],
                "title": "ChartifyText: Automated Chart Generation from Data-Involved Texts via\n  LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChartifyText: Automated Chart Generation from Data-Involved Texts via\n  LLM"
                },
                "summary": "Text documents with numerical values involved are widely used in various\napplications such as scientific research, economy, public health and\njournalism. However, it is difficult for readers to quickly interpret such\ndata-involved texts and gain deep insights. To fill this research gap, this\nwork aims to automatically generate charts to accurately convey the underlying\ndata and ideas to readers, which is essentially a challenging task. The\nchallenges originate from text ambiguities, intrinsic sparsity and uncertainty\nof data in text documents, and subjective sentiment differences. Specifically,\nwe propose ChartifyText, a novel fully-automated approach that leverages Large\nLanguage Models (LLMs) to convert complex data-involved texts to expressive\ncharts. It consists of two major modules: tabular data inference and expressive\nchart generation. The tabular data inference module employs systematic prompt\nengineering to guide the LLM (e.g., GPT-4) to infer table data, where data\nranges, uncertainties, missing data values and corresponding subjective\nsentiments are explicitly considered. The expressive chart generation module\naugments standard charts with intuitive visual encodings and concise texts to\naccurately convey the underlying data and insights. We extensively evaluate the\neffectiveness of ChartifyText on real-world data-involved text documents\nthrough case studies, in-depth interviews with three visualization experts, and\na carefully-designed user study with 15 participants. The results demonstrate\nthe usefulness and effectiveness of ChartifyText in helping readers efficiently\nand effectively make sense of data-involved texts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text documents with numerical values involved are widely used in various\napplications such as scientific research, economy, public health and\njournalism. However, it is difficult for readers to quickly interpret such\ndata-involved texts and gain deep insights. To fill this research gap, this\nwork aims to automatically generate charts to accurately convey the underlying\ndata and ideas to readers, which is essentially a challenging task. The\nchallenges originate from text ambiguities, intrinsic sparsity and uncertainty\nof data in text documents, and subjective sentiment differences. Specifically,\nwe propose ChartifyText, a novel fully-automated approach that leverages Large\nLanguage Models (LLMs) to convert complex data-involved texts to expressive\ncharts. It consists of two major modules: tabular data inference and expressive\nchart generation. The tabular data inference module employs systematic prompt\nengineering to guide the LLM (e.g., GPT-4) to infer table data, where data\nranges, uncertainties, missing data values and corresponding subjective\nsentiments are explicitly considered. The expressive chart generation module\naugments standard charts with intuitive visual encodings and concise texts to\naccurately convey the underlying data and insights. We extensively evaluate the\neffectiveness of ChartifyText on real-world data-involved text documents\nthrough case studies, in-depth interviews with three visualization experts, and\na carefully-designed user study with 15 participants. The results demonstrate\nthe usefulness and effectiveness of ChartifyText in helping readers efficiently\nand effectively make sense of data-involved texts."
                },
                "authors": [
                    {
                        "name": "Songheng Zhang"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Toby Jia-Jun Li"
                    },
                    {
                        "name": "Qiaomu Shen"
                    },
                    {
                        "name": "Yixin Cao"
                    },
                    {
                        "name": "Yong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yong Wang"
                },
                "author": "Yong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17113v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17113v3",
                "updated": "2024-11-07T13:55:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    13,
                    55,
                    50,
                    3,
                    312,
                    0
                ],
                "published": "2024-09-25T17:27:02Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    27,
                    2,
                    2,
                    269,
                    0
                ],
                "title": "Characterizing stable regions in the residual stream of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing stable regions in the residual stream of LLMs"
                },
                "summary": "We identify stable regions in the residual stream of Transformers, where the\nmodel's output remains insensitive to small activation changes, but exhibits\nhigh sensitivity at region boundaries. These regions emerge during training and\nbecome more defined as training progresses or model size increases. The regions\nappear to be much larger than previously studied polytopes. Our analysis\nsuggests that these stable regions align with semantic distinctions, where\nsimilar prompts cluster within regions, and activations from the same region\nlead to similar next token predictions. This work provides a promising research\ndirection for understanding the complexity of neural networks, shedding light\non training dynamics, and advancing interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We identify stable regions in the residual stream of Transformers, where the\nmodel's output remains insensitive to small activation changes, but exhibits\nhigh sensitivity at region boundaries. These regions emerge during training and\nbecome more defined as training progresses or model size increases. The regions\nappear to be much larger than previously studied polytopes. Our analysis\nsuggests that these stable regions align with semantic distinctions, where\nsimilar prompts cluster within regions, and activations from the same region\nlead to similar next token predictions. This work provides a promising research\ndirection for understanding the complexity of neural networks, shedding light\non training dynamics, and advancing interpretability."
                },
                "authors": [
                    {
                        "name": "Jett Janiak"
                    },
                    {
                        "name": "Jacek Karwowski"
                    },
                    {
                        "name": "Chatrik Singh Mangat"
                    },
                    {
                        "name": "Giorgi Giglemiani"
                    },
                    {
                        "name": "Nora Petrova"
                    },
                    {
                        "name": "Stefan Heimersheim"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Heimersheim"
                },
                "author": "Stefan Heimersheim",
                "arxiv_comment": "Published at Scientific Methods for Understanding Deep Learning\n  (SciForDL) workshop at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17113v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17113v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04708v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04708v1",
                "updated": "2024-11-07T13:45:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    13,
                    45,
                    26,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T13:45:26Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    13,
                    45,
                    26,
                    3,
                    312,
                    0
                ],
                "title": "Exploring Hierarchical Molecular Graph Representation in Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Hierarchical Molecular Graph Representation in Multimodal LLMs"
                },
                "summary": "Following the milestones in large language models (LLMs) and multimodal\nmodels, we have seen a surge in applying LLMs to biochemical tasks. Leveraging\ngraph features and molecular text representations, LLMs can tackle various\ntasks, such as predicting chemical reaction outcomes and describing molecular\nproperties. However, most current work overlooks the multi-level nature of\ngraph features. The impact of different feature levels on LLMs and the\nimportance of each level remain unexplored, and it is possible that different\nchemistry tasks require different feature levels. In this work, we first\ninvestigate the effect of feature granularity by fusing GNN-generated feature\ntokens, discovering that even reducing all tokens to a single token does not\nsignificantly impact performance. We then explore the effect of various feature\nlevels on performance, finding that both the quality of LLM-generated molecules\nand performance on different tasks benefit from different feature levels. We\nconclude with two key insights: (1) current molecular Multimodal LLMs(MLLMs)\nlack a comprehensive understanding of graph features, and (2) static processing\nis not sufficient for hierarchical graph feature. Our code will be publicly\navailable soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Following the milestones in large language models (LLMs) and multimodal\nmodels, we have seen a surge in applying LLMs to biochemical tasks. Leveraging\ngraph features and molecular text representations, LLMs can tackle various\ntasks, such as predicting chemical reaction outcomes and describing molecular\nproperties. However, most current work overlooks the multi-level nature of\ngraph features. The impact of different feature levels on LLMs and the\nimportance of each level remain unexplored, and it is possible that different\nchemistry tasks require different feature levels. In this work, we first\ninvestigate the effect of feature granularity by fusing GNN-generated feature\ntokens, discovering that even reducing all tokens to a single token does not\nsignificantly impact performance. We then explore the effect of various feature\nlevels on performance, finding that both the quality of LLM-generated molecules\nand performance on different tasks benefit from different feature levels. We\nconclude with two key insights: (1) current molecular Multimodal LLMs(MLLMs)\nlack a comprehensive understanding of graph features, and (2) static processing\nis not sufficient for hierarchical graph feature. Our code will be publicly\navailable soon."
                },
                "authors": [
                    {
                        "name": "Chengxin Hu"
                    },
                    {
                        "name": "Hao Li"
                    }
                ],
                "author_detail": {
                    "name": "Hao Li"
                },
                "author": "Hao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04708v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04708v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04704v1",
                "updated": "2024-11-07T13:39:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    13,
                    39,
                    14,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T13:39:14Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    13,
                    39,
                    14,
                    3,
                    312,
                    0
                ],
                "title": "Distinguishing LLM-generated from Human-written Code by Contrastive\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distinguishing LLM-generated from Human-written Code by Contrastive\n  Learning"
                },
                "summary": "Large language models (LLMs), such as ChatGPT released by OpenAI, have\nattracted significant attention from both industry and academia due to their\ndemonstrated ability to generate high-quality content for various tasks.\nDespite the impressive capabilities of LLMs, there are growing concerns\nregarding their potential risks in various fields, such as news, education, and\nsoftware engineering. Recently, several commercial and open-source\nLLM-generated content detectors have been proposed, which, however, are\nprimarily designed for detecting natural language content without considering\nthe specific characteristics of program code. This paper aims to fill this gap\nby proposing a novel ChatGPT-generated code detector, CodeGPTSensor, based on a\ncontrastive learning framework and a semantic encoder built with UniXcoder. To\nassess the effectiveness of CodeGPTSensor on differentiating ChatGPT-generated\ncode from human-written code, we first curate a large-scale Human and Machine\ncomparison Corpus (HMCorp), which includes 550K pairs of human-written and\nChatGPT-generated code (i.e., 288K Python code pairs and 222K Java code pairs).\nBased on the HMCorp dataset, our qualitative and quantitative analysis of the\ncharacteristics of ChatGPT-generated code reveals the challenge and opportunity\nof distinguishing ChatGPT-generated code from human-written code with their\nrepresentative features. Our experimental results indicate that CodeGPTSensor\ncan effectively identify ChatGPT-generated code, outperforming all selected\nbaselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), such as ChatGPT released by OpenAI, have\nattracted significant attention from both industry and academia due to their\ndemonstrated ability to generate high-quality content for various tasks.\nDespite the impressive capabilities of LLMs, there are growing concerns\nregarding their potential risks in various fields, such as news, education, and\nsoftware engineering. Recently, several commercial and open-source\nLLM-generated content detectors have been proposed, which, however, are\nprimarily designed for detecting natural language content without considering\nthe specific characteristics of program code. This paper aims to fill this gap\nby proposing a novel ChatGPT-generated code detector, CodeGPTSensor, based on a\ncontrastive learning framework and a semantic encoder built with UniXcoder. To\nassess the effectiveness of CodeGPTSensor on differentiating ChatGPT-generated\ncode from human-written code, we first curate a large-scale Human and Machine\ncomparison Corpus (HMCorp), which includes 550K pairs of human-written and\nChatGPT-generated code (i.e., 288K Python code pairs and 222K Java code pairs).\nBased on the HMCorp dataset, our qualitative and quantitative analysis of the\ncharacteristics of ChatGPT-generated code reveals the challenge and opportunity\nof distinguishing ChatGPT-generated code from human-written code with their\nrepresentative features. Our experimental results indicate that CodeGPTSensor\ncan effectively identify ChatGPT-generated code, outperforming all selected\nbaselines."
                },
                "authors": [
                    {
                        "name": "Xiaodan Xu"
                    },
                    {
                        "name": "Chao Ni"
                    },
                    {
                        "name": "Xinrong Guo"
                    },
                    {
                        "name": "Shaoxuan Liu"
                    },
                    {
                        "name": "Xiaoya Wang"
                    },
                    {
                        "name": "Kui Liu"
                    },
                    {
                        "name": "Xiaohu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohu Yang"
                },
                "author": "Xiaohu Yang",
                "arxiv_comment": "30 pages, 6 figures, Accepted by TOSEM'24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15598v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15598v4",
                "updated": "2024-11-07T13:26:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    13,
                    26,
                    18,
                    3,
                    312,
                    0
                ],
                "published": "2024-05-24T14:30:00Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    14,
                    30,
                    0,
                    4,
                    145,
                    0
                ],
                "title": "MCDFN: Supply Chain Demand Forecasting via an Explainable Multi-Channel\n  Data Fusion Network Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCDFN: Supply Chain Demand Forecasting via an Explainable Multi-Channel\n  Data Fusion Network Model"
                },
                "summary": "Accurate demand forecasting is crucial for optimizing supply chain\nmanagement. Traditional methods often fail to capture complex patterns from\nseasonal variability and special events. Despite advancements in deep learning,\ninterpretable forecasting models remain a challenge. To address this, we\nintroduce the Multi-Channel Data Fusion Network (MCDFN), a hybrid architecture\nthat integrates Convolutional Neural Networks (CNN), Long Short-Term Memory\nnetworks (LSTM), and Gated Recurrent Units (GRU) to enhance predictive\nperformance by extracting spatial and temporal features from time series data.\nOur comparative benchmarking demonstrates that MCDFN outperforms seven other\ndeep-learning models, achieving superior metrics: MSE (23.5738), RMSE (4.8553),\nMAE (3.9991), and MAPE (20.1575%). Additionally, MCDFN's predictions were\nstatistically indistinguishable from actual values, confirmed by a paired\nt-test with a 5% p-value and a 10-fold cross-validated statistical paired\nt-test. We apply explainable AI techniques like ShapTime and Permutation\nFeature Importance to enhance interpretability. This research advances demand\nforecasting methodologies and offers practical guidelines for integrating MCDFN\ninto supply chain systems, highlighting future research directions for\nscalability and user-friendly deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate demand forecasting is crucial for optimizing supply chain\nmanagement. Traditional methods often fail to capture complex patterns from\nseasonal variability and special events. Despite advancements in deep learning,\ninterpretable forecasting models remain a challenge. To address this, we\nintroduce the Multi-Channel Data Fusion Network (MCDFN), a hybrid architecture\nthat integrates Convolutional Neural Networks (CNN), Long Short-Term Memory\nnetworks (LSTM), and Gated Recurrent Units (GRU) to enhance predictive\nperformance by extracting spatial and temporal features from time series data.\nOur comparative benchmarking demonstrates that MCDFN outperforms seven other\ndeep-learning models, achieving superior metrics: MSE (23.5738), RMSE (4.8553),\nMAE (3.9991), and MAPE (20.1575%). Additionally, MCDFN's predictions were\nstatistically indistinguishable from actual values, confirmed by a paired\nt-test with a 5% p-value and a 10-fold cross-validated statistical paired\nt-test. We apply explainable AI techniques like ShapTime and Permutation\nFeature Importance to enhance interpretability. This research advances demand\nforecasting methodologies and offers practical guidelines for integrating MCDFN\ninto supply chain systems, highlighting future research directions for\nscalability and user-friendly deployment."
                },
                "authors": [
                    {
                        "name": "Md Abrar Jahin"
                    },
                    {
                        "name": "Asef Shahriar"
                    },
                    {
                        "name": "Md Al Amin"
                    }
                ],
                "author_detail": {
                    "name": "Md Al Amin"
                },
                "author": "Md Al Amin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15598v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15598v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04679v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04679v1",
                "updated": "2024-11-07T13:08:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    13,
                    8,
                    4,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T13:08:04Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    13,
                    8,
                    4,
                    3,
                    312,
                    0
                ],
                "title": "CaPo: Cooperative Plan Optimization for Efficient Embodied Multi-Agent\n  Cooperation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaPo: Cooperative Plan Optimization for Efficient Embodied Multi-Agent\n  Cooperation"
                },
                "summary": "In this work, we address the cooperation problem among large language model\n(LLM) based embodied agents, where agents must cooperate to achieve a common\ngoal. Previous methods often execute actions extemporaneously and incoherently,\nwithout long-term strategic and cooperative planning, leading to redundant\nsteps, failures, and even serious repercussions in complex tasks like\nsearch-and-rescue missions where discussion and cooperative plan are crucial.\nTo solve this issue, we propose Cooperative Plan Optimization (CaPo) to enhance\nthe cooperation efficiency of LLM-based embodied agents. Inspired by human\ncooperation schemes, CaPo improves cooperation efficiency with two phases: 1)\nmeta-plan generation, and 2) progress-adaptive meta-plan and execution. In the\nfirst phase, all agents analyze the task, discuss, and cooperatively create a\nmeta-plan that decomposes the task into subtasks with detailed steps, ensuring\na long-term strategic and coherent plan for efficient coordination. In the\nsecond phase, agents execute tasks according to the meta-plan and dynamically\nadjust it based on their latest progress (e.g., discovering a target object)\nthrough multi-turn discussions. This progress-based adaptation eliminates\nredundant actions, improving the overall cooperation efficiency of agents.\nExperimental results on the ThreeDworld Multi-Agent Transport and Communicative\nWatch-And-Help tasks demonstrate that CaPo achieves much higher task completion\nrate and efficiency compared with state-of-the-arts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we address the cooperation problem among large language model\n(LLM) based embodied agents, where agents must cooperate to achieve a common\ngoal. Previous methods often execute actions extemporaneously and incoherently,\nwithout long-term strategic and cooperative planning, leading to redundant\nsteps, failures, and even serious repercussions in complex tasks like\nsearch-and-rescue missions where discussion and cooperative plan are crucial.\nTo solve this issue, we propose Cooperative Plan Optimization (CaPo) to enhance\nthe cooperation efficiency of LLM-based embodied agents. Inspired by human\ncooperation schemes, CaPo improves cooperation efficiency with two phases: 1)\nmeta-plan generation, and 2) progress-adaptive meta-plan and execution. In the\nfirst phase, all agents analyze the task, discuss, and cooperatively create a\nmeta-plan that decomposes the task into subtasks with detailed steps, ensuring\na long-term strategic and coherent plan for efficient coordination. In the\nsecond phase, agents execute tasks according to the meta-plan and dynamically\nadjust it based on their latest progress (e.g., discovering a target object)\nthrough multi-turn discussions. This progress-based adaptation eliminates\nredundant actions, improving the overall cooperation efficiency of agents.\nExperimental results on the ThreeDworld Multi-Agent Transport and Communicative\nWatch-And-Help tasks demonstrate that CaPo achieves much higher task completion\nrate and efficiency compared with state-of-the-arts."
                },
                "authors": [
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Pan Zhou"
                    },
                    {
                        "name": "Yingjun Du"
                    },
                    {
                        "name": "Ah-Hwee Tan"
                    },
                    {
                        "name": "Cees G. M. Snoek"
                    },
                    {
                        "name": "Jan-Jakob Sonke"
                    },
                    {
                        "name": "Efstratios Gavves"
                    }
                ],
                "author_detail": {
                    "name": "Efstratios Gavves"
                },
                "author": "Efstratios Gavves",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04679v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04675v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04675v1",
                "updated": "2024-11-07T12:59:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    12,
                    59,
                    48,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T12:59:48Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    12,
                    59,
                    48,
                    3,
                    312,
                    0
                ],
                "title": "Advancing Multi-Connectivity in Satellite-Terrestrial Integrated\n  Networks: Architectures, Challenges, and Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Multi-Connectivity in Satellite-Terrestrial Integrated\n  Networks: Architectures, Challenges, and Applications"
                },
                "summary": "Multi-connectivity (MC) in satellite-terrestrial integrated networks (STINs),\nincluded in 3GPP standards, is regarded as a promising technology for future\nnetworks. The significant advantages of MC in improving coverage,\ncommunication, and sensing through satellite-terrestrial collaboration have\nsparked widespread interest. In this article, we first introduce three\nfundamental deployment architectures of MC systems in STINs, including\nmulti-satellite, single-satellite single-base-station, and multi-satellite\nmulti-base-station configurations. Considering the emerging but still evolving\nsatellite networking, we explore system design challenges such as satellite\nnetworking schemes, e.g., cell-free and multi-tier satellite networks. Then,\nkey technical challenges that severely influence the quality of mutual\ncommunications, including beamforming, channel estimation, and synchronization,\nare discussed subsequently. Furthermore, typical applications such as coverage\nenhancement, traffic offloading, collaborative sensing, and low-altitude\ncommunication are demonstrated, followed by a case study comparing coverage\nperformance in MC and single-connectivity (SC) configurations. Several\nessential future research directions for MC in STINs are presented to\nfacilitate further exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-connectivity (MC) in satellite-terrestrial integrated networks (STINs),\nincluded in 3GPP standards, is regarded as a promising technology for future\nnetworks. The significant advantages of MC in improving coverage,\ncommunication, and sensing through satellite-terrestrial collaboration have\nsparked widespread interest. In this article, we first introduce three\nfundamental deployment architectures of MC systems in STINs, including\nmulti-satellite, single-satellite single-base-station, and multi-satellite\nmulti-base-station configurations. Considering the emerging but still evolving\nsatellite networking, we explore system design challenges such as satellite\nnetworking schemes, e.g., cell-free and multi-tier satellite networks. Then,\nkey technical challenges that severely influence the quality of mutual\ncommunications, including beamforming, channel estimation, and synchronization,\nare discussed subsequently. Furthermore, typical applications such as coverage\nenhancement, traffic offloading, collaborative sensing, and low-altitude\ncommunication are demonstrated, followed by a case study comparing coverage\nperformance in MC and single-connectivity (SC) configurations. Several\nessential future research directions for MC in STINs are presented to\nfacilitate further exploration."
                },
                "authors": [
                    {
                        "name": "Xiangyu Li"
                    },
                    {
                        "name": "Bodong Shang"
                    }
                ],
                "author_detail": {
                    "name": "Bodong Shang"
                },
                "author": "Bodong Shang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04675v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04675v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04671v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04671v1",
                "updated": "2024-11-07T12:55:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    12,
                    55,
                    17,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T12:55:17Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    12,
                    55,
                    17,
                    3,
                    312,
                    0
                ],
                "title": "CUIfy the XR: An Open-Source Package to Embed LLM-powered Conversational\n  Agents in XR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CUIfy the XR: An Open-Source Package to Embed LLM-powered Conversational\n  Agents in XR"
                },
                "summary": "Recent developments in computer graphics, machine learning, and sensor\ntechnologies enable numerous opportunities for extended reality (XR) setups for\neveryday life, from skills training to entertainment. With large corporations\noffering consumer-grade head-mounted displays (HMDs) in an affordable way, it\nis likely that XR will become pervasive, and HMDs will develop as personal\ndevices like smartphones and tablets. However, having intelligent spaces and\nnaturalistic interactions in XR is as important as technological advances so\nthat users grow their engagement in virtual and augmented spaces. To this end,\nlarge language model (LLM)--powered non-player characters (NPCs) with\nspeech-to-text (STT) and text-to-speech (TTS) models bring significant\nadvantages over conventional or pre-scripted NPCs for facilitating more natural\nconversational user interfaces (CUIs) in XR. In this paper, we provide the\ncommunity with an open-source, customizable, extensible, and privacy-aware\nUnity package, CUIfy, that facilitates speech-based NPC-user interaction with\nvarious LLMs, STT, and TTS models. Our package also supports multiple\nLLM-powered NPCs per environment and minimizes the latency between different\ncomputational models through streaming to achieve usable interactions between\nusers and NPCs. We publish our source code in the following repository:\nhttps://gitlab.lrz.de/hctl/cuify",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent developments in computer graphics, machine learning, and sensor\ntechnologies enable numerous opportunities for extended reality (XR) setups for\neveryday life, from skills training to entertainment. With large corporations\noffering consumer-grade head-mounted displays (HMDs) in an affordable way, it\nis likely that XR will become pervasive, and HMDs will develop as personal\ndevices like smartphones and tablets. However, having intelligent spaces and\nnaturalistic interactions in XR is as important as technological advances so\nthat users grow their engagement in virtual and augmented spaces. To this end,\nlarge language model (LLM)--powered non-player characters (NPCs) with\nspeech-to-text (STT) and text-to-speech (TTS) models bring significant\nadvantages over conventional or pre-scripted NPCs for facilitating more natural\nconversational user interfaces (CUIs) in XR. In this paper, we provide the\ncommunity with an open-source, customizable, extensible, and privacy-aware\nUnity package, CUIfy, that facilitates speech-based NPC-user interaction with\nvarious LLMs, STT, and TTS models. Our package also supports multiple\nLLM-powered NPCs per environment and minimizes the latency between different\ncomputational models through streaming to achieve usable interactions between\nusers and NPCs. We publish our source code in the following repository:\nhttps://gitlab.lrz.de/hctl/cuify"
                },
                "authors": [
                    {
                        "name": "Kadir Burak Buldu"
                    },
                    {
                        "name": "Sleyman zdel"
                    },
                    {
                        "name": "Ka Hei Carrie Lau"
                    },
                    {
                        "name": "Mengdi Wang"
                    },
                    {
                        "name": "Daniel Saad"
                    },
                    {
                        "name": "Sofie Schnborn"
                    },
                    {
                        "name": "Auxane Boch"
                    },
                    {
                        "name": "Enkelejda Kasneci"
                    },
                    {
                        "name": "Efe Bozkir"
                    }
                ],
                "author_detail": {
                    "name": "Efe Bozkir"
                },
                "author": "Efe Bozkir",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04671v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04671v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04663v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04663v1",
                "updated": "2024-11-07T12:48:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    12,
                    48,
                    39,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T12:48:39Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    12,
                    48,
                    39,
                    3,
                    312,
                    0
                ],
                "title": "Explainable Search and Discovery of Visual Cultural Heritage Collections\n  with Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable Search and Discovery of Visual Cultural Heritage Collections\n  with Multimodal Large Language Models"
                },
                "summary": "Many cultural institutions have made large digitized visual collections\navailable online, often under permissible re-use licences. Creating interfaces\nfor exploring and searching these collections is difficult, particularly in the\nabsence of granular metadata. In this paper, we introduce a method for using\nstate-of-the-art multimodal large language models (LLMs) to enable an\nopen-ended, explainable search and discovery interface for visual collections.\nWe show how our approach can create novel clustering and recommendation systems\nthat avoid common pitfalls of methods based directly on visual embeddings. Of\nparticular interest is the ability to offer concrete textual explanations of\neach recommendation without the need to preselect the features of interest.\nTogether, these features can create a digital interface that is more open-ended\nand flexible while also being better suited to addressing privacy and ethical\nconcerns. Through a case study using a collection of documentary photographs,\nwe provide several metrics showing the efficacy and possibilities of our\napproach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many cultural institutions have made large digitized visual collections\navailable online, often under permissible re-use licences. Creating interfaces\nfor exploring and searching these collections is difficult, particularly in the\nabsence of granular metadata. In this paper, we introduce a method for using\nstate-of-the-art multimodal large language models (LLMs) to enable an\nopen-ended, explainable search and discovery interface for visual collections.\nWe show how our approach can create novel clustering and recommendation systems\nthat avoid common pitfalls of methods based directly on visual embeddings. Of\nparticular interest is the ability to offer concrete textual explanations of\neach recommendation without the need to preselect the features of interest.\nTogether, these features can create a digital interface that is more open-ended\nand flexible while also being better suited to addressing privacy and ethical\nconcerns. Through a case study using a collection of documentary photographs,\nwe provide several metrics showing the efficacy and possibilities of our\napproach."
                },
                "authors": [
                    {
                        "name": "Taylor Arnold"
                    },
                    {
                        "name": "Lauren Tilton"
                    }
                ],
                "author_detail": {
                    "name": "Lauren Tilton"
                },
                "author": "Lauren Tilton",
                "arxiv_comment": "16 pages, CHR 2024: Computational Humanities Research Conference,\n  December 4 - 6, 2024, Aarhus University, Denmark",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04663v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04663v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04649v1",
                "updated": "2024-11-07T12:12:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    12,
                    12,
                    44,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T12:12:44Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    12,
                    12,
                    44,
                    3,
                    312,
                    0
                ],
                "title": "DISCO: DISCovering Overfittings as Causal Rules for Text Classification\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DISCO: DISCovering Overfittings as Causal Rules for Text Classification\n  Models"
                },
                "summary": "With the rapid advancement of neural language models, the deployment of\nover-parameterized models has surged, increasing the need for interpretable\nexplanations comprehensible to human inspectors. Existing post-hoc\ninterpretability methods, which often focus on unigram features of single input\ntextual instances, fail to capture the models' decision-making process fully.\nAdditionally, many methods do not differentiate between decisions based on\nspurious correlations and those based on a holistic understanding of the input.\nOur paper introduces DISCO, a novel method for discovering global, rule-based\nexplanations by identifying causal n-gram associations with model predictions.\nThis method employs a scalable sequence mining technique to extract relevant\ntext spans from training data, associate them with model predictions, and\nconduct causality checks to distill robust rules that elucidate model behavior.\nThese rules expose potential overfitting and provide insights into misleading\nfeature combinations. We validate DISCO through extensive testing,\ndemonstrating its superiority over existing methods in offering comprehensive\ninsights into complex model behaviors. Our approach successfully identifies all\nshortcuts manually introduced into the training data (100% detection rate on\nthe MultiRC dataset), resulting in an 18.8% regression in model performance --\na capability unmatched by any other method. Furthermore, DISCO supports\ninteractive explanations, enabling human inspectors to distinguish spurious\ncauses in the rule-based output. This alleviates the burden of abundant\ninstance-wise explanations and helps assess the model's risk when encountering\nout-of-distribution (OOD) data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of neural language models, the deployment of\nover-parameterized models has surged, increasing the need for interpretable\nexplanations comprehensible to human inspectors. Existing post-hoc\ninterpretability methods, which often focus on unigram features of single input\ntextual instances, fail to capture the models' decision-making process fully.\nAdditionally, many methods do not differentiate between decisions based on\nspurious correlations and those based on a holistic understanding of the input.\nOur paper introduces DISCO, a novel method for discovering global, rule-based\nexplanations by identifying causal n-gram associations with model predictions.\nThis method employs a scalable sequence mining technique to extract relevant\ntext spans from training data, associate them with model predictions, and\nconduct causality checks to distill robust rules that elucidate model behavior.\nThese rules expose potential overfitting and provide insights into misleading\nfeature combinations. We validate DISCO through extensive testing,\ndemonstrating its superiority over existing methods in offering comprehensive\ninsights into complex model behaviors. Our approach successfully identifies all\nshortcuts manually introduced into the training data (100% detection rate on\nthe MultiRC dataset), resulting in an 18.8% regression in model performance --\na capability unmatched by any other method. Furthermore, DISCO supports\ninteractive explanations, enabling human inspectors to distinguish spurious\ncauses in the rule-based output. This alleviates the burden of abundant\ninstance-wise explanations and helps assess the model's risk when encountering\nout-of-distribution (OOD) data."
                },
                "authors": [
                    {
                        "name": "Zijian Zhang"
                    },
                    {
                        "name": "Vinay Setty"
                    },
                    {
                        "name": "Yumeng Wang"
                    },
                    {
                        "name": "Avishek Anand"
                    }
                ],
                "author_detail": {
                    "name": "Avishek Anand"
                },
                "author": "Avishek Anand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.3; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14125v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14125v3",
                "updated": "2024-11-07T12:07:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    12,
                    7,
                    7,
                    3,
                    312,
                    0
                ],
                "published": "2024-05-23T02:57:42Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    2,
                    57,
                    42,
                    3,
                    144,
                    0
                ],
                "title": "ALI-Agent: Assessing LLMs' Alignment with Human Values via Agent-based\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALI-Agent: Assessing LLMs' Alignment with Human Values via Agent-based\n  Evaluation"
                },
                "summary": "Large Language Models (LLMs) can elicit unintended and even harmful content\nwhen misaligned with human values, posing severe risks to users and society. To\nmitigate these risks, current evaluation benchmarks predominantly employ\nexpert-designed contextual scenarios to assess how well LLMs align with human\nvalues. However, the labor-intensive nature of these benchmarks limits their\ntest scope, hindering their ability to generalize to the extensive variety of\nopen-world use cases and identify rare but crucial long-tail risks.\nAdditionally, these static tests fail to adapt to the rapid evolution of LLMs,\nmaking it hard to evaluate timely alignment issues. To address these\nchallenges, we propose ALI-Agent, an evaluation framework that leverages the\nautonomous abilities of LLM-powered agents to conduct in-depth and adaptive\nalignment assessments. ALI-Agent operates through two principal stages:\nEmulation and Refinement. During the Emulation stage, ALI-Agent automates the\ngeneration of realistic test scenarios. In the Refinement stage, it iteratively\nrefines the scenarios to probe long-tail risks. Specifically, ALI-Agent\nincorporates a memory module to guide test scenario generation, a tool-using\nmodule to reduce human labor in tasks such as evaluating feedback from target\nLLMs, and an action module to refine tests. Extensive experiments across three\naspects of human values--stereotypes, morality, and legality--demonstrate that\nALI-Agent, as a general evaluation framework, effectively identifies model\nmisalignment. Systematic analysis also validates that the generated test\nscenarios represent meaningful use cases, as well as integrate enhanced\nmeasures to probe long-tail risks. Our code is available at\nhttps://github.com/SophieZheng998/ALI-Agent.git",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can elicit unintended and even harmful content\nwhen misaligned with human values, posing severe risks to users and society. To\nmitigate these risks, current evaluation benchmarks predominantly employ\nexpert-designed contextual scenarios to assess how well LLMs align with human\nvalues. However, the labor-intensive nature of these benchmarks limits their\ntest scope, hindering their ability to generalize to the extensive variety of\nopen-world use cases and identify rare but crucial long-tail risks.\nAdditionally, these static tests fail to adapt to the rapid evolution of LLMs,\nmaking it hard to evaluate timely alignment issues. To address these\nchallenges, we propose ALI-Agent, an evaluation framework that leverages the\nautonomous abilities of LLM-powered agents to conduct in-depth and adaptive\nalignment assessments. ALI-Agent operates through two principal stages:\nEmulation and Refinement. During the Emulation stage, ALI-Agent automates the\ngeneration of realistic test scenarios. In the Refinement stage, it iteratively\nrefines the scenarios to probe long-tail risks. Specifically, ALI-Agent\nincorporates a memory module to guide test scenario generation, a tool-using\nmodule to reduce human labor in tasks such as evaluating feedback from target\nLLMs, and an action module to refine tests. Extensive experiments across three\naspects of human values--stereotypes, morality, and legality--demonstrate that\nALI-Agent, as a general evaluation framework, effectively identifies model\nmisalignment. Systematic analysis also validates that the generated test\nscenarios represent meaningful use cases, as well as integrate enhanced\nmeasures to probe long-tail risks. Our code is available at\nhttps://github.com/SophieZheng998/ALI-Agent.git"
                },
                "authors": [
                    {
                        "name": "Jingnan Zheng"
                    },
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "An Zhang"
                    },
                    {
                        "name": "Tai D. Nguyen"
                    },
                    {
                        "name": "Jun Sun"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "arxiv_journal_ref": "2024 Neurips",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14125v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14125v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04642v1",
                "updated": "2024-11-07T11:54:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    11,
                    54,
                    1,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T11:54:01Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    11,
                    54,
                    1,
                    3,
                    312,
                    0
                ],
                "title": "TAP-VL: Text Layout-Aware Pre-training for Enriched Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TAP-VL: Text Layout-Aware Pre-training for Enriched Vision-Language\n  Models"
                },
                "summary": "Vision-Language (VL) models have garnered considerable research interest;\nhowever, they still face challenges in effectively handling text within images.\nTo address this limitation, researchers have developed two approaches. The\nfirst method involves utilizing external Optical Character Recognition (OCR)\ntools to extract textual information from images, which is then prepended to\nother textual inputs. The second strategy focuses on employing extremely\nhigh-resolution images to improve text recognition capabilities. In this paper,\nwe focus on enhancing the first strategy by introducing a novel method, named\nTAP-VL, which treats OCR information as a distinct modality and seamlessly\nintegrates it into any VL model. TAP-VL employs a lightweight transformer-based\nOCR module to receive OCR with layout information, compressing it into a short\nfixed-length sequence for input into the LLM. Initially, we conduct\nmodel-agnostic pretraining of the OCR module on unlabeled documents, followed\nby its integration into any VL architecture through brief fine-tuning.\nExtensive experiments demonstrate consistent performance improvements when\napplying TAP-VL to top-performing VL models, across scene-text and\ndocument-based VL benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language (VL) models have garnered considerable research interest;\nhowever, they still face challenges in effectively handling text within images.\nTo address this limitation, researchers have developed two approaches. The\nfirst method involves utilizing external Optical Character Recognition (OCR)\ntools to extract textual information from images, which is then prepended to\nother textual inputs. The second strategy focuses on employing extremely\nhigh-resolution images to improve text recognition capabilities. In this paper,\nwe focus on enhancing the first strategy by introducing a novel method, named\nTAP-VL, which treats OCR information as a distinct modality and seamlessly\nintegrates it into any VL model. TAP-VL employs a lightweight transformer-based\nOCR module to receive OCR with layout information, compressing it into a short\nfixed-length sequence for input into the LLM. Initially, we conduct\nmodel-agnostic pretraining of the OCR module on unlabeled documents, followed\nby its integration into any VL architecture through brief fine-tuning.\nExtensive experiments demonstrate consistent performance improvements when\napplying TAP-VL to top-performing VL models, across scene-text and\ndocument-based VL benchmarks."
                },
                "authors": [
                    {
                        "name": "Jonathan Fhima"
                    },
                    {
                        "name": "Elad Ben Avraham"
                    },
                    {
                        "name": "Oren Nuriel"
                    },
                    {
                        "name": "Yair Kittenplon"
                    },
                    {
                        "name": "Roy Ganz"
                    },
                    {
                        "name": "Aviad Aberdam"
                    },
                    {
                        "name": "Ron Litman"
                    }
                ],
                "author_detail": {
                    "name": "Ron Litman"
                },
                "author": "Ron Litman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04637v1",
                "updated": "2024-11-07T11:51:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    11,
                    51,
                    14,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T11:51:14Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    11,
                    51,
                    14,
                    3,
                    312,
                    0
                ],
                "title": "Hands-On Tutorial: Labeling with LLM and Human-in-the-Loop",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hands-On Tutorial: Labeling with LLM and Human-in-the-Loop"
                },
                "summary": "Training and deploying machine learning models relies on a large amount of\nhuman-annotated data. As human labeling becomes increasingly expensive and\ntime-consuming, recent research has developed multiple strategies to speed up\nannotation and reduce costs and human workload: generating synthetic training\ndata, active learning, and hybrid labeling. This tutorial is oriented toward\npractical applications: we will present the basics of each strategy, highlight\ntheir benefits and limitations, and discuss in detail real-life case studies.\nAdditionally, we will walk through best practices for managing human annotators\nand controlling the quality of the final dataset. The tutorial includes a\nhands-on workshop, where attendees will be guided in implementing a hybrid\nannotation setup. This tutorial is designed for NLP practitioners from both\nresearch and industry backgrounds who are involved in or interested in\noptimizing data labeling projects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training and deploying machine learning models relies on a large amount of\nhuman-annotated data. As human labeling becomes increasingly expensive and\ntime-consuming, recent research has developed multiple strategies to speed up\nannotation and reduce costs and human workload: generating synthetic training\ndata, active learning, and hybrid labeling. This tutorial is oriented toward\npractical applications: we will present the basics of each strategy, highlight\ntheir benefits and limitations, and discuss in detail real-life case studies.\nAdditionally, we will walk through best practices for managing human annotators\nand controlling the quality of the final dataset. The tutorial includes a\nhands-on workshop, where attendees will be guided in implementing a hybrid\nannotation setup. This tutorial is designed for NLP practitioners from both\nresearch and industry backgrounds who are involved in or interested in\noptimizing data labeling projects."
                },
                "authors": [
                    {
                        "name": "Ekaterina Artemova"
                    },
                    {
                        "name": "Akim Tsvigun"
                    },
                    {
                        "name": "Dominik Schlechtweg"
                    },
                    {
                        "name": "Natalia Fedorova"
                    },
                    {
                        "name": "Sergei Tilga"
                    },
                    {
                        "name": "Boris Obmoroshev"
                    }
                ],
                "author_detail": {
                    "name": "Boris Obmoroshev"
                },
                "author": "Boris Obmoroshev",
                "arxiv_comment": "To be presented at COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15052v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15052v3",
                "updated": "2024-11-07T11:43:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    11,
                    43,
                    4,
                    3,
                    312,
                    0
                ],
                "published": "2024-10-19T09:49:12Z",
                "published_parsed": [
                    2024,
                    10,
                    19,
                    9,
                    49,
                    12,
                    5,
                    293,
                    0
                ],
                "title": "Mining Glitch Tokens in Large Language Models via Gradient-based\n  Discrete Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mining Glitch Tokens in Large Language Models via Gradient-based\n  Discrete Optimization"
                },
                "summary": "Glitch tokens in Large Language Models (LLMs) can trigger unpredictable\nbehaviors, threatening model reliability and safety. Existing detection methods\noften depend on predefined patterns, limiting their adaptability across diverse\nLLM architectures. We propose GlitchMiner, a gradient-based discrete\noptimization framework that efficiently identifies glitch tokens by leveraging\nentropy to quantify prediction uncertainty and a local search strategy for\nexploring the token space. Experiments across multiple LLM architectures show\nthat GlitchMiner outperforms existing methods in both detection accuracy and\nadaptability, achieving over 10% average efficiency improvement. GlitchMiner\nenhances vulnerability assessment in LLMs, contributing to more robust and\nreliable applications. Code is available at\nhttps://github.com/wooozihui/GlitchMiner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Glitch tokens in Large Language Models (LLMs) can trigger unpredictable\nbehaviors, threatening model reliability and safety. Existing detection methods\noften depend on predefined patterns, limiting their adaptability across diverse\nLLM architectures. We propose GlitchMiner, a gradient-based discrete\noptimization framework that efficiently identifies glitch tokens by leveraging\nentropy to quantify prediction uncertainty and a local search strategy for\nexploring the token space. Experiments across multiple LLM architectures show\nthat GlitchMiner outperforms existing methods in both detection accuracy and\nadaptability, achieving over 10% average efficiency improvement. GlitchMiner\nenhances vulnerability assessment in LLMs, contributing to more robust and\nreliable applications. Code is available at\nhttps://github.com/wooozihui/GlitchMiner."
                },
                "authors": [
                    {
                        "name": "Zihui Wu"
                    },
                    {
                        "name": "Haichang Gao"
                    },
                    {
                        "name": "Ping Wang"
                    },
                    {
                        "name": "Shudong Zhang"
                    },
                    {
                        "name": "Zhaoxiang Liu"
                    },
                    {
                        "name": "Shiguo Lian"
                    }
                ],
                "author_detail": {
                    "name": "Shiguo Lian"
                },
                "author": "Shiguo Lian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15052v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15052v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05779v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05779v2",
                "updated": "2024-11-07T10:44:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    10,
                    44,
                    59,
                    3,
                    312,
                    0
                ],
                "published": "2024-10-08T08:00:12Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    8,
                    0,
                    12,
                    1,
                    282,
                    0
                ],
                "title": "LightRAG: Simple and Fast Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightRAG: Simple and Fast Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems enhance large language models\n(LLMs) by integrating external knowledge sources, enabling more accurate and\ncontextually relevant responses tailored to user needs. However, existing RAG\nsystems have significant limitations, including reliance on flat data\nrepresentations and inadequate contextual awareness, which can lead to\nfragmented answers that fail to capture complex inter-dependencies. To address\nthese challenges, we propose LightRAG, which incorporates graph structures into\ntext indexing and retrieval processes. This innovative framework employs a\ndual-level retrieval system that enhances comprehensive information retrieval\nfrom both low-level and high-level knowledge discovery. Additionally, the\nintegration of graph structures with vector representations facilitates\nefficient retrieval of related entities and their relationships, significantly\nimproving response times while maintaining contextual relevance. This\ncapability is further enhanced by an incremental update algorithm that ensures\nthe timely integration of new data, allowing the system to remain effective and\nresponsive in rapidly changing data environments. Extensive experimental\nvalidation demonstrates considerable improvements in retrieval accuracy and\nefficiency compared to existing approaches. We have made our LightRAG\nopen-source and available at the link: https://github.com/HKUDS/LightRAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems enhance large language models\n(LLMs) by integrating external knowledge sources, enabling more accurate and\ncontextually relevant responses tailored to user needs. However, existing RAG\nsystems have significant limitations, including reliance on flat data\nrepresentations and inadequate contextual awareness, which can lead to\nfragmented answers that fail to capture complex inter-dependencies. To address\nthese challenges, we propose LightRAG, which incorporates graph structures into\ntext indexing and retrieval processes. This innovative framework employs a\ndual-level retrieval system that enhances comprehensive information retrieval\nfrom both low-level and high-level knowledge discovery. Additionally, the\nintegration of graph structures with vector representations facilitates\nefficient retrieval of related entities and their relationships, significantly\nimproving response times while maintaining contextual relevance. This\ncapability is further enhanced by an incremental update algorithm that ensures\nthe timely integration of new data, allowing the system to remain effective and\nresponsive in rapidly changing data environments. Extensive experimental\nvalidation demonstrates considerable improvements in retrieval accuracy and\nefficiency compared to existing approaches. We have made our LightRAG\nopen-source and available at the link: https://github.com/HKUDS/LightRAG."
                },
                "authors": [
                    {
                        "name": "Zirui Guo"
                    },
                    {
                        "name": "Lianghao Xia"
                    },
                    {
                        "name": "Yanhua Yu"
                    },
                    {
                        "name": "Tu Ao"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05779v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18624v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18624v3",
                "updated": "2024-11-07T10:35:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    10,
                    35,
                    32,
                    3,
                    312,
                    0
                ],
                "published": "2024-06-26T12:50:55Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    12,
                    50,
                    55,
                    2,
                    178,
                    0
                ],
                "title": "Robust Low-Cost Drone Detection and Classification in Low SNR\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Low-Cost Drone Detection and Classification in Low SNR\n  Environments"
                },
                "summary": "The proliferation of drones, or unmanned aerial vehicles (UAVs), has raised\nsignificant safety concerns due to their potential misuse in activities such as\nespionage, smuggling, and infrastructure disruption. This paper addresses the\ncritical need for effective drone detection and classification systems that\noperate independently of UAV cooperation. We evaluate various convolutional\nneural networks (CNNs) for their ability to detect and classify drones using\nspectrogram data derived from consecutive Fourier transforms of signal\ncomponents. The focus is on model robustness in low signal-to-noise ratio (SNR)\nenvironments, which is critical for real-world applications. A comprehensive\ndataset is provided to support future model development. In addition, we\ndemonstrate a low-cost drone detection system using a standard computer,\nsoftware-defined radio (SDR) and antenna, validated through real-world field\ntesting. On our development dataset, all models consistently achieved an\naverage balanced classification accuracy of >= 85% at SNR > -12dB. In the field\ntest, these models achieved an average balance accuracy of > 80%, depending on\ntransmitter distance and antenna direction. Our contributions include: a\npublicly available dataset for model development, a comparative analysis of CNN\nfor drone detection under low SNR conditions, and the deployment and field\nevaluation of a practical, low-cost detection system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of drones, or unmanned aerial vehicles (UAVs), has raised\nsignificant safety concerns due to their potential misuse in activities such as\nespionage, smuggling, and infrastructure disruption. This paper addresses the\ncritical need for effective drone detection and classification systems that\noperate independently of UAV cooperation. We evaluate various convolutional\nneural networks (CNNs) for their ability to detect and classify drones using\nspectrogram data derived from consecutive Fourier transforms of signal\ncomponents. The focus is on model robustness in low signal-to-noise ratio (SNR)\nenvironments, which is critical for real-world applications. A comprehensive\ndataset is provided to support future model development. In addition, we\ndemonstrate a low-cost drone detection system using a standard computer,\nsoftware-defined radio (SDR) and antenna, validated through real-world field\ntesting. On our development dataset, all models consistently achieved an\naverage balanced classification accuracy of >= 85% at SNR > -12dB. In the field\ntest, these models achieved an average balance accuracy of > 80%, depending on\ntransmitter distance and antenna direction. Our contributions include: a\npublicly available dataset for model development, a comparative analysis of CNN\nfor drone detection under low SNR conditions, and the deployment and field\nevaluation of a practical, low-cost detection system."
                },
                "authors": [
                    {
                        "name": "Stefan Glge"
                    },
                    {
                        "name": "Matthias Nyfeler"
                    },
                    {
                        "name": "Ahmad Aghaebrahimian"
                    },
                    {
                        "name": "Nicola Ramagnano"
                    },
                    {
                        "name": "Christof Schpbach"
                    }
                ],
                "author_detail": {
                    "name": "Christof Schpbach"
                },
                "author": "Christof Schpbach",
                "arxiv_doi": "10.1109/JRFID.2024.3487303",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/JRFID.2024.3487303",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.18624v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18624v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "11 pages, 9 figures",
                "arxiv_journal_ref": "IEEE Journal of Radio Frequency Identification, Volume 8, 2024",
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04602v1",
                "updated": "2024-11-07T10:31:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    10,
                    31,
                    31,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T10:31:31Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    10,
                    31,
                    31,
                    3,
                    312,
                    0
                ],
                "title": "Self-Calibrated Listwise Reranking with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Calibrated Listwise Reranking with Large Language Models"
                },
                "summary": "Large language models (LLMs), with advanced linguistic capabilities, have\nbeen employed in reranking tasks through a sequence-to-sequence approach. In\nthis paradigm, multiple passages are reranked in a listwise manner and a\ntextual reranked permutation is generated. However, due to the limited context\nwindow of LLMs, this reranking paradigm requires a sliding window strategy to\niteratively handle larger candidate sets. This not only increases computational\ncosts but also restricts the LLM from fully capturing all the comparison\ninformation for all candidates. To address these challenges, we propose a novel\nself-calibrated listwise reranking method, which aims to leverage LLMs to\nproduce global relevance scores for ranking. To achieve it, we first propose\nthe relevance-aware listwise reranking framework, which incorporates explicit\nlist-view relevance scores to improve reranking efficiency and enable global\ncomparison across the entire candidate set. Second, to ensure the comparability\nof the computed scores, we propose self-calibrated training that uses\npoint-view relevance assessments generated internally by the LLM itself to\ncalibrate the list-view relevance assessments. Extensive experiments and\ncomprehensive analysis on the BEIR benchmark and TREC Deep Learning Tracks\ndemonstrate the effectiveness and efficiency of our proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), with advanced linguistic capabilities, have\nbeen employed in reranking tasks through a sequence-to-sequence approach. In\nthis paradigm, multiple passages are reranked in a listwise manner and a\ntextual reranked permutation is generated. However, due to the limited context\nwindow of LLMs, this reranking paradigm requires a sliding window strategy to\niteratively handle larger candidate sets. This not only increases computational\ncosts but also restricts the LLM from fully capturing all the comparison\ninformation for all candidates. To address these challenges, we propose a novel\nself-calibrated listwise reranking method, which aims to leverage LLMs to\nproduce global relevance scores for ranking. To achieve it, we first propose\nthe relevance-aware listwise reranking framework, which incorporates explicit\nlist-view relevance scores to improve reranking efficiency and enable global\ncomparison across the entire candidate set. Second, to ensure the comparability\nof the computed scores, we propose self-calibrated training that uses\npoint-view relevance assessments generated internally by the LLM itself to\ncalibrate the list-view relevance assessments. Extensive experiments and\ncomprehensive analysis on the BEIR benchmark and TREC Deep Learning Tracks\ndemonstrate the effectiveness and efficiency of our proposed method."
                },
                "authors": [
                    {
                        "name": "Ruiyang Ren"
                    },
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Kun Zhou"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04620v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04620v4",
                "updated": "2024-11-07T10:16:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    10,
                    16,
                    23,
                    3,
                    312,
                    0
                ],
                "published": "2024-02-07T07:07:02Z",
                "published_parsed": [
                    2024,
                    2,
                    7,
                    7,
                    7,
                    2,
                    2,
                    38,
                    0
                ],
                "title": "CataractBot: An LLM-Powered Expert-in-the-Loop Chatbot for Cataract\n  Patients",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CataractBot: An LLM-Powered Expert-in-the-Loop Chatbot for Cataract\n  Patients"
                },
                "summary": "The healthcare landscape is evolving, with patients seeking reliable\ninformation about their health conditions and available treatment options.\nDespite the abundance of information sources, the digital age overwhelms\nindividuals with excess, often inaccurate information. Patients primarily trust\nmedical professionals, highlighting the need for expert-endorsed health\ninformation. However, increased patient loads on experts has led to reduced\ncommunication time, impacting information sharing. To address this gap, we\ndeveloped CataractBot, an experts-in-the-loop chatbot powered by LLMs, in\ncollaboration with an eye hospital in India. CataractBot answers cataract\nsurgery related questions instantly by querying a curated knowledge base and\nprovides expert-verified responses asynchronously. It has multimodal and\nmultilingual capabilities. In an in-the-wild deployment study with 55\nparticipants, CataractBot proved valuable, providing anytime accessibility,\nsaving time, accommodating diverse literacy levels, alleviating power\ndifferences, and adding a privacy layer between patients and doctors. Users\nreported that their trust in the system was established through expert\nverification. Broadly, our results could inform future work on designing\nexpert-mediated LLM bots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The healthcare landscape is evolving, with patients seeking reliable\ninformation about their health conditions and available treatment options.\nDespite the abundance of information sources, the digital age overwhelms\nindividuals with excess, often inaccurate information. Patients primarily trust\nmedical professionals, highlighting the need for expert-endorsed health\ninformation. However, increased patient loads on experts has led to reduced\ncommunication time, impacting information sharing. To address this gap, we\ndeveloped CataractBot, an experts-in-the-loop chatbot powered by LLMs, in\ncollaboration with an eye hospital in India. CataractBot answers cataract\nsurgery related questions instantly by querying a curated knowledge base and\nprovides expert-verified responses asynchronously. It has multimodal and\nmultilingual capabilities. In an in-the-wild deployment study with 55\nparticipants, CataractBot proved valuable, providing anytime accessibility,\nsaving time, accommodating diverse literacy levels, alleviating power\ndifferences, and adding a privacy layer between patients and doctors. Users\nreported that their trust in the system was established through expert\nverification. Broadly, our results could inform future work on designing\nexpert-mediated LLM bots."
                },
                "authors": [
                    {
                        "name": "Pragnya Ramjee"
                    },
                    {
                        "name": "Bhuvan Sachdeva"
                    },
                    {
                        "name": "Satvik Golechha"
                    },
                    {
                        "name": "Shreyas Kulkarni"
                    },
                    {
                        "name": "Geeta Fulari"
                    },
                    {
                        "name": "Kaushik Murali"
                    },
                    {
                        "name": "Mohit Jain"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Jain"
                },
                "author": "Mohit Jain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04620v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04620v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04576v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04576v1",
                "updated": "2024-11-07T09:58:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    9,
                    58,
                    20,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T09:58:20Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    9,
                    58,
                    20,
                    3,
                    312,
                    0
                ],
                "title": "\"I Always Felt that Something Was Wrong.\": Understanding Compliance\n  Risks and Mitigation Strategies when Professionals Use Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"I Always Felt that Something Was Wrong.\": Understanding Compliance\n  Risks and Mitigation Strategies when Professionals Use Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have been increasingly adopted by professionals\nfor work tasks. However, using LLMs also introduces compliance risks relating\nto privacy, ethics, and regulations. This study investigated the compliance\nrisks professionals perceive with LLM use and their risk mitigation strategies.\nSemi-structured interviews were conducted with 24 law, healthcare, and academia\nprofessionals. Results showed that the main compliance concerns centered around\npotential exposure to sensitive customer/patient information through LLMs. To\naddress risks, professionals reported proactively inputting distorted data to\npreserve privacy. However, full compliance proved challenging, given the\ncomplex interactions between user inputs, LLM behaviors, and regulations. This\nresearch provides valuable insights into designing LLMs with built-in privacy\nand risk controls to support professionals' evaluation and adoption of emerging\nAI technologies while meeting compliance obligations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been increasingly adopted by professionals\nfor work tasks. However, using LLMs also introduces compliance risks relating\nto privacy, ethics, and regulations. This study investigated the compliance\nrisks professionals perceive with LLM use and their risk mitigation strategies.\nSemi-structured interviews were conducted with 24 law, healthcare, and academia\nprofessionals. Results showed that the main compliance concerns centered around\npotential exposure to sensitive customer/patient information through LLMs. To\naddress risks, professionals reported proactively inputting distorted data to\npreserve privacy. However, full compliance proved challenging, given the\ncomplex interactions between user inputs, LLM behaviors, and regulations. This\nresearch provides valuable insights into designing LLMs with built-in privacy\nand risk controls to support professionals' evaluation and adoption of emerging\nAI technologies while meeting compliance obligations."
                },
                "authors": [
                    {
                        "name": "Siying Hu"
                    },
                    {
                        "name": "Piaohong Wang"
                    },
                    {
                        "name": "Yaxing Yao"
                    },
                    {
                        "name": "Zhicong Lu"
                    }
                ],
                "author_detail": {
                    "name": "Zhicong Lu"
                },
                "author": "Zhicong Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04576v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04576v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04574v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04574v1",
                "updated": "2024-11-07T09:58:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    9,
                    58,
                    2,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T09:58:02Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    9,
                    58,
                    2,
                    3,
                    312,
                    0
                ],
                "title": "RIS-Assisted Space Shift Keying with Non-Ideal Transceivers and Greedy\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RIS-Assisted Space Shift Keying with Non-Ideal Transceivers and Greedy\n  Detection"
                },
                "summary": "Reconfigurable intelligent surfaces (RIS) and index modulation (IM) represent\nkey technologies for enabling reliable wireless communication with high energy\nefficiency. However, to fully take advantage of these technologies in practical\ndeployments, comprehending the impact of the non-ideal nature of the underlying\ntransceivers is paramount. In this context, this paper introduces two\nRIS-assisted IM communication models, in which the RIS is part of the\ntransmitter and space-shift keying (SSK) is employed for IM, and assesses their\nperformance in the presence of hardware impairments. In the first model, the\nRIS acts as a passive reflector only, reflecting the oncoming SSK modulated\nsignal intelligently towards the desired receive diversity branch/antenna. The\nsecond model employs RIS as a transmitter, employing M-ary phase-shift keying\nfor reflection phase modulation (RPM), and as a reflector for the incoming SSK\nmodulated signal. Considering transmissions subjected to Nakagami-m fading, and\na greedy detection rule at the receiver, the performance of both the system\nconfigurations is evaluated. Specifically, the pairwise probability of\nerroneous index detection and the probability of erroneous index detection are\nadopted as performance metrics, and their closed-form expressions are derived\nfor the RIS-assisted SSK and RIS-assisted SSK-RPM system models. Monte-Carlo\nsimulation studies are carried out to verify the analytical framework, and\nnumerical results are presented to study the dependency of the error\nperformance on the system parameters. The findings highlight the effect of\nhardware impairment on the performance of the communication system under study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfigurable intelligent surfaces (RIS) and index modulation (IM) represent\nkey technologies for enabling reliable wireless communication with high energy\nefficiency. However, to fully take advantage of these technologies in practical\ndeployments, comprehending the impact of the non-ideal nature of the underlying\ntransceivers is paramount. In this context, this paper introduces two\nRIS-assisted IM communication models, in which the RIS is part of the\ntransmitter and space-shift keying (SSK) is employed for IM, and assesses their\nperformance in the presence of hardware impairments. In the first model, the\nRIS acts as a passive reflector only, reflecting the oncoming SSK modulated\nsignal intelligently towards the desired receive diversity branch/antenna. The\nsecond model employs RIS as a transmitter, employing M-ary phase-shift keying\nfor reflection phase modulation (RPM), and as a reflector for the incoming SSK\nmodulated signal. Considering transmissions subjected to Nakagami-m fading, and\na greedy detection rule at the receiver, the performance of both the system\nconfigurations is evaluated. Specifically, the pairwise probability of\nerroneous index detection and the probability of erroneous index detection are\nadopted as performance metrics, and their closed-form expressions are derived\nfor the RIS-assisted SSK and RIS-assisted SSK-RPM system models. Monte-Carlo\nsimulation studies are carried out to verify the analytical framework, and\nnumerical results are presented to study the dependency of the error\nperformance on the system parameters. The findings highlight the effect of\nhardware impairment on the performance of the communication system under study."
                },
                "authors": [
                    {
                        "name": "Aritra Basu"
                    },
                    {
                        "name": "Soumya P. Dash"
                    },
                    {
                        "name": "Sonia Aissa"
                    }
                ],
                "author_detail": {
                    "name": "Sonia Aissa"
                },
                "author": "Sonia Aissa",
                "arxiv_comment": "12 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04574v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04574v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12096v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12096v3",
                "updated": "2024-11-07T09:29:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    9,
                    29,
                    32,
                    3,
                    312,
                    0
                ],
                "published": "2024-04-18T11:29:23Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    11,
                    29,
                    23,
                    3,
                    109,
                    0
                ],
                "title": "LongEmbed: Extending Embedding Models for Long Context Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongEmbed: Extending Embedding Models for Long Context Retrieval"
                },
                "summary": "Embedding models play a pivot role in modern NLP applications such as IR and\nRAG. While the context limit of LLMs has been pushed beyond 1 million tokens,\nembedding models are still confined to a narrow context window not exceeding 8k\ntokens, refrained from application scenarios requiring long inputs such as\nlegal contracts. This paper explores context window extension of existing\nembedding models, pushing the limit to 32k without requiring additional\ntraining. First, we examine the performance of current embedding models for\nlong context retrieval on our newly constructed LongEmbed benchmark. LongEmbed\ncomprises two synthetic tasks and four carefully chosen real-world tasks,\nfeaturing documents of varying length and dispersed target information.\nBenchmarking results underscore huge room for improvement in these models.\nBased on this, comprehensive experiments show that training-free context window\nextension strategies like position interpolation can effectively extend the\ncontext window of existing embedding models by several folds, regardless of\ntheir original context being 512 or beyond 4k. Furthermore, for models\nemploying absolute position encoding (APE), we show the possibility of further\nfine-tuning to harvest notable performance gains while strictly preserving\noriginal behavior for short inputs. For models using rotary position embedding\n(RoPE), significant enhancements are observed when employing RoPE-specific\nmethods, such as NTK and SelfExtend, indicating RoPE's superiority over APE for\ncontext window extension. To facilitate future research, we release E5-Base-4k\nand E5-RoPE-Base, along with the LongEmbed benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedding models play a pivot role in modern NLP applications such as IR and\nRAG. While the context limit of LLMs has been pushed beyond 1 million tokens,\nembedding models are still confined to a narrow context window not exceeding 8k\ntokens, refrained from application scenarios requiring long inputs such as\nlegal contracts. This paper explores context window extension of existing\nembedding models, pushing the limit to 32k without requiring additional\ntraining. First, we examine the performance of current embedding models for\nlong context retrieval on our newly constructed LongEmbed benchmark. LongEmbed\ncomprises two synthetic tasks and four carefully chosen real-world tasks,\nfeaturing documents of varying length and dispersed target information.\nBenchmarking results underscore huge room for improvement in these models.\nBased on this, comprehensive experiments show that training-free context window\nextension strategies like position interpolation can effectively extend the\ncontext window of existing embedding models by several folds, regardless of\ntheir original context being 512 or beyond 4k. Furthermore, for models\nemploying absolute position encoding (APE), we show the possibility of further\nfine-tuning to harvest notable performance gains while strictly preserving\noriginal behavior for short inputs. For models using rotary position embedding\n(RoPE), significant enhancements are observed when employing RoPE-specific\nmethods, such as NTK and SelfExtend, indicating RoPE's superiority over APE for\ncontext window extension. To facilitate future research, we release E5-Base-4k\nand E5-RoPE-Base, along with the LongEmbed benchmark."
                },
                "authors": [
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Nan Yang"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Wenhao Wu"
                    },
                    {
                        "name": "Furu Wei"
                    },
                    {
                        "name": "Sujian Li"
                    }
                ],
                "author_detail": {
                    "name": "Sujian Li"
                },
                "author": "Sujian Li",
                "arxiv_comment": "EMNLP 2024 Camera Ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12096v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12096v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04539v1",
                "updated": "2024-11-07T08:54:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    8,
                    54,
                    46,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T08:54:46Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    8,
                    54,
                    46,
                    3,
                    312,
                    0
                ],
                "title": "Best Practices for Distilling Large Language Models into BERT for Web\n  Search Ranking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Best Practices for Distilling Large Language Models into BERT for Web\n  Search Ranking"
                },
                "summary": "Recent studies have highlighted the significant potential of Large Language\nModels (LLMs) as zero-shot relevance rankers. These methods predominantly\nutilize prompt learning to assess the relevance between queries and documents\nby generating a ranked list of potential documents. Despite their promise, the\nsubstantial costs associated with LLMs pose a significant challenge for their\ndirect implementation in commercial search systems. To overcome this barrier\nand fully exploit the capabilities of LLMs for text ranking, we explore\ntechniques to transfer the ranking expertise of LLMs to a more compact model\nsimilar to BERT, using a ranking loss to enable the deployment of less\nresource-intensive models. Specifically, we enhance the training of LLMs\nthrough Continued Pre-Training, taking the query as input and the clicked title\nand summary as output. We then proceed with supervised fine-tuning of the LLM\nusing a rank loss, assigning the final token as a representative of the entire\nsentence. Given the inherent characteristics of autoregressive language models,\nonly the final token </s> can encapsulate all preceding tokens. Additionally,\nwe introduce a hybrid point-wise and margin MSE loss to transfer the ranking\nknowledge from LLMs to smaller models like BERT. This method creates a viable\nsolution for environments with strict resource constraints. Both offline and\nonline evaluations have confirmed the efficacy of our approach, and our model\nhas been successfully integrated into a commercial web search engine as of\nFebruary 2024.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have highlighted the significant potential of Large Language\nModels (LLMs) as zero-shot relevance rankers. These methods predominantly\nutilize prompt learning to assess the relevance between queries and documents\nby generating a ranked list of potential documents. Despite their promise, the\nsubstantial costs associated with LLMs pose a significant challenge for their\ndirect implementation in commercial search systems. To overcome this barrier\nand fully exploit the capabilities of LLMs for text ranking, we explore\ntechniques to transfer the ranking expertise of LLMs to a more compact model\nsimilar to BERT, using a ranking loss to enable the deployment of less\nresource-intensive models. Specifically, we enhance the training of LLMs\nthrough Continued Pre-Training, taking the query as input and the clicked title\nand summary as output. We then proceed with supervised fine-tuning of the LLM\nusing a rank loss, assigning the final token as a representative of the entire\nsentence. Given the inherent characteristics of autoregressive language models,\nonly the final token </s> can encapsulate all preceding tokens. Additionally,\nwe introduce a hybrid point-wise and margin MSE loss to transfer the ranking\nknowledge from LLMs to smaller models like BERT. This method creates a viable\nsolution for environments with strict resource constraints. Both offline and\nonline evaluations have confirmed the efficacy of our approach, and our model\nhas been successfully integrated into a commercial web search engine as of\nFebruary 2024."
                },
                "authors": [
                    {
                        "name": "Dezhi Ye"
                    },
                    {
                        "name": "Junwei Hu"
                    },
                    {
                        "name": "Jiabin Fan"
                    },
                    {
                        "name": "Bowen Tian"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Haijin Liang"
                    },
                    {
                        "name": "Jin Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jin Ma"
                },
                "author": "Jin Ma",
                "arxiv_comment": "Arxiv Version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04535v1",
                "updated": "2024-11-07T08:48:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    8,
                    48,
                    33,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T08:48:33Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    8,
                    48,
                    33,
                    3,
                    312,
                    0
                ],
                "title": "Meta-Reasoning Improves Tool Use in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meta-Reasoning Improves Tool Use in Large Language Models"
                },
                "summary": "External tools help large language models (LLMs) succeed at tasks where they\nwould otherwise typically fail. In existing frameworks, LLMs learn tool use\neither by in-context demonstrations or via full model fine-tuning on annotated\ndata. As these approaches do not easily scale, a recent trend is to abandon\nthem in favor of lightweight, parameter-efficient tuning paradigms. These\nmethods allow quickly alternating between the frozen LLM and its specialised\nfine-tuned version, by switching on or off a handful of additional custom\nparameters. Hence, we postulate that the generalization ability of the frozen\nmodel can be leveraged to improve tool selection. We present Tool selECTion via\nmeta-reasONing (TECTON), a two-phase system that first reasons over a task\nusing a custom fine-tuned LM head and outputs candidate tools. Then, with the\ncustom head disabled, it meta-reasons (i.e., it reasons over the previous\nreasoning process) to make a final choice. We show that TECTON results in\nsubstantial gains - both in-distribution and out-of-distribution - on a range\nof math reasoning datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "External tools help large language models (LLMs) succeed at tasks where they\nwould otherwise typically fail. In existing frameworks, LLMs learn tool use\neither by in-context demonstrations or via full model fine-tuning on annotated\ndata. As these approaches do not easily scale, a recent trend is to abandon\nthem in favor of lightweight, parameter-efficient tuning paradigms. These\nmethods allow quickly alternating between the frozen LLM and its specialised\nfine-tuned version, by switching on or off a handful of additional custom\nparameters. Hence, we postulate that the generalization ability of the frozen\nmodel can be leveraged to improve tool selection. We present Tool selECTion via\nmeta-reasONing (TECTON), a two-phase system that first reasons over a task\nusing a custom fine-tuned LM head and outputs candidate tools. Then, with the\ncustom head disabled, it meta-reasons (i.e., it reasons over the previous\nreasoning process) to make a final choice. We show that TECTON results in\nsubstantial gains - both in-distribution and out-of-distribution - on a range\nof math reasoning datasets."
                },
                "authors": [
                    {
                        "name": "Lisa Alazraki"
                    },
                    {
                        "name": "Marek Rei"
                    }
                ],
                "author_detail": {
                    "name": "Marek Rei"
                },
                "author": "Marek Rei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17382v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17382v2",
                "updated": "2024-11-07T08:34:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    8,
                    34,
                    27,
                    3,
                    312,
                    0
                ],
                "published": "2024-05-27T17:38:33Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    17,
                    38,
                    33,
                    0,
                    148,
                    0
                ],
                "title": "ReMoDetect: Reward Models Recognize Aligned LLM's Generations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReMoDetect: Reward Models Recognize Aligned LLM's Generations"
                },
                "summary": "The remarkable capabilities and easy accessibility of large language models\n(LLMs) have significantly increased societal risks (e.g., fake news\ngeneration), necessitating the development of LLM-generated text (LGT)\ndetection methods for safe usage. However, detecting LGTs is challenging due to\nthe vast number of LLMs, making it impractical to account for each LLM\nindividually; hence, it is crucial to identify the common characteristics\nshared by these models. In this paper, we draw attention to a common feature of\nrecent powerful LLMs, namely the alignment training, i.e., training LLMs to\ngenerate human-preferable texts. Our key finding is that as these aligned LLMs\nare trained to maximize the human preferences, they generate texts with higher\nestimated preferences even than human-written texts; thus, such texts are\neasily detected by using the reward model (i.e., an LLM trained to model human\npreference distribution). Based on this finding, we propose two training\nschemes to further improve the detection ability of the reward model, namely\n(i) continual preference fine-tuning to make the reward model prefer aligned\nLGTs even further and (ii) reward modeling of Human/LLM mixed texts (a\nrephrased texts from human-written texts using aligned LLMs), which serves as a\nmedian preference text corpus between LGTs and human-written texts to learn the\ndecision boundary better. We provide an extensive evaluation by considering six\ntext domains across twelve aligned LLMs, where our method demonstrates\nstate-of-the-art results. Code is available at\nhttps://github.com/hyunseoklee-ai/ReMoDetect.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable capabilities and easy accessibility of large language models\n(LLMs) have significantly increased societal risks (e.g., fake news\ngeneration), necessitating the development of LLM-generated text (LGT)\ndetection methods for safe usage. However, detecting LGTs is challenging due to\nthe vast number of LLMs, making it impractical to account for each LLM\nindividually; hence, it is crucial to identify the common characteristics\nshared by these models. In this paper, we draw attention to a common feature of\nrecent powerful LLMs, namely the alignment training, i.e., training LLMs to\ngenerate human-preferable texts. Our key finding is that as these aligned LLMs\nare trained to maximize the human preferences, they generate texts with higher\nestimated preferences even than human-written texts; thus, such texts are\neasily detected by using the reward model (i.e., an LLM trained to model human\npreference distribution). Based on this finding, we propose two training\nschemes to further improve the detection ability of the reward model, namely\n(i) continual preference fine-tuning to make the reward model prefer aligned\nLGTs even further and (ii) reward modeling of Human/LLM mixed texts (a\nrephrased texts from human-written texts using aligned LLMs), which serves as a\nmedian preference text corpus between LGTs and human-written texts to learn the\ndecision boundary better. We provide an extensive evaluation by considering six\ntext domains across twelve aligned LLMs, where our method demonstrates\nstate-of-the-art results. Code is available at\nhttps://github.com/hyunseoklee-ai/ReMoDetect."
                },
                "authors": [
                    {
                        "name": "Hyunseok Lee"
                    },
                    {
                        "name": "Jihoon Tack"
                    },
                    {
                        "name": "Jinwoo Shin"
                    }
                ],
                "author_detail": {
                    "name": "Jinwoo Shin"
                },
                "author": "Jinwoo Shin",
                "arxiv_comment": "Published as a conference proceeding for NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17382v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17382v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.13149v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.13149v2",
                "updated": "2024-11-07T08:20:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    8,
                    20,
                    46,
                    3,
                    312,
                    0
                ],
                "published": "2023-08-25T03:05:33Z",
                "published_parsed": [
                    2023,
                    8,
                    25,
                    3,
                    5,
                    33,
                    4,
                    237,
                    0
                ],
                "title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for\n  Scientific Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for\n  Scientific Research"
                },
                "summary": "Recently, there has been growing interest in using Large Language Models\n(LLMs) for scientific research. Numerous benchmarks have been proposed to\nevaluate the ability of LLMs for scientific research. However, current\nbenchmarks are mostly based on pre-collected objective questions. This design\nsuffers from data leakage problem and lacks the evaluation of subjective Q/A\nability. In this paper, we propose SciEval, a comprehensive and\nmulti-disciplinary evaluation benchmark to address these issues. Based on\nBloom's taxonomy, SciEval covers four dimensions to systematically evaluate\nscientific research ability. In particular, we design a \"dynamic\" subset based\non scientific principles to prevent evaluation from potential data leakage.\nBoth objective and subjective questions are included in SciEval. These\ncharacteristics make SciEval a more effective benchmark for scientific research\nability evaluation of LLMs. Comprehensive experiments on most advanced LLMs\nshow that, although GPT-4 achieves SOTA performance compared to other LLMs,\nthere is still substantial room for improvement, especially for dynamic\nquestions. The codes and data are publicly available on\nhttps://github.com/OpenDFM/SciEval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, there has been growing interest in using Large Language Models\n(LLMs) for scientific research. Numerous benchmarks have been proposed to\nevaluate the ability of LLMs for scientific research. However, current\nbenchmarks are mostly based on pre-collected objective questions. This design\nsuffers from data leakage problem and lacks the evaluation of subjective Q/A\nability. In this paper, we propose SciEval, a comprehensive and\nmulti-disciplinary evaluation benchmark to address these issues. Based on\nBloom's taxonomy, SciEval covers four dimensions to systematically evaluate\nscientific research ability. In particular, we design a \"dynamic\" subset based\non scientific principles to prevent evaluation from potential data leakage.\nBoth objective and subjective questions are included in SciEval. These\ncharacteristics make SciEval a more effective benchmark for scientific research\nability evaluation of LLMs. Comprehensive experiments on most advanced LLMs\nshow that, although GPT-4 achieves SOTA performance compared to other LLMs,\nthere is still substantial room for improvement, especially for dynamic\nquestions. The codes and data are publicly available on\nhttps://github.com/OpenDFM/SciEval."
                },
                "authors": [
                    {
                        "name": "Liangtai Sun"
                    },
                    {
                        "name": "Yang Han"
                    },
                    {
                        "name": "Zihan Zhao"
                    },
                    {
                        "name": "Da Ma"
                    },
                    {
                        "name": "Zhennan Shen"
                    },
                    {
                        "name": "Baocai Chen"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "12 pages, 17 figures, 12 tables. Accepted by AAAI 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.13149v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.13149v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04496v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04496v1",
                "updated": "2024-11-07T07:46:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    7,
                    46,
                    6,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T07:46:06Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    7,
                    46,
                    6,
                    3,
                    312,
                    0
                ],
                "title": "Thanos: Enhancing Conversational Agents with Skill-of-Mind-Infused Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thanos: Enhancing Conversational Agents with Skill-of-Mind-Infused Large\n  Language Model"
                },
                "summary": "To increase social bonding with interlocutors, humans naturally acquire the\nability to respond appropriately in a given situation by considering which\nconversational skill is most suitable for the response - a process we call\nskill-of-mind. For large language model (LLM)-based conversational agents,\nplanning appropriate conversational skills, as humans do, is challenging due to\nthe complexity of social dialogue, especially in interactive scenarios. To\naddress this, we propose a skill-of-mind-annotated conversation dataset, named\nMultifaceted Skill-of-Mind, which includes multi-turn and multifaceted\nconversational skills across various interactive scenarios (e.g., long-term,\ncounseling, task-oriented), grounded in diverse social contexts (e.g.,\ndemographics, persona, rules of thumb). This dataset consists of roughly 100K\nconversations. Using this dataset, we introduce a new family of\nskill-of-mind-infused LLMs, named Thanos, with model sizes of 1B, 3B, and 8B\nparameters. With extensive experiments, these models successfully demonstrate\nthe skill-of-mind process and exhibit strong generalizability in inferring\nmultifaceted skills across a variety of domains. Moreover, we show that Thanos\nsignificantly enhances the quality of responses generated by LLM-based\nconversational agents and promotes prosocial behavior in human evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To increase social bonding with interlocutors, humans naturally acquire the\nability to respond appropriately in a given situation by considering which\nconversational skill is most suitable for the response - a process we call\nskill-of-mind. For large language model (LLM)-based conversational agents,\nplanning appropriate conversational skills, as humans do, is challenging due to\nthe complexity of social dialogue, especially in interactive scenarios. To\naddress this, we propose a skill-of-mind-annotated conversation dataset, named\nMultifaceted Skill-of-Mind, which includes multi-turn and multifaceted\nconversational skills across various interactive scenarios (e.g., long-term,\ncounseling, task-oriented), grounded in diverse social contexts (e.g.,\ndemographics, persona, rules of thumb). This dataset consists of roughly 100K\nconversations. Using this dataset, we introduce a new family of\nskill-of-mind-infused LLMs, named Thanos, with model sizes of 1B, 3B, and 8B\nparameters. With extensive experiments, these models successfully demonstrate\nthe skill-of-mind process and exhibit strong generalizability in inferring\nmultifaceted skills across a variety of domains. Moreover, we show that Thanos\nsignificantly enhances the quality of responses generated by LLM-based\nconversational agents and promotes prosocial behavior in human evaluations."
                },
                "authors": [
                    {
                        "name": "Young-Jun Lee"
                    },
                    {
                        "name": "Dokyong Lee"
                    },
                    {
                        "name": "Junyoung Youn"
                    },
                    {
                        "name": "Kyeongjin Oh"
                    },
                    {
                        "name": "Ho-Jin Choi"
                    }
                ],
                "author_detail": {
                    "name": "Ho-Jin Choi"
                },
                "author": "Ho-Jin Choi",
                "arxiv_comment": "Code: https://github.com/passing2961/Thanos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04496v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04496v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14979v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14979v3",
                "updated": "2024-11-07T07:25:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    7,
                    25,
                    4,
                    3,
                    312,
                    0
                ],
                "published": "2024-10-19T05:01:56Z",
                "published_parsed": [
                    2024,
                    10,
                    19,
                    5,
                    1,
                    56,
                    5,
                    293,
                    0
                ],
                "title": "Do Large Language Models Truly Grasp Mathematics? An Empirical\n  Exploration From A Psychological Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Truly Grasp Mathematics? An Empirical\n  Exploration From A Psychological Perspective"
                },
                "summary": "Despite their proficiency in math tasks, the mechanisms underlying LLMs'\nmathematical reasoning abilities remain a subject of debate. Recent studies\nsuggest that chain-of-thought (CoT) prompts can bolster mathematical reasoning\nby encouraging LLMs to employ human-like logical reasoning (System 2), enabling\nthem to excel on the Cognitive Reflection Test (CRT). To assess whether LLMs\ngenuinely possess System 2-like logical reasoning, we introduced targeted\nmodifications to CRT problems. Our findings reveal that, despite the use of CoT\nprompts, mainstream LLMs, including the latest o1-preview model, continue to\nexhibit a significant error rate. Further analysis indicates that they\npredominantly rely on System 1-like intuitive reasoning and pattern matching\nderived from training data, rather than demonstrating mastery of mathematical\nthinking. This discovery challenges the prevailing notion that LLMs possess\ngenuine logical reasoning abilities and that CoT can enhance them.\nConsequently, this work may temper overly optimistic projections regarding\nLLMs' advancement toward artificial general intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their proficiency in math tasks, the mechanisms underlying LLMs'\nmathematical reasoning abilities remain a subject of debate. Recent studies\nsuggest that chain-of-thought (CoT) prompts can bolster mathematical reasoning\nby encouraging LLMs to employ human-like logical reasoning (System 2), enabling\nthem to excel on the Cognitive Reflection Test (CRT). To assess whether LLMs\ngenuinely possess System 2-like logical reasoning, we introduced targeted\nmodifications to CRT problems. Our findings reveal that, despite the use of CoT\nprompts, mainstream LLMs, including the latest o1-preview model, continue to\nexhibit a significant error rate. Further analysis indicates that they\npredominantly rely on System 1-like intuitive reasoning and pattern matching\nderived from training data, rather than demonstrating mastery of mathematical\nthinking. This discovery challenges the prevailing notion that LLMs possess\ngenuine logical reasoning abilities and that CoT can enhance them.\nConsequently, this work may temper overly optimistic projections regarding\nLLMs' advancement toward artificial general intelligence."
                },
                "authors": [
                    {
                        "name": "Wei Xie"
                    },
                    {
                        "name": "Shuoyoucheng Ma"
                    },
                    {
                        "name": "Zhenhua Wang"
                    },
                    {
                        "name": "Enze Wang"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Xiaobing Sun"
                    },
                    {
                        "name": "Baosheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Baosheng Wang"
                },
                "author": "Baosheng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14979v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14979v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04476v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04476v1",
                "updated": "2024-11-07T07:07:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    7,
                    7,
                    34,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T07:07:34Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    7,
                    7,
                    34,
                    3,
                    312,
                    0
                ],
                "title": "LLM-R: A Framework for Domain-Adaptive Maintenance Scheme Generation\n  Combining Hierarchical Agents and RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-R: A Framework for Domain-Adaptive Maintenance Scheme Generation\n  Combining Hierarchical Agents and RAG"
                },
                "summary": "The increasing use of smart devices has emphasized the critical role of\nmaintenance in production activities. Interactive Electronic Technical Manuals\n(IETMs) are vital tools that support the maintenance of smart equipment.\nHowever, traditional IETMs face challenges such as transitioning from Graphical\nUser Interfaces (GUIs) to natural Language User Interfaces (LUIs) and managing\ncomplex logical relationships. Additionally, they must meet the current demands\nfor higher intelligence. This paper proposes a Maintenance Scheme Generation\nMethod based on Large Language Models (LLM-R). The proposed method includes\nseveral key innovations: We propose the Low Rank Adaptation-Knowledge Retention\n(LORA-KR) loss technology to proportionally adjust mixed maintenance data for\nfine-tuning the LLM. This method prevents knowledge conflicts caused by mixed\ndata, improving the model's adaptability and reasoning ability in specific\nmaintenance domains, Besides, Hierarchical Task-Based Agent and\nInstruction-level Retrieval-Augmented Generation (RAG) technologies are adopted\nto optimize the generation steps and mitigate the phenomenon of hallucination\ncaused by the model's Inability to access contextual information. This\nenhancement improves the model's flexibility and accuracy in handling known or\nunknown maintenance objects and maintenance scheme scenarios. To validate the\nproposed method's effectiveness in maintenance tasks, a maintenance scheme\ndataset was constructed using objects from different fields. The experimental\nresults show that the accuracy of the maintenance schemes generated by the\nproposed method reached 91.59%, indicating which improvement enhances the\nintelligence of maintenance schemes and introduces novel technical approaches\nfor equipment maintenance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing use of smart devices has emphasized the critical role of\nmaintenance in production activities. Interactive Electronic Technical Manuals\n(IETMs) are vital tools that support the maintenance of smart equipment.\nHowever, traditional IETMs face challenges such as transitioning from Graphical\nUser Interfaces (GUIs) to natural Language User Interfaces (LUIs) and managing\ncomplex logical relationships. Additionally, they must meet the current demands\nfor higher intelligence. This paper proposes a Maintenance Scheme Generation\nMethod based on Large Language Models (LLM-R). The proposed method includes\nseveral key innovations: We propose the Low Rank Adaptation-Knowledge Retention\n(LORA-KR) loss technology to proportionally adjust mixed maintenance data for\nfine-tuning the LLM. This method prevents knowledge conflicts caused by mixed\ndata, improving the model's adaptability and reasoning ability in specific\nmaintenance domains, Besides, Hierarchical Task-Based Agent and\nInstruction-level Retrieval-Augmented Generation (RAG) technologies are adopted\nto optimize the generation steps and mitigate the phenomenon of hallucination\ncaused by the model's Inability to access contextual information. This\nenhancement improves the model's flexibility and accuracy in handling known or\nunknown maintenance objects and maintenance scheme scenarios. To validate the\nproposed method's effectiveness in maintenance tasks, a maintenance scheme\ndataset was constructed using objects from different fields. The experimental\nresults show that the accuracy of the maintenance schemes generated by the\nproposed method reached 91.59%, indicating which improvement enhances the\nintelligence of maintenance schemes and introduces novel technical approaches\nfor equipment maintenance."
                },
                "authors": [
                    {
                        "name": "Laifa Tao"
                    },
                    {
                        "name": "Qixuan Huang"
                    },
                    {
                        "name": "Xianjun Wu"
                    },
                    {
                        "name": "Weiwei Zhang"
                    },
                    {
                        "name": "Yunlong Wu"
                    },
                    {
                        "name": "Bin Li"
                    },
                    {
                        "name": "Chen Lu"
                    },
                    {
                        "name": "Xingshuo Hai"
                    }
                ],
                "author_detail": {
                    "name": "Xingshuo Hai"
                },
                "author": "Xingshuo Hai",
                "arxiv_comment": "30 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04476v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04476v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11709v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11709v4",
                "updated": "2024-11-07T07:00:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    7,
                    0,
                    14,
                    3,
                    312,
                    0
                ],
                "published": "2024-06-17T16:28:21Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    16,
                    28,
                    21,
                    0,
                    169,
                    0
                ],
                "title": "Instruct, Not Assist: LLM-based Multi-Turn Planning and Hierarchical\n  Questioning for Socratic Code Debugging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruct, Not Assist: LLM-based Multi-Turn Planning and Hierarchical\n  Questioning for Socratic Code Debugging"
                },
                "summary": "Socratic questioning is an effective teaching strategy, encouraging critical\nthinking and problem-solving. The conversational capabilities of large language\nmodels (LLMs) show great potential for providing scalable, real-time student\nguidance. However, current LLMs often give away solutions directly, making them\nineffective instructors. We tackle this issue in the code debugging domain with\nTreeInstruct, an Instructor agent guided by a novel state space-based planning\nalgorithm. TreeInstruct asks probing questions to help students independently\nidentify and resolve errors. It estimates a student's conceptual and\nsyntactical knowledge to dynamically construct a question tree based on their\nresponses and current knowledge state, effectively addressing both independent\nand dependent mistakes concurrently in a multi-turn interaction setting. In\naddition to using an existing single-bug debugging benchmark, we construct a\nmore challenging multi-bug dataset of 150 coding problems, incorrect solutions,\nand bug fixes -- all carefully constructed and annotated by experts. Extensive\nevaluation shows TreeInstruct's state-of-the-art performance on both datasets,\nproving it to be a more effective instructor than baselines. Furthermore, a\nreal-world case study with five students of varying skill levels further\ndemonstrates TreeInstruct's ability to guide students to debug their code\nefficiently with minimal turns and highly Socratic questioning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Socratic questioning is an effective teaching strategy, encouraging critical\nthinking and problem-solving. The conversational capabilities of large language\nmodels (LLMs) show great potential for providing scalable, real-time student\nguidance. However, current LLMs often give away solutions directly, making them\nineffective instructors. We tackle this issue in the code debugging domain with\nTreeInstruct, an Instructor agent guided by a novel state space-based planning\nalgorithm. TreeInstruct asks probing questions to help students independently\nidentify and resolve errors. It estimates a student's conceptual and\nsyntactical knowledge to dynamically construct a question tree based on their\nresponses and current knowledge state, effectively addressing both independent\nand dependent mistakes concurrently in a multi-turn interaction setting. In\naddition to using an existing single-bug debugging benchmark, we construct a\nmore challenging multi-bug dataset of 150 coding problems, incorrect solutions,\nand bug fixes -- all carefully constructed and annotated by experts. Extensive\nevaluation shows TreeInstruct's state-of-the-art performance on both datasets,\nproving it to be a more effective instructor than baselines. Furthermore, a\nreal-world case study with five students of varying skill levels further\ndemonstrates TreeInstruct's ability to guide students to debug their code\nefficiently with minimal turns and highly Socratic questioning."
                },
                "authors": [
                    {
                        "name": "Priyanka Kargupta"
                    },
                    {
                        "name": "Ishika Agarwal"
                    },
                    {
                        "name": "Dilek Hakkani-Tur"
                    },
                    {
                        "name": "Jiawei Han"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Han"
                },
                "author": "Jiawei Han",
                "arxiv_comment": "Code available at: https://github.com/agarwalishika/TreeInstruct\n  Accepted at EMNLP'24 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11709v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11709v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04070v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04070v5",
                "updated": "2024-11-07T06:21:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    6,
                    21,
                    14,
                    3,
                    312,
                    0
                ],
                "published": "2024-10-05T08:00:55Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    8,
                    0,
                    55,
                    5,
                    279,
                    0
                ],
                "title": "PAD: Personalized Alignment of LLMs at Decoding-Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAD: Personalized Alignment of LLMs at Decoding-Time"
                },
                "summary": "Aligning with personalized preferences, which vary significantly across\ncultural, educational, and political differences, poses a significant challenge\ndue to the computational costs and data demands of traditional alignment\nmethods. In response, this paper presents Personalized Alignment at\nDecoding-time (PAD), a novel framework designed to align LLM outputs with\ndiverse personalized preferences during the inference phase, eliminating the\nneed for additional training. By introducing a unique personalized reward\nmodeling strategy, this framework decouples the text generation process from\npersonalized preferences, facilitating the generation of generalizable\ntoken-level personalized rewards. The PAD algorithm leverages these rewards to\nguide the decoding process, dynamically tailoring the base model's predictions\nto personalized preferences. Extensive experimental results demonstrate that\nPAD not only outperforms existing training-based alignment methods in terms of\naligning with diverse preferences but also shows significant generalizability\nto preferences unseen during training and scalability across different base\nmodels. This work advances the capability of LLMs to meet user needs in\nreal-time applications, presenting a substantial step forward in personalized\nLLM alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning with personalized preferences, which vary significantly across\ncultural, educational, and political differences, poses a significant challenge\ndue to the computational costs and data demands of traditional alignment\nmethods. In response, this paper presents Personalized Alignment at\nDecoding-time (PAD), a novel framework designed to align LLM outputs with\ndiverse personalized preferences during the inference phase, eliminating the\nneed for additional training. By introducing a unique personalized reward\nmodeling strategy, this framework decouples the text generation process from\npersonalized preferences, facilitating the generation of generalizable\ntoken-level personalized rewards. The PAD algorithm leverages these rewards to\nguide the decoding process, dynamically tailoring the base model's predictions\nto personalized preferences. Extensive experimental results demonstrate that\nPAD not only outperforms existing training-based alignment methods in terms of\naligning with diverse preferences but also shows significant generalizability\nto preferences unseen during training and scalability across different base\nmodels. This work advances the capability of LLMs to meet user needs in\nreal-time applications, presenting a substantial step forward in personalized\nLLM alignment."
                },
                "authors": [
                    {
                        "name": "Ruizhe Chen"
                    },
                    {
                        "name": "Xiaotian Zhang"
                    },
                    {
                        "name": "Meng Luo"
                    },
                    {
                        "name": "Wenhao Chai"
                    },
                    {
                        "name": "Zuozhu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zuozhu Liu"
                },
                "author": "Zuozhu Liu",
                "arxiv_comment": "This paper presents Personalized Alignment at Decoding-time (PAD), a\n  novel framework designed to align LLM outputs with diverse personalized\n  preferences during the inference phase",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04070v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04070v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00588v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00588v2",
                "updated": "2024-11-07T05:58:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    5,
                    58,
                    27,
                    3,
                    312,
                    0
                ],
                "published": "2024-09-01T02:47:50Z",
                "published_parsed": [
                    2024,
                    9,
                    1,
                    2,
                    47,
                    50,
                    6,
                    245,
                    0
                ],
                "title": "Diffusion Policy Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policy Policy Optimization"
                },
                "summary": "We introduce Diffusion Policy Policy Optimization, DPPO, an algorithmic\nframework including best practices for fine-tuning diffusion-based policies\n(e.g. Diffusion Policy) in continuous control and robot learning tasks using\nthe policy gradient (PG) method from reinforcement learning (RL). PG methods\nare ubiquitous in training RL policies with other policy parameterizations;\nnevertheless, they had been conjectured to be less efficient for\ndiffusion-based policies. Surprisingly, we show that DPPO achieves the\nstrongest overall performance and efficiency for fine-tuning in common\nbenchmarks compared to other RL methods for diffusion-based policies and also\ncompared to PG fine-tuning of other policy parameterizations. Through\nexperimental investigation, we find that DPPO takes advantage of unique\nsynergies between RL fine-tuning and the diffusion parameterization, leading to\nstructured and on-manifold exploration, stable training, and strong policy\nrobustness. We further demonstrate the strengths of DPPO in a range of\nrealistic settings, including simulated robotic tasks with pixel observations,\nand via zero-shot deployment of simulation-trained policies on robot hardware\nin a long-horizon, multi-stage manipulation task. Website with code:\ndiffusion-ppo.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Diffusion Policy Policy Optimization, DPPO, an algorithmic\nframework including best practices for fine-tuning diffusion-based policies\n(e.g. Diffusion Policy) in continuous control and robot learning tasks using\nthe policy gradient (PG) method from reinforcement learning (RL). PG methods\nare ubiquitous in training RL policies with other policy parameterizations;\nnevertheless, they had been conjectured to be less efficient for\ndiffusion-based policies. Surprisingly, we show that DPPO achieves the\nstrongest overall performance and efficiency for fine-tuning in common\nbenchmarks compared to other RL methods for diffusion-based policies and also\ncompared to PG fine-tuning of other policy parameterizations. Through\nexperimental investigation, we find that DPPO takes advantage of unique\nsynergies between RL fine-tuning and the diffusion parameterization, leading to\nstructured and on-manifold exploration, stable training, and strong policy\nrobustness. We further demonstrate the strengths of DPPO in a range of\nrealistic settings, including simulated robotic tasks with pixel observations,\nand via zero-shot deployment of simulation-trained policies on robot hardware\nin a long-horizon, multi-stage manipulation task. Website with code:\ndiffusion-ppo.github.io"
                },
                "authors": [
                    {
                        "name": "Allen Z. Ren"
                    },
                    {
                        "name": "Justin Lidard"
                    },
                    {
                        "name": "Lars L. Ankile"
                    },
                    {
                        "name": "Anthony Simeonov"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    },
                    {
                        "name": "Anirudha Majumdar"
                    },
                    {
                        "name": "Benjamin Burchfiel"
                    },
                    {
                        "name": "Hongkai Dai"
                    },
                    {
                        "name": "Max Simchowitz"
                    }
                ],
                "author_detail": {
                    "name": "Max Simchowitz"
                },
                "author": "Max Simchowitz",
                "arxiv_comment": "Website: diffusion-ppo.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00588v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00588v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04448v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04448v1",
                "updated": "2024-11-07T05:43:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    5,
                    43,
                    50,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T05:43:50Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    5,
                    43,
                    50,
                    3,
                    312,
                    0
                ],
                "title": "Gradient Localization Improves Lifelong Pretraining of Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradient Localization Improves Lifelong Pretraining of Language Models"
                },
                "summary": "Large Language Models (LLMs) trained on web-scale text corpora have been\nshown to capture world knowledge in their parameters. However, the mechanism by\nwhich language models store different types of knowledge is poorly understood.\nIn this work, we examine two types of knowledge relating to temporally\nsensitive entities and demonstrate that each type is localized to different\nsets of parameters within the LLMs. We hypothesize that the lack of\nconsideration of the locality of knowledge in existing continual learning\nmethods contributes to both: the failed uptake of new information, and\ncatastrophic forgetting of previously learned information. We observe that\nsequences containing references to updated and newly mentioned entities exhibit\nlarger gradient norms in a subset of layers. We demonstrate that targeting\nparameter updates to these relevant layers can improve the performance of\ncontinually pretraining on language containing temporal drift.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) trained on web-scale text corpora have been\nshown to capture world knowledge in their parameters. However, the mechanism by\nwhich language models store different types of knowledge is poorly understood.\nIn this work, we examine two types of knowledge relating to temporally\nsensitive entities and demonstrate that each type is localized to different\nsets of parameters within the LLMs. We hypothesize that the lack of\nconsideration of the locality of knowledge in existing continual learning\nmethods contributes to both: the failed uptake of new information, and\ncatastrophic forgetting of previously learned information. We observe that\nsequences containing references to updated and newly mentioned entities exhibit\nlarger gradient norms in a subset of layers. We demonstrate that targeting\nparameter updates to these relevant layers can improve the performance of\ncontinually pretraining on language containing temporal drift."
                },
                "authors": [
                    {
                        "name": "Jared Fernandez"
                    },
                    {
                        "name": "Yonatan Bisk"
                    },
                    {
                        "name": "Emma Strubell"
                    }
                ],
                "author_detail": {
                    "name": "Emma Strubell"
                },
                "author": "Emma Strubell",
                "arxiv_comment": "EMNLP Findings 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04448v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04448v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18412v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18412v2",
                "updated": "2024-11-07T05:38:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    5,
                    38,
                    31,
                    3,
                    312,
                    0
                ],
                "published": "2024-09-27T03:00:29Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    3,
                    0,
                    29,
                    4,
                    271,
                    0
                ],
                "title": "SciDFM: A Large Language Model with Mixture-of-Experts for Science",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SciDFM: A Large Language Model with Mixture-of-Experts for Science"
                },
                "summary": "Recently, there has been a significant upsurge of interest in leveraging\nlarge language models (LLMs) to assist scientific discovery. However, most LLMs\nonly focus on general science, while they lack domain-specific knowledge, such\nas chemical molecules and amino acid sequences. To bridge these gaps, we\nintroduce SciDFM, a mixture-of-experts LLM, which is trained from scratch and\nis able to conduct college-level scientific reasoning and understand molecules\nand amino acid sequences. We collect a large-scale training corpus containing\nnumerous scientific papers and books from different disciplines as well as data\nfrom domain-specific databases. We further fine-tune the pre-trained model on\nlots of instruction data to improve performances on downstream benchmarks. From\nexperiment results, we show that SciDFM achieves strong performance on general\nscientific benchmarks such as SciEval and SciQ, and it reaches a SOTA\nperformance on domain-specific benchmarks among models of similar size. We\nfurther analyze the expert layers and show that the results of expert selection\nvary with data from different disciplines. To benefit the broader research\ncommunity, we open-source SciDFM at\nhttps://huggingface.co/OpenDFM/SciDFM-MoE-A5.6B-v1.0.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, there has been a significant upsurge of interest in leveraging\nlarge language models (LLMs) to assist scientific discovery. However, most LLMs\nonly focus on general science, while they lack domain-specific knowledge, such\nas chemical molecules and amino acid sequences. To bridge these gaps, we\nintroduce SciDFM, a mixture-of-experts LLM, which is trained from scratch and\nis able to conduct college-level scientific reasoning and understand molecules\nand amino acid sequences. We collect a large-scale training corpus containing\nnumerous scientific papers and books from different disciplines as well as data\nfrom domain-specific databases. We further fine-tune the pre-trained model on\nlots of instruction data to improve performances on downstream benchmarks. From\nexperiment results, we show that SciDFM achieves strong performance on general\nscientific benchmarks such as SciEval and SciQ, and it reaches a SOTA\nperformance on domain-specific benchmarks among models of similar size. We\nfurther analyze the expert layers and show that the results of expert selection\nvary with data from different disciplines. To benefit the broader research\ncommunity, we open-source SciDFM at\nhttps://huggingface.co/OpenDFM/SciDFM-MoE-A5.6B-v1.0."
                },
                "authors": [
                    {
                        "name": "Liangtai Sun"
                    },
                    {
                        "name": "Danyu Luo"
                    },
                    {
                        "name": "Da Ma"
                    },
                    {
                        "name": "Zihan Zhao"
                    },
                    {
                        "name": "Baocai Chen"
                    },
                    {
                        "name": "Zhennan Shen"
                    },
                    {
                        "name": "Su Zhu"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Xin Chen"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "12 pages, 1 figure, 9 tables. Technical Report, accepted by NeurIPS\n  2024 Workshop FM4Science",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18412v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18412v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04444v1",
                "updated": "2024-11-07T05:35:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    5,
                    35,
                    55,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T05:35:55Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    5,
                    35,
                    55,
                    3,
                    312,
                    0
                ],
                "title": "An Empirical Study on the Potential of LLMs in Automated Software\n  Refactoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study on the Potential of LLMs in Automated Software\n  Refactoring"
                },
                "summary": "Recent advances in large language models (LLMs), make it potentially feasible\nto automatically refactor source code with LLMs. However, it remains unclear\nhow well LLMs perform compared to human experts in conducting refactorings\nautomatically and accurately. To fill this gap, in this paper, we conduct an\nempirical study to investigate the potential of LLMs in automated software\nrefactoring, focusing on the identification of refactoring opportunities and\nthe recommendation of refactoring solutions. We first construct a high-quality\nrefactoring dataset comprising 180 real-world refactorings from 20 projects,\nand conduct the empirical study on the dataset. With the to-be-refactored Java\ndocuments as input, ChatGPT and Gemini identified only 28 and 7 respectively\nout of the 180 refactoring opportunities. However, explaining the expected\nrefactoring subcategories and narrowing the search space in the prompts\nsubstantially increased the success rate of ChatGPT from 15.6% to 86.7%.\nConcerning the recommendation of refactoring solutions, ChatGPT recommended 176\nrefactoring solutions for the 180 refactorings, and 63.6% of the recommended\nsolutions were comparable to (even better than) those constructed by human\nexperts. However, 13 out of the 176 solutions suggested by ChatGPT and 9 out of\nthe 137 solutions suggested by Gemini were unsafe in that they either changed\nthe functionality of the source code or introduced syntax errors, which\nindicate the risk of LLM-based refactoring. To this end, we propose a\ndetect-and-reapply tactic, called RefactoringMirror, to avoid such unsafe\nrefactorings. By reapplying the identified refactorings to the original code\nusing thoroughly tested refactoring engines, we can effectively mitigate the\nrisks associated with LLM-based automated refactoring while still leveraging\nLLM's intelligence to obtain valuable refactoring recommendations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs), make it potentially feasible\nto automatically refactor source code with LLMs. However, it remains unclear\nhow well LLMs perform compared to human experts in conducting refactorings\nautomatically and accurately. To fill this gap, in this paper, we conduct an\nempirical study to investigate the potential of LLMs in automated software\nrefactoring, focusing on the identification of refactoring opportunities and\nthe recommendation of refactoring solutions. We first construct a high-quality\nrefactoring dataset comprising 180 real-world refactorings from 20 projects,\nand conduct the empirical study on the dataset. With the to-be-refactored Java\ndocuments as input, ChatGPT and Gemini identified only 28 and 7 respectively\nout of the 180 refactoring opportunities. However, explaining the expected\nrefactoring subcategories and narrowing the search space in the prompts\nsubstantially increased the success rate of ChatGPT from 15.6% to 86.7%.\nConcerning the recommendation of refactoring solutions, ChatGPT recommended 176\nrefactoring solutions for the 180 refactorings, and 63.6% of the recommended\nsolutions were comparable to (even better than) those constructed by human\nexperts. However, 13 out of the 176 solutions suggested by ChatGPT and 9 out of\nthe 137 solutions suggested by Gemini were unsafe in that they either changed\nthe functionality of the source code or introduced syntax errors, which\nindicate the risk of LLM-based refactoring. To this end, we propose a\ndetect-and-reapply tactic, called RefactoringMirror, to avoid such unsafe\nrefactorings. By reapplying the identified refactorings to the original code\nusing thoroughly tested refactoring engines, we can effectively mitigate the\nrisks associated with LLM-based automated refactoring while still leveraging\nLLM's intelligence to obtain valuable refactoring recommendations."
                },
                "authors": [
                    {
                        "name": "Bo Liu"
                    },
                    {
                        "name": "Yanjie Jiang"
                    },
                    {
                        "name": "Yuxia Zhang"
                    },
                    {
                        "name": "Nan Niu"
                    },
                    {
                        "name": "Guangjie Li"
                    },
                    {
                        "name": "Hui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hui Liu"
                },
                "author": "Hui Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04440v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04440v1",
                "updated": "2024-11-07T05:23:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    5,
                    23,
                    31,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T05:23:31Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    5,
                    23,
                    31,
                    3,
                    312,
                    0
                ],
                "title": "AutoProteinEngine: A Large Language Model Driven Agent Framework for\n  Multimodal AutoML in Protein Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoProteinEngine: A Large Language Model Driven Agent Framework for\n  Multimodal AutoML in Protein Engineering"
                },
                "summary": "Protein engineering is important for biomedical applications, but\nconventional approaches are often inefficient and resource-intensive. While\ndeep learning (DL) models have shown promise, their training or implementation\ninto protein engineering remains challenging for biologists without specialized\ncomputational expertise. To address this gap, we propose AutoProteinEngine\n(AutoPE), an agent framework that leverages large language models (LLMs) for\nmultimodal automated machine learning (AutoML) for protein engineering. AutoPE\ninnovatively allows biologists without DL backgrounds to interact with DL\nmodels using natural language, lowering the entry barrier for protein\nengineering tasks. Our AutoPE uniquely integrates LLMs with AutoML to handle\nmodel selection for both protein sequence and graph modalities, automatic\nhyperparameter optimization, and automated data retrieval from protein\ndatabases. We evaluated AutoPE through two real-world protein engineering\ntasks, demonstrating substantial performance improvements compared to\ntraditional zero-shot and manual fine-tuning approaches. By bridging the gap\nbetween DL and biologists' domain expertise, AutoPE empowers researchers to\nleverage DL without extensive programming knowledge. Our code is available at\nhttps://github.com/tsynbio/AutoPE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protein engineering is important for biomedical applications, but\nconventional approaches are often inefficient and resource-intensive. While\ndeep learning (DL) models have shown promise, their training or implementation\ninto protein engineering remains challenging for biologists without specialized\ncomputational expertise. To address this gap, we propose AutoProteinEngine\n(AutoPE), an agent framework that leverages large language models (LLMs) for\nmultimodal automated machine learning (AutoML) for protein engineering. AutoPE\ninnovatively allows biologists without DL backgrounds to interact with DL\nmodels using natural language, lowering the entry barrier for protein\nengineering tasks. Our AutoPE uniquely integrates LLMs with AutoML to handle\nmodel selection for both protein sequence and graph modalities, automatic\nhyperparameter optimization, and automated data retrieval from protein\ndatabases. We evaluated AutoPE through two real-world protein engineering\ntasks, demonstrating substantial performance improvements compared to\ntraditional zero-shot and manual fine-tuning approaches. By bridging the gap\nbetween DL and biologists' domain expertise, AutoPE empowers researchers to\nleverage DL without extensive programming knowledge. Our code is available at\nhttps://github.com/tsynbio/AutoPE."
                },
                "authors": [
                    {
                        "name": "Yungeng Liu"
                    },
                    {
                        "name": "Zan Chen"
                    },
                    {
                        "name": "Yu Guang Wang"
                    },
                    {
                        "name": "Yiqing Shen"
                    }
                ],
                "author_detail": {
                    "name": "Yiqing Shen"
                },
                "author": "Yiqing Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04440v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04440v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18032v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18032v2",
                "updated": "2024-11-07T05:10:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    5,
                    10,
                    20,
                    3,
                    312,
                    0
                ],
                "published": "2024-10-23T17:02:59Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    2,
                    59,
                    2,
                    297,
                    0
                ],
                "title": "GraphTeam: Facilitating Large Language Model-based Graph Analysis via\n  Multi-Agent Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphTeam: Facilitating Large Language Model-based Graph Analysis via\n  Multi-Agent Collaboration"
                },
                "summary": "Graphs are widely used for modeling relational data in real-world scenarios,\nsuch as social networks and urban computing. Existing LLM-based graph analysis\napproaches either integrate graph neural networks (GNNs) for specific machine\nlearning tasks, limiting their transferability, or rely solely on LLMs'\ninternal reasoning ability, resulting in suboptimal performance. To address\nthese limitations, we take advantage of recent advances in LLM-based agents,\nwhich have shown capabilities of utilizing external knowledge or tools for\nproblem solving. By simulating human problem-solving strategies such as analogy\nand collaboration, we propose a multi-agent system based on LLMs named\nGraphTeam, for graph analysis. GraphTeam consists of five LLM-based agents from\nthree modules, and the agents with different specialities can collaborate with\neach other to address complex problems. Specifically, (1) input-output\nnormalization module: the question agent extracts and refines four key\narguments from the original question, facilitating the problem understanding,\nand the answer agent organizes the results to meet the output requirement; (2)\nexternal knowledge retrieval module: we first build a knowledge base consisting\nof relevant documentation and experience information, and then the search agent\nretrieves the most relevant entries for each question. (3) problem-solving\nmodule: given the retrieved information from search agent, the coding agent\nuses established algorithms via programming to generate solutions, and in case\nthe coding agent does not work, the reasoning agent will directly compute the\nresults without programming. Extensive experiments on six graph analysis\nbenchmarks demonstrate that GraphTeam achieves state-of-the-art performance\nwith an average 25.85% improvement over the best baseline in terms of accuracy.\nThe code and data are available at https://github.com/BUPT-GAMMA/GraphTeam.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphs are widely used for modeling relational data in real-world scenarios,\nsuch as social networks and urban computing. Existing LLM-based graph analysis\napproaches either integrate graph neural networks (GNNs) for specific machine\nlearning tasks, limiting their transferability, or rely solely on LLMs'\ninternal reasoning ability, resulting in suboptimal performance. To address\nthese limitations, we take advantage of recent advances in LLM-based agents,\nwhich have shown capabilities of utilizing external knowledge or tools for\nproblem solving. By simulating human problem-solving strategies such as analogy\nand collaboration, we propose a multi-agent system based on LLMs named\nGraphTeam, for graph analysis. GraphTeam consists of five LLM-based agents from\nthree modules, and the agents with different specialities can collaborate with\neach other to address complex problems. Specifically, (1) input-output\nnormalization module: the question agent extracts and refines four key\narguments from the original question, facilitating the problem understanding,\nand the answer agent organizes the results to meet the output requirement; (2)\nexternal knowledge retrieval module: we first build a knowledge base consisting\nof relevant documentation and experience information, and then the search agent\nretrieves the most relevant entries for each question. (3) problem-solving\nmodule: given the retrieved information from search agent, the coding agent\nuses established algorithms via programming to generate solutions, and in case\nthe coding agent does not work, the reasoning agent will directly compute the\nresults without programming. Extensive experiments on six graph analysis\nbenchmarks demonstrate that GraphTeam achieves state-of-the-art performance\nwith an average 25.85% improvement over the best baseline in terms of accuracy.\nThe code and data are available at https://github.com/BUPT-GAMMA/GraphTeam."
                },
                "authors": [
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Qizhi Chu"
                    },
                    {
                        "name": "Yubin Chen"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Yaoqi Liu"
                    },
                    {
                        "name": "Zekai Yu"
                    },
                    {
                        "name": "Weize Chen"
                    },
                    {
                        "name": "Chen Qian"
                    },
                    {
                        "name": "Chuan Shi"
                    },
                    {
                        "name": "Cheng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Cheng Yang"
                },
                "author": "Cheng Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18032v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18032v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04427v1",
                "updated": "2024-11-07T04:38:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    4,
                    38,
                    58,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T04:38:58Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    4,
                    38,
                    58,
                    3,
                    312,
                    0
                ],
                "title": "One fish, two fish, but not the whole sea: Alignment reduces language\n  models' conceptual diversity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One fish, two fish, but not the whole sea: Alignment reduces language\n  models' conceptual diversity"
                },
                "summary": "Researchers in social science and psychology have recently proposed using\nlarge language models (LLMs) as replacements for humans in behavioral research.\nIn addition to arguments about whether LLMs accurately capture population-level\npatterns, this has raised questions about whether LLMs capture human-like\nconceptual diversity. Separately, it is debated whether post-training alignment\n(RLHF or RLAIF) affects models' internal diversity. Inspired by human studies,\nwe use a new way of measuring the conceptual diversity of\nsynthetically-generated LLM \"populations\" by relating the internal variability\nof simulated individuals to the population-level variability. We use this\napproach to evaluate non-aligned and aligned LLMs on two domains with rich\nhuman behavioral data. While no model reaches human-like diversity, aligned\nmodels generally display less diversity than their instruction fine-tuned\ncounterparts. Our findings highlight potential trade-offs between increasing\nmodels' value alignment and decreasing the diversity of their conceptual\nrepresentations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Researchers in social science and psychology have recently proposed using\nlarge language models (LLMs) as replacements for humans in behavioral research.\nIn addition to arguments about whether LLMs accurately capture population-level\npatterns, this has raised questions about whether LLMs capture human-like\nconceptual diversity. Separately, it is debated whether post-training alignment\n(RLHF or RLAIF) affects models' internal diversity. Inspired by human studies,\nwe use a new way of measuring the conceptual diversity of\nsynthetically-generated LLM \"populations\" by relating the internal variability\nof simulated individuals to the population-level variability. We use this\napproach to evaluate non-aligned and aligned LLMs on two domains with rich\nhuman behavioral data. While no model reaches human-like diversity, aligned\nmodels generally display less diversity than their instruction fine-tuned\ncounterparts. Our findings highlight potential trade-offs between increasing\nmodels' value alignment and decreasing the diversity of their conceptual\nrepresentations."
                },
                "authors": [
                    {
                        "name": "Sonia K. Murthy"
                    },
                    {
                        "name": "Tomer Ullman"
                    },
                    {
                        "name": "Jennifer Hu"
                    }
                ],
                "author_detail": {
                    "name": "Jennifer Hu"
                },
                "author": "Jennifer Hu",
                "arxiv_comment": "17 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04425v1",
                "updated": "2024-11-07T04:38:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    4,
                    38,
                    29,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T04:38:29Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    4,
                    38,
                    29,
                    3,
                    312,
                    0
                ],
                "title": "DELIFT: Data Efficient Language model Instruction Fine Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DELIFT: Data Efficient Language model Instruction Fine Tuning"
                },
                "summary": "Fine-tuning large language models (LLMs) is essential for enhancing their\nperformance on specific tasks but is often resource-intensive due to redundant\nor uninformative data. To address this inefficiency, we introduce DELIFT (Data\nEfficient Language model Instruction Fine-Tuning), a novel algorithm that\nsystematically optimizes data selection across the three key stages of\nfine-tuning: (1) instruction tuning, (2) task-specific fine-tuning (e.g.,\nreasoning, question-answering), and (3) continual fine-tuning (e.g.,\nincorporating new data versions). Unlike existing methods that focus on\nsingle-stage optimization or rely on computationally intensive gradient\ncalculations, DELIFT operates efficiently across all stages. Central to our\napproach is a pairwise utility metric that quantifies how beneficial a data\nsample is for improving the model's responses to other samples, effectively\nmeasuring the informational value relative to the model's current capabilities.\nBy leveraging different submodular functions applied to this metric, DELIFT\nselects diverse and optimal subsets that are useful across all stages of\nfine-tuning. Experiments across various tasks and model scales demonstrate that\nDELIFT can reduce the fine-tuning data size by up to 70% without compromising\nperformance, offering significant computational savings and outperforming\nexisting methods in both efficiency and efficacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) is essential for enhancing their\nperformance on specific tasks but is often resource-intensive due to redundant\nor uninformative data. To address this inefficiency, we introduce DELIFT (Data\nEfficient Language model Instruction Fine-Tuning), a novel algorithm that\nsystematically optimizes data selection across the three key stages of\nfine-tuning: (1) instruction tuning, (2) task-specific fine-tuning (e.g.,\nreasoning, question-answering), and (3) continual fine-tuning (e.g.,\nincorporating new data versions). Unlike existing methods that focus on\nsingle-stage optimization or rely on computationally intensive gradient\ncalculations, DELIFT operates efficiently across all stages. Central to our\napproach is a pairwise utility metric that quantifies how beneficial a data\nsample is for improving the model's responses to other samples, effectively\nmeasuring the informational value relative to the model's current capabilities.\nBy leveraging different submodular functions applied to this metric, DELIFT\nselects diverse and optimal subsets that are useful across all stages of\nfine-tuning. Experiments across various tasks and model scales demonstrate that\nDELIFT can reduce the fine-tuning data size by up to 70% without compromising\nperformance, offering significant computational savings and outperforming\nexisting methods in both efficiency and efficacy."
                },
                "authors": [
                    {
                        "name": "Ishika Agarwal"
                    },
                    {
                        "name": "Krishna Killamsetty"
                    },
                    {
                        "name": "Lucian Popa"
                    },
                    {
                        "name": "Marina Danilevksy"
                    }
                ],
                "author_detail": {
                    "name": "Marina Danilevksy"
                },
                "author": "Marina Danilevksy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04424v1",
                "updated": "2024-11-07T04:32:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    4,
                    32,
                    40,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T04:32:40Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    4,
                    32,
                    40,
                    3,
                    312,
                    0
                ],
                "title": "Bayesian Calibration of Win Rate Estimation with LLM Evaluators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Calibration of Win Rate Estimation with LLM Evaluators"
                },
                "summary": "Recent advances in large language models (LLMs) show the potential of using\nLLMs as evaluators for assessing the quality of text generations from LLMs.\nHowever, applying LLM evaluators naively to compare or judge between different\nsystems can lead to unreliable results due to the intrinsic win rate estimation\nbias of LLM evaluators. In order to mitigate this problem, we propose two\ncalibration methods, Bayesian Win Rate Sampling (BWRS) and Bayesian\nDawid-Skene, both of which leverage Bayesian inference to more accurately infer\nthe true win rate of generative language models. We empirically validate our\nmethods on six datasets covering story generation, summarization, and\ninstruction following tasks. We show that both our methods are effective in\nimproving the accuracy of win rate estimation using LLMs as evaluators,\noffering a promising direction for reliable automatic text quality evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) show the potential of using\nLLMs as evaluators for assessing the quality of text generations from LLMs.\nHowever, applying LLM evaluators naively to compare or judge between different\nsystems can lead to unreliable results due to the intrinsic win rate estimation\nbias of LLM evaluators. In order to mitigate this problem, we propose two\ncalibration methods, Bayesian Win Rate Sampling (BWRS) and Bayesian\nDawid-Skene, both of which leverage Bayesian inference to more accurately infer\nthe true win rate of generative language models. We empirically validate our\nmethods on six datasets covering story generation, summarization, and\ninstruction following tasks. We show that both our methods are effective in\nimproving the accuracy of win rate estimation using LLMs as evaluators,\noffering a promising direction for reliable automatic text quality evaluation."
                },
                "authors": [
                    {
                        "name": "Yicheng Gao"
                    },
                    {
                        "name": "Gonghan Xu"
                    },
                    {
                        "name": "Zhe Wang"
                    },
                    {
                        "name": "Arman Cohan"
                    }
                ],
                "author_detail": {
                    "name": "Arman Cohan"
                },
                "author": "Arman Cohan",
                "arxiv_comment": "Accepted by EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18064v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18064v3",
                "updated": "2024-11-07T04:03:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    4,
                    3,
                    4,
                    3,
                    312,
                    0
                ],
                "published": "2024-06-26T04:49:41Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    4,
                    49,
                    41,
                    2,
                    178,
                    0
                ],
                "title": "Evaluating Quality of Answers for Retrieval-Augmented Generation: A\n  Strong LLM Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Quality of Answers for Retrieval-Augmented Generation: A\n  Strong LLM Is All You Need"
                },
                "summary": "We present a comprehensive study of answer quality evaluation in\nRetrieval-Augmented Generation (RAG) applications using vRAG-Eval, a novel\ngrading system that is designed to assess correctness, completeness, and\nhonesty. We further map the grading of quality aspects aforementioned into a\nbinary score, indicating an accept or reject decision, mirroring the intuitive\n\"thumbs-up\" or \"thumbs-down\" gesture commonly used in chat applications. This\napproach suits factual business contexts where a clear decision opinion is\nessential. Our assessment applies vRAG-Eval to two Large Language Models\n(LLMs), evaluating the quality of answers generated by a vanilla RAG\napplication. We compare these evaluations with human expert judgments and find\na substantial alignment between GPT-4's assessments and those of human experts,\nreaching 83% agreement on accept or reject decisions. This study highlights the\npotential of LLMs as reliable evaluators in closed-domain, closed-ended\nsettings, particularly when human evaluations require significant resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a comprehensive study of answer quality evaluation in\nRetrieval-Augmented Generation (RAG) applications using vRAG-Eval, a novel\ngrading system that is designed to assess correctness, completeness, and\nhonesty. We further map the grading of quality aspects aforementioned into a\nbinary score, indicating an accept or reject decision, mirroring the intuitive\n\"thumbs-up\" or \"thumbs-down\" gesture commonly used in chat applications. This\napproach suits factual business contexts where a clear decision opinion is\nessential. Our assessment applies vRAG-Eval to two Large Language Models\n(LLMs), evaluating the quality of answers generated by a vanilla RAG\napplication. We compare these evaluations with human expert judgments and find\na substantial alignment between GPT-4's assessments and those of human experts,\nreaching 83% agreement on accept or reject decisions. This study highlights the\npotential of LLMs as reliable evaluators in closed-domain, closed-ended\nsettings, particularly when human evaluations require significant resources."
                },
                "authors": [
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Alberto Garcia Hernandez"
                    },
                    {
                        "name": "Roman Kyslyi"
                    },
                    {
                        "name": "Nicholas Kersting"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas Kersting"
                },
                "author": "Nicholas Kersting",
                "arxiv_comment": "13 pages, 8 figures, 12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18064v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18064v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04105v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04105v2",
                "updated": "2024-11-07T03:50:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    3,
                    50,
                    19,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-06T18:35:32Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    18,
                    35,
                    32,
                    2,
                    311,
                    0
                ],
                "title": "How Transformers Solve Propositional Logic Problems: A Mechanistic\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Transformers Solve Propositional Logic Problems: A Mechanistic\n  Analysis"
                },
                "summary": "Large language models (LLMs) have shown amazing performance on tasks that\nrequire planning and reasoning. Motivated by this, we investigate the internal\nmechanisms that underpin a network's ability to perform complex logical\nreasoning. We first construct a synthetic propositional logic problem that\nserves as a concrete test-bed for network training and evaluation. Crucially,\nthis problem demands nontrivial planning to solve, but we can train a small\ntransformer to achieve perfect accuracy. Building on our set-up, we then pursue\nan understanding of precisely how a three-layer transformer, trained from\nscratch, solves this problem. We are able to identify certain \"planning\" and\n\"reasoning\" circuits in the network that necessitate cooperation between the\nattention blocks to implement the desired logic. To expand our findings, we\nthen study a larger model, Mistral 7B. Using activation patching, we\ncharacterize internal components that are critical in solving our logic\nproblem. Overall, our work systemically uncovers novel aspects of small and\nlarge transformers, and continues the study of how they plan and reason.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown amazing performance on tasks that\nrequire planning and reasoning. Motivated by this, we investigate the internal\nmechanisms that underpin a network's ability to perform complex logical\nreasoning. We first construct a synthetic propositional logic problem that\nserves as a concrete test-bed for network training and evaluation. Crucially,\nthis problem demands nontrivial planning to solve, but we can train a small\ntransformer to achieve perfect accuracy. Building on our set-up, we then pursue\nan understanding of precisely how a three-layer transformer, trained from\nscratch, solves this problem. We are able to identify certain \"planning\" and\n\"reasoning\" circuits in the network that necessitate cooperation between the\nattention blocks to implement the desired logic. To expand our findings, we\nthen study a larger model, Mistral 7B. Using activation patching, we\ncharacterize internal components that are critical in solving our logic\nproblem. Overall, our work systemically uncovers novel aspects of small and\nlarge transformers, and continues the study of how they plan and reason."
                },
                "authors": [
                    {
                        "name": "Guan Zhe Hong"
                    },
                    {
                        "name": "Nishanth Dikkala"
                    },
                    {
                        "name": "Enming Luo"
                    },
                    {
                        "name": "Cyrus Rashtchian"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Rina Panigrahy"
                    }
                ],
                "author_detail": {
                    "name": "Rina Panigrahy"
                },
                "author": "Rina Panigrahy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04105v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04105v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02059v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02059v3",
                "updated": "2024-11-07T03:32:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    3,
                    32,
                    44,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-04T13:03:13Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    13,
                    3,
                    13,
                    0,
                    309,
                    0
                ],
                "title": "TableGPT2: A Large Multimodal Model with Tabular Data Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TableGPT2: A Large Multimodal Model with Tabular Data Integration"
                },
                "summary": "The emergence of models like GPTs, Claude, LLaMA, and Qwen has reshaped AI\napplications, presenting vast new opportunities across industries. Yet, the\nintegration of tabular data remains notably underdeveloped, despite its\nfoundational role in numerous real-world domains.\n  This gap is critical for three main reasons. First, database or data\nwarehouse data integration is essential for advanced applications; second, the\nvast and largely untapped resource of tabular data offers immense potential for\nanalysis; and third, the business intelligence domain specifically demands\nadaptable, precise solutions that many current LLMs may struggle to provide.\n  In response, we introduce TableGPT2, a model rigorously pre-trained and\nfine-tuned with over 593.8K tables and 2.36M high-quality query-table-output\ntuples, a scale of table-related data unprecedented in prior research. This\nextensive training enables TableGPT2 to excel in table-centric tasks while\nmaintaining strong general language and coding abilities.\n  One of TableGPT2's key innovations is its novel table encoder, specifically\ndesigned to capture schema-level and cell-level information. This encoder\nstrengthens the model's ability to handle ambiguous queries, missing column\nnames, and irregular tables commonly encountered in real-world applications.\nSimilar to visual language models, this pioneering approach integrates with the\ndecoder to form a robust large multimodal model.\n  We believe the results are compelling: over 23 benchmarking metrics,\nTableGPT2 achieves an average performance improvement of 35.20% in the 7B model\nand 49.32% in the 72B model over prior benchmark-neutral LLMs, with robust\ngeneral-purpose capabilities intact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of models like GPTs, Claude, LLaMA, and Qwen has reshaped AI\napplications, presenting vast new opportunities across industries. Yet, the\nintegration of tabular data remains notably underdeveloped, despite its\nfoundational role in numerous real-world domains.\n  This gap is critical for three main reasons. First, database or data\nwarehouse data integration is essential for advanced applications; second, the\nvast and largely untapped resource of tabular data offers immense potential for\nanalysis; and third, the business intelligence domain specifically demands\nadaptable, precise solutions that many current LLMs may struggle to provide.\n  In response, we introduce TableGPT2, a model rigorously pre-trained and\nfine-tuned with over 593.8K tables and 2.36M high-quality query-table-output\ntuples, a scale of table-related data unprecedented in prior research. This\nextensive training enables TableGPT2 to excel in table-centric tasks while\nmaintaining strong general language and coding abilities.\n  One of TableGPT2's key innovations is its novel table encoder, specifically\ndesigned to capture schema-level and cell-level information. This encoder\nstrengthens the model's ability to handle ambiguous queries, missing column\nnames, and irregular tables commonly encountered in real-world applications.\nSimilar to visual language models, this pioneering approach integrates with the\ndecoder to form a robust large multimodal model.\n  We believe the results are compelling: over 23 benchmarking metrics,\nTableGPT2 achieves an average performance improvement of 35.20% in the 7B model\nand 49.32% in the 72B model over prior benchmark-neutral LLMs, with robust\ngeneral-purpose capabilities intact."
                },
                "authors": [
                    {
                        "name": "Aofeng Su"
                    },
                    {
                        "name": "Aowen Wang"
                    },
                    {
                        "name": "Chao Ye"
                    },
                    {
                        "name": "Chen Zhou"
                    },
                    {
                        "name": "Ga Zhang"
                    },
                    {
                        "name": "Gang Chen"
                    },
                    {
                        "name": "Guangcheng Zhu"
                    },
                    {
                        "name": "Haobo Wang"
                    },
                    {
                        "name": "Haokai Xu"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Haoze Li"
                    },
                    {
                        "name": "Haoxuan Lan"
                    },
                    {
                        "name": "Jiaming Tian"
                    },
                    {
                        "name": "Jing Yuan"
                    },
                    {
                        "name": "Junbo Zhao"
                    },
                    {
                        "name": "Junlin Zhou"
                    },
                    {
                        "name": "Kaizhe Shou"
                    },
                    {
                        "name": "Liangyu Zha"
                    },
                    {
                        "name": "Lin Long"
                    },
                    {
                        "name": "Liyao Li"
                    },
                    {
                        "name": "Pengzuo Wu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Qingyi Huang"
                    },
                    {
                        "name": "Saisai Yang"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Wentao Ye"
                    },
                    {
                        "name": "Wufang Zhu"
                    },
                    {
                        "name": "Xiaomeng Hu"
                    },
                    {
                        "name": "Xijun Gu"
                    },
                    {
                        "name": "Xinjie Sun"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Yuhang Yang"
                    },
                    {
                        "name": "Zhiqing Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqing Xiao"
                },
                "author": "Zhiqing Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02059v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02059v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15186v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15186v4",
                "updated": "2024-11-07T03:26:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    3,
                    26,
                    58,
                    3,
                    312,
                    0
                ],
                "published": "2024-07-21T14:48:23Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    14,
                    48,
                    23,
                    6,
                    203,
                    0
                ],
                "title": "A Survey on Employing Large Language Models for Text-to-SQL Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Employing Large Language Models for Text-to-SQL Tasks"
                },
                "summary": "The increasing volume of data in relational databases and the expertise\nneeded for writing SQL queries pose challenges for users to access and analyze\ndata. Text-to-SQL (Text2SQL) solves the issues by utilizing natural language\nprocessing (NLP) techniques to convert natural language into SQL queries. With\nthe development of Large Language Models (LLMs), a range of LLM-based Text2SQL\nmethods have emerged. This survey provides a comprehensive review of LLMs in\nText2SQL tasks. We review benchmark datasets, prompt engineering methods,\nfine-tuning methods, and base models in LLM-based Text2SQL methods. We provide\ninsights in each part and discuss future directions in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing volume of data in relational databases and the expertise\nneeded for writing SQL queries pose challenges for users to access and analyze\ndata. Text-to-SQL (Text2SQL) solves the issues by utilizing natural language\nprocessing (NLP) techniques to convert natural language into SQL queries. With\nthe development of Large Language Models (LLMs), a range of LLM-based Text2SQL\nmethods have emerged. This survey provides a comprehensive review of LLMs in\nText2SQL tasks. We review benchmark datasets, prompt engineering methods,\nfine-tuning methods, and base models in LLM-based Text2SQL methods. We provide\ninsights in each part and discuss future directions in this field."
                },
                "authors": [
                    {
                        "name": "Liang Shi"
                    },
                    {
                        "name": "Zhengju Tang"
                    },
                    {
                        "name": "Nan Zhang"
                    },
                    {
                        "name": "Xiaotong Zhang"
                    },
                    {
                        "name": "Zhi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Yang"
                },
                "author": "Zhi Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15186v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15186v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11484v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11484v8",
                "updated": "2024-11-07T03:23:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    3,
                    23,
                    16,
                    3,
                    312,
                    0
                ],
                "published": "2024-07-16T08:20:39Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    8,
                    20,
                    39,
                    1,
                    198,
                    0
                ],
                "title": "The Oscars of AI Theater: A Survey on Role-Playing with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Oscars of AI Theater: A Survey on Role-Playing with Language Models"
                },
                "summary": "This survey explores the burgeoning field of role-playing with language\nmodels, focusing on their development from early persona-based models to\nadvanced character-driven simulations facilitated by Large Language Models\n(LLMs). Initially confined to simple persona consistency due to limited model\ncapabilities, role-playing tasks have now expanded to embrace complex character\nportrayals involving character consistency, behavioral alignment, and overall\nattractiveness. We provide a comprehensive taxonomy of the critical components\nin designing these systems, including data, models and alignment, agent\narchitecture and evaluation. This survey not only outlines the current\nmethodologies and challenges, such as managing dynamic personal profiles and\nachieving high-level persona consistency but also suggests avenues for future\nresearch in improving the depth and realism of role-playing applications. The\ngoal is to guide future research by offering a structured overview of current\nmethodologies and identifying potential areas for improvement. Related\nresources and papers are available at\nhttps://github.com/nuochenpku/Awesome-Role-Play-Papers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This survey explores the burgeoning field of role-playing with language\nmodels, focusing on their development from early persona-based models to\nadvanced character-driven simulations facilitated by Large Language Models\n(LLMs). Initially confined to simple persona consistency due to limited model\ncapabilities, role-playing tasks have now expanded to embrace complex character\nportrayals involving character consistency, behavioral alignment, and overall\nattractiveness. We provide a comprehensive taxonomy of the critical components\nin designing these systems, including data, models and alignment, agent\narchitecture and evaluation. This survey not only outlines the current\nmethodologies and challenges, such as managing dynamic personal profiles and\nachieving high-level persona consistency but also suggests avenues for future\nresearch in improving the depth and realism of role-playing applications. The\ngoal is to guide future research by offering a structured overview of current\nmethodologies and identifying potential areas for improvement. Related\nresources and papers are available at\nhttps://github.com/nuochenpku/Awesome-Role-Play-Papers."
                },
                "authors": [
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "Jia Li"
                    }
                ],
                "author_detail": {
                    "name": "Jia Li"
                },
                "author": "Jia Li",
                "arxiv_comment": "28 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11484v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11484v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00132v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00132v2",
                "updated": "2024-11-07T03:22:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    3,
                    22,
                    56,
                    3,
                    312,
                    0
                ],
                "published": "2024-10-31T18:33:39Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    18,
                    33,
                    39,
                    3,
                    305,
                    0
                ],
                "title": "Beyond Accuracy: Ensuring Correct Predictions With Correct Rationales",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Accuracy: Ensuring Correct Predictions With Correct Rationales"
                },
                "summary": "Large pretrained foundation models demonstrate exceptional performance and,\nin some high-stakes applications, even surpass human experts. However, most of\nthese models are currently evaluated primarily on prediction accuracy,\noverlooking the validity of the rationales behind their accurate predictions.\nFor the safe deployment of foundation models, there is a pressing need to\nensure double-correct predictions, i.e., correct prediction backed by correct\nrationales. To achieve this, we propose a two-phase scheme: First, we curate a\nnew dataset that offers structured rationales for visual recognition tasks.\nSecond, we propose a rationale-informed optimization method to guide the model\nin disentangling and localizing visual evidence for each rationale, without\nrequiring manual annotations. Extensive experiments and ablation studies\ndemonstrate that our model outperforms state-of-the-art models by up to 10.1%\nin prediction accuracy across a wide range of tasks. Furthermore, our method\nsignificantly improves the model's rationale correctness, improving\nlocalization by 7.5% and disentanglement by 36.5%. Our dataset, source code,\nand pretrained weights: https://github.com/deep-real/DCP",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large pretrained foundation models demonstrate exceptional performance and,\nin some high-stakes applications, even surpass human experts. However, most of\nthese models are currently evaluated primarily on prediction accuracy,\noverlooking the validity of the rationales behind their accurate predictions.\nFor the safe deployment of foundation models, there is a pressing need to\nensure double-correct predictions, i.e., correct prediction backed by correct\nrationales. To achieve this, we propose a two-phase scheme: First, we curate a\nnew dataset that offers structured rationales for visual recognition tasks.\nSecond, we propose a rationale-informed optimization method to guide the model\nin disentangling and localizing visual evidence for each rationale, without\nrequiring manual annotations. Extensive experiments and ablation studies\ndemonstrate that our model outperforms state-of-the-art models by up to 10.1%\nin prediction accuracy across a wide range of tasks. Furthermore, our method\nsignificantly improves the model's rationale correctness, improving\nlocalization by 7.5% and disentanglement by 36.5%. Our dataset, source code,\nand pretrained weights: https://github.com/deep-real/DCP"
                },
                "authors": [
                    {
                        "name": "Tang Li"
                    },
                    {
                        "name": "Mengmeng Ma"
                    },
                    {
                        "name": "Xi Peng"
                    }
                ],
                "author_detail": {
                    "name": "Xi Peng"
                },
                "author": "Xi Peng",
                "arxiv_comment": "In Proceedings of the 38th Conference on Neural Information\n  Processing Systems (NeurIPS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00132v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00132v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02603v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02603v3",
                "updated": "2024-11-07T03:17:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    3,
                    17,
                    42,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-04T20:53:04Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    20,
                    53,
                    4,
                    0,
                    309,
                    0
                ],
                "title": "FactTest: Factuality Testing in Large Language Models with Finite-Sample\n  and Distribution-Free Guarantees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FactTest: Factuality Testing in Large Language Models with Finite-Sample\n  and Distribution-Free Guarantees"
                },
                "summary": "The propensity of Large Language Models (LLMs) to generate hallucinations and\nnon-factual content undermines their reliability in high-stakes domains, where\nrigorous control over Type I errors (the conditional probability of incorrectly\nclassifying hallucinations as truthful content) is essential. Despite its\nimportance, formal verification of LLM factuality with such guarantees remains\nlargely unexplored. In this paper, we introduce FactTest, a novel framework\nthat statistically assesses whether a LLM can confidently provide correct\nanswers to given questions with high-probability correctness guarantees. We\nformulate factuality testing as hypothesis testing problem to enforce an upper\nbound of Type I errors at user-specified significance levels. Notably, we prove\nthat our framework also ensures strong Type II error control under mild\nconditions and can be extended to maintain its effectiveness when covariate\nshifts exist. Our approach is distribution-free and works for any number of\nhuman-annotated samples. It is model-agnostic and applies to any black-box or\nwhite-box LM. Extensive experiments on question-answering (QA) and\nmultiple-choice benchmarks demonstrate that FactTest effectively detects\nhallucinations and improves the model's ability to abstain from answering\nunknown questions, leading to an over 40% accuracy improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The propensity of Large Language Models (LLMs) to generate hallucinations and\nnon-factual content undermines their reliability in high-stakes domains, where\nrigorous control over Type I errors (the conditional probability of incorrectly\nclassifying hallucinations as truthful content) is essential. Despite its\nimportance, formal verification of LLM factuality with such guarantees remains\nlargely unexplored. In this paper, we introduce FactTest, a novel framework\nthat statistically assesses whether a LLM can confidently provide correct\nanswers to given questions with high-probability correctness guarantees. We\nformulate factuality testing as hypothesis testing problem to enforce an upper\nbound of Type I errors at user-specified significance levels. Notably, we prove\nthat our framework also ensures strong Type II error control under mild\nconditions and can be extended to maintain its effectiveness when covariate\nshifts exist. Our approach is distribution-free and works for any number of\nhuman-annotated samples. It is model-agnostic and applies to any black-box or\nwhite-box LM. Extensive experiments on question-answering (QA) and\nmultiple-choice benchmarks demonstrate that FactTest effectively detects\nhallucinations and improves the model's ability to abstain from answering\nunknown questions, leading to an over 40% accuracy improvement."
                },
                "authors": [
                    {
                        "name": "Fan Nie"
                    },
                    {
                        "name": "Xiaotian Hou"
                    },
                    {
                        "name": "Shuhang Lin"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Huaxiu Yao"
                    },
                    {
                        "name": "Linjun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linjun Zhang"
                },
                "author": "Linjun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02603v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02603v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17767v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17767v2",
                "updated": "2024-11-07T03:16:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    3,
                    16,
                    2,
                    3,
                    312,
                    0
                ],
                "published": "2024-05-28T02:46:11Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    2,
                    46,
                    11,
                    1,
                    149,
                    0
                ],
                "title": "Linguistic Collapse: Neural Collapse in (Large) Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linguistic Collapse: Neural Collapse in (Large) Language Models"
                },
                "summary": "Neural collapse ($\\mathcal{NC}$) is a phenomenon observed in classification\ntasks where top-layer representations collapse into their class means, which\nbecome equinorm, equiangular and aligned with the classifiers. These behaviors\n-- associated with generalization and robustness -- would manifest under\nspecific conditions: models are trained towards zero loss, with noise-free\nlabels belonging to balanced classes, which do not outnumber the model's hidden\ndimension. Recent studies have explored $\\mathcal{NC}$ in the absence of one or\nmore of these conditions to extend and capitalize on the associated benefits of\nideal geometries. Language modeling presents a curious frontier, as\n\\textit{training by token prediction} constitutes a classification task where\nnone of the conditions exist: the vocabulary is imbalanced and exceeds the\nembedding dimension; different tokens might correspond to similar contextual\nembeddings; and large language models (LLMs) in particular are typically only\ntrained for a few epochs. This paper empirically investigates the impact of\nscaling the architectures and training of causal language models (CLMs) on\ntheir progression towards $\\mathcal{NC}$. We find that $\\mathcal{NC}$\nproperties that develop with scale (and regularization) are linked to\ngeneralization. Moreover, there is evidence of some relationship between\n$\\mathcal{NC}$ and generalization independent of scale. Our work thereby\nunderscores the generality of $\\mathcal{NC}$ as it extends to the novel and\nmore challenging setting of language modeling. Downstream, we seek to inspire\nfurther research on the phenomenon to deepen our understanding of LLMs -- and\nneural networks at large -- and improve existing architectures based on\n$\\mathcal{NC}$-related properties. Our code is hosted on GitHub at\nhttps://github.com/rhubarbwu/linguistic-collapse .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural collapse ($\\mathcal{NC}$) is a phenomenon observed in classification\ntasks where top-layer representations collapse into their class means, which\nbecome equinorm, equiangular and aligned with the classifiers. These behaviors\n-- associated with generalization and robustness -- would manifest under\nspecific conditions: models are trained towards zero loss, with noise-free\nlabels belonging to balanced classes, which do not outnumber the model's hidden\ndimension. Recent studies have explored $\\mathcal{NC}$ in the absence of one or\nmore of these conditions to extend and capitalize on the associated benefits of\nideal geometries. Language modeling presents a curious frontier, as\n\\textit{training by token prediction} constitutes a classification task where\nnone of the conditions exist: the vocabulary is imbalanced and exceeds the\nembedding dimension; different tokens might correspond to similar contextual\nembeddings; and large language models (LLMs) in particular are typically only\ntrained for a few epochs. This paper empirically investigates the impact of\nscaling the architectures and training of causal language models (CLMs) on\ntheir progression towards $\\mathcal{NC}$. We find that $\\mathcal{NC}$\nproperties that develop with scale (and regularization) are linked to\ngeneralization. Moreover, there is evidence of some relationship between\n$\\mathcal{NC}$ and generalization independent of scale. Our work thereby\nunderscores the generality of $\\mathcal{NC}$ as it extends to the novel and\nmore challenging setting of language modeling. Downstream, we seek to inspire\nfurther research on the phenomenon to deepen our understanding of LLMs -- and\nneural networks at large -- and improve existing architectures based on\n$\\mathcal{NC}$-related properties. Our code is hosted on GitHub at\nhttps://github.com/rhubarbwu/linguistic-collapse ."
                },
                "authors": [
                    {
                        "name": "Robert Wu"
                    },
                    {
                        "name": "Vardan Papyan"
                    }
                ],
                "author_detail": {
                    "name": "Vardan Papyan"
                },
                "author": "Vardan Papyan",
                "arxiv_comment": "NeurIPS 2024; 36 pages; 30 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17767v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17767v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07 (Primary) 68T50 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.18802v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.18802v4",
                "updated": "2024-11-07T03:14:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    3,
                    14,
                    38,
                    3,
                    312,
                    0
                ],
                "published": "2024-03-27T17:48:55Z",
                "published_parsed": [
                    2024,
                    3,
                    27,
                    17,
                    48,
                    55,
                    2,
                    87,
                    0
                ],
                "title": "Long-form factuality in large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-form factuality in large language models"
                },
                "summary": "Large language models (LLMs) often generate content that contains factual\nerrors when responding to fact-seeking prompts on open-ended topics. To\nbenchmark a model's long-form factuality in open domains, we first use GPT-4 to\ngenerate LongFact, a prompt set comprising thousands of questions spanning 38\ntopics. We then propose that LLM agents can be used as automated evaluators for\nlong-form factuality through a method which we call Search-Augmented Factuality\nEvaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into\na set of individual facts and to evaluate the accuracy of each fact using a\nmulti-step reasoning process comprising sending search queries to Google Search\nand determining whether a fact is supported by the search results. Furthermore,\nwe propose extending F1 score as an aggregated metric for long-form factuality.\nTo do so, we balance the percentage of supported facts in a response\n(precision) with the percentage of provided facts relative to a hyperparameter\nrepresenting a user's preferred response length (recall).\n  Empirically, we demonstrate that LLM agents can outperform crowdsourced human\nannotators - on a set of ~16k individual facts, SAFE agrees with crowdsourced\nhuman annotators 72% of the time, and on a random subset of 100 disagreement\ncases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times\ncheaper than human annotators. We also benchmark thirteen language models on\nLongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding\nthat larger language models generally achieve better long-form factuality.\nLongFact, SAFE, and all experimental code are available at\nhttps://github.com/google-deepmind/long-form-factuality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often generate content that contains factual\nerrors when responding to fact-seeking prompts on open-ended topics. To\nbenchmark a model's long-form factuality in open domains, we first use GPT-4 to\ngenerate LongFact, a prompt set comprising thousands of questions spanning 38\ntopics. We then propose that LLM agents can be used as automated evaluators for\nlong-form factuality through a method which we call Search-Augmented Factuality\nEvaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into\na set of individual facts and to evaluate the accuracy of each fact using a\nmulti-step reasoning process comprising sending search queries to Google Search\nand determining whether a fact is supported by the search results. Furthermore,\nwe propose extending F1 score as an aggregated metric for long-form factuality.\nTo do so, we balance the percentage of supported facts in a response\n(precision) with the percentage of provided facts relative to a hyperparameter\nrepresenting a user's preferred response length (recall).\n  Empirically, we demonstrate that LLM agents can outperform crowdsourced human\nannotators - on a set of ~16k individual facts, SAFE agrees with crowdsourced\nhuman annotators 72% of the time, and on a random subset of 100 disagreement\ncases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times\ncheaper than human annotators. We also benchmark thirteen language models on\nLongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding\nthat larger language models generally achieve better long-form factuality.\nLongFact, SAFE, and all experimental code are available at\nhttps://github.com/google-deepmind/long-form-factuality."
                },
                "authors": [
                    {
                        "name": "Jerry Wei"
                    },
                    {
                        "name": "Chengrun Yang"
                    },
                    {
                        "name": "Xinying Song"
                    },
                    {
                        "name": "Yifeng Lu"
                    },
                    {
                        "name": "Nathan Hu"
                    },
                    {
                        "name": "Jie Huang"
                    },
                    {
                        "name": "Dustin Tran"
                    },
                    {
                        "name": "Daiyi Peng"
                    },
                    {
                        "name": "Ruibo Liu"
                    },
                    {
                        "name": "Da Huang"
                    },
                    {
                        "name": "Cosmo Du"
                    },
                    {
                        "name": "Quoc V. Le"
                    }
                ],
                "author_detail": {
                    "name": "Quoc V. Le"
                },
                "author": "Quoc V. Le",
                "arxiv_comment": "NeurIPS 2024; 72 pages, 18 figures, 30 tables. Code at\n  https://github.com/google-deepmind/long-form-factuality",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.18802v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.18802v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19487v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19487v3",
                "updated": "2024-11-07T03:05:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    3,
                    5,
                    18,
                    3,
                    312,
                    0
                ],
                "published": "2024-09-28T23:59:46Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    23,
                    59,
                    46,
                    5,
                    272,
                    0
                ],
                "title": "HealthQ: Unveiling Questioning Capabilities of LLM Chains in Healthcare\n  Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HealthQ: Unveiling Questioning Capabilities of LLM Chains in Healthcare\n  Conversations"
                },
                "summary": "In digital healthcare, large language models (LLMs) have primarily been\nutilized to enhance question-answering capabilities and improve patient\ninteractions. However, effective patient care necessitates LLM chains that can\nactively gather information by posing relevant questions. This paper presents\nHealthQ, a novel framework designed to evaluate the questioning capabilities of\nLLM healthcare chains. We implemented several LLM chains, including\nRetrieval-Augmented Generation (RAG), Chain of Thought (CoT), and reflective\nchains, and introduced an LLM judge to assess the relevance and informativeness\nof the generated questions. To validate HealthQ, we employed traditional\nNatural Language Processing (NLP) metrics such as Recall-Oriented Understudy\nfor Gisting Evaluation (ROUGE) and Named Entity Recognition (NER)-based set\ncomparison, and constructed two custom datasets from public medical note\ndatasets, ChatDoctor and MTS-Dialog. Our contributions are threefold: we\nprovide the first comprehensive study on the questioning capabilities of LLMs\nin healthcare conversations, develop a novel dataset generation pipeline, and\npropose a detailed evaluation methodology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In digital healthcare, large language models (LLMs) have primarily been\nutilized to enhance question-answering capabilities and improve patient\ninteractions. However, effective patient care necessitates LLM chains that can\nactively gather information by posing relevant questions. This paper presents\nHealthQ, a novel framework designed to evaluate the questioning capabilities of\nLLM healthcare chains. We implemented several LLM chains, including\nRetrieval-Augmented Generation (RAG), Chain of Thought (CoT), and reflective\nchains, and introduced an LLM judge to assess the relevance and informativeness\nof the generated questions. To validate HealthQ, we employed traditional\nNatural Language Processing (NLP) metrics such as Recall-Oriented Understudy\nfor Gisting Evaluation (ROUGE) and Named Entity Recognition (NER)-based set\ncomparison, and constructed two custom datasets from public medical note\ndatasets, ChatDoctor and MTS-Dialog. Our contributions are threefold: we\nprovide the first comprehensive study on the questioning capabilities of LLMs\nin healthcare conversations, develop a novel dataset generation pipeline, and\npropose a detailed evaluation methodology."
                },
                "authors": [
                    {
                        "name": "Ziyu Wang"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Di Huang"
                    },
                    {
                        "name": "Amir M. Rahmani"
                    }
                ],
                "author_detail": {
                    "name": "Amir M. Rahmani"
                },
                "author": "Amir M. Rahmani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19487v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19487v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04388v1",
                "updated": "2024-11-07T03:02:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    3,
                    2,
                    9,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T03:02:09Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    3,
                    2,
                    9,
                    3,
                    312,
                    0
                ],
                "title": "Unlearning in- vs. out-of-distribution data in LLMs under gradient-based\n  method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlearning in- vs. out-of-distribution data in LLMs under gradient-based\n  method"
                },
                "summary": "Machine unlearning aims to solve the problem of removing the influence of\nselected training examples from a learned model. Despite the increasing\nattention to this problem, it remains an open research question how to evaluate\nunlearning in large language models (LLMs), and what are the critical\nproperties of the data to be unlearned that affect the quality and efficiency\nof unlearning. This work formalizes a metric to evaluate unlearning quality in\ngenerative models, and uses it to assess the trade-offs between unlearning\nquality and performance. We demonstrate that unlearning out-of-distribution\nexamples requires more unlearning steps but overall presents a better trade-off\noverall. For in-distribution examples, however, we observe a rapid decay in\nperformance as unlearning progresses. We further evaluate how example's\nmemorization and difficulty affect unlearning under a classical gradient\nascent-based approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine unlearning aims to solve the problem of removing the influence of\nselected training examples from a learned model. Despite the increasing\nattention to this problem, it remains an open research question how to evaluate\nunlearning in large language models (LLMs), and what are the critical\nproperties of the data to be unlearned that affect the quality and efficiency\nof unlearning. This work formalizes a metric to evaluate unlearning quality in\ngenerative models, and uses it to assess the trade-offs between unlearning\nquality and performance. We demonstrate that unlearning out-of-distribution\nexamples requires more unlearning steps but overall presents a better trade-off\noverall. For in-distribution examples, however, we observe a rapid decay in\nperformance as unlearning progresses. We further evaluate how example's\nmemorization and difficulty affect unlearning under a classical gradient\nascent-based approach."
                },
                "authors": [
                    {
                        "name": "Teodora Baluta"
                    },
                    {
                        "name": "Pascal Lamblin"
                    },
                    {
                        "name": "Daniel Tarlow"
                    },
                    {
                        "name": "Fabian Pedregosa"
                    },
                    {
                        "name": "Gintare Karolina Dziugaite"
                    }
                ],
                "author_detail": {
                    "name": "Gintare Karolina Dziugaite"
                },
                "author": "Gintare Karolina Dziugaite",
                "arxiv_comment": "Accepted at Safe Generative AI Workshop @ NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04387v1",
                "updated": "2024-11-07T03:01:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    3,
                    1,
                    50,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T03:01:50Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    3,
                    1,
                    50,
                    3,
                    312,
                    0
                ],
                "title": "Automated Update of Android Deprecated API Usages with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Update of Android Deprecated API Usages with Large Language\n  Models"
                },
                "summary": "Android apps rely on application programming interfaces (APIs) to access\nvarious functionalities of Android devices. These APIs however are regularly\nupdated to incorporate new features while the old APIs get deprecated. Even\nthough the importance of updating deprecated API usages with the recommended\nreplacement APIs has been widely recognized, it is non-trivial to update the\ndeprecated API usages. Therefore, the usages of deprecated APIs linger in\nAndroid apps and cause compatibility issues in practice. This paper introduces\nGUPPY, an automated approach that utilizes large language models (LLMs) to\nupdate Android deprecated API usages. By employing carefully crafted prompts,\nGUPPY leverages GPT-4, one of the most powerful LLMs, to update deprecated-API\nusages, ensuring compatibility in both the old and new API levels.\nAdditionally, GUPPY uses GPT-4 to generate tests, identify incorrect updates,\nand refine the API usage through an iterative process until the tests pass or a\nspecified limit is reached. Our evaluation, conducted on 360 benchmark API\nusages from 20 deprecated APIs and an additional 156 deprecated API usages from\nthe latest API levels 33 and 34, demonstrates GUPPY's advantages over the\nstate-of-the-art techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Android apps rely on application programming interfaces (APIs) to access\nvarious functionalities of Android devices. These APIs however are regularly\nupdated to incorporate new features while the old APIs get deprecated. Even\nthough the importance of updating deprecated API usages with the recommended\nreplacement APIs has been widely recognized, it is non-trivial to update the\ndeprecated API usages. Therefore, the usages of deprecated APIs linger in\nAndroid apps and cause compatibility issues in practice. This paper introduces\nGUPPY, an automated approach that utilizes large language models (LLMs) to\nupdate Android deprecated API usages. By employing carefully crafted prompts,\nGUPPY leverages GPT-4, one of the most powerful LLMs, to update deprecated-API\nusages, ensuring compatibility in both the old and new API levels.\nAdditionally, GUPPY uses GPT-4 to generate tests, identify incorrect updates,\nand refine the API usage through an iterative process until the tests pass or a\nspecified limit is reached. Our evaluation, conducted on 360 benchmark API\nusages from 20 deprecated APIs and an additional 156 deprecated API usages from\nthe latest API levels 33 and 34, demonstrates GUPPY's advantages over the\nstate-of-the-art techniques."
                },
                "authors": [
                    {
                        "name": "Tarek Mahmud"
                    },
                    {
                        "name": "Bin Duan"
                    },
                    {
                        "name": "Meiru Che"
                    },
                    {
                        "name": "Awatif Yasmin"
                    },
                    {
                        "name": "Anne H. H. Ngu"
                    },
                    {
                        "name": "Guowei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guowei Yang"
                },
                "author": "Guowei Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04372v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04372v1",
                "updated": "2024-11-07T02:05:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    2,
                    5,
                    43,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T02:05:43Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    2,
                    5,
                    43,
                    3,
                    312,
                    0
                ],
                "title": "Benchmarking Large Language Models with Integer Sequence Generation\n  Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Large Language Models with Integer Sequence Generation\n  Tasks"
                },
                "summary": "This paper presents a novel benchmark where the large language model (LLM)\nmust write code that computes integer sequences from the Online Encyclopedia of\nInteger Sequences (OEIS), a widely-used resource for mathematical sequences.\nThe benchmark is designed to evaluate both the correctness of the generated\ncode and its computational efficiency. Our benchmark reveals that the o1 series\nof models outperform other frontier models from OpenAI, Anthropic, Meta, and\nGoogle in accuracy and cheating rates across both easy and hard integer\nsequences. In order to ensure models do not exploit memorized sequence values,\nwe introduce an automated cheating detection mechanism that flags the use of\nlookup tables and validated this automation against human cheating evaluations.\nThis benchmark provides a meaningful challenge for current LLMs, offering\ninsights into their mathematical reasoning and code writing capabilities, which\ncan guide future research directions and model development in mathematical\nreasoning and code synthesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel benchmark where the large language model (LLM)\nmust write code that computes integer sequences from the Online Encyclopedia of\nInteger Sequences (OEIS), a widely-used resource for mathematical sequences.\nThe benchmark is designed to evaluate both the correctness of the generated\ncode and its computational efficiency. Our benchmark reveals that the o1 series\nof models outperform other frontier models from OpenAI, Anthropic, Meta, and\nGoogle in accuracy and cheating rates across both easy and hard integer\nsequences. In order to ensure models do not exploit memorized sequence values,\nwe introduce an automated cheating detection mechanism that flags the use of\nlookup tables and validated this automation against human cheating evaluations.\nThis benchmark provides a meaningful challenge for current LLMs, offering\ninsights into their mathematical reasoning and code writing capabilities, which\ncan guide future research directions and model development in mathematical\nreasoning and code synthesis."
                },
                "authors": [
                    {
                        "name": "Daniel O'Malley"
                    },
                    {
                        "name": "Manish Bhattarai"
                    },
                    {
                        "name": "Javier Santos"
                    }
                ],
                "author_detail": {
                    "name": "Javier Santos"
                },
                "author": "Javier Santos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04372v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04372v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15520v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15520v2",
                "updated": "2024-11-07T01:52:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    1,
                    52,
                    17,
                    3,
                    312,
                    0
                ],
                "published": "2024-09-23T20:14:09Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    20,
                    14,
                    9,
                    0,
                    267,
                    0
                ],
                "title": "Enabling Efficient On-Device Fine-Tuning of LLMs Using Only Inference\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Efficient On-Device Fine-Tuning of LLMs Using Only Inference\n  Engines"
                },
                "summary": "Large Language Models (LLMs) are currently pre-trained and fine-tuned on\nlarge cloud servers. The next frontier is LLM personalization, where a\nfoundation model can be fine-tuned with user/task-specific data. Given the\nsensitive nature of such private data, it is desirable to fine-tune these\nmodels on edge devices to improve user trust. However, fine-tuning on\nresource-constrained edge devices presents significant challenges due to\nsubstantial memory and computational demands, as well as limited infrastructure\nsupport. We observe that inference engines (e.g., ExecuTorch) can be repurposed\nfor fine-tuning by leveraging zeroth-order (ZO) optimization, which uses\nmultiple forward passes to approximate gradients. However, directly applying ZO\nmethods on edge devices is impractical due to the high computational cost of\nmultiple model perturbations required to achieve accuracy improvements. Based\non these observations, we propose a memory- and computation-efficient LLM\nfine-tuning method for edge devices. Our approach has three key innovations:\n(1) We introduce a parallelized randomized gradient estimation (P-RGE)\ntechnique that achieves high parallel efficiency by leveraging outer-loop and\ninner-loop parallelization. This enables multiple function queries and forward\npasses to be executed in parallel, reducing training time. (2) We integrate\nP-RGE with parameter-efficient fine-tuning methods (e.g. LoRA) to further\nreduce computational and memory overhead. (3) We implement a P-RGE LoRA-FA\nmodule that fully supports fine-tuning with ExecuTorch. Our approach requires\nno modifications to ExecuTorch's runtime code, as it can be implemented with\nserver-side code changes only. Experiments demonstrate that P-RGE achieves\nsubstantial runtime speedups and memory savings while improving fine-tuning\naccuracy, paving the way for practical deployment of LLMs in real-time,\non-device applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are currently pre-trained and fine-tuned on\nlarge cloud servers. The next frontier is LLM personalization, where a\nfoundation model can be fine-tuned with user/task-specific data. Given the\nsensitive nature of such private data, it is desirable to fine-tune these\nmodels on edge devices to improve user trust. However, fine-tuning on\nresource-constrained edge devices presents significant challenges due to\nsubstantial memory and computational demands, as well as limited infrastructure\nsupport. We observe that inference engines (e.g., ExecuTorch) can be repurposed\nfor fine-tuning by leveraging zeroth-order (ZO) optimization, which uses\nmultiple forward passes to approximate gradients. However, directly applying ZO\nmethods on edge devices is impractical due to the high computational cost of\nmultiple model perturbations required to achieve accuracy improvements. Based\non these observations, we propose a memory- and computation-efficient LLM\nfine-tuning method for edge devices. Our approach has three key innovations:\n(1) We introduce a parallelized randomized gradient estimation (P-RGE)\ntechnique that achieves high parallel efficiency by leveraging outer-loop and\ninner-loop parallelization. This enables multiple function queries and forward\npasses to be executed in parallel, reducing training time. (2) We integrate\nP-RGE with parameter-efficient fine-tuning methods (e.g. LoRA) to further\nreduce computational and memory overhead. (3) We implement a P-RGE LoRA-FA\nmodule that fully supports fine-tuning with ExecuTorch. Our approach requires\nno modifications to ExecuTorch's runtime code, as it can be implemented with\nserver-side code changes only. Experiments demonstrate that P-RGE achieves\nsubstantial runtime speedups and memory savings while improving fine-tuning\naccuracy, paving the way for practical deployment of LLMs in real-time,\non-device applications."
                },
                "authors": [
                    {
                        "name": "Lei Gao"
                    },
                    {
                        "name": "Amir Ziashahabi"
                    },
                    {
                        "name": "Yue Niu"
                    },
                    {
                        "name": "Salman Avestimehr"
                    },
                    {
                        "name": "Murali Annavaram"
                    }
                ],
                "author_detail": {
                    "name": "Murali Annavaram"
                },
                "author": "Murali Annavaram",
                "arxiv_comment": "Accepted at NeurIPS 2024 ENLSP-IV workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15520v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15520v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04358v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04358v1",
                "updated": "2024-11-07T01:31:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    1,
                    31,
                    48,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T01:31:48Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    1,
                    31,
                    48,
                    3,
                    312,
                    0
                ],
                "title": "Robust and Efficient Fine-tuning of LLMs with Bayesian\n  Reparameterization of Low-Rank Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust and Efficient Fine-tuning of LLMs with Bayesian\n  Reparameterization of Low-Rank Adaptation"
                },
                "summary": "Large Language Models (LLMs) are highly resource-intensive to fine-tune due\nto their enormous size. While low-rank adaptation is a prominent\nparameter-efficient fine-tuning approach, it suffers from sensitivity to\nhyperparameter choices, leading to instability in model performance on\nfine-tuning downstream tasks. This paper highlights the importance of effective\nparameterization in low-rank fine-tuning to reduce estimator variance and\nenhance the stability of final model outputs. We propose MonteCLoRA, an\nefficient fine-tuning technique, employing Monte Carlo estimation to learn an\nunbiased posterior estimation of low-rank parameters with low expected\nvariance, which stabilizes fine-tuned LLMs with only O(1) additional\nparameters. MonteCLoRA shows significant improvements in accuracy and\nrobustness, achieving up to 3.8% higher accuracy and 8.6% greater robustness\nthan existing efficient fine-tuning methods on natural language understanding\ntasks with pre-trained RoBERTa-base. Furthermore, in generative tasks with\npre-trained LLaMA-1-7B, MonteCLoRA demonstrates robust zero-shot performance\nwith 50% lower variance than the contemporary efficient fine-tuning methods.\nThe theoretical and empirical results presented in the paper underscore how\nparameterization and hyperpriors balance exploration-exploitation in the\nlow-rank parametric space, therefore leading to more optimal and robust\nparameter estimation during efficient fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are highly resource-intensive to fine-tune due\nto their enormous size. While low-rank adaptation is a prominent\nparameter-efficient fine-tuning approach, it suffers from sensitivity to\nhyperparameter choices, leading to instability in model performance on\nfine-tuning downstream tasks. This paper highlights the importance of effective\nparameterization in low-rank fine-tuning to reduce estimator variance and\nenhance the stability of final model outputs. We propose MonteCLoRA, an\nefficient fine-tuning technique, employing Monte Carlo estimation to learn an\nunbiased posterior estimation of low-rank parameters with low expected\nvariance, which stabilizes fine-tuned LLMs with only O(1) additional\nparameters. MonteCLoRA shows significant improvements in accuracy and\nrobustness, achieving up to 3.8% higher accuracy and 8.6% greater robustness\nthan existing efficient fine-tuning methods on natural language understanding\ntasks with pre-trained RoBERTa-base. Furthermore, in generative tasks with\npre-trained LLaMA-1-7B, MonteCLoRA demonstrates robust zero-shot performance\nwith 50% lower variance than the contemporary efficient fine-tuning methods.\nThe theoretical and empirical results presented in the paper underscore how\nparameterization and hyperpriors balance exploration-exploitation in the\nlow-rank parametric space, therefore leading to more optimal and robust\nparameter estimation during efficient fine-tuning."
                },
                "authors": [
                    {
                        "name": "Vaibhav Seth"
                    },
                    {
                        "name": "Arinjay Pathak"
                    },
                    {
                        "name": "Ayan Sengupta"
                    },
                    {
                        "name": "Natraj Raman"
                    },
                    {
                        "name": "Sriram Gopalakrishnan"
                    },
                    {
                        "name": "Tanmoy Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Chakraborty"
                },
                "author": "Tanmoy Chakraborty",
                "arxiv_comment": "48 pages, 10 figures, 10 tables, Code:\n  https://github.com/LCS2-IIITD/MonteCLoRA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04358v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01222v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01222v3",
                "updated": "2024-11-07T01:26:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    1,
                    26,
                    43,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-02T12:01:44Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    12,
                    1,
                    44,
                    5,
                    307,
                    0
                ],
                "title": "$B^4$: A Black-Box Scrubbing Attack on LLM Watermarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$B^4$: A Black-Box Scrubbing Attack on LLM Watermarks"
                },
                "summary": "Watermarking has emerged as a prominent technique for LLM-generated content\ndetection by embedding imperceptible patterns. Despite supreme performance, its\nrobustness against adversarial attacks remains underexplored. Previous work\ntypically considers a grey-box attack setting, where the specific type of\nwatermark is already known. Some even necessitates knowledge about\nhyperparameters of the watermarking method. Such prerequisites are unattainable\nin real-world scenarios. Targeting at a more realistic black-box threat model\nwith fewer assumptions, we here propose $B^4$, a black-box scrubbing attack on\nwatermarks. Specifically, we formulate the watermark scrubbing attack as a\nconstrained optimization problem by capturing its objectives with two\ndistributions, a Watermark Distribution and a Fidelity Distribution. This\noptimization problem can be approximately solved using two proxy distributions.\nExperimental results across 12 different settings demonstrate the superior\nperformance of $B^4$ compared with other baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watermarking has emerged as a prominent technique for LLM-generated content\ndetection by embedding imperceptible patterns. Despite supreme performance, its\nrobustness against adversarial attacks remains underexplored. Previous work\ntypically considers a grey-box attack setting, where the specific type of\nwatermark is already known. Some even necessitates knowledge about\nhyperparameters of the watermarking method. Such prerequisites are unattainable\nin real-world scenarios. Targeting at a more realistic black-box threat model\nwith fewer assumptions, we here propose $B^4$, a black-box scrubbing attack on\nwatermarks. Specifically, we formulate the watermark scrubbing attack as a\nconstrained optimization problem by capturing its objectives with two\ndistributions, a Watermark Distribution and a Fidelity Distribution. This\noptimization problem can be approximately solved using two proxy distributions.\nExperimental results across 12 different settings demonstrate the superior\nperformance of $B^4$ compared with other baselines."
                },
                "authors": [
                    {
                        "name": "Baizhou Huang"
                    },
                    {
                        "name": "Xiao Pu"
                    },
                    {
                        "name": "Xiaojun Wan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Wan"
                },
                "author": "Xiaojun Wan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01222v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01222v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.06567v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.06567v3",
                "updated": "2024-11-07T00:54:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    0,
                    54,
                    30,
                    3,
                    312,
                    0
                ],
                "published": "2024-07-09T05:52:26Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    5,
                    52,
                    26,
                    1,
                    191,
                    0
                ],
                "title": "FinCon: A Synthesized LLM Multi-Agent System with Conceptual Verbal\n  Reinforcement for Enhanced Financial Decision Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FinCon: A Synthesized LLM Multi-Agent System with Conceptual Verbal\n  Reinforcement for Enhanced Financial Decision Making"
                },
                "summary": "Large language models (LLMs) have demonstrated notable potential in\nconducting complex tasks and are increasingly utilized in various financial\napplications. However, high-quality sequential financial investment\ndecision-making remains challenging. These tasks require multiple interactions\nwith a volatile environment for every decision, demanding sufficient\nintelligence to maximize returns and manage risks. Although LLMs have been used\nto develop agent systems that surpass human teams and yield impressive\ninvestment returns, opportunities to enhance multi-sourced information\nsynthesis and optimize decision-making outcomes through timely experience\nrefinement remain unexplored. Here, we introduce the FinCon, an LLM-based\nmulti-agent framework with CONceptual verbal reinforcement tailored for diverse\nFINancial tasks. Inspired by effective real-world investment firm\norganizational structures, FinCon utilizes a manager-analyst communication\nhierarchy. This structure allows for synchronized cross-functional agent\ncollaboration towards unified goals through natural language interactions and\nequips each agent with greater memory capacity than humans. Additionally, a\nrisk-control component in FinCon enhances decision quality by episodically\ninitiating a self-critiquing mechanism to update systematic investment beliefs.\nThe conceptualized beliefs serve as verbal reinforcement for the future agent's\nbehavior and can be selectively propagated to the appropriate node that\nrequires knowledge updates. This feature significantly improves performance\nwhile reducing unnecessary peer-to-peer communication costs. Moreover, FinCon\ndemonstrates strong generalization capabilities in various financial tasks,\nincluding single stock trading and portfolio management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated notable potential in\nconducting complex tasks and are increasingly utilized in various financial\napplications. However, high-quality sequential financial investment\ndecision-making remains challenging. These tasks require multiple interactions\nwith a volatile environment for every decision, demanding sufficient\nintelligence to maximize returns and manage risks. Although LLMs have been used\nto develop agent systems that surpass human teams and yield impressive\ninvestment returns, opportunities to enhance multi-sourced information\nsynthesis and optimize decision-making outcomes through timely experience\nrefinement remain unexplored. Here, we introduce the FinCon, an LLM-based\nmulti-agent framework with CONceptual verbal reinforcement tailored for diverse\nFINancial tasks. Inspired by effective real-world investment firm\norganizational structures, FinCon utilizes a manager-analyst communication\nhierarchy. This structure allows for synchronized cross-functional agent\ncollaboration towards unified goals through natural language interactions and\nequips each agent with greater memory capacity than humans. Additionally, a\nrisk-control component in FinCon enhances decision quality by episodically\ninitiating a self-critiquing mechanism to update systematic investment beliefs.\nThe conceptualized beliefs serve as verbal reinforcement for the future agent's\nbehavior and can be selectively propagated to the appropriate node that\nrequires knowledge updates. This feature significantly improves performance\nwhile reducing unnecessary peer-to-peer communication costs. Moreover, FinCon\ndemonstrates strong generalization capabilities in various financial tasks,\nincluding single stock trading and portfolio management."
                },
                "authors": [
                    {
                        "name": "Yangyang Yu"
                    },
                    {
                        "name": "Zhiyuan Yao"
                    },
                    {
                        "name": "Haohang Li"
                    },
                    {
                        "name": "Zhiyang Deng"
                    },
                    {
                        "name": "Yupeng Cao"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Jordan W. Suchow"
                    },
                    {
                        "name": "Rong Liu"
                    },
                    {
                        "name": "Zhenyu Cui"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Denghui Zhang"
                    },
                    {
                        "name": "Koduvayur Subbalakshmi"
                    },
                    {
                        "name": "Guojun Xiong"
                    },
                    {
                        "name": "Yueru He"
                    },
                    {
                        "name": "Jimin Huang"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Qianqian Xie"
                    }
                ],
                "author_detail": {
                    "name": "Qianqian Xie"
                },
                "author": "Qianqian Xie",
                "arxiv_comment": "LLM Applications, LLM Agents, Financial Technology, Quantitative\n  Finance, Algorithmic Trading, Cognitive Science",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.06567v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.06567v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04341v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04341v1",
                "updated": "2024-11-07T00:39:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    0,
                    39,
                    34,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T00:39:34Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    0,
                    39,
                    34,
                    3,
                    312,
                    0
                ],
                "title": "Enhancing classroom teaching with LLMs and RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing classroom teaching with LLMs and RAG"
                },
                "summary": "Large Language Models have become a valuable source of information for our\ndaily inquiries. However, after training, its data source quickly becomes\nout-of-date, making RAG a useful tool for providing even more recent or\npertinent data. In this work, we investigate how RAG pipelines, with the course\nmaterials serving as a data source, might help students in K-12 education. The\ninitial research utilizes Reddit as a data source for up-to-date cybersecurity\ninformation. Chunk size is evaluated to determine the optimal amount of context\nneeded to generate accurate answers. After running the experiment for different\nchunk sizes, answer correctness was evaluated using RAGAs with average answer\ncorrectness not exceeding 50 percent for any chunk size. This suggests that\nReddit is not a good source to mine for data for questions about cybersecurity\nthreats. The methodology was successful in evaluating the data source, which\nhas implications for its use to evaluate educational resources for\neffectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have become a valuable source of information for our\ndaily inquiries. However, after training, its data source quickly becomes\nout-of-date, making RAG a useful tool for providing even more recent or\npertinent data. In this work, we investigate how RAG pipelines, with the course\nmaterials serving as a data source, might help students in K-12 education. The\ninitial research utilizes Reddit as a data source for up-to-date cybersecurity\ninformation. Chunk size is evaluated to determine the optimal amount of context\nneeded to generate accurate answers. After running the experiment for different\nchunk sizes, answer correctness was evaluated using RAGAs with average answer\ncorrectness not exceeding 50 percent for any chunk size. This suggests that\nReddit is not a good source to mine for data for questions about cybersecurity\nthreats. The methodology was successful in evaluating the data source, which\nhas implications for its use to evaluate educational resources for\neffectiveness."
                },
                "authors": [
                    {
                        "name": "Elizabeth A Mullins"
                    },
                    {
                        "name": "Adrian Portillo"
                    },
                    {
                        "name": "Kristalys Ruiz-Rohena"
                    },
                    {
                        "name": "Aritran Piplai"
                    }
                ],
                "author_detail": {
                    "name": "Aritran Piplai"
                },
                "author": "Aritran Piplai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04341v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04341v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04329v1",
                "updated": "2024-11-07T00:09:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    0,
                    9,
                    54,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T00:09:54Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    0,
                    9,
                    54,
                    3,
                    312,
                    0
                ],
                "title": "CodeTree: Agent-guided Tree Search for Code Generation with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeTree: Agent-guided Tree Search for Code Generation with Large\n  Language Models"
                },
                "summary": "Pre-trained on massive amounts of code and text data, large language models\n(LLMs) have demonstrated remarkable achievements in performing code generation\ntasks. With additional execution-based feedback, these models can act as agents\nwith capabilities to self-refine and improve generated code autonomously.\nHowever, on challenging coding tasks with extremely large search space, current\nagentic approaches still struggle with multi-stage planning, generating, and\ndebugging. To address this problem, we propose CodeTree, a framework for LLM\nagents to efficiently explore the search space in different stages of the code\ngeneration process. Specifically, we adopted a unified tree structure to\nexplicitly explore different coding strategies, generate corresponding coding\nsolutions, and subsequently refine the solutions. In each stage, critical\ndecision-making (ranking, termination, expanding) of the exploration process is\nguided by both the environmental execution-based feedback and\nLLM-agent-generated feedback. We comprehensively evaluated CodeTree on 7 code\ngeneration benchmarks and demonstrated the significant performance gains of\nCodeTree against strong baselines. Using GPT-4o as the base model, we\nconsistently achieved top results of 95.1 on HumanEval, 98.7 on MBPP, and 43.0\non CodeContests. On the challenging SWEBench benchmark, our approach led to\nsignificant performance gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-trained on massive amounts of code and text data, large language models\n(LLMs) have demonstrated remarkable achievements in performing code generation\ntasks. With additional execution-based feedback, these models can act as agents\nwith capabilities to self-refine and improve generated code autonomously.\nHowever, on challenging coding tasks with extremely large search space, current\nagentic approaches still struggle with multi-stage planning, generating, and\ndebugging. To address this problem, we propose CodeTree, a framework for LLM\nagents to efficiently explore the search space in different stages of the code\ngeneration process. Specifically, we adopted a unified tree structure to\nexplicitly explore different coding strategies, generate corresponding coding\nsolutions, and subsequently refine the solutions. In each stage, critical\ndecision-making (ranking, termination, expanding) of the exploration process is\nguided by both the environmental execution-based feedback and\nLLM-agent-generated feedback. We comprehensively evaluated CodeTree on 7 code\ngeneration benchmarks and demonstrated the significant performance gains of\nCodeTree against strong baselines. Using GPT-4o as the base model, we\nconsistently achieved top results of 95.1 on HumanEval, 98.7 on MBPP, and 43.0\non CodeContests. On the challenging SWEBench benchmark, our approach led to\nsignificant performance gains."
                },
                "authors": [
                    {
                        "name": "Jierui Li"
                    },
                    {
                        "name": "Hung Le"
                    },
                    {
                        "name": "Yinbo Zhou"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Doyen Sahoo"
                    }
                ],
                "author_detail": {
                    "name": "Doyen Sahoo"
                },
                "author": "Doyen Sahoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04324v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04324v1",
                "updated": "2024-11-06T23:54:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    23,
                    54,
                    9,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T23:54:09Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    23,
                    54,
                    9,
                    2,
                    311,
                    0
                ],
                "title": "Gradient Boosting Trees and Large Language Models for Tabular Data\n  Few-Shot Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradient Boosting Trees and Large Language Models for Tabular Data\n  Few-Shot Learning"
                },
                "summary": "Large Language Models (LLM) have brought numerous of new applications to\nMachine Learning (ML). In the context of tabular data (TD), recent studies show\nthat TabLLM is a very powerful mechanism for few-shot-learning (FSL)\napplications, even if gradient boosting decisions trees (GBDT) have\nhistorically dominated the TD field. In this work we demonstrate that although\nLLMs are a viable alternative, the evidence suggests that baselines used to\ngauge performance can be improved. We replicated public benchmarks and our\nmethodology improves LightGBM by 290%, this is mainly driven by forcing node\nsplitting with few samples, a critical step in FSL with GBDT. Our results show\nan advantage to TabLLM for 8 or fewer shots, but as the number of samples\nincreases GBDT provides competitive performance at a fraction of runtime. For\nother real-life applications with vast number of samples, we found FSL still\nuseful to improve model diversity, and when combined with ExtraTrees it\nprovides strong resilience to overfitting, our proposal was validated in a ML\ncompetition setting ranking first place.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLM) have brought numerous of new applications to\nMachine Learning (ML). In the context of tabular data (TD), recent studies show\nthat TabLLM is a very powerful mechanism for few-shot-learning (FSL)\napplications, even if gradient boosting decisions trees (GBDT) have\nhistorically dominated the TD field. In this work we demonstrate that although\nLLMs are a viable alternative, the evidence suggests that baselines used to\ngauge performance can be improved. We replicated public benchmarks and our\nmethodology improves LightGBM by 290%, this is mainly driven by forcing node\nsplitting with few samples, a critical step in FSL with GBDT. Our results show\nan advantage to TabLLM for 8 or fewer shots, but as the number of samples\nincreases GBDT provides competitive performance at a fraction of runtime. For\nother real-life applications with vast number of samples, we found FSL still\nuseful to improve model diversity, and when combined with ExtraTrees it\nprovides strong resilience to overfitting, our proposal was validated in a ML\ncompetition setting ranking first place."
                },
                "authors": [
                    {
                        "name": "Carlos Huertas"
                    }
                ],
                "author_detail": {
                    "name": "Carlos Huertas"
                },
                "author": "Carlos Huertas",
                "arxiv_comment": "FedCSIS 2024 - Data Mining Competition - 1st Place Winner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04324v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04324v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04319v1",
                "updated": "2024-11-06T23:47:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    23,
                    47,
                    54,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T23:47:54Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    23,
                    47,
                    54,
                    2,
                    311,
                    0
                ],
                "title": "Towards Optimizing SQL Generation via LLM Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Optimizing SQL Generation via LLM Routing"
                },
                "summary": "Text-to-SQL enables users to interact with databases through natural\nlanguage, simplifying access to structured data. Although highly capable large\nlanguage models (LLMs) achieve strong accuracy for complex queries, they incur\nunnecessary latency and dollar cost for simpler ones. In this paper, we\nintroduce the first LLM routing approach for Text-to-SQL, which dynamically\nselects the most cost-effective LLM capable of generating accurate SQL for each\nquery. We present two routing strategies (score- and classification-based) that\nachieve accuracy comparable to the most capable LLM while reducing costs. We\ndesign the routers for ease of training and efficient inference. In our\nexperiments, we highlight a practical and explainable accuracy-cost trade-off\non the BIRD dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL enables users to interact with databases through natural\nlanguage, simplifying access to structured data. Although highly capable large\nlanguage models (LLMs) achieve strong accuracy for complex queries, they incur\nunnecessary latency and dollar cost for simpler ones. In this paper, we\nintroduce the first LLM routing approach for Text-to-SQL, which dynamically\nselects the most cost-effective LLM capable of generating accurate SQL for each\nquery. We present two routing strategies (score- and classification-based) that\nachieve accuracy comparable to the most capable LLM while reducing costs. We\ndesign the routers for ease of training and efficient inference. In our\nexperiments, we highlight a practical and explainable accuracy-cost trade-off\non the BIRD dataset."
                },
                "authors": [
                    {
                        "name": "Mohammadhossein Malekpour"
                    },
                    {
                        "name": "Nour Shaheen"
                    },
                    {
                        "name": "Foutse Khomh"
                    },
                    {
                        "name": "Amine Mhedhbi"
                    }
                ],
                "author_detail": {
                    "name": "Amine Mhedhbi"
                },
                "author": "Amine Mhedhbi",
                "arxiv_comment": "Table Representation Learning Workshop at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04316v1",
                "updated": "2024-11-06T23:41:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    23,
                    41,
                    18,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T23:41:18Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    23,
                    41,
                    18,
                    2,
                    311,
                    0
                ],
                "title": "A Multilingual Sentiment Lexicon for Low-Resource Language Translation\n  using Large Languages Models and Explainable AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multilingual Sentiment Lexicon for Low-Resource Language Translation\n  using Large Languages Models and Explainable AI"
                },
                "summary": "South Africa and the Democratic Republic of Congo (DRC) present a complex\nlinguistic landscape with languages such as Zulu, Sepedi, Afrikaans, French,\nEnglish, and Tshiluba (Ciluba), which creates unique challenges for AI-driven\ntranslation and sentiment analysis systems due to a lack of accurately labeled\ndata. This study seeks to address these challenges by developing a multilingual\nlexicon designed for French and Tshiluba, now expanded to include translations\nin English, Afrikaans, Sepedi, and Zulu. The lexicon enhances cultural\nrelevance in sentiment classification by integrating language-specific\nsentiment scores. A comprehensive testing corpus is created to support\ntranslation and sentiment analysis tasks, with machine learning models such as\nRandom Forest, Support Vector Machine (SVM), Decision Trees, and Gaussian Naive\nBayes (GNB) trained to predict sentiment across low resource languages (LRLs).\nAmong them, the Random Forest model performed particularly well, capturing\nsentiment polarity and handling language-specific nuances effectively.\nFurthermore, Bidirectional Encoder Representations from Transformers (BERT), a\nLarge Language Model (LLM), is applied to predict context-based sentiment with\nhigh accuracy, achieving 99% accuracy and 98% precision, outperforming other\nmodels. The BERT predictions were clarified using Explainable AI (XAI),\nimproving transparency and fostering confidence in sentiment classification.\nOverall, findings demonstrate that the proposed lexicon and machine learning\nmodels significantly enhance translation and sentiment analysis for LRLs in\nSouth Africa and the DRC, laying a foundation for future AI models that support\nunderrepresented languages, with applications across education, governance, and\nbusiness in multilingual contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "South Africa and the Democratic Republic of Congo (DRC) present a complex\nlinguistic landscape with languages such as Zulu, Sepedi, Afrikaans, French,\nEnglish, and Tshiluba (Ciluba), which creates unique challenges for AI-driven\ntranslation and sentiment analysis systems due to a lack of accurately labeled\ndata. This study seeks to address these challenges by developing a multilingual\nlexicon designed for French and Tshiluba, now expanded to include translations\nin English, Afrikaans, Sepedi, and Zulu. The lexicon enhances cultural\nrelevance in sentiment classification by integrating language-specific\nsentiment scores. A comprehensive testing corpus is created to support\ntranslation and sentiment analysis tasks, with machine learning models such as\nRandom Forest, Support Vector Machine (SVM), Decision Trees, and Gaussian Naive\nBayes (GNB) trained to predict sentiment across low resource languages (LRLs).\nAmong them, the Random Forest model performed particularly well, capturing\nsentiment polarity and handling language-specific nuances effectively.\nFurthermore, Bidirectional Encoder Representations from Transformers (BERT), a\nLarge Language Model (LLM), is applied to predict context-based sentiment with\nhigh accuracy, achieving 99% accuracy and 98% precision, outperforming other\nmodels. The BERT predictions were clarified using Explainable AI (XAI),\nimproving transparency and fostering confidence in sentiment classification.\nOverall, findings demonstrate that the proposed lexicon and machine learning\nmodels significantly enhance translation and sentiment analysis for LRLs in\nSouth Africa and the DRC, laying a foundation for future AI models that support\nunderrepresented languages, with applications across education, governance, and\nbusiness in multilingual contexts."
                },
                "authors": [
                    {
                        "name": "Melusi Malinga"
                    },
                    {
                        "name": "Isaac Lupanda"
                    },
                    {
                        "name": "Mike Wa Nkongolo"
                    },
                    {
                        "name": "Phil van Deventer"
                    }
                ],
                "author_detail": {
                    "name": "Phil van Deventer"
                },
                "author": "Phil van Deventer",
                "arxiv_comment": "This work is part of a PhD proposal in Information Technology at the\n  University of Pretoria, supervised by Dr. Mike Wa Nkongolo and co-supervised\n  by Dr. Phil van Deventer, under the Low-Resource Language Processing Lab in\n  the Department of Informatics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02316v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02316v2",
                "updated": "2024-11-06T23:27:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    23,
                    27,
                    24,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-04T17:40:39Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    40,
                    39,
                    0,
                    309,
                    0
                ],
                "title": "Evaluating Creative Short Story Generation in Humans and Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Creative Short Story Generation in Humans and Large Language\n  Models"
                },
                "summary": "Storytelling is a fundamental aspect of human communication, relying heavily\non creativity to produce narratives that are novel, appropriate, and\nsurprising. While large language models (LLMs) have recently demonstrated the\nability to generate high-quality stories, their creative capabilities remain\nunderexplored. Previous research has either focused on creativity tests\nrequiring short responses or primarily compared model performance in story\ngeneration to that of professional writers. However, the question of whether\nLLMs exhibit creativity in writing short stories on par with the average human\nremains unanswered. In this work, we conduct a systematic analysis of\ncreativity in short story generation across LLMs and everyday people. Using a\nfive-sentence creative story task, commonly employed in psychology to assess\nhuman creativity, we automatically evaluate model- and human-generated stories\nacross several dimensions of creativity, including novelty, surprise, and\ndiversity. Our findings reveal that while LLMs can generate stylistically\ncomplex stories, they tend to fall short in terms of creativity when compared\nto average human writers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Storytelling is a fundamental aspect of human communication, relying heavily\non creativity to produce narratives that are novel, appropriate, and\nsurprising. While large language models (LLMs) have recently demonstrated the\nability to generate high-quality stories, their creative capabilities remain\nunderexplored. Previous research has either focused on creativity tests\nrequiring short responses or primarily compared model performance in story\ngeneration to that of professional writers. However, the question of whether\nLLMs exhibit creativity in writing short stories on par with the average human\nremains unanswered. In this work, we conduct a systematic analysis of\ncreativity in short story generation across LLMs and everyday people. Using a\nfive-sentence creative story task, commonly employed in psychology to assess\nhuman creativity, we automatically evaluate model- and human-generated stories\nacross several dimensions of creativity, including novelty, surprise, and\ndiversity. Our findings reveal that while LLMs can generate stylistically\ncomplex stories, they tend to fall short in terms of creativity when compared\nto average human writers."
                },
                "authors": [
                    {
                        "name": "Mete Ismayilzada"
                    },
                    {
                        "name": "Claire Stevenson"
                    },
                    {
                        "name": "Lonneke van der Plas"
                    }
                ],
                "author_detail": {
                    "name": "Lonneke van der Plas"
                },
                "author": "Lonneke van der Plas",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02316v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02316v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04308v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04308v1",
                "updated": "2024-11-06T23:16:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    23,
                    16,
                    25,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T23:16:25Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    23,
                    16,
                    25,
                    2,
                    311,
                    0
                ],
                "title": "Improving Bilingual Capabilities of Language Models to Support Diverse\n  Linguistic Practices in Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Bilingual Capabilities of Language Models to Support Diverse\n  Linguistic Practices in Education"
                },
                "summary": "Large language models (LLMs) offer promise in generating educational content,\nproviding instructor feedback, and reducing teacher workload on assessments.\nWhile prior studies have focused on studying LLM-powered learning analytics,\nlimited research has examined how effective LLMs are in a bilingual context. In\nthis paper, we study the effectiveness of multilingual large language models\n(MLLMs) across monolingual (English-only, Spanish-only) and bilingual\n(Spanglish) student writing. We present a learning analytics use case that\ndetails LLM performance in assessing acceptable and unacceptable explanations\nof Science and Social Science concepts. Our findings reveal a significant bias\nin the grading performance of pre-trained models for bilingual writing compared\nto English-only and Spanish-only writing. Following this, we fine-tune\nopen-source MLLMs including Llama 3.1 and Mistral NeMo using synthetic datasets\ngenerated in English, Spanish, and Spanglish. Our experiments indicate that the\nmodels perform significantly better for all three languages after fine-tuning\nwith bilingual data. This study highlights the potential of enhancing MLLM\neffectiveness to support authentic language practices amongst bilingual\nlearners. It also aims to illustrate the value of incorporating non-English\nlanguages into the design and implementation of language models in education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) offer promise in generating educational content,\nproviding instructor feedback, and reducing teacher workload on assessments.\nWhile prior studies have focused on studying LLM-powered learning analytics,\nlimited research has examined how effective LLMs are in a bilingual context. In\nthis paper, we study the effectiveness of multilingual large language models\n(MLLMs) across monolingual (English-only, Spanish-only) and bilingual\n(Spanglish) student writing. We present a learning analytics use case that\ndetails LLM performance in assessing acceptable and unacceptable explanations\nof Science and Social Science concepts. Our findings reveal a significant bias\nin the grading performance of pre-trained models for bilingual writing compared\nto English-only and Spanish-only writing. Following this, we fine-tune\nopen-source MLLMs including Llama 3.1 and Mistral NeMo using synthetic datasets\ngenerated in English, Spanish, and Spanglish. Our experiments indicate that the\nmodels perform significantly better for all three languages after fine-tuning\nwith bilingual data. This study highlights the potential of enhancing MLLM\neffectiveness to support authentic language practices amongst bilingual\nlearners. It also aims to illustrate the value of incorporating non-English\nlanguages into the design and implementation of language models in education."
                },
                "authors": [
                    {
                        "name": "Anand Syamkumar"
                    },
                    {
                        "name": "Nora Tseng"
                    },
                    {
                        "name": "Kaycie Barron"
                    },
                    {
                        "name": "Shanglin Yang"
                    },
                    {
                        "name": "Shamya Karumbaiah"
                    },
                    {
                        "name": "Rheeya Uppal"
                    },
                    {
                        "name": "Junjie Hu"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Hu"
                },
                "author": "Junjie Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04308v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04308v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04299v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04299v1",
                "updated": "2024-11-06T22:48:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    22,
                    48,
                    18,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T22:48:18Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    22,
                    48,
                    18,
                    2,
                    311,
                    0
                ],
                "title": "An Empirical Study on Automatically Detecting AI-Generated Source Code:\n  How Far Are We?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study on Automatically Detecting AI-Generated Source Code:\n  How Far Are We?"
                },
                "summary": "Artificial Intelligence (AI) techniques, especially Large Language Models\n(LLMs), have started gaining popularity among researchers and software\ndevelopers for generating source code. However, LLMs have been shown to\ngenerate code with quality issues and also incurred copyright/licensing\ninfringements. Therefore, detecting whether a piece of source code is written\nby humans or AI has become necessary. This study first presents an empirical\nanalysis to investigate the effectiveness of the existing AI detection tools in\ndetecting AI-generated code. The results show that they all perform poorly and\nlack sufficient generalizability to be practically deployed. Then, to improve\nthe performance of AI-generated code detection, we propose a range of\napproaches, including fine-tuning the LLMs and machine learning-based\nclassification with static code metrics or code embedding generated from\nAbstract Syntax Tree (AST). Our best model outperforms state-of-the-art\nAI-generated code detector (GPTSniffer) and achieves an F1 score of 82.55. We\nalso conduct an ablation study on our best-performing model to investigate the\nimpact of different source code features on its performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence (AI) techniques, especially Large Language Models\n(LLMs), have started gaining popularity among researchers and software\ndevelopers for generating source code. However, LLMs have been shown to\ngenerate code with quality issues and also incurred copyright/licensing\ninfringements. Therefore, detecting whether a piece of source code is written\nby humans or AI has become necessary. This study first presents an empirical\nanalysis to investigate the effectiveness of the existing AI detection tools in\ndetecting AI-generated code. The results show that they all perform poorly and\nlack sufficient generalizability to be practically deployed. Then, to improve\nthe performance of AI-generated code detection, we propose a range of\napproaches, including fine-tuning the LLMs and machine learning-based\nclassification with static code metrics or code embedding generated from\nAbstract Syntax Tree (AST). Our best model outperforms state-of-the-art\nAI-generated code detector (GPTSniffer) and achieves an F1 score of 82.55. We\nalso conduct an ablation study on our best-performing model to investigate the\nimpact of different source code features on its performance."
                },
                "authors": [
                    {
                        "name": "Hyunjae Suh"
                    },
                    {
                        "name": "Mahan Tafreshipour"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Adithya Bhattiprolu"
                    },
                    {
                        "name": "Iftekhar Ahmed"
                    }
                ],
                "author_detail": {
                    "name": "Iftekhar Ahmed"
                },
                "author": "Iftekhar Ahmed",
                "arxiv_comment": "Accepted at The 47th IEEE/ACM International Conference on Software\n  Engineering (ICSE 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04299v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00369v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00369v2",
                "updated": "2024-11-06T22:41:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    22,
                    41,
                    31,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-01T05:14:03Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    5,
                    14,
                    3,
                    4,
                    306,
                    0
                ],
                "title": "GRSQA -- Graph Reasoning-Structured Question Answering Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GRSQA -- Graph Reasoning-Structured Question Answering Dataset"
                },
                "summary": "Large Language Models (LLMs) have excelled in multi-hop question-answering\n(M-QA) due to their advanced reasoning abilities. However, the impact of the\ninherent reasoning structures on LLM M-QA performance remains unclear, largely\ndue to the absence of QA datasets that provide fine-grained reasoning\nstructures. To address this gap, we introduce the Graph Reasoning-Structured\nQuestion Answering Dataset (GRS-QA), which includes both semantic contexts and\nreasoning structures for QA pairs. Unlike existing M-QA datasets, where\ndifferent reasoning structures are entangled together, GRS-QA explicitly\ncaptures intricate reasoning pathways by constructing reasoning graphs, where\nnodes represent textual contexts and edges denote logical flows. These\nreasoning graphs of different structures enable a fine-grained evaluation of\nLLM reasoning capabilities across various reasoning structures. Our empirical\nanalysis reveals that LLMs perform differently when handling questions with\nvarying reasoning structures. This finding facilitates the exploration of\ntextual structures as compared with semantics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have excelled in multi-hop question-answering\n(M-QA) due to their advanced reasoning abilities. However, the impact of the\ninherent reasoning structures on LLM M-QA performance remains unclear, largely\ndue to the absence of QA datasets that provide fine-grained reasoning\nstructures. To address this gap, we introduce the Graph Reasoning-Structured\nQuestion Answering Dataset (GRS-QA), which includes both semantic contexts and\nreasoning structures for QA pairs. Unlike existing M-QA datasets, where\ndifferent reasoning structures are entangled together, GRS-QA explicitly\ncaptures intricate reasoning pathways by constructing reasoning graphs, where\nnodes represent textual contexts and edges denote logical flows. These\nreasoning graphs of different structures enable a fine-grained evaluation of\nLLM reasoning capabilities across various reasoning structures. Our empirical\nanalysis reveals that LLMs perform differently when handling questions with\nvarying reasoning structures. This finding facilitates the exploration of\ntextual structures as compared with semantics."
                },
                "authors": [
                    {
                        "name": "Anish Pahilajani"
                    },
                    {
                        "name": "Devasha Trivedi"
                    },
                    {
                        "name": "Jincen Shuai"
                    },
                    {
                        "name": "Khin S. Yone"
                    },
                    {
                        "name": "Samyak Rajesh Jain"
                    },
                    {
                        "name": "Namyong Park"
                    },
                    {
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "name": "Nesreen K. Ahmed"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "arxiv_comment": "15 pages, 24 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00369v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00369v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]