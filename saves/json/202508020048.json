[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.22701v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22701v2",
                "updated": "2025-07-31T16:21:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    16,
                    21,
                    3,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-30T14:10:16Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    14,
                    10,
                    16,
                    2,
                    211,
                    0
                ],
                "title": "SAM: A Stability-Aware Cache Manager for Multi-Tenant Embedded Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAM: A Stability-Aware Cache Manager for Multi-Tenant Embedded Databases"
                },
                "summary": "The co-location of multiple database instances on resource constrained edge\nnodes creates significant cache contention, where traditional schemes are\ninefficient and unstable under dynamic workloads. To address this, we present\nSAM(a Stability-Aware Manager), an autonomic cache manager that establishes\ndecision stability as a first-class design principle. It achieves this through\nits core control policy, AURA(Autonomic Utility-balancing Resource Allocator),\nwhich resolves the classic exploitation-exploration dilemma by synthesizing two\northogonal factors: the H-factor, representing proven historical efficiency\n(exploitation), and the V-factor, for estimated marginal gain (exploration).\nThrough this practical synthesis and adaptive control, SAM achieves sustained\nhigh performance with strategic stability and robustness in volatile\nconditions.\n  Extensive experiments against 14 diverse baselines demonstrate SAM's\nsuperiority. It achieves top-tier throughput while being uniquely resilient to\ncomplex workload shifts and adversarial workloads like cache pollution.\nFurthermore, its decision latency is highly scalable, remaining nearly constant\nas the system grows to 120 databases. Crucially, SAM achieves superior decision\nstability -- maintaining consistent optimization directions despite noise,\navoiding performance oscillations while ensuring predictable Quality of\nService. These results prove that a principled, stability-aware design is\nessential for sustained high performance in real-world, large-scale systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The co-location of multiple database instances on resource constrained edge\nnodes creates significant cache contention, where traditional schemes are\ninefficient and unstable under dynamic workloads. To address this, we present\nSAM(a Stability-Aware Manager), an autonomic cache manager that establishes\ndecision stability as a first-class design principle. It achieves this through\nits core control policy, AURA(Autonomic Utility-balancing Resource Allocator),\nwhich resolves the classic exploitation-exploration dilemma by synthesizing two\northogonal factors: the H-factor, representing proven historical efficiency\n(exploitation), and the V-factor, for estimated marginal gain (exploration).\nThrough this practical synthesis and adaptive control, SAM achieves sustained\nhigh performance with strategic stability and robustness in volatile\nconditions.\n  Extensive experiments against 14 diverse baselines demonstrate SAM's\nsuperiority. It achieves top-tier throughput while being uniquely resilient to\ncomplex workload shifts and adversarial workloads like cache pollution.\nFurthermore, its decision latency is highly scalable, remaining nearly constant\nas the system grows to 120 databases. Crucially, SAM achieves superior decision\nstability -- maintaining consistent optimization directions despite noise,\navoiding performance oscillations while ensuring predictable Quality of\nService. These results prove that a principled, stability-aware design is\nessential for sustained high performance in real-world, large-scale systems."
                },
                "authors": [
                    {
                        "name": "Haoran Zhang"
                    },
                    {
                        "name": "Decheng Zuo"
                    },
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Zhiyu Liang"
                    },
                    {
                        "name": "Hongzhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hongzhi Wang"
                },
                "author": "Hongzhi Wang",
                "arxiv_comment": "17 pages, 10 figures. An extended version of a paper under review at\n  the VLDB 2026 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22701v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22701v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.4; H.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23674v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23674v1",
                "updated": "2025-07-31T15:50:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    50,
                    57,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T15:50:57Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    50,
                    57,
                    3,
                    212,
                    0
                ],
                "title": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses"
                },
                "summary": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience."
                },
                "authors": [
                    {
                        "name": "Muhammad Taha Cheema"
                    },
                    {
                        "name": "Abeer Aamir"
                    },
                    {
                        "name": "Khawaja Gul Muhammad"
                    },
                    {
                        "name": "Naveed Anwar Bhatti"
                    },
                    {
                        "name": "Ihsan Ayyub Qazi"
                    },
                    {
                        "name": "Zafar Ayyub Qazi"
                    }
                ],
                "author_detail": {
                    "name": "Zafar Ayyub Qazi"
                },
                "author": "Zafar Ayyub Qazi",
                "arxiv_comment": "13 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23674v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23387v1",
                "updated": "2025-07-31T10:02:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    10,
                    2,
                    26,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T10:02:26Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    10,
                    2,
                    26,
                    3,
                    212,
                    0
                ],
                "title": "H2SGEMM: Emulating FP32 GEMM on Ascend NPUs using FP16 Units with\n  Precision Recovery and Cache-Aware Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "H2SGEMM: Emulating FP32 GEMM on Ascend NPUs using FP16 Units with\n  Precision Recovery and Cache-Aware Optimization"
                },
                "summary": "Low-precision matrix engines, such as FP16 cube, offer high throughput but\nlack support for full-precision computation. In this work, we propose H2SGEMM,\na high-performance algorithm for emulating FP32 general matrix-matrix\nmultiplication (GEMM) using only FP16 computation units on a representative AI\naccelerator. The method decomposes each FP32 operand into two FP16 values and\ncompensates for numerical errors through a tunable scaling strategy. A detailed\nanalysis of numerical errors, including underflow conditions and precision\nloss, guides the selection of scaling parameters to preserve up to 22 bits of\nmantissa accuracy. We further investigate the effect of computation order on\naccuracy and demonstrate that a term-wise accumulation scheme improves\nnumerical stability over conventional FP32 GEMM in low-exponent regimes.\nFinally, a cache-aware blocking strategy and double-buffered pipeline are\nintroduced to overlap memory transfers with computation, enabling H2SGEMM to\nachieve up to 77% of the theoretical FP32-equivalent peak performance on Ascend\n910A NPU lacking native FP32 support. Extensive numerical experiments confirm\nthat our method not only recovers the accuracy of native FP32 GEMM but also\nexhibits superior numerical stability under certain conditions, due to its\nstructured and error-aware computation order.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-precision matrix engines, such as FP16 cube, offer high throughput but\nlack support for full-precision computation. In this work, we propose H2SGEMM,\na high-performance algorithm for emulating FP32 general matrix-matrix\nmultiplication (GEMM) using only FP16 computation units on a representative AI\naccelerator. The method decomposes each FP32 operand into two FP16 values and\ncompensates for numerical errors through a tunable scaling strategy. A detailed\nanalysis of numerical errors, including underflow conditions and precision\nloss, guides the selection of scaling parameters to preserve up to 22 bits of\nmantissa accuracy. We further investigate the effect of computation order on\naccuracy and demonstrate that a term-wise accumulation scheme improves\nnumerical stability over conventional FP32 GEMM in low-exponent regimes.\nFinally, a cache-aware blocking strategy and double-buffered pipeline are\nintroduced to overlap memory transfers with computation, enabling H2SGEMM to\nachieve up to 77% of the theoretical FP32-equivalent peak performance on Ascend\n910A NPU lacking native FP32 support. Extensive numerical experiments confirm\nthat our method not only recovers the accuracy of native FP32 GEMM but also\nexhibits superior numerical stability under certain conditions, due to its\nstructured and error-aware computation order."
                },
                "authors": [
                    {
                        "name": "Weicheng Xue"
                    },
                    {
                        "name": "Baisong Xu"
                    },
                    {
                        "name": "Kai Yang"
                    },
                    {
                        "name": "Yongxiang Liu"
                    },
                    {
                        "name": "Dengdeng Fan"
                    },
                    {
                        "name": "Pengxiang Xu"
                    },
                    {
                        "name": "Yonghong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yonghong Tian"
                },
                "author": "Yonghong Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21433v2",
                "updated": "2025-07-31T07:53:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    7,
                    53,
                    53,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-29T02:05:51Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    2,
                    5,
                    51,
                    1,
                    210,
                    0
                ],
                "title": "MemShare: Memory Efficient Inference for Large Reasoning Models through\n  KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemShare: Memory Efficient Inference for Large Reasoning Models through\n  KV Cache Reuse"
                },
                "summary": "Large Reasoning Models (LRMs) have achieved significant advances in\nmathematical reasoning and formal logic tasks. However, their tendency to\ngenerate lengthy chain-of-thought sequences leads to substantial memory\noverhead during inference. We observe that LRMs frequently produce highly\nsimilar intermediate reasoning steps, which correspond to similar KV cache\nstates across layers. Motivated by this observation, we propose MemShare, a\nnovel KV cache management approach that effectively reduces memory overhead.\nMemShare employs a collaborative filtering algorithm to efficiently identify\nreusable KV cache blocks and enables zero copy cache reuse to significantly\nreduce memory overhead, improve throughput while maintaining accuracy.\nExperimental results demonstrate that MemShare delivers up to 84.79\\%\nimprovement in throughput while maintaining better accuracy compared to\nexisting KV cache management methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models (LRMs) have achieved significant advances in\nmathematical reasoning and formal logic tasks. However, their tendency to\ngenerate lengthy chain-of-thought sequences leads to substantial memory\noverhead during inference. We observe that LRMs frequently produce highly\nsimilar intermediate reasoning steps, which correspond to similar KV cache\nstates across layers. Motivated by this observation, we propose MemShare, a\nnovel KV cache management approach that effectively reduces memory overhead.\nMemShare employs a collaborative filtering algorithm to efficiently identify\nreusable KV cache blocks and enables zero copy cache reuse to significantly\nreduce memory overhead, improve throughput while maintaining accuracy.\nExperimental results demonstrate that MemShare delivers up to 84.79\\%\nimprovement in throughput while maintaining better accuracy compared to\nexisting KV cache management methods."
                },
                "authors": [
                    {
                        "name": "Kaiwen Chen"
                    },
                    {
                        "name": "Xin Tan"
                    },
                    {
                        "name": "Minchen Yu"
                    },
                    {
                        "name": "Hong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Hong Xu"
                },
                "author": "Hong Xu",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01199v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01199v2",
                "updated": "2025-07-31T07:35:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    7,
                    35,
                    4,
                    3,
                    212,
                    0
                ],
                "published": "2025-03-03T05:52:02Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    5,
                    52,
                    2,
                    0,
                    62,
                    0
                ],
                "title": "LiteGS: A High-performance Framework to Train 3DGS in Subminutes via\n  System and Algorithm Codesign",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiteGS: A High-performance Framework to Train 3DGS in Subminutes via\n  System and Algorithm Codesign"
                },
                "summary": "3D Gaussian Splatting (3DGS) has emerged as promising alternative in 3D\nrepresentation. However, it still suffers from high training cost. This paper\nintroduces LiteGS, a high performance framework that systematically optimizes\nthe 3DGS training pipeline from multiple aspects. At the low-level computation\nlayer, we design a ``warp-based raster'' associated with two hardware-aware\noptimizations to significantly reduce gradient reduction overhead. At the\nmid-level data management layer, we introduce dynamic spatial sorting based on\nMorton coding to enable a performant ``Cluster-Cull-Compact'' pipeline and\nimprove data locality, therefore reducing cache misses. At the top-level\nalgorithm layer, we establish a new robust densification criterion based on the\nvariance of the opacity gradient, paired with a more stable opacity control\nmechanism, to achieve more precise parameter growth. Experimental results\ndemonstrate that LiteGS accelerates the original 3DGS training by up to 13.4x\nwith comparable or superior quality and surpasses the current SOTA in\nlightweight models by up to 1.4x speedup. For high-quality reconstruction\ntasks, LiteGS sets a new accuracy record and decreases the training time by an\norder of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Gaussian Splatting (3DGS) has emerged as promising alternative in 3D\nrepresentation. However, it still suffers from high training cost. This paper\nintroduces LiteGS, a high performance framework that systematically optimizes\nthe 3DGS training pipeline from multiple aspects. At the low-level computation\nlayer, we design a ``warp-based raster'' associated with two hardware-aware\noptimizations to significantly reduce gradient reduction overhead. At the\nmid-level data management layer, we introduce dynamic spatial sorting based on\nMorton coding to enable a performant ``Cluster-Cull-Compact'' pipeline and\nimprove data locality, therefore reducing cache misses. At the top-level\nalgorithm layer, we establish a new robust densification criterion based on the\nvariance of the opacity gradient, paired with a more stable opacity control\nmechanism, to achieve more precise parameter growth. Experimental results\ndemonstrate that LiteGS accelerates the original 3DGS training by up to 13.4x\nwith comparable or superior quality and surpasses the current SOTA in\nlightweight models by up to 1.4x speedup. For high-quality reconstruction\ntasks, LiteGS sets a new accuracy record and decreases the training time by an\norder of magnitude."
                },
                "authors": [
                    {
                        "name": "Kaimin Liao"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Luchao Wang"
                    },
                    {
                        "name": "Yaohua Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yaohua Tang"
                },
                "author": "Yaohua Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01199v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01199v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23292v1",
                "updated": "2025-07-31T07:10:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    7,
                    10,
                    39,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T07:10:39Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    7,
                    10,
                    39,
                    3,
                    212,
                    0
                ],
                "title": "SequenceLayers: Sequence Processing and Streaming Neural Networks Made\n  Easy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SequenceLayers: Sequence Processing and Streaming Neural Networks Made\n  Easy"
                },
                "summary": "We introduce a neural network layer API and library for sequence modeling,\ndesigned for easy creation of sequence models that can be executed both\nlayer-by-layer (e.g., teacher-forced training) and step-by-step (e.g.,\nautoregressive sampling). To achieve this, layers define an explicit\nrepresentation of their state over time (e.g., a Transformer KV cache, a\nconvolution buffer, an RNN hidden state), and a step method that evolves that\nstate, tested to give identical results to a stateless layer-wise invocation.\nThis and other aspects of the SequenceLayers contract enables complex models to\nbe immediately streamable, mitigates a wide range of common bugs arising in\nboth streaming and parallel sequence processing, and can be implemented in any\ndeep learning library. A composable and declarative API, along with a\ncomprehensive suite of layers and combinators, streamlines the construction of\nproduction-scale models from simple streamable components while preserving\nstrong correctness guarantees. Our current implementations of SequenceLayers\n(JAX, TensorFlow 2) are available at https://github.com/google/sequence-layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a neural network layer API and library for sequence modeling,\ndesigned for easy creation of sequence models that can be executed both\nlayer-by-layer (e.g., teacher-forced training) and step-by-step (e.g.,\nautoregressive sampling). To achieve this, layers define an explicit\nrepresentation of their state over time (e.g., a Transformer KV cache, a\nconvolution buffer, an RNN hidden state), and a step method that evolves that\nstate, tested to give identical results to a stateless layer-wise invocation.\nThis and other aspects of the SequenceLayers contract enables complex models to\nbe immediately streamable, mitigates a wide range of common bugs arising in\nboth streaming and parallel sequence processing, and can be implemented in any\ndeep learning library. A composable and declarative API, along with a\ncomprehensive suite of layers and combinators, streamlines the construction of\nproduction-scale models from simple streamable components while preserving\nstrong correctness guarantees. Our current implementations of SequenceLayers\n(JAX, TensorFlow 2) are available at https://github.com/google/sequence-layers."
                },
                "authors": [
                    {
                        "name": "RJ Skerry-Ryan"
                    },
                    {
                        "name": "Julian Salazar"
                    },
                    {
                        "name": "Soroosh Mariooryad"
                    },
                    {
                        "name": "David Kao"
                    },
                    {
                        "name": "Daisy Stanton"
                    },
                    {
                        "name": "Eric Battenberg"
                    },
                    {
                        "name": "Matt Shannon"
                    },
                    {
                        "name": "Ron J. Weiss"
                    },
                    {
                        "name": "Robin Scheibler"
                    },
                    {
                        "name": "Jonas Rothfuss"
                    },
                    {
                        "name": "Tom Bagby"
                    }
                ],
                "author_detail": {
                    "name": "Tom Bagby"
                },
                "author": "Tom Bagby",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07966v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07966v3",
                "updated": "2025-07-30T16:55:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    55,
                    33,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-10T17:47:40Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    47,
                    40,
                    3,
                    191,
                    0
                ],
                "title": "Scaling RL to Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling RL to Long Videos"
                },
                "summary": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B\nsupports processing up to 8,192 video frames per video, and configurable FPS\nsettings. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B\nsupports processing up to 8,192 video frames per video, and configurable FPS\nsettings. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames)."
                },
                "authors": [
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Baifeng Shi"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Hanrong Ye"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Xiaojuan Qi"
                    },
                    {
                        "name": "Sifei Liu"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Code at https://github.com/NVlabs/Long-RL and model at\n  https://huggingface.co/Efficient-Large-Model/LongVILA-R1-7B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07966v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07966v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22801v1",
                "updated": "2025-07-30T16:04:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    4,
                    1,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T16:04:01Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    4,
                    1,
                    2,
                    211,
                    0
                ],
                "title": "DSPE: Profit Maximization in Edge-Cloud Storage System using Dynamic\n  Space Partitioning with Erasure Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DSPE: Profit Maximization in Edge-Cloud Storage System using Dynamic\n  Space Partitioning with Erasure Code"
                },
                "summary": "Edge Storage Systems have emerged as a critical enabler of low latency data\naccess in modern cloud networks by bringing storage and computation closer to\nend users. However, the limited storage capacity of edge servers poses\nsignificant challenges in handling high volume and latency sensitive data\naccess requests, particularly under dynamic workloads. In this work, we propose\na profit driven framework that integrates three key mechanisms which are\ncollaborative caching, erasure coding, and elastic storage partitioning. Unlike\ntraditional replication, erasure coding enables space efficient redundancy,\nallowing data to be reconstructed from any subset of K out of K plus M coded\nblocks. We dynamically partition each edge server s storage into private and\npublic regions. The private region is further subdivided among access points\nbased on their incoming request rates, enabling adaptive control over data\nlocality and ownership. We design a data placement and replacement policy that\ndetermines how and where to store or evict coded data blocks to maximize data\naccess within deadlines. While the private region serves requests from local\nAPs, the public region handles cooperative storage requests from neighboring\nservers. Our proposed Dynamic Space Partitioning and Elastic caching strategy\nis evaluated on both synthetic and real world traces from Netflix and Spotify.\nExperimental results show that our method improves overall system profitability\nby approximately 5 to 8% compared to state of the art approaches under varied\nworkload conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Storage Systems have emerged as a critical enabler of low latency data\naccess in modern cloud networks by bringing storage and computation closer to\nend users. However, the limited storage capacity of edge servers poses\nsignificant challenges in handling high volume and latency sensitive data\naccess requests, particularly under dynamic workloads. In this work, we propose\na profit driven framework that integrates three key mechanisms which are\ncollaborative caching, erasure coding, and elastic storage partitioning. Unlike\ntraditional replication, erasure coding enables space efficient redundancy,\nallowing data to be reconstructed from any subset of K out of K plus M coded\nblocks. We dynamically partition each edge server s storage into private and\npublic regions. The private region is further subdivided among access points\nbased on their incoming request rates, enabling adaptive control over data\nlocality and ownership. We design a data placement and replacement policy that\ndetermines how and where to store or evict coded data blocks to maximize data\naccess within deadlines. While the private region serves requests from local\nAPs, the public region handles cooperative storage requests from neighboring\nservers. Our proposed Dynamic Space Partitioning and Elastic caching strategy\nis evaluated on both synthetic and real world traces from Netflix and Spotify.\nExperimental results show that our method improves overall system profitability\nby approximately 5 to 8% compared to state of the art approaches under varied\nworkload conditions."
                },
                "authors": [
                    {
                        "name": "Shubhradeep Roy"
                    },
                    {
                        "name": "Suvarthi Sarkar"
                    },
                    {
                        "name": "Vivek Verma"
                    },
                    {
                        "name": "Aryabartta Sahu"
                    }
                ],
                "author_detail": {
                    "name": "Aryabartta Sahu"
                },
                "author": "Aryabartta Sahu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22746v1",
                "updated": "2025-07-30T15:03:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    15,
                    3,
                    36,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T15:03:36Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    15,
                    3,
                    36,
                    2,
                    211,
                    0
                ],
                "title": "Next Tokens Denoising for Speech Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next Tokens Denoising for Speech Synthesis"
                },
                "summary": "While diffusion and autoregressive (AR) models have significantly advanced\ngenerative modeling, they each present distinct limitations. AR models, which\nrely on causal attention, cannot exploit future context and suffer from slow\ngeneration speeds. Conversely, diffusion models struggle with key-value (KV)\ncaching. To overcome these challenges, we introduce Dragon-FM, a novel\ntext-to-speech (TTS) design that unifies AR and flow-matching. This model\nprocesses 48 kHz audio codec tokens in chunks at a compact 12.5 tokens per\nsecond rate. This design enables AR modeling across chunks, ensuring global\ncoherence, while parallel flow-matching within chunks facilitates fast\niterative denoising. Consequently, the proposed model can utilize KV-cache\nacross chunks and incorporate future context within each chunk. Furthermore, it\nbridges continuous and discrete feature modeling, demonstrating that continuous\nAR flow-matching can predict discrete tokens with finite scalar quantizers.\nThis efficient codec and fast chunk-autoregressive architecture also makes the\nproposed model particularly effective for generating extended content.\nExperiment for demos of our work} on podcast datasets demonstrate its\ncapability to efficiently generate high-quality zero-shot podcasts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While diffusion and autoregressive (AR) models have significantly advanced\ngenerative modeling, they each present distinct limitations. AR models, which\nrely on causal attention, cannot exploit future context and suffer from slow\ngeneration speeds. Conversely, diffusion models struggle with key-value (KV)\ncaching. To overcome these challenges, we introduce Dragon-FM, a novel\ntext-to-speech (TTS) design that unifies AR and flow-matching. This model\nprocesses 48 kHz audio codec tokens in chunks at a compact 12.5 tokens per\nsecond rate. This design enables AR modeling across chunks, ensuring global\ncoherence, while parallel flow-matching within chunks facilitates fast\niterative denoising. Consequently, the proposed model can utilize KV-cache\nacross chunks and incorporate future context within each chunk. Furthermore, it\nbridges continuous and discrete feature modeling, demonstrating that continuous\nAR flow-matching can predict discrete tokens with finite scalar quantizers.\nThis efficient codec and fast chunk-autoregressive architecture also makes the\nproposed model particularly effective for generating extended content.\nExperiment for demos of our work} on podcast datasets demonstrate its\ncapability to efficiently generate high-quality zero-shot podcasts."
                },
                "authors": [
                    {
                        "name": "Yanqing Liu"
                    },
                    {
                        "name": "Ruiqing Xue"
                    },
                    {
                        "name": "Chong Zhang"
                    },
                    {
                        "name": "Yufei Liu"
                    },
                    {
                        "name": "Gang Wang"
                    },
                    {
                        "name": "Bohan Li"
                    },
                    {
                        "name": "Yao Qian"
                    },
                    {
                        "name": "Lei He"
                    },
                    {
                        "name": "Shujie Liu"
                    },
                    {
                        "name": "Sheng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Zhao"
                },
                "author": "Sheng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22636v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22636v1",
                "updated": "2025-07-30T12:55:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    55,
                    55,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T12:55:55Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    55,
                    55,
                    2,
                    211,
                    0
                ],
                "title": "All-gluon amplitudes with off-shell recursion in multiplet bases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All-gluon amplitudes with off-shell recursion in multiplet bases"
                },
                "summary": "The efficient computation of color-summed QCD amplitudes at high parton\nmultiplicities remains a central challenge for precision collider predictions.\nExisting approaches using trace, color-flow, or adjoint bases suffer from\nnon-orthogonality, which complicates the color algebra and scales poorly with\nmultiplicity. In this work, we present an off-shell recursive framework for\ncomputing all-gluon tree-level amplitudes directly in orthogonal multiplet\nbases. Utilizing Wigner $6j$ coefficients, we construct an algorithm that\nbuilds multiplet-projected off-shell currents from lower-point currents. By\noptimizing the recursion through partial summation and caching, we find that\nthe computational complexity of calculating $n$-gluon color-summed squared\namplitudes scales as $\\mathcal{O}(17^n)$. This demonstrates the potential\ncompetitiveness of multiplet bases for high-multiplicity processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficient computation of color-summed QCD amplitudes at high parton\nmultiplicities remains a central challenge for precision collider predictions.\nExisting approaches using trace, color-flow, or adjoint bases suffer from\nnon-orthogonality, which complicates the color algebra and scales poorly with\nmultiplicity. In this work, we present an off-shell recursive framework for\ncomputing all-gluon tree-level amplitudes directly in orthogonal multiplet\nbases. Utilizing Wigner $6j$ coefficients, we construct an algorithm that\nbuilds multiplet-projected off-shell currents from lower-point currents. By\noptimizing the recursion through partial summation and caching, we find that\nthe computational complexity of calculating $n$-gluon color-summed squared\namplitudes scales as $\\mathcal{O}(17^n)$. This demonstrates the potential\ncompetitiveness of multiplet bases for high-multiplicity processes."
                },
                "authors": [
                    {
                        "name": "Oskar Bolinder"
                    },
                    {
                        "name": "Rikkert Frederix"
                    },
                    {
                        "name": "Malin Sjodahl"
                    }
                ],
                "author_detail": {
                    "name": "Malin Sjodahl"
                },
                "author": "Malin Sjodahl",
                "arxiv_comment": "15 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22636v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22636v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20984v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20984v2",
                "updated": "2025-07-30T06:29:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    6,
                    29,
                    40,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-28T16:45:14Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    45,
                    14,
                    0,
                    209,
                    0
                ],
                "title": "SmallThinker: A Family of Efficient Large Language Models Natively\n  Trained for Local Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmallThinker: A Family of Efficient Large Language Models Natively\n  Trained for Local Deployment"
                },
                "summary": "While frontier large language models (LLMs) continue to push capability\nboundaries, their deployment remains confined to GPU-powered cloud\ninfrastructure. We challenge this paradigm with SmallThinker, a family of LLMs\nnatively designed - not adapted - for the unique constraints of local devices:\nweak computational power, limited memory, and slow storage. Unlike traditional\napproaches that mainly compress existing models built for clouds, we architect\nSmallThinker from the ground up to thrive within these limitations. Our\ninnovation lies in a deployment-aware architecture that transforms constraints\ninto design principles. First, We introduce a two-level sparse structure\ncombining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward\nnetworks, drastically reducing computational demands without sacrificing model\ncapacity. Second, to conquer the I/O bottleneck of slow storage, we design a\npre-attention router that enables our co-designed inference engine to prefetch\nexpert parameters from storage while computing attention, effectively hiding\nstorage latency that would otherwise cripple on-device inference. Third, for\nmemory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to\nslash KV cache requirements. We release SmallThinker-4B-A0.6B and\nSmallThinker-21B-A3B, which achieve state-of-the-art performance scores and\neven outperform larger LLMs. Remarkably, our co-designed system mostly\neliminates the need for expensive GPU hardware: with Q4_0 quantization, both\nmodels exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB\nand 8GB of memory respectively. SmallThinker is publicly available at\nhf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and\nhf.co/PowerInfer/SmallThinker-21BA3B-Instruct.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While frontier large language models (LLMs) continue to push capability\nboundaries, their deployment remains confined to GPU-powered cloud\ninfrastructure. We challenge this paradigm with SmallThinker, a family of LLMs\nnatively designed - not adapted - for the unique constraints of local devices:\nweak computational power, limited memory, and slow storage. Unlike traditional\napproaches that mainly compress existing models built for clouds, we architect\nSmallThinker from the ground up to thrive within these limitations. Our\ninnovation lies in a deployment-aware architecture that transforms constraints\ninto design principles. First, We introduce a two-level sparse structure\ncombining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward\nnetworks, drastically reducing computational demands without sacrificing model\ncapacity. Second, to conquer the I/O bottleneck of slow storage, we design a\npre-attention router that enables our co-designed inference engine to prefetch\nexpert parameters from storage while computing attention, effectively hiding\nstorage latency that would otherwise cripple on-device inference. Third, for\nmemory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to\nslash KV cache requirements. We release SmallThinker-4B-A0.6B and\nSmallThinker-21B-A3B, which achieve state-of-the-art performance scores and\neven outperform larger LLMs. Remarkably, our co-designed system mostly\neliminates the need for expensive GPU hardware: with Q4_0 quantization, both\nmodels exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB\nand 8GB of memory respectively. SmallThinker is publicly available at\nhf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and\nhf.co/PowerInfer/SmallThinker-21BA3B-Instruct."
                },
                "authors": [
                    {
                        "name": "Yixin Song"
                    },
                    {
                        "name": "Zhenliang Xue"
                    },
                    {
                        "name": "Dongliang Wei"
                    },
                    {
                        "name": "Feiyang Chen"
                    },
                    {
                        "name": "Jianxiang Gao"
                    },
                    {
                        "name": "Junchen Liu"
                    },
                    {
                        "name": "Hangyu Liang"
                    },
                    {
                        "name": "Guangshuo Qin"
                    },
                    {
                        "name": "Chengrong Tian"
                    },
                    {
                        "name": "Bo Wen"
                    },
                    {
                        "name": "Longyu Zhao"
                    },
                    {
                        "name": "Xinrui Zheng"
                    },
                    {
                        "name": "Zeyu Mi"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20984v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20984v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19442v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19442v3",
                "updated": "2025-07-30T05:24:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    5,
                    24,
                    46,
                    2,
                    211,
                    0
                ],
                "published": "2024-12-27T04:17:57Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    17,
                    57,
                    4,
                    362,
                    0
                ],
                "title": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management"
                },
                "summary": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Tianhao Tang"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Nicole Hu"
                    },
                    {
                        "name": "Wei Dong"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "arxiv_comment": "Accepted to TMLR 2025. The revised version incorporates more papers\n  and has been further polished",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19442v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19442v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21492v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21492v1",
                "updated": "2025-07-29T04:21:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    29,
                    4,
                    21,
                    11,
                    1,
                    210,
                    0
                ],
                "published": "2025-07-29T04:21:11Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    4,
                    21,
                    11,
                    1,
                    210,
                    0
                ],
                "title": "Bridging Cache-Friendliness and Concurrency: A Locality-Optimized\n  In-Memory B-Skiplist",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Cache-Friendliness and Concurrency: A Locality-Optimized\n  In-Memory B-Skiplist"
                },
                "summary": "Skiplists are widely used for in-memory indexing in many key-value stores,\nsuch as RocksDB and LevelDB, due to their ease of implementation and simple\nconcurrency control mechanisms. However, traditional skiplists suffer from poor\ncache locality, as they store only a single element per node, leaving\nperformance on the table. Minimizing last-level cache misses is key to\nmaximizing in-memory index performance, making high cache locality essential.\nIn this paper, we present a practical concurrent B-skiplist that enhances cache\nlocality and performance while preserving the simplicity of traditional\nskiplist structures and concurrency control schemes. Our key contributions\ninclude a top-down, single-pass insertion algorithm for B-skiplists and a\ncorresponding simple and efficient top-down concurrency control scheme. On 128\nthreads, the proposed concurrent B-skiplist achieves between 2x-9x higher\nthroughput compared to state-of-the-art concurrent skiplist implementations,\nincluding Facebook's concurrent skiplist from Folly and the Java\nConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves\ncompetitive (0.9x-1.7x) throughput on point workloads compared to\nstate-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a\nmore complete picture of the performance, we also measure the latency of\nskiplist and tree-based indices and find that the B-skiplist achieves between\n3.5x-103x lower 99% latency compared to other concurrent skiplists and between\n0.85x-64x lower 99% latency compared to tree-based indices on point workloads\nwith inserts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skiplists are widely used for in-memory indexing in many key-value stores,\nsuch as RocksDB and LevelDB, due to their ease of implementation and simple\nconcurrency control mechanisms. However, traditional skiplists suffer from poor\ncache locality, as they store only a single element per node, leaving\nperformance on the table. Minimizing last-level cache misses is key to\nmaximizing in-memory index performance, making high cache locality essential.\nIn this paper, we present a practical concurrent B-skiplist that enhances cache\nlocality and performance while preserving the simplicity of traditional\nskiplist structures and concurrency control schemes. Our key contributions\ninclude a top-down, single-pass insertion algorithm for B-skiplists and a\ncorresponding simple and efficient top-down concurrency control scheme. On 128\nthreads, the proposed concurrent B-skiplist achieves between 2x-9x higher\nthroughput compared to state-of-the-art concurrent skiplist implementations,\nincluding Facebook's concurrent skiplist from Folly and the Java\nConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves\ncompetitive (0.9x-1.7x) throughput on point workloads compared to\nstate-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a\nmore complete picture of the performance, we also measure the latency of\nskiplist and tree-based indices and find that the B-skiplist achieves between\n3.5x-103x lower 99% latency compared to other concurrent skiplists and between\n0.85x-64x lower 99% latency compared to tree-based indices on point workloads\nwith inserts."
                },
                "authors": [
                    {
                        "name": "Yicong Luo"
                    },
                    {
                        "name": "Senhe Hao"
                    },
                    {
                        "name": "Brian Wheatman"
                    },
                    {
                        "name": "Prashant Pandey"
                    },
                    {
                        "name": "Helen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Helen Xu"
                },
                "author": "Helen Xu",
                "arxiv_doi": "10.1145/3754598.3754655",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3754598.3754655",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.21492v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21492v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted into ICPP 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24358v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24358v2",
                "updated": "2025-07-28T20:44:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    20,
                    44,
                    23,
                    0,
                    209,
                    0
                ],
                "published": "2025-03-31T17:37:32Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    37,
                    32,
                    0,
                    90,
                    0
                ],
                "title": "SQuat: Subspace-orthogonal KV Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQuat: Subspace-orthogonal KV Cache Quantization"
                },
                "summary": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms."
                },
                "authors": [
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Ligong Han"
                    },
                    {
                        "name": "Kai Xu"
                    },
                    {
                        "name": "Akash Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "Akash Srivastava"
                },
                "author": "Akash Srivastava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24358v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24358v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.13349v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.13349v3",
                "updated": "2025-07-28T14:11:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    11,
                    53,
                    0,
                    209,
                    0
                ],
                "published": "2023-11-22T12:34:51Z",
                "published_parsed": [
                    2023,
                    11,
                    22,
                    12,
                    34,
                    51,
                    2,
                    326,
                    0
                ],
                "title": "REDS: Resource-Efficient Deep Subnetworks for Dynamic Resource\n  Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REDS: Resource-Efficient Deep Subnetworks for Dynamic Resource\n  Constraints"
                },
                "summary": "Deep learning models deployed on edge devices frequently encounter resource\nvariability, which arises from fluctuating energy levels, timing constraints,\nor prioritization of other critical tasks within the system. State-of-the-art\nmachine learning pipelines generate resource-agnostic models that are not\ncapable to adapt at runtime. In this work, we introduce Resource-Efficient Deep\nSubnetworks (REDS) to tackle model adaptation to variable resources. In\ncontrast to the state-of-the-art, REDS leverages structured sparsity\nconstructively by exploiting permutation invariance of neurons, which allows\nfor hardware-specific optimizations. Specifically, REDS achieves computational\nefficiency by (1) skipping sequential computational blocks identified by a\nnovel iterative knapsack optimizer, and (2) taking advantage of data cache by\nre-arranging the order of operations in REDS computational graph. REDS supports\nconventional deep networks frequently deployed on the edge and provides\ncomputational benefits even for small and simple networks. We evaluate REDS on\neight benchmark architectures trained on the Visual Wake Words, Google Speech\nCommands, Fashion-MNIST, CIFAR-10 and ImageNet-1K datasets, and test on four\noff-the-shelf mobile and embedded hardware platforms. We provide a theoretical\nresult and empirical evidence demonstrating REDS' outstanding performance in\nterms of submodels' test set accuracy, and demonstrate an adaptation time in\nresponse to dynamic resource constraints of under 40$\\mu$s, utilizing a\nfully-connected network on Arduino Nano 33 BLE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models deployed on edge devices frequently encounter resource\nvariability, which arises from fluctuating energy levels, timing constraints,\nor prioritization of other critical tasks within the system. State-of-the-art\nmachine learning pipelines generate resource-agnostic models that are not\ncapable to adapt at runtime. In this work, we introduce Resource-Efficient Deep\nSubnetworks (REDS) to tackle model adaptation to variable resources. In\ncontrast to the state-of-the-art, REDS leverages structured sparsity\nconstructively by exploiting permutation invariance of neurons, which allows\nfor hardware-specific optimizations. Specifically, REDS achieves computational\nefficiency by (1) skipping sequential computational blocks identified by a\nnovel iterative knapsack optimizer, and (2) taking advantage of data cache by\nre-arranging the order of operations in REDS computational graph. REDS supports\nconventional deep networks frequently deployed on the edge and provides\ncomputational benefits even for small and simple networks. We evaluate REDS on\neight benchmark architectures trained on the Visual Wake Words, Google Speech\nCommands, Fashion-MNIST, CIFAR-10 and ImageNet-1K datasets, and test on four\noff-the-shelf mobile and embedded hardware platforms. We provide a theoretical\nresult and empirical evidence demonstrating REDS' outstanding performance in\nterms of submodels' test set accuracy, and demonstrate an adaptation time in\nresponse to dynamic resource constraints of under 40$\\mu$s, utilizing a\nfully-connected network on Arduino Nano 33 BLE."
                },
                "authors": [
                    {
                        "name": "Francesco Corti"
                    },
                    {
                        "name": "Balz Maag"
                    },
                    {
                        "name": "Joachim Schauer"
                    },
                    {
                        "name": "Ulrich Pferschy"
                    },
                    {
                        "name": "Olga Saukh"
                    }
                ],
                "author_detail": {
                    "name": "Olga Saukh"
                },
                "author": "Olga Saukh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.13349v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.13349v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20677v1",
                "updated": "2025-07-28T09:59:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    59,
                    22,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T09:59:22Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    59,
                    22,
                    0,
                    209,
                    0
                ],
                "title": "Quantum Circuit Caches and Compressors for Low Latency, High Throughput\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Circuit Caches and Compressors for Low Latency, High Throughput\n  Computing"
                },
                "summary": "Utility-scale quantum programs contain operations on the order of $>10^{15}$\nwhich must be prepared and piped from a classical co-processor to the control\nunit of the quantum device. The latency of this process significantly increases\nwith the size of the program: existing high-level classical representations of\nquantum programs are typically memory intensive and do not na\\\"ively\nefficiently scale to the degree required to execute utility-scale programs in\nreal-time. To combat this limitation, we propose the utilization of high-level\nquantum circuit caches and compressors. The first save on the time associated\nwith repetitive tasks and sub-circuits, and the latter are useful for\nrepresenting the programs/circuits in memory-efficient formats. We present\nnumerical evidence that caches and compressors can offer five orders of\nmagnitude lower latencies during the automatic transpilation of extremely large\nquantum circuits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utility-scale quantum programs contain operations on the order of $>10^{15}$\nwhich must be prepared and piped from a classical co-processor to the control\nunit of the quantum device. The latency of this process significantly increases\nwith the size of the program: existing high-level classical representations of\nquantum programs are typically memory intensive and do not na\\\"ively\nefficiently scale to the degree required to execute utility-scale programs in\nreal-time. To combat this limitation, we propose the utilization of high-level\nquantum circuit caches and compressors. The first save on the time associated\nwith repetitive tasks and sub-circuits, and the latter are useful for\nrepresenting the programs/circuits in memory-efficient formats. We present\nnumerical evidence that caches and compressors can offer five orders of\nmagnitude lower latencies during the automatic transpilation of extremely large\nquantum circuits."
                },
                "authors": [
                    {
                        "name": "Ioana Moflic"
                    },
                    {
                        "name": "Alan Robertson"
                    },
                    {
                        "name": "Simon J. Devitt"
                    },
                    {
                        "name": "Alexandru Paler"
                    }
                ],
                "author_detail": {
                    "name": "Alexandru Paler"
                },
                "author": "Alexandru Paler",
                "arxiv_comment": "accepted at Q-CORE workshop of the QCE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20613v1",
                "updated": "2025-07-28T08:27:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    8,
                    27,
                    40,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T08:27:40Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    8,
                    27,
                    40,
                    0,
                    209,
                    0
                ],
                "title": "Enhancing Large Multimodal Models with Adaptive Sparsity and KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Large Multimodal Models with Adaptive Sparsity and KV Cache\n  Compression"
                },
                "summary": "Large multimodal models (LMMs) have advanced significantly by integrating\nvisual encoders with extensive language models, enabling robust reasoning\ncapabilities. However, compressing LMMs for deployment on edge devices remains\na critical challenge. In this work, we propose an adaptive search algorithm\nthat optimizes sparsity and KV cache compression to enhance LMM efficiency.\nUtilizing the Tree-structured Parzen Estimator, our method dynamically adjusts\npruning ratios and KV cache quantization bandwidth across different LMM layers,\nusing model performance as the optimization objective. This approach uniquely\ncombines pruning with key-value cache quantization and incorporates a fast\npruning technique that eliminates the need for additional fine-tuning or weight\nadjustments, achieving efficient compression without compromising accuracy.\nComprehensive evaluations on benchmark datasets, including LLaVA-1.5 7B and\n13B, demonstrate our method superiority over state-of-the-art techniques such\nas SparseGPT and Wanda across various compression levels. Notably, our\nframework automatic allocation of KV cache compression resources sets a new\nstandard in LMM optimization, delivering memory efficiency without sacrificing\nmuch performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large multimodal models (LMMs) have advanced significantly by integrating\nvisual encoders with extensive language models, enabling robust reasoning\ncapabilities. However, compressing LMMs for deployment on edge devices remains\na critical challenge. In this work, we propose an adaptive search algorithm\nthat optimizes sparsity and KV cache compression to enhance LMM efficiency.\nUtilizing the Tree-structured Parzen Estimator, our method dynamically adjusts\npruning ratios and KV cache quantization bandwidth across different LMM layers,\nusing model performance as the optimization objective. This approach uniquely\ncombines pruning with key-value cache quantization and incorporates a fast\npruning technique that eliminates the need for additional fine-tuning or weight\nadjustments, achieving efficient compression without compromising accuracy.\nComprehensive evaluations on benchmark datasets, including LLaVA-1.5 7B and\n13B, demonstrate our method superiority over state-of-the-art techniques such\nas SparseGPT and Wanda across various compression levels. Notably, our\nframework automatic allocation of KV cache compression resources sets a new\nstandard in LMM optimization, delivering memory efficiency without sacrificing\nmuch performance."
                },
                "authors": [
                    {
                        "name": "Te Zhang"
                    },
                    {
                        "name": "Yuheng Li"
                    },
                    {
                        "name": "Junxiang Wang"
                    },
                    {
                        "name": "Lujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Lujun Li"
                },
                "author": "Lujun Li",
                "arxiv_comment": "6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01760v2",
                "updated": "2025-07-28T04:25:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    4,
                    25,
                    58,
                    0,
                    209,
                    0
                ],
                "published": "2024-10-02T17:14:47Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    14,
                    47,
                    2,
                    276,
                    0
                ],
                "title": "Learning-Augmented Online Caching: New Upper Bounds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-Augmented Online Caching: New Upper Bounds"
                },
                "summary": "We address the problem of learning-augmented online caching in the scenario\nwhen each request is accompanied by a prediction of the next occurrence of the\nrequested page. We improve currently known bounds on the competitive ratio of\nthe BlindOracle algorithm, which evicts a page predicted to be requested last.\nWe also prove a lower bound on the competitive ratio of any randomized\nalgorithm and show that a combination of the BlindOracle with the Marker\nalgorithm achieves a competitive ratio that is optimal up to some constant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the problem of learning-augmented online caching in the scenario\nwhen each request is accompanied by a prediction of the next occurrence of the\nrequested page. We improve currently known bounds on the competitive ratio of\nthe BlindOracle algorithm, which evicts a page predicted to be requested last.\nWe also prove a lower bound on the competitive ratio of any randomized\nalgorithm and show that a combination of the BlindOracle with the Marker\nalgorithm achieves a competitive ratio that is optimal up to some constant."
                },
                "authors": [
                    {
                        "name": "Daniel Skachkov"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    },
                    {
                        "name": "Yuri Dorn"
                    },
                    {
                        "name": "Alexander Demin"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Demin"
                },
                "author": "Alexander Demin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08161v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08161v2",
                "updated": "2025-07-27T09:58:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    27,
                    9,
                    58,
                    25,
                    6,
                    208,
                    0
                ],
                "published": "2025-06-09T19:13:16Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    19,
                    13,
                    16,
                    0,
                    160,
                    0
                ],
                "title": "GATE: Geometry-Aware Trained Encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GATE: Geometry-Aware Trained Encoding"
                },
                "summary": "The encoding of input parameters is one of the fundamental building blocks of\nneural network algorithms. Its goal is to map the input data to a\nhigher-dimensional space, typically supported by trained feature vectors. The\nmapping is crucial for the efficiency and approximation quality of neural\nnetworks. We propose a novel geometry-aware encoding called GATE that stores\nfeature vectors on the surface of triangular meshes. Our encoding is suitable\nfor neural rendering-related algorithms, for example, neural radiance caching.\nIt also avoids limitations of previous hash-based encoding schemes, such as\nhash collisions, selection of resolution versus scene size, and divergent\nmemory access. Our approach decouples feature vector density from geometry\ndensity using mesh colors, while allowing for finer control over neural network\ntraining and adaptive level-of-detail.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The encoding of input parameters is one of the fundamental building blocks of\nneural network algorithms. Its goal is to map the input data to a\nhigher-dimensional space, typically supported by trained feature vectors. The\nmapping is crucial for the efficiency and approximation quality of neural\nnetworks. We propose a novel geometry-aware encoding called GATE that stores\nfeature vectors on the surface of triangular meshes. Our encoding is suitable\nfor neural rendering-related algorithms, for example, neural radiance caching.\nIt also avoids limitations of previous hash-based encoding schemes, such as\nhash collisions, selection of resolution versus scene size, and divergent\nmemory access. Our approach decouples feature vector density from geometry\ndensity using mesh colors, while allowing for finer control over neural network\ntraining and adaptive level-of-detail."
                },
                "authors": [
                    {
                        "name": "Jakub Bokansk"
                    },
                    {
                        "name": "Daniel Meister"
                    },
                    {
                        "name": "Carsten Benthin"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Benthin"
                },
                "author": "Carsten Benthin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08161v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08161v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20173v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20173v1",
                "updated": "2025-07-27T08:25:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    27,
                    8,
                    25,
                    8,
                    6,
                    208,
                    0
                ],
                "published": "2025-07-27T08:25:08Z",
                "published_parsed": [
                    2025,
                    7,
                    27,
                    8,
                    25,
                    8,
                    6,
                    208,
                    0
                ],
                "title": "High-Performance Parallel Optimization of the Fish School Behaviour on\n  the Setonix Platform Using OpenMP",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Performance Parallel Optimization of the Fish School Behaviour on\n  the Setonix Platform Using OpenMP"
                },
                "summary": "This paper presents an in-depth investigation into the high-performance\nparallel optimization of the Fish School Behaviour (FSB) algorithm on the\nSetonix supercomputing platform using the OpenMP framework. Given the\nincreasing demand for enhanced computational capabilities for complex,\nlarge-scale calculations across diverse domains, there's an imperative need for\noptimized parallel algorithms and computing structures. The FSB algorithm,\ninspired by nature's social behavior patterns, provides an ideal platform for\nparallelization due to its iterative and computationally intensive nature. This\nstudy leverages the capabilities of the Setonix platform and the OpenMP\nframework to analyze various aspects of multi-threading, such as thread counts,\nscheduling strategies, and OpenMP constructs, aiming to discern patterns and\nstrategies that can elevate program performance. Experiments were designed to\nrigorously test different configurations, and our results not only offer\ninsights for parallel optimization of FSB on Setonix but also provide valuable\nreferences for other parallel computational research using OpenMP. Looking\nforward, other factors, such as cache behavior and thread scheduling strategies\nat micro and macro levels, hold potential for further exploration and\noptimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an in-depth investigation into the high-performance\nparallel optimization of the Fish School Behaviour (FSB) algorithm on the\nSetonix supercomputing platform using the OpenMP framework. Given the\nincreasing demand for enhanced computational capabilities for complex,\nlarge-scale calculations across diverse domains, there's an imperative need for\noptimized parallel algorithms and computing structures. The FSB algorithm,\ninspired by nature's social behavior patterns, provides an ideal platform for\nparallelization due to its iterative and computationally intensive nature. This\nstudy leverages the capabilities of the Setonix platform and the OpenMP\nframework to analyze various aspects of multi-threading, such as thread counts,\nscheduling strategies, and OpenMP constructs, aiming to discern patterns and\nstrategies that can elevate program performance. Experiments were designed to\nrigorously test different configurations, and our results not only offer\ninsights for parallel optimization of FSB on Setonix but also provide valuable\nreferences for other parallel computational research using OpenMP. Looking\nforward, other factors, such as cache behavior and thread scheduling strategies\nat micro and macro levels, hold potential for further exploration and\noptimization."
                },
                "authors": [
                    {
                        "name": "Haitian Wang"
                    },
                    {
                        "name": "Long Qin"
                    }
                ],
                "author_detail": {
                    "name": "Long Qin"
                },
                "author": "Long Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20173v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20116v1",
                "updated": "2025-07-27T03:45:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    27,
                    3,
                    45,
                    7,
                    6,
                    208,
                    0
                ],
                "published": "2025-07-27T03:45:07Z",
                "published_parsed": [
                    2025,
                    7,
                    27,
                    3,
                    45,
                    7,
                    6,
                    208,
                    0
                ],
                "title": "Accelerating Containerized Service Delivery at the Network Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Containerized Service Delivery at the Network Edge"
                },
                "summary": "Efficient container image distribution is crucial for enabling machine\nlearning inference at the network edge, where resource limitations and dynamic\nnetwork conditions create significant challenges. In this paper, we present\nPeerSync, a decentralized P2P-based system designed to optimize image\ndistribution in edge environments. PeerSync employs a popularity- and\nnetwork-aware download engine that dynamically adapts to content popularity and\nreal-time network conditions using a sliding window mechanism. PeerSync further\nintegrates automated tracker election for rapid peer discovery and dynamic\ncache management for efficient storage utilization. We implement PeerSync with\n8000+ lines of Rust code and test its performance extensively on both physical\nedge devices and Docker-based emulations. Experimental results show that\nPeerSync delivers a remarkable speed increase of 2.72$\\times$, 1.79$\\times$,\nand 1.28$\\times$ compared to the Baseline, Dragonfly, and Kraken, respectively,\nwhile significantly reducing peak cross-network traffic by 90.72\\% under\ncongested and varying network conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient container image distribution is crucial for enabling machine\nlearning inference at the network edge, where resource limitations and dynamic\nnetwork conditions create significant challenges. In this paper, we present\nPeerSync, a decentralized P2P-based system designed to optimize image\ndistribution in edge environments. PeerSync employs a popularity- and\nnetwork-aware download engine that dynamically adapts to content popularity and\nreal-time network conditions using a sliding window mechanism. PeerSync further\nintegrates automated tracker election for rapid peer discovery and dynamic\ncache management for efficient storage utilization. We implement PeerSync with\n8000+ lines of Rust code and test its performance extensively on both physical\nedge devices and Docker-based emulations. Experimental results show that\nPeerSync delivers a remarkable speed increase of 2.72$\\times$, 1.79$\\times$,\nand 1.28$\\times$ compared to the Baseline, Dragonfly, and Kraken, respectively,\nwhile significantly reducing peak cross-network traffic by 90.72\\% under\ncongested and varying network conditions."
                },
                "authors": [
                    {
                        "name": "Yinuo Deng"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Dongjing Wang"
                    },
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Wenzhuo Qian"
                    },
                    {
                        "name": "Jianwei Yin"
                    },
                    {
                        "name": "Schahram Dustdar"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18300v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18300v3",
                "updated": "2025-07-27T00:40:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    27,
                    0,
                    40,
                    47,
                    6,
                    208,
                    0
                ],
                "published": "2025-05-23T18:46:10Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    18,
                    46,
                    10,
                    4,
                    143,
                    0
                ],
                "title": "Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient\n  Nonlinear MCMC on General Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient\n  Nonlinear MCMC on General Graphs"
                },
                "summary": "We propose a history-driven target (HDT) framework in Markov Chain Monte\nCarlo (MCMC) to improve any random walk algorithm on discrete state spaces,\nsuch as general undirected graphs, for efficient sampling from target\ndistribution $\\boldsymbol{\\mu}$. With broad applications in network science and\ndistributed optimization, recent innovations like the self-repellent random\nwalk (SRRW) achieve near-zero variance by prioritizing under-sampled states\nthrough transition kernel modifications based on past visit frequencies.\nHowever, SRRW's reliance on explicit computation of transition probabilities\nfor all neighbors at each step introduces substantial computational overhead,\nwhile its strict dependence on time-reversible Markov chains excludes advanced\nnon-reversible MCMC methods. To overcome these limitations, instead of direct\nmodification of transition kernel, HDT introduces a history-dependent target\ndistribution $\\boldsymbol{\\pi}[\\mathbf{x}]$ to replace the original target\n$\\boldsymbol{\\mu}$ in any graph sampler, where $\\mathbf{x}$ represents the\nempirical measure of past visits. This design preserves lightweight\nimplementation by requiring only local information between the current and\nproposed states and achieves compatibility with both reversible and\nnon-reversible MCMC samplers, while retaining unbiased samples with target\ndistribution $\\boldsymbol{\\mu}$ and near-zero variance performance. Extensive\nexperiments in graph sampling demonstrate consistent performance gains, and a\nmemory-efficient Least Recently Used (LRU) cache ensures scalability to large\ngeneral graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a history-driven target (HDT) framework in Markov Chain Monte\nCarlo (MCMC) to improve any random walk algorithm on discrete state spaces,\nsuch as general undirected graphs, for efficient sampling from target\ndistribution $\\boldsymbol{\\mu}$. With broad applications in network science and\ndistributed optimization, recent innovations like the self-repellent random\nwalk (SRRW) achieve near-zero variance by prioritizing under-sampled states\nthrough transition kernel modifications based on past visit frequencies.\nHowever, SRRW's reliance on explicit computation of transition probabilities\nfor all neighbors at each step introduces substantial computational overhead,\nwhile its strict dependence on time-reversible Markov chains excludes advanced\nnon-reversible MCMC methods. To overcome these limitations, instead of direct\nmodification of transition kernel, HDT introduces a history-dependent target\ndistribution $\\boldsymbol{\\pi}[\\mathbf{x}]$ to replace the original target\n$\\boldsymbol{\\mu}$ in any graph sampler, where $\\mathbf{x}$ represents the\nempirical measure of past visits. This design preserves lightweight\nimplementation by requiring only local information between the current and\nproposed states and achieves compatibility with both reversible and\nnon-reversible MCMC samplers, while retaining unbiased samples with target\ndistribution $\\boldsymbol{\\mu}$ and near-zero variance performance. Extensive\nexperiments in graph sampling demonstrate consistent performance gains, and a\nmemory-efficient Least Recently Used (LRU) cache ensures scalability to large\ngeneral graphs."
                },
                "authors": [
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Yi-Ting Ma"
                    },
                    {
                        "name": "Do Young Eun"
                    }
                ],
                "author_detail": {
                    "name": "Do Young Eun"
                },
                "author": "Do Young Eun",
                "arxiv_comment": "Accepted at ICML 2025 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18300v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18300v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20030v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20030v1",
                "updated": "2025-07-26T18:20:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    18,
                    20,
                    25,
                    5,
                    207,
                    0
                ],
                "published": "2025-07-26T18:20:25Z",
                "published_parsed": [
                    2025,
                    7,
                    26,
                    18,
                    20,
                    25,
                    5,
                    207,
                    0
                ],
                "title": "FAEDKV: Infinite-Window Fourier Transform for Unbiased KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAEDKV: Infinite-Window Fourier Transform for Unbiased KV Cache\n  Compression"
                },
                "summary": "The efficacy of Large Language Models (LLMs) in long-context tasks is often\nhampered by the substantial memory footprint and computational demands of the\nKey-Value (KV) cache. Current compression strategies, including token eviction\nand learned projections, frequently lead to biased representations -- either by\noveremphasizing recent/high-attention tokens or by repeatedly degrading\ninformation from earlier context -- and may require costly model retraining. We\npresent FAEDKV (Frequency-Adaptive Infinite-Window for KV cache), a novel,\ntraining-free KV cache compression framework that ensures unbiased information\nretention. FAEDKV operates by transforming the KV cache into the frequency\ndomain using a proposed Infinite-Window Fourier Transform (IWDFT). This\napproach allows for the equalized contribution of all tokens to the compressed\nrepresentation, effectively preserving both early and recent contextual\ninformation. A preliminary frequency ablation study identifies critical\nspectral components for layer-wise, targeted compression. Experiments on\nLongBench benchmark demonstrate FAEDKV's superiority over existing methods by\nup to 22\\%. In addition, our method shows superior, position-agnostic retrieval\naccuracy on the Needle-In-A-Haystack task compared to compression based\napproaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficacy of Large Language Models (LLMs) in long-context tasks is often\nhampered by the substantial memory footprint and computational demands of the\nKey-Value (KV) cache. Current compression strategies, including token eviction\nand learned projections, frequently lead to biased representations -- either by\noveremphasizing recent/high-attention tokens or by repeatedly degrading\ninformation from earlier context -- and may require costly model retraining. We\npresent FAEDKV (Frequency-Adaptive Infinite-Window for KV cache), a novel,\ntraining-free KV cache compression framework that ensures unbiased information\nretention. FAEDKV operates by transforming the KV cache into the frequency\ndomain using a proposed Infinite-Window Fourier Transform (IWDFT). This\napproach allows for the equalized contribution of all tokens to the compressed\nrepresentation, effectively preserving both early and recent contextual\ninformation. A preliminary frequency ablation study identifies critical\nspectral components for layer-wise, targeted compression. Experiments on\nLongBench benchmark demonstrate FAEDKV's superiority over existing methods by\nup to 22\\%. In addition, our method shows superior, position-agnostic retrieval\naccuracy on the Needle-In-A-Haystack task compared to compression based\napproaches."
                },
                "authors": [
                    {
                        "name": "Runchao Li"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Mu Sheng"
                    },
                    {
                        "name": "Xianxuan Long"
                    },
                    {
                        "name": "Haotian Yu"
                    },
                    {
                        "name": "Pan Li"
                    }
                ],
                "author_detail": {
                    "name": "Pan Li"
                },
                "author": "Pan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20030v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20030v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03140v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03140v2",
                "updated": "2025-07-26T15:25:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    15,
                    25,
                    22,
                    5,
                    207,
                    0
                ],
                "published": "2025-04-04T03:30:15Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    3,
                    30,
                    15,
                    4,
                    94,
                    0
                ],
                "title": "Model Reveals What to Cache: Profiling-Based Feature Reuse for Video\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Reveals What to Cache: Profiling-Based Feature Reuse for Video\n  Diffusion Models"
                },
                "summary": "Recent advances in diffusion models have demonstrated remarkable capabilities\nin video generation. However, the computational intensity remains a significant\nchallenge for practical applications. While feature caching has been proposed\nto reduce the computational burden of diffusion models, existing methods\ntypically overlook the heterogeneous significance of individual blocks,\nresulting in suboptimal reuse and degraded output quality. To this end, we\naddress this gap by introducing ProfilingDiT, a novel adaptive caching strategy\nthat explicitly disentangles foreground and background-focused blocks. Through\na systematic analysis of attention distributions in diffusion models, we reveal\na key observation: 1) Most layers exhibit a consistent preference for either\nforeground or background regions. 2) Predicted noise shows low inter-step\nsimilarity initially, which stabilizes as denoising progresses. This finding\ninspires us to formulate a selective caching strategy that preserves full\ncomputation for dynamic foreground elements while efficiently caching static\nbackground features. Our approach substantially reduces computational overhead\nwhile preserving visual fidelity. Extensive experiments demonstrate that our\nframework achieves significant acceleration (e.g., 2.01 times speedup for\nWan2.1) while maintaining visual fidelity across comprehensive quality metrics,\nestablishing a viable method for efficient video generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in diffusion models have demonstrated remarkable capabilities\nin video generation. However, the computational intensity remains a significant\nchallenge for practical applications. While feature caching has been proposed\nto reduce the computational burden of diffusion models, existing methods\ntypically overlook the heterogeneous significance of individual blocks,\nresulting in suboptimal reuse and degraded output quality. To this end, we\naddress this gap by introducing ProfilingDiT, a novel adaptive caching strategy\nthat explicitly disentangles foreground and background-focused blocks. Through\na systematic analysis of attention distributions in diffusion models, we reveal\na key observation: 1) Most layers exhibit a consistent preference for either\nforeground or background regions. 2) Predicted noise shows low inter-step\nsimilarity initially, which stabilizes as denoising progresses. This finding\ninspires us to formulate a selective caching strategy that preserves full\ncomputation for dynamic foreground elements while efficiently caching static\nbackground features. Our approach substantially reduces computational overhead\nwhile preserving visual fidelity. Extensive experiments demonstrate that our\nframework achieves significant acceleration (e.g., 2.01 times speedup for\nWan2.1) while maintaining visual fidelity across comprehensive quality metrics,\nestablishing a viable method for efficient video generation."
                },
                "authors": [
                    {
                        "name": "Xuran Ma"
                    },
                    {
                        "name": "Yexin Liu"
                    },
                    {
                        "name": "Yaofu Liu"
                    },
                    {
                        "name": "Xianfeng Wu"
                    },
                    {
                        "name": "Mingzhe Zheng"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Ser-Nam Lim"
                    },
                    {
                        "name": "Harry Yang"
                    }
                ],
                "author_detail": {
                    "name": "Harry Yang"
                },
                "author": "Harry Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03140v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03140v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09720v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09720v3",
                "updated": "2025-07-26T15:13:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    15,
                    13,
                    56,
                    5,
                    207,
                    0
                ],
                "published": "2025-02-13T19:11:40Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    11,
                    40,
                    3,
                    44,
                    0
                ],
                "title": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs"
                },
                "summary": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent works have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Metas SpinQuant (perplexity 7.3), OstQuant (7.3) and QuaRot\n(8.2). Comparisons on bigger models (up to 70B) and on various LLM evaluation\nbenchmarks confirm uniform superiority of NestQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent works have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Metas SpinQuant (perplexity 7.3), OstQuant (7.3) and QuaRot\n(8.2). Comparisons on bigger models (up to 70B) and on various LLM evaluation\nbenchmarks confirm uniform superiority of NestQuant."
                },
                "authors": [
                    {
                        "name": "Semyon Savkin"
                    },
                    {
                        "name": "Eitan Porat"
                    },
                    {
                        "name": "Or Ordentlich"
                    },
                    {
                        "name": "Yury Polyanskiy"
                    }
                ],
                "author_detail": {
                    "name": "Yury Polyanskiy"
                },
                "author": "Yury Polyanskiy",
                "arxiv_comment": "23 pages; Accepted at the 42nd International Conference on Machine\n  Learning (ICML 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09720v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09720v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20677v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20677v2",
                "updated": "2025-07-26T13:33:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    13,
                    33,
                    6,
                    5,
                    207,
                    0
                ],
                "published": "2024-12-30T03:05:45Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    5,
                    45,
                    0,
                    365,
                    0
                ],
                "title": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional performance across\ndiverse natural language processing tasks. However, as the model size and the\ninput sequence's length increase, the linearly increasing key-value (KV) cache\nsignificantly degrades inference throughput. Therefore, grouped-query attention\n(GQA), as an alternative to multi-head attention (MHA), has been widely\nintroduced into LLMs. In this work, we propose a cost-effective method for\nconverting MHA into GQA with any compression ratio of KV heads. The key point\nof our method lies in the application of Procrustes analysis to the attention\nheads, which enhances the similarity among attention heads while preserving\ncomputational invariance, thereby improving the model's post-training\nperformance. Subsequently, we employ $\\mathit{L_0}$ regularization to prune\nredundant parameters. The model after pruning can be adapted to the standard\nGQA framework. Experimental results show that our strategy can compress up to\n87.5\\% KV heads of LLaMA2-7B model and 75\\% KV heads of Sheared-LLaMA-1.3B with\nacceptable performance degradation. Our code is released at\nhttps://github.com/fpcsong/mha2gqa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional performance across\ndiverse natural language processing tasks. However, as the model size and the\ninput sequence's length increase, the linearly increasing key-value (KV) cache\nsignificantly degrades inference throughput. Therefore, grouped-query attention\n(GQA), as an alternative to multi-head attention (MHA), has been widely\nintroduced into LLMs. In this work, we propose a cost-effective method for\nconverting MHA into GQA with any compression ratio of KV heads. The key point\nof our method lies in the application of Procrustes analysis to the attention\nheads, which enhances the similarity among attention heads while preserving\ncomputational invariance, thereby improving the model's post-training\nperformance. Subsequently, we employ $\\mathit{L_0}$ regularization to prune\nredundant parameters. The model after pruning can be adapted to the standard\nGQA framework. Experimental results show that our strategy can compress up to\n87.5\\% KV heads of LLaMA2-7B model and 75\\% KV heads of Sheared-LLaMA-1.3B with\nacceptable performance degradation. Our code is released at\nhttps://github.com/fpcsong/mha2gqa."
                },
                "authors": [
                    {
                        "name": "Qingyun Jin"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Feng Zhou"
                    },
                    {
                        "name": "Zengchang Qin"
                    }
                ],
                "author_detail": {
                    "name": "Zengchang Qin"
                },
                "author": "Zengchang Qin",
                "arxiv_comment": "13 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20677v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20677v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19906v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19906v1",
                "updated": "2025-07-26T10:34:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    10,
                    34,
                    53,
                    5,
                    207,
                    0
                ],
                "published": "2025-07-26T10:34:53Z",
                "published_parsed": [
                    2025,
                    7,
                    26,
                    10,
                    34,
                    53,
                    5,
                    207,
                    0
                ],
                "title": "CaliDrop: KV Cache Compression with Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaliDrop: KV Cache Compression with Calibration"
                },
                "summary": "Large Language Models (LLMs) require substantial computational resources\nduring generation. While the Key-Value (KV) cache significantly accelerates\nthis process by storing attention intermediates, its memory footprint grows\nlinearly with sequence length, batch size, and model size, creating a\nbottleneck in long-context scenarios. Various KV cache compression techniques,\nincluding token eviction, quantization, and low-rank projection, have been\nproposed to mitigate this bottleneck, often complementing each other. This\npaper focuses on enhancing token eviction strategies. Token eviction leverages\nthe observation that the attention patterns are often sparse, allowing for the\nremoval of less critical KV entries to save memory. However, this reduction\nusually comes at the cost of notable accuracy degradation, particularly under\nhigh compression ratios. To address this issue, we propose \\textbf{CaliDrop}, a\nnovel strategy that enhances token eviction through calibration. Our\npreliminary experiments show that queries at nearby positions exhibit high\nsimilarity. Building on this observation, CaliDrop performs speculative\ncalibration on the discarded tokens to mitigate the accuracy loss caused by\ntoken eviction. Extensive experiments demonstrate that CaliDrop significantly\nimproves the accuracy of existing token eviction methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) require substantial computational resources\nduring generation. While the Key-Value (KV) cache significantly accelerates\nthis process by storing attention intermediates, its memory footprint grows\nlinearly with sequence length, batch size, and model size, creating a\nbottleneck in long-context scenarios. Various KV cache compression techniques,\nincluding token eviction, quantization, and low-rank projection, have been\nproposed to mitigate this bottleneck, often complementing each other. This\npaper focuses on enhancing token eviction strategies. Token eviction leverages\nthe observation that the attention patterns are often sparse, allowing for the\nremoval of less critical KV entries to save memory. However, this reduction\nusually comes at the cost of notable accuracy degradation, particularly under\nhigh compression ratios. To address this issue, we propose \\textbf{CaliDrop}, a\nnovel strategy that enhances token eviction through calibration. Our\npreliminary experiments show that queries at nearby positions exhibit high\nsimilarity. Building on this observation, CaliDrop performs speculative\ncalibration on the discarded tokens to mitigate the accuracy loss caused by\ntoken eviction. Extensive experiments demonstrate that CaliDrop significantly\nimproves the accuracy of existing token eviction methods."
                },
                "authors": [
                    {
                        "name": "Yi Su"
                    },
                    {
                        "name": "Quantong Qiu"
                    },
                    {
                        "name": "Yuechi Zhou"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Qingrong Xia"
                    },
                    {
                        "name": "Ping Li"
                    },
                    {
                        "name": "Xinyu Duan"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19906v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19906v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19823v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19823v1",
                "updated": "2025-07-26T06:43:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    6,
                    43,
                    14,
                    5,
                    207,
                    0
                ],
                "published": "2025-07-26T06:43:14Z",
                "published_parsed": [
                    2025,
                    7,
                    26,
                    6,
                    43,
                    14,
                    5,
                    207,
                    0
                ],
                "title": "HCAttention: Extreme KV Cache Compression via Heterogeneous Attention\n  Computing for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HCAttention: Extreme KV Cache Compression via Heterogeneous Attention\n  Computing for LLMs"
                },
                "summary": "Processing long-context inputs with large language models presents a\nsignificant challenge due to the enormous memory requirements of the Key-Value\n(KV) cache during inference. Existing KV cache compression methods exhibit\nnoticeable performance degradation when memory is reduced by more than 85%.\nAdditionally, strategies that leverage GPU-CPU collaboration for approximate\nattention remain underexplored in this setting. We propose HCAttention, a\nheterogeneous attention computation framework that integrates key quantization,\nvalue offloading, and dynamic KV eviction to enable efficient inference under\nextreme memory constraints. The method is compatible with existing transformer\narchitectures and does not require model fine-tuning. Experimental results on\nthe LongBench benchmark demonstrate that our approach preserves the accuracy of\nfull-attention model while shrinking the KV cache memory footprint to 25% of\nits original size. Remarkably, it stays competitive with only 12.5% of the\ncache, setting a new state-of-the-art in LLM KV cache compression. To the best\nof our knowledge, HCAttention is the first to extend the Llama-3-8B model to\nprocess 4 million tokens on a single A100 GPU with 80GB memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long-context inputs with large language models presents a\nsignificant challenge due to the enormous memory requirements of the Key-Value\n(KV) cache during inference. Existing KV cache compression methods exhibit\nnoticeable performance degradation when memory is reduced by more than 85%.\nAdditionally, strategies that leverage GPU-CPU collaboration for approximate\nattention remain underexplored in this setting. We propose HCAttention, a\nheterogeneous attention computation framework that integrates key quantization,\nvalue offloading, and dynamic KV eviction to enable efficient inference under\nextreme memory constraints. The method is compatible with existing transformer\narchitectures and does not require model fine-tuning. Experimental results on\nthe LongBench benchmark demonstrate that our approach preserves the accuracy of\nfull-attention model while shrinking the KV cache memory footprint to 25% of\nits original size. Remarkably, it stays competitive with only 12.5% of the\ncache, setting a new state-of-the-art in LLM KV cache compression. To the best\nof our knowledge, HCAttention is the first to extend the Llama-3-8B model to\nprocess 4 million tokens on a single A100 GPU with 80GB memory."
                },
                "authors": [
                    {
                        "name": "Dongquan Yang"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Xiaotian Yu"
                    },
                    {
                        "name": "Xianbiao Qi"
                    },
                    {
                        "name": "Rong Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Rong Xiao"
                },
                "author": "Rong Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19823v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19823v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19718v1",
                "updated": "2025-07-25T23:55:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    23,
                    55,
                    54,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T23:55:54Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    23,
                    55,
                    54,
                    4,
                    206,
                    0
                ],
                "title": "GSCache: Real-Time Radiance Caching for Volume Path Tracing using 3D\n  Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GSCache: Real-Time Radiance Caching for Volume Path Tracing using 3D\n  Gaussian Splatting"
                },
                "summary": "Real-time path tracing is rapidly becoming the standard for rendering in\nentertainment and professional applications. In scientific visualization,\nvolume rendering plays a crucial role in helping researchers analyze and\ninterpret complex 3D data. Recently, photorealistic rendering techniques have\ngained popularity in scientific visualization, yet they face significant\nchallenges. One of the most prominent issues is slow rendering performance and\nhigh pixel variance caused by Monte Carlo integration. In this work, we\nintroduce a novel radiance caching approach for path-traced volume rendering.\nOur method leverages advances in volumetric scene representation and adapts 3D\nGaussian splatting to function as a multi-level, path-space radiance cache.\nThis cache is designed to be trainable on the fly, dynamically adapting to\nchanges in scene parameters such as lighting configurations and transfer\nfunctions. By incorporating our cache, we achieve less noisy, higher-quality\nimages without increasing rendering costs. To evaluate our approach, we compare\nit against a baseline path tracer that supports uniform sampling and next-event\nestimation and the state-of-the-art for neural radiance caching. Through both\nquantitative and qualitative analyses, we demonstrate that our path-space\nradiance cache is a robust solution that is easy to integrate and significantly\nenhances the rendering quality of volumetric visualization applications while\nmaintaining comparable computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time path tracing is rapidly becoming the standard for rendering in\nentertainment and professional applications. In scientific visualization,\nvolume rendering plays a crucial role in helping researchers analyze and\ninterpret complex 3D data. Recently, photorealistic rendering techniques have\ngained popularity in scientific visualization, yet they face significant\nchallenges. One of the most prominent issues is slow rendering performance and\nhigh pixel variance caused by Monte Carlo integration. In this work, we\nintroduce a novel radiance caching approach for path-traced volume rendering.\nOur method leverages advances in volumetric scene representation and adapts 3D\nGaussian splatting to function as a multi-level, path-space radiance cache.\nThis cache is designed to be trainable on the fly, dynamically adapting to\nchanges in scene parameters such as lighting configurations and transfer\nfunctions. By incorporating our cache, we achieve less noisy, higher-quality\nimages without increasing rendering costs. To evaluate our approach, we compare\nit against a baseline path tracer that supports uniform sampling and next-event\nestimation and the state-of-the-art for neural radiance caching. Through both\nquantitative and qualitative analyses, we demonstrate that our path-space\nradiance cache is a robust solution that is easy to integrate and significantly\nenhances the rendering quality of volumetric visualization applications while\nmaintaining comparable computational efficiency."
                },
                "authors": [
                    {
                        "name": "David Bauer"
                    },
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "Hamid Gadirov"
                    },
                    {
                        "name": "Kwan-Liu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Liu Ma"
                },
                "author": "Kwan-Liu Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19427v1",
                "updated": "2025-07-25T16:53:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    16,
                    53,
                    13,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T16:53:13Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    16,
                    53,
                    13,
                    4,
                    206,
                    0
                ],
                "title": "Step-3 is Large yet Affordable: Model-system Co-design for\n  Cost-effective Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step-3 is Large yet Affordable: Model-system Co-design for\n  Cost-effective Decoding"
                },
                "summary": "Large language models (LLMs) face low hardware efficiency during decoding,\nespecially for long-context reasoning tasks. This paper introduces Step-3, a\n321B-parameter VLM with hardware-aware model-system co-design optimized for\nminimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel\nMulti-Matrix Factorization Attention (MFA) mechanism that significantly reduces\nboth KV cache size and computation while maintaining high attention\nexpressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed\ninference system that decouples attention and Feed-Forward Network (FFN) layers\ninto specialized subsystems. This co-design achieves unprecedented cost\nefficiency: Step-3 significantly reduces theoretical decoding costs compared\nwith models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at\nlonger context. Step-3 achieves low cost while activating 38B parameters per\ntoken (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that\nhardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are\ncritical to cost-effectiveness. We perform a head-to-head comparison with\nDeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs\nachieves a decoding throughput of up to 4,039 tokens per second per GPU under\n50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324\nin the same setup and sets a new Pareto frontier for LLM decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face low hardware efficiency during decoding,\nespecially for long-context reasoning tasks. This paper introduces Step-3, a\n321B-parameter VLM with hardware-aware model-system co-design optimized for\nminimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel\nMulti-Matrix Factorization Attention (MFA) mechanism that significantly reduces\nboth KV cache size and computation while maintaining high attention\nexpressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed\ninference system that decouples attention and Feed-Forward Network (FFN) layers\ninto specialized subsystems. This co-design achieves unprecedented cost\nefficiency: Step-3 significantly reduces theoretical decoding costs compared\nwith models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at\nlonger context. Step-3 achieves low cost while activating 38B parameters per\ntoken (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that\nhardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are\ncritical to cost-effectiveness. We perform a head-to-head comparison with\nDeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs\nachieves a decoding throughput of up to 4,039 tokens per second per GPU under\n50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324\nin the same setup and sets a new Pareto frontier for LLM decoding."
                },
                "authors": [
                    {
                        "name": "StepFun"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Bojun Wang"
                    },
                    {
                        "name": "Changyi Wan"
                    },
                    {
                        "name": "Guanzhe Huang"
                    },
                    {
                        "name": "Hanpeng Hu"
                    },
                    {
                        "name": "Haonan Jia"
                    },
                    {
                        "name": "Hao Nie"
                    },
                    {
                        "name": "Mingliang Li"
                    },
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Siyu Chen"
                    },
                    {
                        "name": "Song Yuan"
                    },
                    {
                        "name": "Wuxun Xie"
                    },
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Xing Chen"
                    },
                    {
                        "name": "Xingping Yang"
                    },
                    {
                        "name": "Xuelin Zhang"
                    },
                    {
                        "name": "Yanbo Yu"
                    },
                    {
                        "name": "Yaoyu Wang"
                    },
                    {
                        "name": "Yibo Zhu"
                    },
                    {
                        "name": "Yimin Jiang"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Yuanwei Lu"
                    },
                    {
                        "name": "Houyi Li"
                    },
                    {
                        "name": "Jingcheng Hu"
                    },
                    {
                        "name": "Ka Man Lo"
                    },
                    {
                        "name": "Ailin Huang"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Boyu Chen"
                    },
                    {
                        "name": "Changxin Miao"
                    },
                    {
                        "name": "Chang Lou"
                    },
                    {
                        "name": "Chen Hu"
                    },
                    {
                        "name": "Chen Xu"
                    },
                    {
                        "name": "Chenfeng Yu"
                    },
                    {
                        "name": "Chengyuan Yao"
                    },
                    {
                        "name": "Daokuan Lv"
                    },
                    {
                        "name": "Dapeng Shi"
                    },
                    {
                        "name": "Deshan Sun"
                    },
                    {
                        "name": "Ding Huang"
                    },
                    {
                        "name": "Dingyuan Hu"
                    },
                    {
                        "name": "Dongqing Pang"
                    },
                    {
                        "name": "Enle Liu"
                    },
                    {
                        "name": "Fajie Zhang"
                    },
                    {
                        "name": "Fanqi Wan"
                    },
                    {
                        "name": "Gulin Yan"
                    },
                    {
                        "name": "Han Zhang"
                    },
                    {
                        "name": "Han Zhou"
                    },
                    {
                        "name": "Hanghao Wu"
                    },
                    {
                        "name": "Hangyu Guo"
                    },
                    {
                        "name": "Hanqi Chen"
                    },
                    {
                        "name": "Hanshan Zhang"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Haocheng Zhang"
                    },
                    {
                        "name": "Haolong Yan"
                    },
                    {
                        "name": "Haoran Lv"
                    },
                    {
                        "name": "Haoran Wei"
                    },
                    {
                        "name": "Hebin Zhou"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Hongxin Li"
                    },
                    {
                        "name": "Hongyu Zhou"
                    },
                    {
                        "name": "Hongyuan Wang"
                    },
                    {
                        "name": "Huiyong Guo"
                    },
                    {
                        "name": "Jia Wang"
                    },
                    {
                        "name": "Jiahao Gong"
                    },
                    {
                        "name": "Jialing Xie"
                    },
                    {
                        "name": "Jian Zhou"
                    },
                    {
                        "name": "Jianjian Sun"
                    },
                    {
                        "name": "Jiaoren Wu"
                    },
                    {
                        "name": "Jiaran Zhang"
                    },
                    {
                        "name": "Jiayu Liu"
                    },
                    {
                        "name": "Jie Cheng"
                    },
                    {
                        "name": "Jie Luo"
                    },
                    {
                        "name": "Jie Yan"
                    },
                    {
                        "name": "Jie Yang"
                    },
                    {
                        "name": "Jieyi Hou"
                    },
                    {
                        "name": "Jinguang Zhang"
                    },
                    {
                        "name": "Jinlan Cao"
                    },
                    {
                        "name": "Jisheng Yin"
                    },
                    {
                        "name": "Junfeng Liu"
                    },
                    {
                        "name": "Junhao Huang"
                    },
                    {
                        "name": "Junzhe Lin"
                    },
                    {
                        "name": "Kaijun Tan"
                    },
                    {
                        "name": "Kaixiang Li"
                    },
                    {
                        "name": "Kang An"
                    },
                    {
                        "name": "Kangheng Lin"
                    },
                    {
                        "name": "Kenkun Liu"
                    },
                    {
                        "name": "Lei Yang"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Liangyu Chen"
                    },
                    {
                        "name": "Lieyu Shi"
                    },
                    {
                        "name": "Liguo Tan"
                    },
                    {
                        "name": "Lin Lin"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Lina Chen"
                    },
                    {
                        "name": "Liwen Huang"
                    },
                    {
                        "name": "Liying Shi"
                    },
                    {
                        "name": "Longlong Gu"
                    },
                    {
                        "name": "Mei Chen"
                    },
                    {
                        "name": "Mengqiang Ren"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Mingzhe Chen"
                    },
                    {
                        "name": "Na Wang"
                    },
                    {
                        "name": "Nan Wu"
                    },
                    {
                        "name": "Qi Han"
                    },
                    {
                        "name": "Qian Zhao"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Qianni Liu"
                    },
                    {
                        "name": "Qiaohui Chen"
                    },
                    {
                        "name": "Qiling Wu"
                    },
                    {
                        "name": "Qinglin He"
                    },
                    {
                        "name": "Qinyuan Tan"
                    },
                    {
                        "name": "Qiufeng Wang"
                    },
                    {
                        "name": "Qiuping Wu"
                    },
                    {
                        "name": "Qiuyan Liang"
                    },
                    {
                        "name": "Quan Sun"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Ruihang Miao"
                    },
                    {
                        "name": "Ruosi Wan"
                    },
                    {
                        "name": "Ruyan Guo"
                    },
                    {
                        "name": "Shangwu Zhong"
                    },
                    {
                        "name": "Shaoliang Pang"
                    },
                    {
                        "name": "Shengjie Fan"
                    },
                    {
                        "name": "Shijie Shang"
                    },
                    {
                        "name": "Shilei Jiang"
                    },
                    {
                        "name": "Shiliang Yang"
                    },
                    {
                        "name": "Shiming Hao"
                    },
                    {
                        "name": "Shuli Gao"
                    },
                    {
                        "name": "Siming Huang"
                    },
                    {
                        "name": "Siqi Liu"
                    },
                    {
                        "name": "Tiancheng Cao"
                    },
                    {
                        "name": "Tianhao Cheng"
                    },
                    {
                        "name": "Tianhao Peng"
                    },
                    {
                        "name": "Wang You"
                    },
                    {
                        "name": "Wei Ji"
                    },
                    {
                        "name": "Wen Sun"
                    },
                    {
                        "name": "Wenjin Deng"
                    },
                    {
                        "name": "Wenqing He"
                    },
                    {
                        "name": "Wenzhen Zheng"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Xiangwen Kong"
                    },
                    {
                        "name": "Xianzhen Luo"
                    },
                    {
                        "name": "Xiaobo Yang"
                    },
                    {
                        "name": "Xiaojia Liu"
                    },
                    {
                        "name": "Xiaoxiao Ren"
                    },
                    {
                        "name": "Xin Han"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Xin Wu"
                    },
                    {
                        "name": "Xu Zhao"
                    },
                    {
                        "name": "Yanan Wei"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Yangguang Li"
                    },
                    {
                        "name": "Yangshijie Xu"
                    },
                    {
                        "name": "Yanming Xu"
                    },
                    {
                        "name": "Yaqiang Shi"
                    },
                    {
                        "name": "Yeqing Shen"
                    },
                    {
                        "name": "Yi Yang"
                    },
                    {
                        "name": "Yifei Yang"
                    },
                    {
                        "name": "Yifeng Gong"
                    },
                    {
                        "name": "Yihan Chen"
                    },
                    {
                        "name": "Yijing Yang"
                    },
                    {
                        "name": "Yinmin Zhang"
                    },
                    {
                        "name": "Yizhuang Zhou"
                    },
                    {
                        "name": "Yuanhao Ding"
                    },
                    {
                        "name": "Yuantao Fan"
                    },
                    {
                        "name": "Yuanzhen Yang"
                    },
                    {
                        "name": "Yuchu Luo"
                    },
                    {
                        "name": "Yue Peng"
                    },
                    {
                        "name": "Yufan Lu"
                    },
                    {
                        "name": "Yuhang Deng"
                    },
                    {
                        "name": "Yuhe Yin"
                    },
                    {
                        "name": "Yujie Liu"
                    },
                    {
                        "name": "Yukun Chen"
                    },
                    {
                        "name": "Yuling Zhao"
                    },
                    {
                        "name": "Yun Mou"
                    },
                    {
                        "name": "Yunlong Li"
                    },
                    {
                        "name": "Yunzhou Ju"
                    },
                    {
                        "name": "Yusheng Li"
                    },
                    {
                        "name": "Yuxiang Yang"
                    },
                    {
                        "name": "Yuxiang Zhang"
                    },
                    {
                        "name": "Yuyang Chen"
                    },
                    {
                        "name": "Zejia Weng"
                    },
                    {
                        "name": "Zhe Xie"
                    },
                    {
                        "name": "Zheng Ge"
                    },
                    {
                        "name": "Zheng Gong"
                    },
                    {
                        "name": "Zhenyi Lu"
                    },
                    {
                        "name": "Zhewei Huang"
                    },
                    {
                        "name": "Zhichao Chang"
                    },
                    {
                        "name": "Zhiguo Huang"
                    },
                    {
                        "name": "Zhirui Wang"
                    },
                    {
                        "name": "Zidong Yang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Ziqi Wang"
                    },
                    {
                        "name": "Zixin Zhang"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Daxin Jiang"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhang"
                },
                "author": "Xiangyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19367v1",
                "updated": "2025-07-25T15:17:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    17,
                    29,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T15:17:29Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    17,
                    29,
                    4,
                    206,
                    0
                ],
                "title": "Empowering IoT Firmware Secure Update with Customization Rights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering IoT Firmware Secure Update with Customization Rights"
                },
                "summary": "Firmware updates remain the primary line of defense for IoT devices; however,\nthe update channel itself has become a well-established attack vector. Existing\ndefenses mainly focus on securing monolithic firmware images, leaving\nmodule-level customization -a growing user demand-largely unprotected and\ninsufficiently explored. To address this gap, we conduct a pilot study on the\nupdate workflows of 200 Linux-based IoT devices across 23 vendors, uncovering\nfive previously undocumented vulnerabilities caused by customization practices.\nA broader analysis of update-related CVEs from 2020 to 2024 reveals that over\nhalf originate from customization-induced issues. These findings highlight a\ncritical yet underexamined reality: as customization increases, so does the\nattack surface, while current defenses fail to keep pace. We propose IMUP\n(Integrity-Centric Modular Update Platform), the first framework to address two\nkey challenges: constructing a trustworthy cross-module integrity chain and\nscaling update performance under mass customization. IMUP combines three\ntechniques: per-module chameleon hashing for integrity, server-side\nproof-of-work offloading to reduce device overhead, and server-side caching to\nreuse module combinations, minimizing rebuild costs. Security analysis shows\nthat even when 95 percent of secret keys are exposed, forging a valid image\nincurs over 300 times the cost of the legitimate server. Experiments on\nheterogeneous IoT devices demonstrate that IMUP reduces server-side generation\ntime by 2.9 times and device downtime by 5.9 times compared to a\npackage-manager baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Firmware updates remain the primary line of defense for IoT devices; however,\nthe update channel itself has become a well-established attack vector. Existing\ndefenses mainly focus on securing monolithic firmware images, leaving\nmodule-level customization -a growing user demand-largely unprotected and\ninsufficiently explored. To address this gap, we conduct a pilot study on the\nupdate workflows of 200 Linux-based IoT devices across 23 vendors, uncovering\nfive previously undocumented vulnerabilities caused by customization practices.\nA broader analysis of update-related CVEs from 2020 to 2024 reveals that over\nhalf originate from customization-induced issues. These findings highlight a\ncritical yet underexamined reality: as customization increases, so does the\nattack surface, while current defenses fail to keep pace. We propose IMUP\n(Integrity-Centric Modular Update Platform), the first framework to address two\nkey challenges: constructing a trustworthy cross-module integrity chain and\nscaling update performance under mass customization. IMUP combines three\ntechniques: per-module chameleon hashing for integrity, server-side\nproof-of-work offloading to reduce device overhead, and server-side caching to\nreuse module combinations, minimizing rebuild costs. Security analysis shows\nthat even when 95 percent of secret keys are exposed, forging a valid image\nincurs over 300 times the cost of the legitimate server. Experiments on\nheterogeneous IoT devices demonstrate that IMUP reduces server-side generation\ntime by 2.9 times and device downtime by 5.9 times compared to a\npackage-manager baseline."
                },
                "authors": [
                    {
                        "name": "Weihao Chen"
                    },
                    {
                        "name": "Yansong Gao"
                    },
                    {
                        "name": "Boyu Kuang"
                    },
                    {
                        "name": "Jin B. Hong"
                    },
                    {
                        "name": "Yuqing Zhang"
                    },
                    {
                        "name": "Anmin Fu"
                    }
                ],
                "author_detail": {
                    "name": "Anmin Fu"
                },
                "author": "Anmin Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01802v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01802v2",
                "updated": "2025-07-24T19:44:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    19,
                    44,
                    36,
                    3,
                    205,
                    0
                ],
                "published": "2025-02-03T20:30:25Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    20,
                    30,
                    25,
                    0,
                    34,
                    0
                ],
                "title": "General kinetic ion induced electron emission model for metallic walls\n  applied to biased Z-pinch electrodes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General kinetic ion induced electron emission model for metallic walls\n  applied to biased Z-pinch electrodes"
                },
                "summary": "A kinetic ion induced electron emission (IIEE) model for general applications\nis developed to obtain the emitted electron energy spectrum for a distribution\nof ion impacts on a metallic surface. We assume an ionization cascade mechanism\nand use empirical models for the ion and electron stopping powers. The emission\nspectrum and the secondary electron yield (SEY) are validated for a variety of\nmaterials. The IIEE model is used to study the effect of IIEE on the\nplasma-material interactions of Z-pinch electrodes. Un-magnetized\nBoltzmann-Poisson simulations are performed for a Z-pinch plasma doubly bounded\nby two biased copper electrodes with and without IIEE at bias potentials from 0\nto 9 kV. At the anode, the SEY decreases from 0 to 1 kV, but then increases at\nhigher bias potentials. At the cathode, the SEY is much larger due to higher\nenergy ion bombardment and grows with bias potential. As the bias potential\nincreases, the emitted cathode electrons are accelerated to higher energies\ninto the domain collisionally heating the plasma. Above 1 kV, the heating is\nstrong enough to increase the plasma potential. Despite SEY greater than 1,\nonly a classical sheath forms as opposed to a space-charge limited or inverse\nsheath due to the emitted electron flux not reaching the space charge current\nsaturation limits. Furthermore, the current in the emissionless cases saturates\nto a value lower than experiment. With IIEE, the current does not saturate and\ncontinues to increase with the 4 kV case matching most closely with experiment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A kinetic ion induced electron emission (IIEE) model for general applications\nis developed to obtain the emitted electron energy spectrum for a distribution\nof ion impacts on a metallic surface. We assume an ionization cascade mechanism\nand use empirical models for the ion and electron stopping powers. The emission\nspectrum and the secondary electron yield (SEY) are validated for a variety of\nmaterials. The IIEE model is used to study the effect of IIEE on the\nplasma-material interactions of Z-pinch electrodes. Un-magnetized\nBoltzmann-Poisson simulations are performed for a Z-pinch plasma doubly bounded\nby two biased copper electrodes with and without IIEE at bias potentials from 0\nto 9 kV. At the anode, the SEY decreases from 0 to 1 kV, but then increases at\nhigher bias potentials. At the cathode, the SEY is much larger due to higher\nenergy ion bombardment and grows with bias potential. As the bias potential\nincreases, the emitted cathode electrons are accelerated to higher energies\ninto the domain collisionally heating the plasma. Above 1 kV, the heating is\nstrong enough to increase the plasma potential. Despite SEY greater than 1,\nonly a classical sheath forms as opposed to a space-charge limited or inverse\nsheath due to the emitted electron flux not reaching the space charge current\nsaturation limits. Furthermore, the current in the emissionless cases saturates\nto a value lower than experiment. With IIEE, the current does not saturate and\ncontinues to increase with the 4 kV case matching most closely with experiment."
                },
                "authors": [
                    {
                        "name": "Chirag R. Skolar"
                    },
                    {
                        "name": "Kolter Bradshaw"
                    },
                    {
                        "name": "Manaure Francisquez"
                    },
                    {
                        "name": "Lucio Murillo"
                    },
                    {
                        "name": "Vignesh Krishna Kumar"
                    },
                    {
                        "name": "Bhuvana Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Bhuvana Srinivasan"
                },
                "author": "Bhuvana Srinivasan",
                "arxiv_comment": "21 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01802v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01802v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16870v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16870v2",
                "updated": "2025-07-24T17:30:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    17,
                    30,
                    12,
                    3,
                    205,
                    0
                ],
                "published": "2025-03-21T05:58:18Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    5,
                    58,
                    18,
                    4,
                    80,
                    0
                ],
                "title": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs"
                },
                "summary": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B."
                },
                "authors": [
                    {
                        "name": "Anshumann"
                    },
                    {
                        "name": "Mohd Abbas Zaidi"
                    },
                    {
                        "name": "Akhil Kedia"
                    },
                    {
                        "name": "Jinwoo Ahn"
                    },
                    {
                        "name": "Taehwak Kwon"
                    },
                    {
                        "name": "Kangwook Lee"
                    },
                    {
                        "name": "Haejun Lee"
                    },
                    {
                        "name": "Joohyung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Joohyung Lee"
                },
                "author": "Joohyung Lee",
                "arxiv_comment": "Accepted as Oral paper at ACL 2025. Source code is available at\n  https://github.com/akhilkedia/RandomSamplingKD . Anshumann, Mohd Abbas Zaidi\n  and Akhil Kedia have Equal Contribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16870v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16870v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04704v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04704v2",
                "updated": "2025-07-24T16:25:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    25,
                    51,
                    3,
                    205,
                    0
                ],
                "published": "2025-04-07T03:22:15Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    3,
                    22,
                    15,
                    0,
                    97,
                    0
                ],
                "title": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important"
                },
                "summary": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modification of the inference infrastructure and\nsignificant computation overhead. Based on the fact that the Large Language\nmodels are autoregressive models, we propose LagKV, a KV compression strategy\nonly relying on straight forward comparison among KV themselves. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on RULER benchmark show that, our approach\noutperforms SnapKV and StreamingLLM in different compression ratios. Especially\nin the 64-digit passkey retrieval task, our method outperforms the attention\nweight based method $H_2O$ over $50\\%$ with same compression ratios. Our code\nis available at https://github.com/AI-Lab-China-Merchants-Bank/LagKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modification of the inference infrastructure and\nsignificant computation overhead. Based on the fact that the Large Language\nmodels are autoregressive models, we propose LagKV, a KV compression strategy\nonly relying on straight forward comparison among KV themselves. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on RULER benchmark show that, our approach\noutperforms SnapKV and StreamingLLM in different compression ratios. Especially\nin the 64-digit passkey retrieval task, our method outperforms the attention\nweight based method $H_2O$ over $50\\%$ with same compression ratios. Our code\nis available at https://github.com/AI-Lab-China-Merchants-Bank/LagKV."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "JiaMing Zhang"
                    },
                    {
                        "name": "Xiong Li"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04704v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04704v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18446v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18446v1",
                "updated": "2025-07-24T14:30:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    30,
                    48,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T14:30:48Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    30,
                    48,
                    3,
                    205,
                    0
                ],
                "title": "Streaming Sortformer: Speaker Cache-Based Online Speaker Diarization\n  with Arrival-Time Ordering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming Sortformer: Speaker Cache-Based Online Speaker Diarization\n  with Arrival-Time Ordering"
                },
                "summary": "This paper presents a streaming extension for the Sortformer speaker\ndiarization framework, whose key property is the arrival-time ordering of\noutput speakers. The proposed approach employs an Arrival-Order Speaker Cache\n(AOSC) to store frame-level acoustic embeddings of previously observed\nspeakers. Unlike conventional speaker-tracing buffers, AOSC orders embeddings\nby speaker index corresponding to their arrival time order, and is dynamically\nupdated by selecting frames with the highest scores based on the model's past\npredictions. Notably, the number of stored embeddings per speaker is determined\ndynamically by the update mechanism, ensuring efficient cache utilization and\nprecise speaker tracking. Experiments on benchmark datasets confirm the\neffectiveness and flexibility of our approach, even in low-latency setups.\nThese results establish Streaming Sortformer as a robust solution for real-time\nmulti-speaker tracking and a foundation for streaming multi-talker speech\nprocessing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a streaming extension for the Sortformer speaker\ndiarization framework, whose key property is the arrival-time ordering of\noutput speakers. The proposed approach employs an Arrival-Order Speaker Cache\n(AOSC) to store frame-level acoustic embeddings of previously observed\nspeakers. Unlike conventional speaker-tracing buffers, AOSC orders embeddings\nby speaker index corresponding to their arrival time order, and is dynamically\nupdated by selecting frames with the highest scores based on the model's past\npredictions. Notably, the number of stored embeddings per speaker is determined\ndynamically by the update mechanism, ensuring efficient cache utilization and\nprecise speaker tracking. Experiments on benchmark datasets confirm the\neffectiveness and flexibility of our approach, even in low-latency setups.\nThese results establish Streaming Sortformer as a robust solution for real-time\nmulti-speaker tracking and a foundation for streaming multi-talker speech\nprocessing."
                },
                "authors": [
                    {
                        "name": "Ivan Medennikov"
                    },
                    {
                        "name": "Taejin Park"
                    },
                    {
                        "name": "Weiqing Wang"
                    },
                    {
                        "name": "He Huang"
                    },
                    {
                        "name": "Kunal Dhawan"
                    },
                    {
                        "name": "Jinhan Wang"
                    },
                    {
                        "name": "Jagadeesh Balam"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Accepted to Interspeech 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18446v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18446v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18028v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18028v1",
                "updated": "2025-07-24T02:00:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    2,
                    0,
                    9,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T02:00:09Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    2,
                    0,
                    9,
                    3,
                    205,
                    0
                ],
                "title": "NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural\n  KV Database",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural\n  KV Database"
                },
                "summary": "Efficiently editing knowledge stored in large language models (LLMs) enables\nmodel updates without large-scale training. One possible solution is\nLocate-and-Edit (L\\&E), allowing simultaneous modifications of a massive number\nof facts. However, such editing may compromise the general abilities of LLMs\nand even result in forgetting edited facts when scaling up to thousands of\nedits. In this paper, we model existing linear L\\&E methods as querying a\nKey-Value (KV) database. From this perspective, we then propose NeuralDB, an\nediting framework that explicitly represents the edited facts as a neural KV\ndatabase equipped with a non-linear gated retrieval module, % In particular,\nour gated module only operates when inference involves the edited facts,\neffectively preserving the general abilities of LLMs. Comprehensive experiments\ninvolving the editing of 10,000 facts were conducted on the ZsRE and\nCounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results\ndemonstrate that NeuralDB not only excels in editing efficacy, generalization,\nspecificity, fluency, and consistency, but also preserves overall performance\nacross six representative text understanding and generation tasks. Further\nexperiments indicate that NeuralDB maintains its effectiveness even when scaled\nto 100,000 facts (\\textbf{50x} more than in prior work).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently editing knowledge stored in large language models (LLMs) enables\nmodel updates without large-scale training. One possible solution is\nLocate-and-Edit (L\\&E), allowing simultaneous modifications of a massive number\nof facts. However, such editing may compromise the general abilities of LLMs\nand even result in forgetting edited facts when scaling up to thousands of\nedits. In this paper, we model existing linear L\\&E methods as querying a\nKey-Value (KV) database. From this perspective, we then propose NeuralDB, an\nediting framework that explicitly represents the edited facts as a neural KV\ndatabase equipped with a non-linear gated retrieval module, % In particular,\nour gated module only operates when inference involves the edited facts,\neffectively preserving the general abilities of LLMs. Comprehensive experiments\ninvolving the editing of 10,000 facts were conducted on the ZsRE and\nCounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results\ndemonstrate that NeuralDB not only excels in editing efficacy, generalization,\nspecificity, fluency, and consistency, but also preserves overall performance\nacross six representative text understanding and generation tasks. Further\nexperiments indicate that NeuralDB maintains its effectiveness even when scaled\nto 100,000 facts (\\textbf{50x} more than in prior work)."
                },
                "authors": [
                    {
                        "name": "Weizhi Fei"
                    },
                    {
                        "name": "Hao Shi"
                    },
                    {
                        "name": "Jing Xu"
                    },
                    {
                        "name": "Jingchen Peng"
                    },
                    {
                        "name": "Jiazheng Li"
                    },
                    {
                        "name": "Jingzhao Zhang"
                    },
                    {
                        "name": "Bo Bai"
                    },
                    {
                        "name": "Wei Han"
                    },
                    {
                        "name": "Zhenyuan Chen"
                    },
                    {
                        "name": "Xueyan Niu"
                    }
                ],
                "author_detail": {
                    "name": "Xueyan Niu"
                },
                "author": "Xueyan Niu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18028v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18028v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17744v1",
                "updated": "2025-07-23T17:57:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    17,
                    57,
                    9,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-23T17:57:09Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    17,
                    57,
                    9,
                    2,
                    204,
                    0
                ],
                "title": "Yume: An Interactive World Generation Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yume: An Interactive World Generation Model"
                },
                "summary": "Yume aims to use images, text, or videos to create an interactive, realistic,\nand dynamic world, which allows exploration and control using peripheral\ndevices or neural signals. In this report, we present a preview version of\n\\method, which creates a dynamic world from an input image and allows\nexploration of the world using keyboard actions. To achieve this high-fidelity\nand interactive video world generation, we introduce a well-designed framework,\nwhich consists of four main components, including camera motion quantization,\nvideo generation architecture, advanced sampler, and model acceleration. First,\nwe quantize camera motions for stable training and user-friendly interaction\nusing keyboard inputs. Then, we introduce the Masked Video Diffusion\nTransformer~(MVDT) with a memory module for infinite video generation in an\nautoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM)\nand Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE)\nare introduced to the sampler for better visual quality and more precise\ncontrol. Moreover, we investigate model acceleration by synergistic\noptimization of adversarial distillation and caching mechanisms. We use the\nhigh-quality world exploration dataset \\sekai to train \\method, and it achieves\nremarkable results in diverse scenes and applications. All data, codebase, and\nmodel weights are available on https://github.com/stdstu12/YUME. Yume will\nupdate monthly to achieve its original goal. Project page:\nhttps://stdstu12.github.io/YUME-Project/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yume aims to use images, text, or videos to create an interactive, realistic,\nand dynamic world, which allows exploration and control using peripheral\ndevices or neural signals. In this report, we present a preview version of\n\\method, which creates a dynamic world from an input image and allows\nexploration of the world using keyboard actions. To achieve this high-fidelity\nand interactive video world generation, we introduce a well-designed framework,\nwhich consists of four main components, including camera motion quantization,\nvideo generation architecture, advanced sampler, and model acceleration. First,\nwe quantize camera motions for stable training and user-friendly interaction\nusing keyboard inputs. Then, we introduce the Masked Video Diffusion\nTransformer~(MVDT) with a memory module for infinite video generation in an\nautoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM)\nand Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE)\nare introduced to the sampler for better visual quality and more precise\ncontrol. Moreover, we investigate model acceleration by synergistic\noptimization of adversarial distillation and caching mechanisms. We use the\nhigh-quality world exploration dataset \\sekai to train \\method, and it achieves\nremarkable results in diverse scenes and applications. All data, codebase, and\nmodel weights are available on https://github.com/stdstu12/YUME. Yume will\nupdate monthly to achieve its original goal. Project page:\nhttps://stdstu12.github.io/YUME-Project/."
                },
                "authors": [
                    {
                        "name": "Xiaofeng Mao"
                    },
                    {
                        "name": "Shaoheng Lin"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Chuanhao Li"
                    },
                    {
                        "name": "Wenshuo Peng"
                    },
                    {
                        "name": "Tong He"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    },
                    {
                        "name": "Mingmin Chi"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kaipeng Zhang"
                },
                "author": "Kaipeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17647v1",
                "updated": "2025-07-23T16:09:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    16,
                    9,
                    10,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-23T16:09:10Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    16,
                    9,
                    10,
                    2,
                    204,
                    0
                ],
                "title": "SHINE: A Scalable HNSW Index in Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SHINE: A Scalable HNSW Index in Disaggregated Memory"
                },
                "summary": "Approximate nearest neighbor (ANN) search is a fundamental problem in\ncomputer science for which in-memory graph-based methods, such as Hierarchical\nNavigable Small World (HNSW), perform exceptionally well. To scale beyond\nbillions of high-dimensional vectors, the index must be distributed. The\ndisaggregated memory architecture physically separates compute and memory into\ntwo distinct hardware units and has become popular in modern data centers. Both\nunits are connected via RDMA networks that allow compute nodes to directly\naccess remote memory and perform all the computations, posing unique challenges\nfor disaggregated indexes.\n  In this work, we propose a scalable HNSW index for ANN search in\ndisaggregated memory. In contrast to existing distributed approaches, which\npartition the graph at the cost of accuracy, our method builds a\ngraph-preserving index that reaches the same accuracy as a single-machine HNSW.\nContinuously fetching high-dimensional vector data from remote memory leads to\nsevere network bandwidth limitations, which we overcome by employing an\nefficient caching mechanism. Since answering a single query involves processing\nnumerous unique graph nodes, caching alone is not sufficient to achieve high\nscalability. We logically combine the caches of the compute nodes to increase\nthe overall cache effectiveness and confirm the efficiency and scalability of\nour method in our evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate nearest neighbor (ANN) search is a fundamental problem in\ncomputer science for which in-memory graph-based methods, such as Hierarchical\nNavigable Small World (HNSW), perform exceptionally well. To scale beyond\nbillions of high-dimensional vectors, the index must be distributed. The\ndisaggregated memory architecture physically separates compute and memory into\ntwo distinct hardware units and has become popular in modern data centers. Both\nunits are connected via RDMA networks that allow compute nodes to directly\naccess remote memory and perform all the computations, posing unique challenges\nfor disaggregated indexes.\n  In this work, we propose a scalable HNSW index for ANN search in\ndisaggregated memory. In contrast to existing distributed approaches, which\npartition the graph at the cost of accuracy, our method builds a\ngraph-preserving index that reaches the same accuracy as a single-machine HNSW.\nContinuously fetching high-dimensional vector data from remote memory leads to\nsevere network bandwidth limitations, which we overcome by employing an\nefficient caching mechanism. Since answering a single query involves processing\nnumerous unique graph nodes, caching alone is not sufficient to achieve high\nscalability. We logically combine the caches of the compute nodes to increase\nthe overall cache effectiveness and confirm the efficiency and scalability of\nour method in our evaluation."
                },
                "authors": [
                    {
                        "name": "Manuel Widmoser"
                    },
                    {
                        "name": "Daniel Kocher"
                    },
                    {
                        "name": "Nikolaus Augsten"
                    }
                ],
                "author_detail": {
                    "name": "Nikolaus Augsten"
                },
                "author": "Nikolaus Augsten",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16242v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16242v2",
                "updated": "2025-07-23T15:59:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    15,
                    59,
                    38,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-22T05:26:28Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    26,
                    28,
                    1,
                    203,
                    0
                ],
                "title": "Toward a Lightweight and Robust Design for Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward a Lightweight and Robust Design for Caching"
                },
                "summary": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice."
                },
                "authors": [
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Jiaji Zhang"
                    },
                    {
                        "name": "Xueyan Tang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16242v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16242v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17554v1",
                "updated": "2025-07-23T14:43:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    14,
                    43,
                    22,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-23T14:43:22Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    14,
                    43,
                    22,
                    2,
                    204,
                    0
                ],
                "title": "An h-space Based Adversarial Attack for Protection Against Few-shot\n  Personalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An h-space Based Adversarial Attack for Protection Against Few-shot\n  Personalization"
                },
                "summary": "The versatility of diffusion models in generating customized images from few\nsamples raises significant privacy concerns, particularly regarding\nunauthorized modifications of private content. This concerning issue has\nrenewed the efforts in developing protection mechanisms based on adversarial\nattacks, which generate effective perturbations to poison diffusion models. Our\nwork is motivated by the observation that these models exhibit a high degree of\nabstraction within their semantic latent space (`h-space'), which encodes\ncritical high-level features for generating coherent and meaningful content. In\nthis paper, we propose a novel anti-customization approach, called HAAD\n(h-space based Adversarial Attack for Diffusion models), that leverages\nadversarial attacks to craft perturbations based on the h-space that can\nefficiently degrade the image generation process. Building upon HAAD, we\nfurther introduce a more efficient variant, HAAD-KV, that constructs\nperturbations solely based on the KV parameters of the h-space. This strategy\noffers a stronger protection, that is computationally less expensive. Despite\ntheir simplicity, our methods outperform state-of-the-art adversarial attacks,\nhighlighting their effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The versatility of diffusion models in generating customized images from few\nsamples raises significant privacy concerns, particularly regarding\nunauthorized modifications of private content. This concerning issue has\nrenewed the efforts in developing protection mechanisms based on adversarial\nattacks, which generate effective perturbations to poison diffusion models. Our\nwork is motivated by the observation that these models exhibit a high degree of\nabstraction within their semantic latent space (`h-space'), which encodes\ncritical high-level features for generating coherent and meaningful content. In\nthis paper, we propose a novel anti-customization approach, called HAAD\n(h-space based Adversarial Attack for Diffusion models), that leverages\nadversarial attacks to craft perturbations based on the h-space that can\nefficiently degrade the image generation process. Building upon HAAD, we\nfurther introduce a more efficient variant, HAAD-KV, that constructs\nperturbations solely based on the KV parameters of the h-space. This strategy\noffers a stronger protection, that is computationally less expensive. Despite\ntheir simplicity, our methods outperform state-of-the-art adversarial attacks,\nhighlighting their effectiveness."
                },
                "authors": [
                    {
                        "name": "Xide Xu"
                    },
                    {
                        "name": "Sandesh Kamath"
                    },
                    {
                        "name": "Muhammad Atif Butt"
                    },
                    {
                        "name": "Bogdan Raducanu"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Raducanu"
                },
                "author": "Bogdan Raducanu",
                "arxiv_comment": "32 pages, 15 figures. Accepted by ACM Multimedia 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23956v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23956v3",
                "updated": "2025-07-23T11:42:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    11,
                    42,
                    3,
                    2,
                    204,
                    0
                ],
                "published": "2025-03-31T11:13:18Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    13,
                    18,
                    0,
                    90,
                    0
                ],
                "title": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference"
                },
                "summary": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches."
                },
                "authors": [
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Hao Zou"
                    },
                    {
                        "name": "Bochen Wang"
                    },
                    {
                        "name": "Ye Xi"
                    },
                    {
                        "name": "Zhen Xie"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23956v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23956v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17411v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17411v1",
                "updated": "2025-07-23T11:12:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    11,
                    12,
                    8,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-23T11:12:08Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    11,
                    12,
                    8,
                    2,
                    204,
                    0
                ],
                "title": "Multiprocessor Scheduling with Memory Constraints: Fundamental\n  Properties and Finding Optimal Solutions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiprocessor Scheduling with Memory Constraints: Fundamental\n  Properties and Finding Optimal Solutions"
                },
                "summary": "We study the problem of scheduling a general computational DAG on multiple\nprocessors in a 2-level memory hierarchy. This setting is a natural\ngeneralization of several prominent models in the literature, and it\nsimultaneously captures workload balancing, communication, and data movement\ndue to cache size limitations. We first analyze the fundamental properties of\nthis problem from a theoretical perspective, such as its computational\ncomplexity. We also prove that optimizing parallelization and memory management\nseparately, as done in many applications, can result in a solution that is a\nlinear factor away from the optimum.\n  On the algorithmic side, we discuss a natural technique to represent and\nsolve the problem as an Integer Linear Program (ILP). We develop a holistic\nscheduling algorithm based on this approach, and we experimentally study its\nperformance and properties on a small benchmark of computational tasks. Our\nresults confirm that the ILP-based method can indeed find considerably better\nsolutions than a baseline which combines classical scheduling algorithms and\nmemory management policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of scheduling a general computational DAG on multiple\nprocessors in a 2-level memory hierarchy. This setting is a natural\ngeneralization of several prominent models in the literature, and it\nsimultaneously captures workload balancing, communication, and data movement\ndue to cache size limitations. We first analyze the fundamental properties of\nthis problem from a theoretical perspective, such as its computational\ncomplexity. We also prove that optimizing parallelization and memory management\nseparately, as done in many applications, can result in a solution that is a\nlinear factor away from the optimum.\n  On the algorithmic side, we discuss a natural technique to represent and\nsolve the problem as an Integer Linear Program (ILP). We develop a holistic\nscheduling algorithm based on this approach, and we experimentally study its\nperformance and properties on a small benchmark of computational tasks. Our\nresults confirm that the ILP-based method can indeed find considerably better\nsolutions than a baseline which combines classical scheduling algorithms and\nmemory management policies."
                },
                "authors": [
                    {
                        "name": "Pl Andrs Papp"
                    },
                    {
                        "name": "Toni Bhnlein"
                    },
                    {
                        "name": "A. N. Yzelman"
                    }
                ],
                "author_detail": {
                    "name": "A. N. Yzelman"
                },
                "author": "A. N. Yzelman",
                "arxiv_doi": "10.1145/3754598.3754676",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3754598.3754676",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.17411v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17411v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in the 54th International Conference on Parallel Processing\n  (ICPP 2025)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "90B35, 90C10, 68Q10, 68W10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13373v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13373v2",
                "updated": "2025-07-23T10:10:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    10,
                    10,
                    53,
                    2,
                    204,
                    0
                ],
                "published": "2024-11-20T14:52:36Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    52,
                    36,
                    2,
                    325,
                    0
                ],
                "title": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment"
                },
                "summary": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Nuclear\nPhysics (BINP), was based on a source of positive hydrogen ions, accelerated to\n50 keV and for a maximum ion current of 5 A. The beam could be modulated and\nthe maximum overall duration was 50 ms. With the upgrade of RFX-mod to the\npresent RFX-mod2 machine, the DNBI is being renovated to solve several power\nunits faults and improve the overall reliability of the system. The 50 kV power\nsupply is being improved, as well as the power supplies in the high voltage\ndeck and its insulation transformer. Magnetic field survival tests were\nperformed on the toroidal-core-based DC-DC converters that should power the\nelectronic boards in a reliable way. The control system, originally based on\nCAMAC technology, was redesigned to be fully replaced. This contribution\nreviews the technical criticalities emerged in the DNBI check-up and the new\nsolutions adopted to make the DNBI operative and more reliable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Nuclear\nPhysics (BINP), was based on a source of positive hydrogen ions, accelerated to\n50 keV and for a maximum ion current of 5 A. The beam could be modulated and\nthe maximum overall duration was 50 ms. With the upgrade of RFX-mod to the\npresent RFX-mod2 machine, the DNBI is being renovated to solve several power\nunits faults and improve the overall reliability of the system. The 50 kV power\nsupply is being improved, as well as the power supplies in the high voltage\ndeck and its insulation transformer. Magnetic field survival tests were\nperformed on the toroidal-core-based DC-DC converters that should power the\nelectronic boards in a reliable way. The control system, originally based on\nCAMAC technology, was redesigned to be fully replaced. This contribution\nreviews the technical criticalities emerged in the DNBI check-up and the new\nsolutions adopted to make the DNBI operative and more reliable."
                },
                "authors": [
                    {
                        "name": "Marco Barbisan"
                    },
                    {
                        "name": "Marco Boldrin"
                    },
                    {
                        "name": "Luca Cinnirella"
                    },
                    {
                        "name": "Bruno Laterza"
                    },
                    {
                        "name": "Alberto Maistrello"
                    },
                    {
                        "name": "Lionello Marrelli"
                    },
                    {
                        "name": "Federico Molon"
                    },
                    {
                        "name": "Simone Peruzzo"
                    },
                    {
                        "name": "Cesare Taliercio"
                    },
                    {
                        "name": "Marco Valisa"
                    },
                    {
                        "name": "Enrico Zampiva"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Zampiva"
                },
                "author": "Enrico Zampiva",
                "arxiv_doi": "10.1016/j.fusengdes.2025.115320",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.fusengdes.2025.115320",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.13373v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13373v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages (excl. highlights), 8 figures. Contribution to the 33rd\n  Symposium on Fusion Technology (SOFT), 22-27 September 2024. This is the\n  accepted manuscript for the \"Fusion Engineering and Design\" journal",
                "arxiv_journal_ref": "Fusion Engineering and Design, Volume 220, November 2025, 115320",
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16391v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16391v2",
                "updated": "2025-07-23T09:31:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    9,
                    31,
                    1,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-22T09:35:59Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    9,
                    35,
                    59,
                    1,
                    203,
                    0
                ],
                "title": "Ironman: Accelerating Oblivious Transfer Extension for\n  Privacy-Preserving AI with Near-Memory Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ironman: Accelerating Oblivious Transfer Extension for\n  Privacy-Preserving AI with Near-Memory Processing"
                },
                "summary": "With the wide application of machine learning (ML), privacy concerns arise\nwith user data as they may contain sensitive information. Privacy-preserving ML\n(PPML) based on cryptographic primitives has emerged as a promising solution in\nwhich an ML model is directly computed on the encrypted data to provide a\nformal privacy guarantee. However, PPML frameworks heavily rely on the\noblivious transfer (OT) primitive to compute nonlinear functions. OT mainly\ninvolves the computation of single-point correlated OT (SPCOT) and learning\nparity with noise (LPN) operations. As OT is still computed extensively on\ngeneral-purpose CPUs, it becomes the latency bottleneck of modern PPML\nframeworks.\n  In this paper, we propose a novel OT accelerator, dubbed Ironman, to\nsignificantly increase the efficiency of OT and the overall PPML framework. We\nobserve that SPCOT is computation-bounded, and thus propose a hardware-friendly\nSPCOT algorithm with a customized accelerator to improve SPCOT computation\nthroughput. In contrast, LPN is memory-bandwidth-bounded due to irregular\nmemory access patterns. Hence, we further leverage the near-memory processing\n(NMP) architecture equipped with memory-side cache and index sorting to improve\neffective memory bandwidth. With extensive experiments, we demonstrate Ironman\nachieves a 39.2-237.4 times improvement in OT throughput across different NMP\nconfigurations compared to the full-thread CPU implementation. For different\nPPML frameworks, Ironman demonstrates a 2.1-3.4 times reduction in end-to-end\nlatency for both CNN and Transformer models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the wide application of machine learning (ML), privacy concerns arise\nwith user data as they may contain sensitive information. Privacy-preserving ML\n(PPML) based on cryptographic primitives has emerged as a promising solution in\nwhich an ML model is directly computed on the encrypted data to provide a\nformal privacy guarantee. However, PPML frameworks heavily rely on the\noblivious transfer (OT) primitive to compute nonlinear functions. OT mainly\ninvolves the computation of single-point correlated OT (SPCOT) and learning\nparity with noise (LPN) operations. As OT is still computed extensively on\ngeneral-purpose CPUs, it becomes the latency bottleneck of modern PPML\nframeworks.\n  In this paper, we propose a novel OT accelerator, dubbed Ironman, to\nsignificantly increase the efficiency of OT and the overall PPML framework. We\nobserve that SPCOT is computation-bounded, and thus propose a hardware-friendly\nSPCOT algorithm with a customized accelerator to improve SPCOT computation\nthroughput. In contrast, LPN is memory-bandwidth-bounded due to irregular\nmemory access patterns. Hence, we further leverage the near-memory processing\n(NMP) architecture equipped with memory-side cache and index sorting to improve\neffective memory bandwidth. With extensive experiments, we demonstrate Ironman\nachieves a 39.2-237.4 times improvement in OT throughput across different NMP\nconfigurations compared to the full-thread CPU implementation. For different\nPPML frameworks, Ironman demonstrates a 2.1-3.4 times reduction in end-to-end\nlatency for both CNN and Transformer models."
                },
                "authors": [
                    {
                        "name": "Chenqi Lin"
                    },
                    {
                        "name": "Kang Yang"
                    },
                    {
                        "name": "Tianshi Xu"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Zhaohui Chen"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Mingyu Gao"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16391v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16391v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02634v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02634v4",
                "updated": "2025-07-23T08:07:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    8,
                    7,
                    19,
                    2,
                    204,
                    0
                ],
                "published": "2025-06-03T08:51:38Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    8,
                    51,
                    38,
                    1,
                    154,
                    0
                ],
                "title": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider"
                },
                "summary": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity."
                },
                "authors": [
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Jinbo Han"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Sijie Shen"
                    },
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Chenguang Fang"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by USENIX ATC'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02634v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02634v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17286v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17286v2",
                "updated": "2025-07-23T05:57:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    5,
                    57,
                    32,
                    2,
                    204,
                    0
                ],
                "published": "2025-06-15T07:19:33Z",
                "published_parsed": [
                    2025,
                    6,
                    15,
                    7,
                    19,
                    33,
                    6,
                    166,
                    0
                ],
                "title": "GTA: Grouped-head latenT Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTA: Grouped-head latenT Attention"
                },
                "summary": "Attention mechanisms underpin the success of large language models (LLMs),\nyet their substantial computational and memory overhead poses challenges for\noptimizing efficiency and performance. A critical bottleneck arises as KV cache\nand attention computations scale rapidly with text length, challenging\ndeployment on hardware with limited computational and memory resources. We\nobserve that attention mechanisms exhibit substantial redundancy, since the KV\ncache can be significantly compressed and attention maps across heads display\nhigh similarity, revealing that much of the computation and storage is\nunnecessary. Leveraging these insights, we propose \\textbf{G}rouped-Head\nLaten\\textbf{T} \\textbf{A}ttention (GTA), a novel attention mechanism that\nreduces memory usage and computational complexity while maintaining\nperformance. GTA comprises two components: (1) a shared attention map mechanism\nthat reuses attention scores across multiple heads, decreasing the key cache\nsize; and (2) a nonlinear value decoder with learned projections that\ncompresses the value cache into a latent space, further cutting memory needs.\nGTA cuts attention computation FLOPs by up to \\emph{62.5\\%} versus\nGrouped-Query Attention and shrink the KV cache by up to \\emph{70\\%}, all while\navoiding the extra overhead of Multi-Head Latent Attention to improve LLM\ndeployment efficiency. Consequently, GTA models achieve a \\emph{2x} increase in\nend-to-end inference speed, with prefill benefiting from reduced computational\ncost and decoding benefiting from the smaller cache footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention mechanisms underpin the success of large language models (LLMs),\nyet their substantial computational and memory overhead poses challenges for\noptimizing efficiency and performance. A critical bottleneck arises as KV cache\nand attention computations scale rapidly with text length, challenging\ndeployment on hardware with limited computational and memory resources. We\nobserve that attention mechanisms exhibit substantial redundancy, since the KV\ncache can be significantly compressed and attention maps across heads display\nhigh similarity, revealing that much of the computation and storage is\nunnecessary. Leveraging these insights, we propose \\textbf{G}rouped-Head\nLaten\\textbf{T} \\textbf{A}ttention (GTA), a novel attention mechanism that\nreduces memory usage and computational complexity while maintaining\nperformance. GTA comprises two components: (1) a shared attention map mechanism\nthat reuses attention scores across multiple heads, decreasing the key cache\nsize; and (2) a nonlinear value decoder with learned projections that\ncompresses the value cache into a latent space, further cutting memory needs.\nGTA cuts attention computation FLOPs by up to \\emph{62.5\\%} versus\nGrouped-Query Attention and shrink the KV cache by up to \\emph{70\\%}, all while\navoiding the extra overhead of Multi-Head Latent Attention to improve LLM\ndeployment efficiency. Consequently, GTA models achieve a \\emph{2x} increase in\nend-to-end inference speed, with prefill benefiting from reduced computational\ncost and decoding benefiting from the smaller cache footprint."
                },
                "authors": [
                    {
                        "name": "Luoyang Sun"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Jiwen Jiang"
                    },
                    {
                        "name": "Xinjian Wu"
                    },
                    {
                        "name": "Haifeng Zhang"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Lionel Ni"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17286v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17286v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11046v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11046v2",
                "updated": "2025-07-23T01:42:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    1,
                    42,
                    19,
                    2,
                    204,
                    0
                ],
                "published": "2025-02-16T09:08:36Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    9,
                    8,
                    36,
                    6,
                    47,
                    0
                ],
                "title": "Enabling Efficient Transaction Processing on CXL-Based Memory Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Efficient Transaction Processing on CXL-Based Memory Sharing"
                },
                "summary": "Transaction processing systems are the crux for modern data-center\napplications, yet current multi-node systems are slow due to network overheads.\nThis paper advocates for Compute Express Link (CXL) as a network alternative,\nwhich enables low-latency and cache-coherent shared memory accesses. However,\ndirectly adopting standard CXL primitives leads to performance degradation due\nto the high cost of maintaining cross-node cache coherence. To address the CXL\nchallenges, this paper introduces CtXnL, a software-hardware co-designed system\nthat implements a novel hybrid coherence primitive tailored to the loosely\ncoherent nature of transactional data. The core innovation of CtXnL is\nempowering transaction system developers with the ability to selectively\nachieve data coherence. Our evaluations on OLTP workloads demonstrate that\nCtXnL enhances performance, outperforming current network-based systems and\nachieves with up to 2.08x greater throughput than vanilla CXL memory sharing\narchitectures across universal transaction processing policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transaction processing systems are the crux for modern data-center\napplications, yet current multi-node systems are slow due to network overheads.\nThis paper advocates for Compute Express Link (CXL) as a network alternative,\nwhich enables low-latency and cache-coherent shared memory accesses. However,\ndirectly adopting standard CXL primitives leads to performance degradation due\nto the high cost of maintaining cross-node cache coherence. To address the CXL\nchallenges, this paper introduces CtXnL, a software-hardware co-designed system\nthat implements a novel hybrid coherence primitive tailored to the loosely\ncoherent nature of transactional data. The core innovation of CtXnL is\nempowering transaction system developers with the ability to selectively\nachieve data coherence. Our evaluations on OLTP workloads demonstrate that\nCtXnL enhances performance, outperforming current network-based systems and\nachieves with up to 2.08x greater throughput than vanilla CXL memory sharing\narchitectures across universal transaction processing policies."
                },
                "authors": [
                    {
                        "name": "Zhao Wang"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Cong Li"
                    },
                    {
                        "name": "Dimin Niu"
                    },
                    {
                        "name": "Tianchan Guan"
                    },
                    {
                        "name": "Zhaoyang Du"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11046v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11046v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17033v1",
                "updated": "2025-07-22T21:41:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    41,
                    43,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T21:41:43Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    41,
                    43,
                    1,
                    203,
                    0
                ],
                "title": "GATEBLEED: Exploiting On-Core Accelerator Power Gating for High\n  Performance & Stealthy Attacks on AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GATEBLEED: Exploiting On-Core Accelerator Power Gating for High\n  Performance & Stealthy Attacks on AI"
                },
                "summary": "As power consumption from AI training and inference continues to increase, AI\naccelerators are being integrated directly into the CPU. Intel's Advanced\nMatrix Extensions (AMX) is one such example, debuting on the 4th generation\nIntel Xeon Scalable CPU. We discover a timing side and covert channel,\nGATEBLEED, caused by the aggressive power gating utilized to keep the CPU\nwithin operating limits. We show that the GATEBLEED side channel is a threat to\nAI privacy as many ML models such as transformers and CNNs make critical\ncomputationally-heavy decisions based on private values like confidence\nthresholds and routing logits. Timing delays from selective powering down of\nAMX components mean that each matrix multiplication is a potential leakage\npoint when executed on the AMX accelerator. Our research identifies over a\ndozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,\nTensorFlow, etc.), revealing that they can leak sensitive and private\ninformation. GATEBLEED poses a risk for local and remote timing inference, even\nunder previous protective measures. GATEBLEED can be used as a high\nperformance, stealthy remote covert channel and a generic magnifier for timing\ntransmission channels, capable of bypassing traditional cache defenses to leak\narbitrary memory addresses and evading state of the art microarchitectural\nattack detectors under realistic network conditions and system configurations\nin which previous attacks fail. We implement an end-to-end microarchitectural\ninference attack on a transformer model optimized with Intel AMX, achieving a\nmembership inference accuracy of 81% and a precision of 0.89. In a CNN-based or\ntransformer-based mixture-of-experts model optimized with Intel AMX, we leak\nexpert choice with 100% accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As power consumption from AI training and inference continues to increase, AI\naccelerators are being integrated directly into the CPU. Intel's Advanced\nMatrix Extensions (AMX) is one such example, debuting on the 4th generation\nIntel Xeon Scalable CPU. We discover a timing side and covert channel,\nGATEBLEED, caused by the aggressive power gating utilized to keep the CPU\nwithin operating limits. We show that the GATEBLEED side channel is a threat to\nAI privacy as many ML models such as transformers and CNNs make critical\ncomputationally-heavy decisions based on private values like confidence\nthresholds and routing logits. Timing delays from selective powering down of\nAMX components mean that each matrix multiplication is a potential leakage\npoint when executed on the AMX accelerator. Our research identifies over a\ndozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,\nTensorFlow, etc.), revealing that they can leak sensitive and private\ninformation. GATEBLEED poses a risk for local and remote timing inference, even\nunder previous protective measures. GATEBLEED can be used as a high\nperformance, stealthy remote covert channel and a generic magnifier for timing\ntransmission channels, capable of bypassing traditional cache defenses to leak\narbitrary memory addresses and evading state of the art microarchitectural\nattack detectors under realistic network conditions and system configurations\nin which previous attacks fail. We implement an end-to-end microarchitectural\ninference attack on a transformer model optimized with Intel AMX, achieving a\nmembership inference accuracy of 81% and a precision of 0.89. In a CNN-based or\ntransformer-based mixture-of-experts model optimized with Intel AMX, we leak\nexpert choice with 100% accuracy."
                },
                "authors": [
                    {
                        "name": "Joshua Kalyanapu"
                    },
                    {
                        "name": "Farshad Dizani"
                    },
                    {
                        "name": "Darsh Asher"
                    },
                    {
                        "name": "Azam Ghanbari"
                    },
                    {
                        "name": "Rosario Cammarota"
                    },
                    {
                        "name": "Aydin Aysu"
                    },
                    {
                        "name": "Samira Mirbagher Ajorpaz"
                    }
                ],
                "author_detail": {
                    "name": "Samira Mirbagher Ajorpaz"
                },
                "author": "Samira Mirbagher Ajorpaz",
                "arxiv_comment": "Accepted at MICRO 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17029v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17029v1",
                "updated": "2025-07-22T21:33:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    33,
                    30,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T21:33:30Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    33,
                    30,
                    1,
                    203,
                    0
                ],
                "title": "StreamME: Simplify 3D Gaussian Avatar within Live Stream",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamME: Simplify 3D Gaussian Avatar within Live Stream"
                },
                "summary": "We propose StreamME, a method focuses on fast 3D avatar reconstruction. The\nStreamME synchronously records and reconstructs a head avatar from live video\nstreams without any pre-cached data, enabling seamless integration of the\nreconstructed appearance into downstream applications. This exceptionally fast\ntraining strategy, which we refer to as on-the-fly training, is central to our\napproach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminating\nthe reliance on MLPs in deformable 3DGS and relying solely on geometry, which\nsignificantly improves the adaptation speed to facial expression. To further\nensure high efficiency in on-the-fly training, we introduced a simplification\nstrategy based on primary points, which distributes the point clouds more\nsparsely across the facial surface, optimizing points number while maintaining\nrendering quality. Leveraging the on-the-fly training capabilities, our method\nprotects the facial privacy and reduces communication bandwidth in VR system or\nonline conference. Additionally, it can be directly applied to downstream\napplication such as animation, toonify, and relighting. Please refer to our\nproject page for more details: https://songluchuan.github.io/StreamME/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose StreamME, a method focuses on fast 3D avatar reconstruction. The\nStreamME synchronously records and reconstructs a head avatar from live video\nstreams without any pre-cached data, enabling seamless integration of the\nreconstructed appearance into downstream applications. This exceptionally fast\ntraining strategy, which we refer to as on-the-fly training, is central to our\napproach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminating\nthe reliance on MLPs in deformable 3DGS and relying solely on geometry, which\nsignificantly improves the adaptation speed to facial expression. To further\nensure high efficiency in on-the-fly training, we introduced a simplification\nstrategy based on primary points, which distributes the point clouds more\nsparsely across the facial surface, optimizing points number while maintaining\nrendering quality. Leveraging the on-the-fly training capabilities, our method\nprotects the facial privacy and reduces communication bandwidth in VR system or\nonline conference. Additionally, it can be directly applied to downstream\napplication such as animation, toonify, and relighting. Please refer to our\nproject page for more details: https://songluchuan.github.io/StreamME/."
                },
                "authors": [
                    {
                        "name": "Luchuan Song"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Zhan Xu"
                    },
                    {
                        "name": "Yi Zhou"
                    },
                    {
                        "name": "Deepali Aneja"
                    },
                    {
                        "name": "Chenliang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chenliang Xu"
                },
                "author": "Chenliang Xu",
                "arxiv_comment": "12 pages, 15 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17029v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17029v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16933v1",
                "updated": "2025-07-22T18:17:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    18,
                    17,
                    53,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T18:17:53Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    18,
                    17,
                    53,
                    1,
                    203,
                    0
                ],
                "title": "SiLQ: Simple Large Language Model Quantization-Aware Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SiLQ: Simple Large Language Model Quantization-Aware Training"
                },
                "summary": "Large language models can be quantized to reduce inference time latency,\nmodel size, and energy consumption, thereby delivering a better user experience\nat lower cost. A challenge exists to deliver quantized models with minimal loss\nof accuracy in reasonable time, and in particular to do so without requiring\nmechanisms incompatible with specialized inference accelerators. Here, we\ndemonstrate a simple, end-to-end quantization-aware training approach that,\nwith an increase in total model training budget of less than 0.1%, outperforms\nthe leading published quantization methods by large margins on several modern\nbenchmarks, with both base and instruct model variants. The approach easily\ngeneralizes across different model architectures, can be applied to\nactivations, cache, and weights, and requires the introduction of no additional\noperations to the model other than the quantization itself.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models can be quantized to reduce inference time latency,\nmodel size, and energy consumption, thereby delivering a better user experience\nat lower cost. A challenge exists to deliver quantized models with minimal loss\nof accuracy in reasonable time, and in particular to do so without requiring\nmechanisms incompatible with specialized inference accelerators. Here, we\ndemonstrate a simple, end-to-end quantization-aware training approach that,\nwith an increase in total model training budget of less than 0.1%, outperforms\nthe leading published quantization methods by large margins on several modern\nbenchmarks, with both base and instruct model variants. The approach easily\ngeneralizes across different model architectures, can be applied to\nactivations, cache, and weights, and requires the introduction of no additional\noperations to the model other than the quantization itself."
                },
                "authors": [
                    {
                        "name": "Steven K. Esser"
                    },
                    {
                        "name": "Jeffrey L. McKinstry"
                    },
                    {
                        "name": "Deepika Bablani"
                    },
                    {
                        "name": "Rathinakumar Appuswamy"
                    },
                    {
                        "name": "Dharmendra S. Modha"
                    }
                ],
                "author_detail": {
                    "name": "Dharmendra S. Modha"
                },
                "author": "Dharmendra S. Modha",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16784v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16784v1",
                "updated": "2025-07-22T17:30:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    17,
                    30,
                    4,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T17:30:04Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    17,
                    30,
                    4,
                    1,
                    203,
                    0
                ],
                "title": "Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning"
                },
                "summary": "To break the context limits of large language models (LLMs) that bottleneck\nreasoning accuracy and efficiency, we propose the Thread Inference Model (TIM),\na family of LLMs trained for recursive and decompositional problem solving, and\nTIMRUN, an inference runtime enabling long-horizon structured reasoning beyond\ncontext limits. Together, TIM hosted on TIMRUN supports virtually unlimited\nworking memory and multi-hop tool calls within a single language model\ninference, overcoming output limits, positional-embedding constraints, and\nGPU-memory bottlenecks. Performance is achieved by modeling natural language as\nreasoning trees measured by both length and depth instead of linear sequences.\nThe reasoning trees consist of tasks with thoughts, recursive subtasks, and\nconclusions based on the concept we proposed in Schroeder et al, 2025. During\ngeneration, we maintain a working memory that retains only the key-value states\nof the most relevant context tokens, selected by a rule-based subtask-pruning\nmechanism, enabling reuse of positional embeddings and GPU memory pages\nthroughout reasoning. Experimental results show that our system sustains high\ninference throughput, even when manipulating up to 90% of the KV cache in GPU\nmemory. It also delivers accurate reasoning on mathematical tasks and handles\ninformation retrieval challenges that require long-horizon reasoning and\nmulti-hop tool use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To break the context limits of large language models (LLMs) that bottleneck\nreasoning accuracy and efficiency, we propose the Thread Inference Model (TIM),\na family of LLMs trained for recursive and decompositional problem solving, and\nTIMRUN, an inference runtime enabling long-horizon structured reasoning beyond\ncontext limits. Together, TIM hosted on TIMRUN supports virtually unlimited\nworking memory and multi-hop tool calls within a single language model\ninference, overcoming output limits, positional-embedding constraints, and\nGPU-memory bottlenecks. Performance is achieved by modeling natural language as\nreasoning trees measured by both length and depth instead of linear sequences.\nThe reasoning trees consist of tasks with thoughts, recursive subtasks, and\nconclusions based on the concept we proposed in Schroeder et al, 2025. During\ngeneration, we maintain a working memory that retains only the key-value states\nof the most relevant context tokens, selected by a rule-based subtask-pruning\nmechanism, enabling reuse of positional embeddings and GPU memory pages\nthroughout reasoning. Experimental results show that our system sustains high\ninference throughput, even when manipulating up to 90% of the KV cache in GPU\nmemory. It also delivers accurate reasoning on mathematical tasks and handles\ninformation retrieval challenges that require long-horizon reasoning and\nmulti-hop tool use."
                },
                "authors": [
                    {
                        "name": "Hongyin Luo"
                    },
                    {
                        "name": "Nathaniel Morgan"
                    },
                    {
                        "name": "Tina Li"
                    },
                    {
                        "name": "Derek Zhao"
                    },
                    {
                        "name": "Ai Vy Ngo"
                    },
                    {
                        "name": "Philip Schroeder"
                    },
                    {
                        "name": "Lijie Yang"
                    },
                    {
                        "name": "Assaf Ben-Kish"
                    },
                    {
                        "name": "Jack O'Brien"
                    },
                    {
                        "name": "James Glass"
                    }
                ],
                "author_detail": {
                    "name": "James Glass"
                },
                "author": "James Glass",
                "arxiv_comment": "Research preview",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16784v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16784v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16768v1",
                "updated": "2025-07-22T17:13:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    17,
                    13,
                    47,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T17:13:47Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    17,
                    13,
                    47,
                    1,
                    203,
                    0
                ],
                "title": "WGRAMMAR: Leverage Prior Knowledge to Accelerate Structured Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WGRAMMAR: Leverage Prior Knowledge to Accelerate Structured Decoding"
                },
                "summary": "Structured decoding enables large language models (LLMs) to generate outputs\nin formats required by downstream systems, such as HTML or JSON. However,\nexisting methods suffer from efficiency bottlenecks due to grammar compilation,\nstate tracking, and mask creation. We observe that many real-world tasks embed\nstrong prior knowledge about output structure. Leveraging this, we propose a\ndecomposition of constraints into static and dynamic components -- precompiling\nstatic structures offline and instantiating dynamic arguments at runtime using\ngrammar snippets. Instead of relying on pushdown automata, we employ a\ncompositional set of operators to model regular formats, achieving lower\ntransition latency. We introduce wgrammar, a lightweight decoding engine that\nintegrates domain-aware simplification, constraint decomposition, and mask\ncaching, achieving up to 250x speedup over existing systems. wgrammar's source\ncode is publicly available at https://github.com/wrran/wgrammar.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured decoding enables large language models (LLMs) to generate outputs\nin formats required by downstream systems, such as HTML or JSON. However,\nexisting methods suffer from efficiency bottlenecks due to grammar compilation,\nstate tracking, and mask creation. We observe that many real-world tasks embed\nstrong prior knowledge about output structure. Leveraging this, we propose a\ndecomposition of constraints into static and dynamic components -- precompiling\nstatic structures offline and instantiating dynamic arguments at runtime using\ngrammar snippets. Instead of relying on pushdown automata, we employ a\ncompositional set of operators to model regular formats, achieving lower\ntransition latency. We introduce wgrammar, a lightweight decoding engine that\nintegrates domain-aware simplification, constraint decomposition, and mask\ncaching, achieving up to 250x speedup over existing systems. wgrammar's source\ncode is publicly available at https://github.com/wrran/wgrammar."
                },
                "authors": [
                    {
                        "name": "Ran Wang"
                    },
                    {
                        "name": "Xiaoxuan Liu"
                    },
                    {
                        "name": "Hao Ren"
                    },
                    {
                        "name": "Gang Chen"
                    },
                    {
                        "name": "Fanchao Qi"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.10131v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.10131v3",
                "updated": "2025-07-22T16:49:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    16,
                    49,
                    24,
                    1,
                    203,
                    0
                ],
                "published": "2022-12-20T09:58:39Z",
                "published_parsed": [
                    2022,
                    12,
                    20,
                    9,
                    58,
                    39,
                    1,
                    354,
                    0
                ],
                "title": "Hydra: Virtualized Multi-Language Runtime for High-Density Serverless\n  Platforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hydra: Virtualized Multi-Language Runtime for High-Density Serverless\n  Platforms"
                },
                "summary": "Serverless is an attractive computing model that offers seamless scalability\nand elasticity; it takes the infrastructure management burden away from users\nand enables a pay-as-you-use billing model. As a result, serverless is becoming\nincreasingly popular to support highly elastic and bursty workloads. However,\nexisting platforms are supported by bloated virtualization stacks, which,\ncombined with bursty and irregular invocations, lead to high memory and latency\noverheads.\n  To reduce the virtualization stack bloat, we propose Hydra, a virtualized\nmulti-language runtime and platform capable of hosting multiple sandboxes\nrunning concurrently. To fully leverage Hydra's virtualized runtime, we revisit\nthe existing serverless platform design to make it colocation-aware across\nowners and functions, and to feature a caching layer of pre-allocated Hydra\ninstances that can be used by different functions written in different\nlanguages to reduce cold starts. We also propose a snapshotting mechanism to\ncheckpoint and restore individual sandboxes.\n  By consolidating multiple serverless function invocations through Hydra, we\nimprove the overall function density (ops/GB-sec) by 2.41x on average compared\nto OpenWhisk runtimes, the state-of-the-art single-language runtimes used in\nmost serverless platforms, and by 1.43x on average compared to Knative runtimes\nsupporting invocation colocation within the same function. When reproducing the\nAzure Functions trace, our serverless platform operating Hydra instances\nreduces the overall memory footprint by 21.3-43.9% compared to operating\nOpenWhisk instances and by 14.5-30% compared to operating Knative instances.\nHydra eliminates cold starts thanks to the pool of pre-warmed runtime\ninstances, reducing p99 latency by 45.3-375.5x compared to OpenWhisk and by\n1.9-51.4x compared to Knative.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless is an attractive computing model that offers seamless scalability\nand elasticity; it takes the infrastructure management burden away from users\nand enables a pay-as-you-use billing model. As a result, serverless is becoming\nincreasingly popular to support highly elastic and bursty workloads. However,\nexisting platforms are supported by bloated virtualization stacks, which,\ncombined with bursty and irregular invocations, lead to high memory and latency\noverheads.\n  To reduce the virtualization stack bloat, we propose Hydra, a virtualized\nmulti-language runtime and platform capable of hosting multiple sandboxes\nrunning concurrently. To fully leverage Hydra's virtualized runtime, we revisit\nthe existing serverless platform design to make it colocation-aware across\nowners and functions, and to feature a caching layer of pre-allocated Hydra\ninstances that can be used by different functions written in different\nlanguages to reduce cold starts. We also propose a snapshotting mechanism to\ncheckpoint and restore individual sandboxes.\n  By consolidating multiple serverless function invocations through Hydra, we\nimprove the overall function density (ops/GB-sec) by 2.41x on average compared\nto OpenWhisk runtimes, the state-of-the-art single-language runtimes used in\nmost serverless platforms, and by 1.43x on average compared to Knative runtimes\nsupporting invocation colocation within the same function. When reproducing the\nAzure Functions trace, our serverless platform operating Hydra instances\nreduces the overall memory footprint by 21.3-43.9% compared to operating\nOpenWhisk instances and by 14.5-30% compared to operating Knative instances.\nHydra eliminates cold starts thanks to the pool of pre-warmed runtime\ninstances, reducing p99 latency by 45.3-375.5x compared to OpenWhisk and by\n1.9-51.4x compared to Knative."
                },
                "authors": [
                    {
                        "name": "Serhii Ivanenko"
                    },
                    {
                        "name": "Vasyl Lanko"
                    },
                    {
                        "name": "Rudi Horn"
                    },
                    {
                        "name": "Vojin Jovanovic"
                    },
                    {
                        "name": "Rodrigo Bruno"
                    }
                ],
                "author_detail": {
                    "name": "Rodrigo Bruno"
                },
                "author": "Rodrigo Bruno",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2212.10131v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.10131v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16243v1",
                "updated": "2025-07-22T05:34:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    34,
                    3,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T05:34:03Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    34,
                    3,
                    1,
                    203,
                    0
                ],
                "title": "Genus Zero Kashiwara-Vergne Solutions from Braids",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genus Zero Kashiwara-Vergne Solutions from Braids"
                },
                "summary": "Using the language of moperads-monoids in the category of right modules over\nan operad-we reinterpret the Alekseev-Enriquez-Torossian construction of\nKashiwara-Vergne (KV) solutions from associators. We show that any isomorphism\nbetween the moperad of parenthesized braids with a frozen strand and the\nmoperad of chord diagrams gives rise to a family of genus zero KV solutions\noperadically generated by a single classical KV solution. We show that the\nGrothendieck-Teichm\\\"uller module groups act on the latter, intertwining the\nactions of the KV symmetry groups. In the other direction, we show that any\nsymmetric KV solution gives rise to a morphism from the moperad of\nparenthesized braids with a frozen strand to the moperad of tangential\nautomorphisms of free Lie algebras. This morphism factors through the moperad\nof chord diagrams if and only if the associated KV associator is a Drinfeld\nassociator.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using the language of moperads-monoids in the category of right modules over\nan operad-we reinterpret the Alekseev-Enriquez-Torossian construction of\nKashiwara-Vergne (KV) solutions from associators. We show that any isomorphism\nbetween the moperad of parenthesized braids with a frozen strand and the\nmoperad of chord diagrams gives rise to a family of genus zero KV solutions\noperadically generated by a single classical KV solution. We show that the\nGrothendieck-Teichm\\\"uller module groups act on the latter, intertwining the\nactions of the KV symmetry groups. In the other direction, we show that any\nsymmetric KV solution gives rise to a morphism from the moperad of\nparenthesized braids with a frozen strand to the moperad of tangential\nautomorphisms of free Lie algebras. This morphism factors through the moperad\nof chord diagrams if and only if the associated KV associator is a Drinfeld\nassociator."
                },
                "authors": [
                    {
                        "name": "Zsuzsanna Dancso"
                    },
                    {
                        "name": "Iva Halacheva"
                    },
                    {
                        "name": "Guillaume Laplante-Anfossi"
                    },
                    {
                        "name": "Marcy Robertson"
                    },
                    {
                        "name": "Chandan Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandan Singh"
                },
                "author": "Chandan Singh",
                "arxiv_comment": "comments welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.AT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.AT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.QA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "18M60, 17B, 55",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16217v1",
                "updated": "2025-07-22T04:21:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    4,
                    21,
                    3,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T04:21:03Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    4,
                    21,
                    3,
                    1,
                    203,
                    0
                ],
                "title": "Towards Compute-Optimal Many-Shot In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Compute-Optimal Many-Shot In-Context Learning"
                },
                "summary": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL."
                },
                "authors": [
                    {
                        "name": "Shahriar Golchin"
                    },
                    {
                        "name": "Yanfei Chen"
                    },
                    {
                        "name": "Rujun Han"
                    },
                    {
                        "name": "Manan Gandhi"
                    },
                    {
                        "name": "Tianli Yu"
                    },
                    {
                        "name": "Swaroop Mishra"
                    },
                    {
                        "name": "Mihai Surdeanu"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    },
                    {
                        "name": "Chen-Yu Lee"
                    },
                    {
                        "name": "Tomas Pfister"
                    }
                ],
                "author_detail": {
                    "name": "Tomas Pfister"
                },
                "author": "Tomas Pfister",
                "arxiv_comment": "Final version; accepted at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10789v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10789v2",
                "updated": "2025-07-21T19:31:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    19,
                    31,
                    37,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-14T20:38:09Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    20,
                    38,
                    9,
                    0,
                    195,
                    0
                ],
                "title": "Dissecting the NVIDIA Blackwell Architecture with Microbenchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting the NVIDIA Blackwell Architecture with Microbenchmarks"
                },
                "summary": "The rapid development in scientific research provides a need for more compute\npower, which is partly being solved by GPUs. This paper presents a\nmicroarchitectural analysis of the modern NVIDIA Blackwell architecture by\nstudying GPU performance\n  features with thought through microbenchmarks. We unveil key subsystems,\nincluding the memory hierarchy, SM execution\n  pipelines, and the SM sub-core units, including the 5th generation tensor\ncores supporting FP4 and FP6 precisions.\n  To understand the different key features of the NVIDIA GPU, we study latency,\nthroughput, cache behavior, and scheduling\n  details, revealing subtle tuning metrics in the design of Blackwell. To\ndevelop a comprehensive analysis, we compare the\n  Blackwell architecture with the previous Hopper architecture by using the\nGeForce RTX 5080 and H100 PCIe, respectively. We\n  evaluate and compare results, presenting both generational improvements and\nperformance regressions. Additionally, we\n  investigate the role of power efficiency and energy consumption under varied\nworkloads. Our findings provide actionable insights\n  for application developers, compiler writers, and performance engineers to\noptimize workloads on Blackwell-based platforms,\n  and contribute new data to the growing research on GPU architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development in scientific research provides a need for more compute\npower, which is partly being solved by GPUs. This paper presents a\nmicroarchitectural analysis of the modern NVIDIA Blackwell architecture by\nstudying GPU performance\n  features with thought through microbenchmarks. We unveil key subsystems,\nincluding the memory hierarchy, SM execution\n  pipelines, and the SM sub-core units, including the 5th generation tensor\ncores supporting FP4 and FP6 precisions.\n  To understand the different key features of the NVIDIA GPU, we study latency,\nthroughput, cache behavior, and scheduling\n  details, revealing subtle tuning metrics in the design of Blackwell. To\ndevelop a comprehensive analysis, we compare the\n  Blackwell architecture with the previous Hopper architecture by using the\nGeForce RTX 5080 and H100 PCIe, respectively. We\n  evaluate and compare results, presenting both generational improvements and\nperformance regressions. Additionally, we\n  investigate the role of power efficiency and energy consumption under varied\nworkloads. Our findings provide actionable insights\n  for application developers, compiler writers, and performance engineers to\noptimize workloads on Blackwell-based platforms,\n  and contribute new data to the growing research on GPU architectures."
                },
                "authors": [
                    {
                        "name": "Aaron Jarmusch"
                    },
                    {
                        "name": "Nathan Graddon"
                    },
                    {
                        "name": "Sunita Chandrasekaran"
                    }
                ],
                "author_detail": {
                    "name": "Sunita Chandrasekaran"
                },
                "author": "Sunita Chandrasekaran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10789v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10789v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14003v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14003v2",
                "updated": "2025-07-21T19:05:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    19,
                    5,
                    1,
                    0,
                    202,
                    0
                ],
                "published": "2024-10-17T20:11:34Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    20,
                    11,
                    34,
                    3,
                    291,
                    0
                ],
                "title": "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time\n  Systems"
                },
                "summary": "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\nmemory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial for high performance. To support high MLP, shared last-level caches\n(LLCs) are divided into multiple banks, allowing parallel access. However,\nuneven distribution of cache requests from the cores, especially when requests\nfrom multiple cores are concentrated on a single bank, can result in\nsignificant contention affecting all cores that access the cache. Such cache\nbank contention can even be maliciously induced -- known as cache bank-aware\ndenial-of-service (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked shared LLC based multicore real-time systems. By regulating\nbandwidth on a per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache accesses to non-contended banks, thus improving overall\nperformance (throughput) without compromising isolation benefits of throttling.\nWe implement our approach on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively using both synthetic and real-world workloads.\nOur evaluation results show that the proposed per-bank regulation approach\neffectively protects real-time tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\times$ performance improvement for the\nthrottled benign best-effort tasks compared to prior bank-oblivious bandwidth\nthrottling approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\nmemory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial for high performance. To support high MLP, shared last-level caches\n(LLCs) are divided into multiple banks, allowing parallel access. However,\nuneven distribution of cache requests from the cores, especially when requests\nfrom multiple cores are concentrated on a single bank, can result in\nsignificant contention affecting all cores that access the cache. Such cache\nbank contention can even be maliciously induced -- known as cache bank-aware\ndenial-of-service (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked shared LLC based multicore real-time systems. By regulating\nbandwidth on a per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache accesses to non-contended banks, thus improving overall\nperformance (throughput) without compromising isolation benefits of throttling.\nWe implement our approach on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively using both synthetic and real-world workloads.\nOur evaluation results show that the proposed per-bank regulation approach\neffectively protects real-time tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\times$ performance improvement for the\nthrottled benign best-effort tasks compared to prior bank-oblivious bandwidth\nthrottling approaches."
                },
                "authors": [
                    {
                        "name": "Connor Sullivan"
                    },
                    {
                        "name": "Alex Manley"
                    },
                    {
                        "name": "Mohammad Alian"
                    },
                    {
                        "name": "Heechul Yun"
                    }
                ],
                "author_detail": {
                    "name": "Heechul Yun"
                },
                "author": "Heechul Yun",
                "arxiv_doi": "10.1109/RTSS62706.2024.00036",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/RTSS62706.2024.00036",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.14003v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14003v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Update to Fig. 11: The previous version used mismatched cache\n  capacities between the 2-bank and 4-bank configurations in the simulation\n  setup. This has been corrected to ensure both configurations have equal total\n  cache capacity. As a result, the specific numerical results in Fig. 11 have\n  changed. However, the overall trend shown in Fig. 11 and key findings of the\n  paper remain consistent",
                "arxiv_journal_ref": "IEEE Real-Time Systems Symposium (RTSS), 2024, pp. 336-348",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18974v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18974v3",
                "updated": "2025-07-21T14:50:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    14,
                    50,
                    41,
                    0,
                    202,
                    0
                ],
                "published": "2025-03-22T06:14:33Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    6,
                    14,
                    33,
                    5,
                    81,
                    0
                ],
                "title": "An Efficient Frequency-Based Approach for Maximal Square Detection in\n  Binary Matrices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Frequency-Based Approach for Maximal Square Detection in\n  Binary Matrices"
                },
                "summary": "Detecting maximal square submatrices of ones in binary matrices is a\nfundamental problem with applications in computer vision and pattern\nrecognition. While the standard dynamic programming (DP) solution achieves\noptimal asymptotic complexity, its practical performance suffers from repeated\nminimum operations and inefficient memory access patterns that degrade cache\nutilization. To address these limitations, we introduce a novel frequency-based\nalgorithm that employs a greedy approach to track the columnar continuity of\nones through an adaptive frequency array and a dynamic thresholding mechanism.\nExtensive benchmarking demonstrates that the frequency-based algorithm achieves\nfaster performance than the standard DP in 100% of test cases with an average\nspeedup of 3.32x, a maximum speedup of 4.60x, and a minimum speedup of 2.31x\nacross matrices up to 5000x5000 with densities from 0.1 to 0.9. The algorithm's\naverage speedup exceeds 2.5x for all densities and rises to over 3.5x for\ndensities of 0.7 and higher across all matrix sizes. These results demonstrate\nthat the frequency-based approach is a superior alternative to standard DP and\nopens new possibilities for efficient matrix analysis in performance-critical\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting maximal square submatrices of ones in binary matrices is a\nfundamental problem with applications in computer vision and pattern\nrecognition. While the standard dynamic programming (DP) solution achieves\noptimal asymptotic complexity, its practical performance suffers from repeated\nminimum operations and inefficient memory access patterns that degrade cache\nutilization. To address these limitations, we introduce a novel frequency-based\nalgorithm that employs a greedy approach to track the columnar continuity of\nones through an adaptive frequency array and a dynamic thresholding mechanism.\nExtensive benchmarking demonstrates that the frequency-based algorithm achieves\nfaster performance than the standard DP in 100% of test cases with an average\nspeedup of 3.32x, a maximum speedup of 4.60x, and a minimum speedup of 2.31x\nacross matrices up to 5000x5000 with densities from 0.1 to 0.9. The algorithm's\naverage speedup exceeds 2.5x for all densities and rises to over 3.5x for\ndensities of 0.7 and higher across all matrix sizes. These results demonstrate\nthat the frequency-based approach is a superior alternative to standard DP and\nopens new possibilities for efficient matrix analysis in performance-critical\napplications."
                },
                "authors": [
                    {
                        "name": "Swastik Bhandari"
                    }
                ],
                "author_detail": {
                    "name": "Swastik Bhandari"
                },
                "author": "Swastik Bhandari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18974v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18974v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10524v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10524v2",
                "updated": "2025-07-21T07:45:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    7,
                    45,
                    14,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-14T17:49:00Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    49,
                    0,
                    0,
                    195,
                    0
                ],
                "title": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation"
                },
                "summary": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to decrease prefill latency and memory\nfootprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms\na new Pareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to decrease prefill latency and memory\nfootprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms\na new Pareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost."
                },
                "authors": [
                    {
                        "name": "Sangmin Bae"
                    },
                    {
                        "name": "Yujin Kim"
                    },
                    {
                        "name": "Reza Bayat"
                    },
                    {
                        "name": "Sungnyun Kim"
                    },
                    {
                        "name": "Jiyoun Ha"
                    },
                    {
                        "name": "Tal Schuster"
                    },
                    {
                        "name": "Adam Fisch"
                    },
                    {
                        "name": "Hrayr Harutyunyan"
                    },
                    {
                        "name": "Ziwei Ji"
                    },
                    {
                        "name": "Aaron Courville"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "36 pages, 9 figures, 14 tables, codes at\n  https://github.com/raymin0223/mixture_of_recursions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10524v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10524v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09025v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09025v2",
                "updated": "2025-07-20T03:49:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    20,
                    3,
                    49,
                    3,
                    6,
                    201,
                    0
                ],
                "published": "2025-07-11T21:19:18Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    21,
                    19,
                    18,
                    4,
                    192,
                    0
                ],
                "title": "Lizard: An Efficient Linearization Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lizard: An Efficient Linearization Framework for Large Language Models"
                },
                "summary": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\narchitectures for infinite-context generation. Transformer-based LLMs face\nsignificant memory and computational bottlenecks as context lengths increase,\ndue to the quadratic complexity of softmax attention and the growing key-value\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\nattention mechanism that closely approximates softmax attention while\npreserving the output quality. Unlike previous linearization methods, which are\noften limited by fixed model structures and therefore exclude gating\nmechanisms, Lizard incorporates a gating module inspired by recent\nstate-of-the-art linear models. This enables adaptive memory control, supports\nconstant-memory inference, offers strong length generalization, and allows more\nflexible model design. Lizard combines gated linear attention for global\ncontext compression with sliding window attention enhanced by meta memory,\nforming a hybrid mechanism that captures both long-range dependencies and\nfine-grained local interactions. Moreover, we introduce a hardware-aware\nalgorithm that accelerates the training speed of our models. Extensive\nexperiments show that Lizard achieves near-lossless recovery of the teacher\nmodel's performance across standard language modeling tasks, while\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\nbenchmark, Lizard improves over prior models by 18 points and shows significant\nimprovements on associative recall tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\narchitectures for infinite-context generation. Transformer-based LLMs face\nsignificant memory and computational bottlenecks as context lengths increase,\ndue to the quadratic complexity of softmax attention and the growing key-value\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\nattention mechanism that closely approximates softmax attention while\npreserving the output quality. Unlike previous linearization methods, which are\noften limited by fixed model structures and therefore exclude gating\nmechanisms, Lizard incorporates a gating module inspired by recent\nstate-of-the-art linear models. This enables adaptive memory control, supports\nconstant-memory inference, offers strong length generalization, and allows more\nflexible model design. Lizard combines gated linear attention for global\ncontext compression with sliding window attention enhanced by meta memory,\nforming a hybrid mechanism that captures both long-range dependencies and\nfine-grained local interactions. Moreover, we introduce a hardware-aware\nalgorithm that accelerates the training speed of our models. Extensive\nexperiments show that Lizard achieves near-lossless recovery of the teacher\nmodel's performance across standard language modeling tasks, while\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\nbenchmark, Lizard improves over prior models by 18 points and shows significant\nimprovements on associative recall tasks."
                },
                "authors": [
                    {
                        "name": "Chien Van Nguyen"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Hanieh Deilamsalehy"
                    },
                    {
                        "name": "Puneet Mathur"
                    },
                    {
                        "name": "Viet Dac Lai"
                    },
                    {
                        "name": "Haoliang Wang"
                    },
                    {
                        "name": "Jayakumar Subramanian"
                    },
                    {
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "name": "Trung Bui"
                    },
                    {
                        "name": "Nikos Vlassis"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Thien Huu Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Thien Huu Nguyen"
                },
                "author": "Thien Huu Nguyen",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09025v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09025v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11092v2",
                "updated": "2025-07-19T17:46:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    19,
                    17,
                    46,
                    19,
                    5,
                    200,
                    0
                ],
                "published": "2025-06-05T19:47:22Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    19,
                    47,
                    22,
                    3,
                    156,
                    0
                ],
                "title": "Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing\n  Multi-Turn Planning and Tool Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing\n  Multi-Turn Planning and Tool Adaptation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has significantly advanced large\nlanguage models (LLMs) by grounding their outputs in external tools and\nknowledge sources. However, existing RAG systems are typically constrained to\nstatic, single-turn interactions with fixed toolsets, making them ill-suited\nfor dynamic domains such as healthcare and smart homes, where user intent,\navailable tools, and contextual factors evolve over time. We present Dynamic\nContext Tuning (DCT), a lightweight framework that extends RAG to support\nmulti-turn dialogue and evolving tool environments without requiring\nretraining. DCT integrates an attention-based context cache to track relevant\npast information, LoRA-based retrieval to dynamically select domain-specific\ntools, and efficient context compression to maintain inputs within LLM context\nlimits. Experiments on both synthetic and real-world benchmarks show that DCT\nimproves plan accuracy by 14% and reduces hallucinations by 37%, while matching\nGPT-4 performance at significantly lower cost. Furthermore, DCT generalizes to\npreviously unseen tools, enabling scalable and adaptable AI assistants across a\nwide range of dynamic environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has significantly advanced large\nlanguage models (LLMs) by grounding their outputs in external tools and\nknowledge sources. However, existing RAG systems are typically constrained to\nstatic, single-turn interactions with fixed toolsets, making them ill-suited\nfor dynamic domains such as healthcare and smart homes, where user intent,\navailable tools, and contextual factors evolve over time. We present Dynamic\nContext Tuning (DCT), a lightweight framework that extends RAG to support\nmulti-turn dialogue and evolving tool environments without requiring\nretraining. DCT integrates an attention-based context cache to track relevant\npast information, LoRA-based retrieval to dynamically select domain-specific\ntools, and efficient context compression to maintain inputs within LLM context\nlimits. Experiments on both synthetic and real-world benchmarks show that DCT\nimproves plan accuracy by 14% and reduces hallucinations by 37%, while matching\nGPT-4 performance at significantly lower cost. Furthermore, DCT generalizes to\npreviously unseen tools, enabling scalable and adaptable AI assistants across a\nwide range of dynamic environments."
                },
                "authors": [
                    {
                        "name": "Jubin Abhishek Soni"
                    },
                    {
                        "name": "Amit Anand"
                    },
                    {
                        "name": "Rajesh Kumar Pandey"
                    },
                    {
                        "name": "Aniket Abhishek Soni"
                    }
                ],
                "author_detail": {
                    "name": "Aniket Abhishek Soni"
                },
                "author": "Aniket Abhishek Soni",
                "arxiv_comment": "We are withdrawing the submission in order to thoroughly revise the\n  work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17772v1",
                "updated": "2025-07-19T17:02:15Z",
                "updated_parsed": [
                    2025,
                    7,
                    19,
                    17,
                    2,
                    15,
                    5,
                    200,
                    0
                ],
                "published": "2025-07-19T17:02:15Z",
                "published_parsed": [
                    2025,
                    7,
                    19,
                    17,
                    2,
                    15,
                    5,
                    200,
                    0
                ],
                "title": "Caching Techniques for Reducing the Communication Cost of Federated\n  Learning in IoT Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching Techniques for Reducing the Communication Cost of Federated\n  Learning in IoT Environments"
                },
                "summary": "Federated Learning (FL) allows multiple distributed devices to jointly train\na shared model without centralizing data, but communication cost remains a\nmajor bottleneck, especially in resource-constrained environments. This paper\nintroduces caching strategies - FIFO, LRU, and Priority-Based - to reduce\nunnecessary model update transmissions. By selectively forwarding significant\nupdates, our approach lowers bandwidth usage while maintaining model accuracy.\nExperiments on CIFAR-10 and medical datasets show reduced communication with\nminimal accuracy loss. Results confirm that intelligent caching improves\nscalability, memory efficiency, and supports reliable FL in edge IoT networks,\nmaking it practical for deployment in smart cities, healthcare, and other\nlatency-sensitive applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) allows multiple distributed devices to jointly train\na shared model without centralizing data, but communication cost remains a\nmajor bottleneck, especially in resource-constrained environments. This paper\nintroduces caching strategies - FIFO, LRU, and Priority-Based - to reduce\nunnecessary model update transmissions. By selectively forwarding significant\nupdates, our approach lowers bandwidth usage while maintaining model accuracy.\nExperiments on CIFAR-10 and medical datasets show reduced communication with\nminimal accuracy loss. Results confirm that intelligent caching improves\nscalability, memory efficiency, and supports reliable FL in edge IoT networks,\nmaking it practical for deployment in smart cities, healthcare, and other\nlatency-sensitive applications."
                },
                "authors": [
                    {
                        "name": "Ahmad Alhonainy"
                    },
                    {
                        "name": "Praveen Rao"
                    }
                ],
                "author_detail": {
                    "name": "Praveen Rao"
                },
                "arxiv_affiliation": "University of Missouri, USA",
                "author": "Praveen Rao",
                "arxiv_comment": "Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16002v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16002v3",
                "updated": "2025-07-19T07:41:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    19,
                    7,
                    41,
                    3,
                    5,
                    200,
                    0
                ],
                "published": "2025-02-21T23:34:29Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    23,
                    34,
                    29,
                    4,
                    52,
                    0
                ],
                "title": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse"
                },
                "summary": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we investigate a\nnew strategy to eliminate such inefficiency, where the KV cache of each\ndocument is precomputed independently. During inference, the KV caches of\nretrieved documents are concatenated, allowing the model to reuse cached\nrepresentations instead of recomputing them. To mitigate the performance\ndegradation when using KV caches computed independently for each document,\nKVLink introduces two key techniques: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, and using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments. Experiments across 7 datasets demonstrate that KVLink improves\nquestion answering accuracy by an average of 4% over state-of-the-art methods.\nFurthermore, by leveraging precomputed KV caches, our approach reduces\ntime-to-first-token by up to 96% compared to standard LLM inference, making it\na scalable and efficient solution for context reuse. Additionally, KVLink can\nbe combined with KV cache compression to further save cache loading and storage\noverhead while outperforming the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we investigate a\nnew strategy to eliminate such inefficiency, where the KV cache of each\ndocument is precomputed independently. During inference, the KV caches of\nretrieved documents are concatenated, allowing the model to reuse cached\nrepresentations instead of recomputing them. To mitigate the performance\ndegradation when using KV caches computed independently for each document,\nKVLink introduces two key techniques: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, and using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments. Experiments across 7 datasets demonstrate that KVLink improves\nquestion answering accuracy by an average of 4% over state-of-the-art methods.\nFurthermore, by leveraging precomputed KV caches, our approach reduces\ntime-to-first-token by up to 96% compared to standard LLM inference, making it\na scalable and efficient solution for context reuse. Additionally, KVLink can\nbe combined with KV cache compression to further save cache loading and storage\noverhead while outperforming the baselines."
                },
                "authors": [
                    {
                        "name": "Jingbo Yang"
                    },
                    {
                        "name": "Bairu Hou"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Yujia Bao"
                    },
                    {
                        "name": "Shiyu Chang"
                    }
                ],
                "author_detail": {
                    "name": "Shiyu Chang"
                },
                "author": "Shiyu Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16002v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16002v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08373v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08373v2",
                "updated": "2025-07-19T03:40:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    19,
                    3,
                    40,
                    40,
                    5,
                    200,
                    0
                ],
                "published": "2025-06-10T02:37:46Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    2,
                    37,
                    46,
                    1,
                    161,
                    0
                ],
                "title": "Draft-based Approximate Inference for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Draft-based Approximate Inference for LLMs"
                },
                "summary": "Optimizing inference for long-context Large Language Models (LLMs) is\nincreasingly important due to the quadratic compute and linear memory\ncomplexity of Transformers. Existing approximation methods, such as key-value\n(KV) cache dropping, sparse attention, and prompt compression, typically rely\non rough predictions of token or KV pair importance. We propose a novel\nframework for approximate LLM inference that leverages small draft models to\nmore accurately predict the importance of tokens and KV pairs. Specifically, we\nintroduce two instantiations of our proposed framework: (i) SpecKV, the first\nmethod that leverages a draft output to accurately assess the importance of\neach KV pair for more effective KV cache dropping, and (ii) SpecPC, which uses\nthe draft model's attention activations to identify and discard unimportant\nprompt tokens. We motivate our methods with theoretical and empirical analyses,\nand show a strong correlation between the attention patterns of draft and\ntarget models. Extensive experiments on long-context benchmarks show that our\nmethods consistently achieve higher accuracy than existing baselines, while\npreserving the same improvements in memory usage, latency, and throughput. Our\ncode is available at https://github.com/furiosa-ai/draft-based-approx-llm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing inference for long-context Large Language Models (LLMs) is\nincreasingly important due to the quadratic compute and linear memory\ncomplexity of Transformers. Existing approximation methods, such as key-value\n(KV) cache dropping, sparse attention, and prompt compression, typically rely\non rough predictions of token or KV pair importance. We propose a novel\nframework for approximate LLM inference that leverages small draft models to\nmore accurately predict the importance of tokens and KV pairs. Specifically, we\nintroduce two instantiations of our proposed framework: (i) SpecKV, the first\nmethod that leverages a draft output to accurately assess the importance of\neach KV pair for more effective KV cache dropping, and (ii) SpecPC, which uses\nthe draft model's attention activations to identify and discard unimportant\nprompt tokens. We motivate our methods with theoretical and empirical analyses,\nand show a strong correlation between the attention patterns of draft and\ntarget models. Extensive experiments on long-context benchmarks show that our\nmethods consistently achieve higher accuracy than existing baselines, while\npreserving the same improvements in memory usage, latency, and throughput. Our\ncode is available at https://github.com/furiosa-ai/draft-based-approx-llm."
                },
                "authors": [
                    {
                        "name": "Kevin Galim"
                    },
                    {
                        "name": "Ethan Ewer"
                    },
                    {
                        "name": "Wonjun Kang"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Hyung Il Koo"
                    },
                    {
                        "name": "Kangwook Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kangwook Lee"
                },
                "author": "Kangwook Lee",
                "arxiv_comment": "Added discussion and comparison with SpecPrefill",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08373v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08373v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17771v1",
                "updated": "2025-07-19T00:57:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    19,
                    0,
                    57,
                    54,
                    5,
                    200,
                    0
                ],
                "published": "2025-07-19T00:57:54Z",
                "published_parsed": [
                    2025,
                    7,
                    19,
                    0,
                    57,
                    54,
                    5,
                    200,
                    0
                ],
                "title": "Flexible Vector Integration in Embedded RISC-V SoCs for End to End CNN\n  Inference Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flexible Vector Integration in Embedded RISC-V SoCs for End to End CNN\n  Inference Acceleration"
                },
                "summary": "The emergence of heterogeneity and domain-specific architectures targeting\ndeep learning inference show great potential for enabling the deployment of\nmodern CNNs on resource-constrained embedded platforms. A significant\ndevelopment is the diversification of custom hardware solely targeting the most\nexpensive parts of CNNs. DLAs (deep learning accelerators) and NPUs (neural\nprocessing units), among others, can overcome the approaching limits of\ntraditional silicon scaling and provide a solution to the power/performance\ntradeoff within embedded SoCs. Efficient DSA utilization requires proper system\nintegration and a compilation/execution model for balanced execution in these\nheterogeneous architectures. There is a critical need for proper system\nintegration and an efficient compilation/execution model for balanced execution\nin these heterogeneous architectures. This work highlights the hardware\nintegration challenges for efficiently placing these units within the memory\nhierarchy and correct proximity to other execution blocks. We experimentally\nverify performance bottlenecks in CNN execution and pre/post-processing at\nruntime, where previous attention has generally been given to accelerator\nspeedup alone. This work takes advantage of the ratification of the RISC-V\nVector 1.0 extension and demonstrates its potential as a flexible target within\na well-suited cache hierarchy scheme to reduce pre-processing bottlenecks and\nCPU fallback processes. Our results show up to a 9x speedup of image\npre-processing and YOLOv3 fallback layer execution by up to 3x compared to CPU.\nWe demonstrate RVV-1.0 in exposing a flexible programming model that can enable\na balanced computation and memory footprint on accelerator-rich embedded SoCs\nsupporting modern deep-learning dataflows while consuming less power than\ntraditional parallel execution platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of heterogeneity and domain-specific architectures targeting\ndeep learning inference show great potential for enabling the deployment of\nmodern CNNs on resource-constrained embedded platforms. A significant\ndevelopment is the diversification of custom hardware solely targeting the most\nexpensive parts of CNNs. DLAs (deep learning accelerators) and NPUs (neural\nprocessing units), among others, can overcome the approaching limits of\ntraditional silicon scaling and provide a solution to the power/performance\ntradeoff within embedded SoCs. Efficient DSA utilization requires proper system\nintegration and a compilation/execution model for balanced execution in these\nheterogeneous architectures. There is a critical need for proper system\nintegration and an efficient compilation/execution model for balanced execution\nin these heterogeneous architectures. This work highlights the hardware\nintegration challenges for efficiently placing these units within the memory\nhierarchy and correct proximity to other execution blocks. We experimentally\nverify performance bottlenecks in CNN execution and pre/post-processing at\nruntime, where previous attention has generally been given to accelerator\nspeedup alone. This work takes advantage of the ratification of the RISC-V\nVector 1.0 extension and demonstrates its potential as a flexible target within\na well-suited cache hierarchy scheme to reduce pre-processing bottlenecks and\nCPU fallback processes. Our results show up to a 9x speedup of image\npre-processing and YOLOv3 fallback layer execution by up to 3x compared to CPU.\nWe demonstrate RVV-1.0 in exposing a flexible programming model that can enable\na balanced computation and memory footprint on accelerator-rich embedded SoCs\nsupporting modern deep-learning dataflows while consuming less power than\ntraditional parallel execution platforms."
                },
                "authors": [
                    {
                        "name": "Dmitri Lyalikov"
                    }
                ],
                "author_detail": {
                    "name": "Dmitri Lyalikov"
                },
                "author": "Dmitri Lyalikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13961v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13961v1",
                "updated": "2025-07-18T14:24:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    24,
                    29,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T14:24:29Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    24,
                    29,
                    4,
                    199,
                    0
                ],
                "title": "Secretive Hotplug Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secretive Hotplug Coded Caching"
                },
                "summary": "In this work, we consider a coded caching model called \\textit{hotplug coded\ncaching}, in which some users are offline during the delivery phase. The\nconcept of Hotplug Placement Delivery Arrays (HpPDAs) for hotplug coded caching\nsystems has been introduced in the literature, and two classes of HpPDAs are\nknown. In this paper, we consider a secrecy constraint in hotplug coded caching\nsetup, where users should not learn anything about any file from their cache\ncontent, and active users should not gain any information about files other\nthan their demanded file from either their cache content or the server\ntransmissions. We propose two secretive schemes for the two classes of HpPDAs\nand compare them with a baseline scheme, which is a secretive scheme using PDAs\nfor the classical coded caching setup and can be trivially adapted for the\nhotplug coded caching setup. We numerically show that our schemes outperform\nthe baseline scheme in certain memory regions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we consider a coded caching model called \\textit{hotplug coded\ncaching}, in which some users are offline during the delivery phase. The\nconcept of Hotplug Placement Delivery Arrays (HpPDAs) for hotplug coded caching\nsystems has been introduced in the literature, and two classes of HpPDAs are\nknown. In this paper, we consider a secrecy constraint in hotplug coded caching\nsetup, where users should not learn anything about any file from their cache\ncontent, and active users should not gain any information about files other\nthan their demanded file from either their cache content or the server\ntransmissions. We propose two secretive schemes for the two classes of HpPDAs\nand compare them with a baseline scheme, which is a secretive scheme using PDAs\nfor the classical coded caching setup and can be trivially adapted for the\nhotplug coded caching setup. We numerically show that our schemes outperform\nthe baseline scheme in certain memory regions."
                },
                "authors": [
                    {
                        "name": "Mallikharjuna Chinnapadamala"
                    },
                    {
                        "name": "Charul Rajput"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "11 pages and 2 figures. arXiv admin note: text overlap with\n  arXiv:2404.06433",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13961v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13961v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04421v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04421v2",
                "updated": "2025-07-18T13:29:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    13,
                    29,
                    47,
                    4,
                    199,
                    0
                ],
                "published": "2025-05-07T13:54:26Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    54,
                    26,
                    2,
                    127,
                    0
                ],
                "title": "LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders"
                },
                "summary": "Modeling ultra-long user behavior sequences is critical for capturing both\nlong- and short-term preferences in industrial recommender systems. Existing\nsolutions typically rely on two-stage retrieval or indirect modeling paradigms,\nincuring upstream-downstream inconsistency and computational inefficiency. In\nthis paper, we present LONGER, a Long-sequence Optimized traNsformer for\nGPU-Efficient Recommenders. LONGER incorporates (i) a global token mechanism\nfor stabilizing attention over long contexts, (ii) a token merge module with\nlightweight InnerTransformers and hybrid attention strategy to reduce quadratic\ncomplexity, and (iii) a series of engineering optimizations, including training\nwith mixed-precision and activation recomputation, KV cache serving, and the\nfully synchronous model training and serving framework for unified GPU-based\ndense and sparse parameter updates. LONGER consistently outperforms strong\nbaselines in both offline metrics and online A/B testing in both advertising\nand e-commerce services at ByteDance, validating its consistent effectiveness\nand industrial-level scaling laws. Currently, LONGER has been fully deployed at\nmore than 10 influential scenarios at ByteDance, serving billion users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling ultra-long user behavior sequences is critical for capturing both\nlong- and short-term preferences in industrial recommender systems. Existing\nsolutions typically rely on two-stage retrieval or indirect modeling paradigms,\nincuring upstream-downstream inconsistency and computational inefficiency. In\nthis paper, we present LONGER, a Long-sequence Optimized traNsformer for\nGPU-Efficient Recommenders. LONGER incorporates (i) a global token mechanism\nfor stabilizing attention over long contexts, (ii) a token merge module with\nlightweight InnerTransformers and hybrid attention strategy to reduce quadratic\ncomplexity, and (iii) a series of engineering optimizations, including training\nwith mixed-precision and activation recomputation, KV cache serving, and the\nfully synchronous model training and serving framework for unified GPU-based\ndense and sparse parameter updates. LONGER consistently outperforms strong\nbaselines in both offline metrics and online A/B testing in both advertising\nand e-commerce services at ByteDance, validating its consistent effectiveness\nand industrial-level scaling laws. Currently, LONGER has been fully deployed at\nmore than 10 influential scenarios at ByteDance, serving billion users."
                },
                "authors": [
                    {
                        "name": "Zheng Chai"
                    },
                    {
                        "name": "Qin Ren"
                    },
                    {
                        "name": "Xijun Xiao"
                    },
                    {
                        "name": "Huizhi Yang"
                    },
                    {
                        "name": "Bo Han"
                    },
                    {
                        "name": "Sijun Zhang"
                    },
                    {
                        "name": "Di Chen"
                    },
                    {
                        "name": "Hui Lu"
                    },
                    {
                        "name": "Wenlin Zhao"
                    },
                    {
                        "name": "Lele Yu"
                    },
                    {
                        "name": "Xionghang Xie"
                    },
                    {
                        "name": "Shiru Ren"
                    },
                    {
                        "name": "Xiang Sun"
                    },
                    {
                        "name": "Yaocheng Tan"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Yuchao Zheng"
                    },
                    {
                        "name": "Di Wu"
                    }
                ],
                "author_detail": {
                    "name": "Di Wu"
                },
                "author": "Di Wu",
                "arxiv_journal_ref": "Proceedings of the Nineteenth ACM Conference on Recommender\n  Systems (RecSys '25), September 22--26, 2025, Prague, Czech Republic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04421v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04421v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13681v1",
                "updated": "2025-07-18T06:12:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    6,
                    12,
                    8,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T06:12:08Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    6,
                    12,
                    8,
                    4,
                    199,
                    0
                ],
                "title": "LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for\n  Multi-Turn Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for\n  Multi-Turn Dialogues"
                },
                "summary": "Multi-turn dialogues are essential in many real-world applications of large\nlanguage models, such as chatbots and virtual assistants. As conversation\nhistories become longer, existing large language models face increasing\ncomputational and memory challenges, which hinder their ability to provide\nefficient and responsive interactions. Most current acceleration methods either\ncompress the context or optimize key value caching, but they often rely on\nfixed or position-based heuristics that do not adapt well to the dynamic and\nunpredictable patterns found in actual multi-turn conversations. In this paper,\nwe present LoopServe, an adaptive dual-phase inference acceleration framework\nfor large language models in multi-turn dialogues. LoopServe introduces two\nmain innovations. First, it performs online sparsification during the\nprefilling phase by dynamically selecting the most important parts of the\nattention matrix for each new input. Second, it uses progressive key value\ncompression during decoding by adaptively maintaining a relevant and efficient\ncache based on the most recently generated output tokens. We also propose a\n\\href{https://huggingface.co/datasets/TreeAILab/Multi-turn_Long-context_Benchmark_for_LLMs}{new\nbenchmark} with eleven multi-turn datasets that reflect realistic query\npositions and conversational dependencies. Extensive experiments demonstrate\nthat LoopServe consistently achieves superior effectiveness compared to\nexisting baselines and significantly accelerates LLM inference across a wide\nrange of long-context dialogue tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn dialogues are essential in many real-world applications of large\nlanguage models, such as chatbots and virtual assistants. As conversation\nhistories become longer, existing large language models face increasing\ncomputational and memory challenges, which hinder their ability to provide\nefficient and responsive interactions. Most current acceleration methods either\ncompress the context or optimize key value caching, but they often rely on\nfixed or position-based heuristics that do not adapt well to the dynamic and\nunpredictable patterns found in actual multi-turn conversations. In this paper,\nwe present LoopServe, an adaptive dual-phase inference acceleration framework\nfor large language models in multi-turn dialogues. LoopServe introduces two\nmain innovations. First, it performs online sparsification during the\nprefilling phase by dynamically selecting the most important parts of the\nattention matrix for each new input. Second, it uses progressive key value\ncompression during decoding by adaptively maintaining a relevant and efficient\ncache based on the most recently generated output tokens. We also propose a\n\\href{https://huggingface.co/datasets/TreeAILab/Multi-turn_Long-context_Benchmark_for_LLMs}{new\nbenchmark} with eleven multi-turn datasets that reflect realistic query\npositions and conversational dependencies. Extensive experiments demonstrate\nthat LoopServe consistently achieves superior effectiveness compared to\nexisting baselines and significantly accelerates LLM inference across a wide\nrange of long-context dialogue tasks."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Darian Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Qingfa Xiao"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19243v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19243v3",
                "updated": "2025-07-18T01:49:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    1,
                    49,
                    36,
                    4,
                    199,
                    0
                ],
                "published": "2025-01-31T15:58:15Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    58,
                    15,
                    4,
                    31,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Error-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Error-Optimized Cache"
                },
                "summary": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the \\textbf{E}rror-\\textbf{O}ptimized\n\\textbf{C}ache (\\textbf{EOC}). This method introduces three key improvements:\n\\textbf{(1)} Prior knowledge extraction: Extract and process the caching\ndifferences; \\textbf{(2)} A judgment method for cache optimization: Determine\nwhether certain caching steps need to be optimized; \\textbf{(3)} Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching, especially\nexcessive caching. On the ImageNet dataset, without substantially increasing\nthe computational load, this method improves the FID of the generated images\nwhen the rule-based model FORA has a caching level of \\textbf{75}\\%,\n\\textbf{50}\\%, and \\textbf{25}\\%, and the training-based model\nLearning-to-cache has a caching level of \\textbf{22}\\%. Specifically, the FID\nvalues change from 30.454 to 21.690 (\\textbf{28.8}\\%), from 6.857 to 5.821\n(\\textbf{15.1}\\%), from 3.870 to 3.692 (\\textbf{4.6}\\%), and from 3.539 to\n3.451 (\\textbf{2.5}\\%) respectively. Code is available at\nhttps://github.com/qiujx0520/EOC_MM2025.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the \\textbf{E}rror-\\textbf{O}ptimized\n\\textbf{C}ache (\\textbf{EOC}). This method introduces three key improvements:\n\\textbf{(1)} Prior knowledge extraction: Extract and process the caching\ndifferences; \\textbf{(2)} A judgment method for cache optimization: Determine\nwhether certain caching steps need to be optimized; \\textbf{(3)} Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching, especially\nexcessive caching. On the ImageNet dataset, without substantially increasing\nthe computational load, this method improves the FID of the generated images\nwhen the rule-based model FORA has a caching level of \\textbf{75}\\%,\n\\textbf{50}\\%, and \\textbf{25}\\%, and the training-based model\nLearning-to-cache has a caching level of \\textbf{22}\\%. Specifically, the FID\nvalues change from 30.454 to 21.690 (\\textbf{28.8}\\%), from 6.857 to 5.821\n(\\textbf{15.1}\\%), from 3.870 to 3.692 (\\textbf{4.6}\\%), and from 3.539 to\n3.451 (\\textbf{2.5}\\%) respectively. Code is available at\nhttps://github.com/qiujx0520/EOC_MM2025.git."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Houcheng Jiang"
                    },
                    {
                        "name": "Xingyu Zhu"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "arxiv_journal_ref": "ACM MM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19243v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19243v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05156v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05156v2",
                "updated": "2025-07-18T01:36:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    1,
                    36,
                    3,
                    4,
                    199,
                    0
                ],
                "published": "2025-03-07T05:31:47Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    5,
                    31,
                    47,
                    4,
                    66,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Gradient-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Gradient-Optimized Cache"
                },
                "summary": "Feature caching has emerged as an effective strategy to accelerate diffusion\ntransformer (DiT) sampling through temporal feature reuse. It is a challenging\nproblem since (1) Progressive error accumulation from cached blocks\nsignificantly degrades generation quality, particularly when over 50\\% of\nblocks are cached; (2) Current error compensation approaches neglect dynamic\nperturbation patterns during the caching process, leading to suboptimal error\ncorrection. To solve these problems, we propose the Gradient-Optimized Cache\n(GOC) with two key innovations: (1) Cached Gradient Propagation: A gradient\nqueue dynamically computes the gradient differences between cached and\nrecomputed features. These gradients are weighted and propagated to subsequent\nsteps, directly compensating for the approximation errors introduced by\ncaching. (2) Inflection-Aware Optimization: Through statistical analysis of\nfeature variation patterns, we identify critical inflection points where the\ndenoising trajectory changes direction. By aligning gradient updates with these\ndetected phases, we prevent conflicting gradient directions during error\ncorrection. Extensive evaluations on ImageNet demonstrate GOC's superior\ntrade-off between efficiency and quality. With 50\\% cached blocks, GOC achieves\nIS 216.28 (26.3\\% higher) and FID 3.907 (43\\% lower) compared to baseline DiT,\nwhile maintaining identical computational costs. These improvements persist\nacross various cache ratios, demonstrating robust adaptability to different\nacceleration requirements. Code is available at\nhttps://github.com/qiujx0520/GOC_ICCV2025.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature caching has emerged as an effective strategy to accelerate diffusion\ntransformer (DiT) sampling through temporal feature reuse. It is a challenging\nproblem since (1) Progressive error accumulation from cached blocks\nsignificantly degrades generation quality, particularly when over 50\\% of\nblocks are cached; (2) Current error compensation approaches neglect dynamic\nperturbation patterns during the caching process, leading to suboptimal error\ncorrection. To solve these problems, we propose the Gradient-Optimized Cache\n(GOC) with two key innovations: (1) Cached Gradient Propagation: A gradient\nqueue dynamically computes the gradient differences between cached and\nrecomputed features. These gradients are weighted and propagated to subsequent\nsteps, directly compensating for the approximation errors introduced by\ncaching. (2) Inflection-Aware Optimization: Through statistical analysis of\nfeature variation patterns, we identify critical inflection points where the\ndenoising trajectory changes direction. By aligning gradient updates with these\ndetected phases, we prevent conflicting gradient directions during error\ncorrection. Extensive evaluations on ImageNet demonstrate GOC's superior\ntrade-off between efficiency and quality. With 50\\% cached blocks, GOC achieves\nIS 216.28 (26.3\\% higher) and FID 3.907 (43\\% lower) compared to baseline DiT,\nwhile maintaining identical computational costs. These improvements persist\nacross various cache ratios, demonstrating robust adaptability to different\nacceleration requirements. Code is available at\nhttps://github.com/qiujx0520/GOC_ICCV2025.git."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Kezhou Chen"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "arxiv_journal_ref": "ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05156v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05156v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13575v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13575v1",
                "updated": "2025-07-17T23:37:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    23,
                    37,
                    19,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T23:37:19Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    23,
                    37,
                    19,
                    3,
                    198,
                    0
                ],
                "title": "Apple Intelligence Foundation Language Models: Tech Report 2025",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apple Intelligence Foundation Language Models: Tech Report 2025"
                },
                "summary": "We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute."
                },
                "authors": [
                    {
                        "name": "Hanzhi Zhou"
                    },
                    {
                        "name": "Erik Hornberger"
                    },
                    {
                        "name": "Pengsheng Guo"
                    },
                    {
                        "name": "Xiyou Zhou"
                    },
                    {
                        "name": "Saiwen Wang"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Yifei He"
                    },
                    {
                        "name": "Xuankai Chang"
                    },
                    {
                        "name": "Rene Rauch"
                    },
                    {
                        "name": "Louis D'hauwe"
                    },
                    {
                        "name": "John Peebles"
                    },
                    {
                        "name": "Alec Doane"
                    },
                    {
                        "name": "Kohen Chia"
                    },
                    {
                        "name": "Jenna Thibodeau"
                    },
                    {
                        "name": "Zi-Yi Dou"
                    },
                    {
                        "name": "Yuanyang Zhang"
                    },
                    {
                        "name": "Ruoming Pang"
                    },
                    {
                        "name": "Reed Li"
                    },
                    {
                        "name": "Zhifeng Chen"
                    },
                    {
                        "name": "Jeremy Warner"
                    },
                    {
                        "name": "Zhaoyang Xu"
                    },
                    {
                        "name": "Sophy Lee"
                    },
                    {
                        "name": "David Mizrahi"
                    },
                    {
                        "name": "Ramsey Tantawi"
                    },
                    {
                        "name": "Chris Chaney"
                    },
                    {
                        "name": "Kelsey Peterson"
                    },
                    {
                        "name": "Jun Qin"
                    },
                    {
                        "name": "Alex Dombrowski"
                    },
                    {
                        "name": "Mira Chiang"
                    },
                    {
                        "name": "Aiswarya Raghavan"
                    },
                    {
                        "name": "Gerard Casamayor"
                    },
                    {
                        "name": "Qibin Chen"
                    },
                    {
                        "name": "Aonan Zhang"
                    },
                    {
                        "name": "Nathalie Tran"
                    },
                    {
                        "name": "Jianyu Wang"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Thomas Voice"
                    },
                    {
                        "name": "Alessandro Pappalardo"
                    },
                    {
                        "name": "Brycen Wershing"
                    },
                    {
                        "name": "Prasanth Yadla"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Priyal Chhatrapati"
                    },
                    {
                        "name": "Ismael Fernandez"
                    },
                    {
                        "name": "Yusuf Goren"
                    },
                    {
                        "name": "Xin Zheng"
                    },
                    {
                        "name": "Forrest Huang"
                    },
                    {
                        "name": "Tao Lei"
                    },
                    {
                        "name": "Eray Yildiz"
                    },
                    {
                        "name": "Alper Kokmen"
                    },
                    {
                        "name": "Gokul Santhanam"
                    },
                    {
                        "name": "Areeba Kamal"
                    },
                    {
                        "name": "Kaan Elgin"
                    },
                    {
                        "name": "Dian Ang Yap"
                    },
                    {
                        "name": "Jeremy Liu"
                    },
                    {
                        "name": "Peter Gray"
                    },
                    {
                        "name": "Howard Xing"
                    },
                    {
                        "name": "Kieran Liu"
                    },
                    {
                        "name": "Matteo Ronchi"
                    },
                    {
                        "name": "Moritz Schwarzer-Becker"
                    },
                    {
                        "name": "Yun Zhu"
                    },
                    {
                        "name": "Mandana Saebi"
                    },
                    {
                        "name": "Jeremy Snow"
                    },
                    {
                        "name": "David Griffiths"
                    },
                    {
                        "name": "Guillaume Tartavel"
                    },
                    {
                        "name": "Erin Feldman"
                    },
                    {
                        "name": "Simon Lehnerer"
                    },
                    {
                        "name": "Fernando Bermdez-Medina"
                    },
                    {
                        "name": "Hans Han"
                    },
                    {
                        "name": "Joe Zhou"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Sujeeth Reddy"
                    },
                    {
                        "name": "Zirui Wang"
                    },
                    {
                        "name": "Tom Gunter"
                    },
                    {
                        "name": "Albert Antony"
                    },
                    {
                        "name": "Yuanzhi Li"
                    },
                    {
                        "name": "John Dennison"
                    },
                    {
                        "name": "Tony Sun"
                    },
                    {
                        "name": "Yena Han"
                    },
                    {
                        "name": "Yi Qin"
                    },
                    {
                        "name": "Sam Davarnia"
                    },
                    {
                        "name": "Jeffrey Bigham"
                    },
                    {
                        "name": "Wayne Shan"
                    },
                    {
                        "name": "Hannah Gillis Coleman"
                    },
                    {
                        "name": "Guillaume Klein"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Muyang Yu"
                    },
                    {
                        "name": "Jack Cackler"
                    },
                    {
                        "name": "Yuan Gao"
                    },
                    {
                        "name": "Crystal Xiao"
                    },
                    {
                        "name": "Binazir Karimzadeh"
                    },
                    {
                        "name": "Zhengdong Zhang"
                    },
                    {
                        "name": "Felix Bai"
                    },
                    {
                        "name": "Albin Madappally Jose"
                    },
                    {
                        "name": "Feng Nan"
                    },
                    {
                        "name": "Nazir Kamaldin"
                    },
                    {
                        "name": "Dong Yin"
                    },
                    {
                        "name": "Hans Hao"
                    },
                    {
                        "name": "Yanchao Sun"
                    },
                    {
                        "name": "Yi Hua"
                    },
                    {
                        "name": "Charles Maalouf"
                    },
                    {
                        "name": "Alex Guillen Garcia"
                    },
                    {
                        "name": "Guoli Yin"
                    },
                    {
                        "name": "Lezhi Li"
                    },
                    {
                        "name": "Mohana Prasad Sathya Moorthy"
                    },
                    {
                        "name": "Hongbin Gao"
                    },
                    {
                        "name": "Jay Tang"
                    },
                    {
                        "name": "Joanna Arreaza-Taylor"
                    },
                    {
                        "name": "Faye Lao"
                    },
                    {
                        "name": "Carina Peng"
                    },
                    {
                        "name": "Josh Shaffer"
                    },
                    {
                        "name": "Dan Masi"
                    },
                    {
                        "name": "Sushma Rao"
                    },
                    {
                        "name": "Tommi Vehvilainen"
                    },
                    {
                        "name": "Senyu Tong"
                    },
                    {
                        "name": "Dongcai Shen"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Chris Bartels"
                    },
                    {
                        "name": "Peter Fu"
                    },
                    {
                        "name": "Qingqing Cao"
                    },
                    {
                        "name": "Christopher Neubauer"
                    },
                    {
                        "name": "Ethan Li"
                    },
                    {
                        "name": "Mingfei Gao"
                    },
                    {
                        "name": "Rebecca Callahan"
                    },
                    {
                        "name": "Richard Wei"
                    },
                    {
                        "name": "Patrick Dong"
                    },
                    {
                        "name": "Alex Braunstein"
                    },
                    {
                        "name": "Sachin Ravi"
                    },
                    {
                        "name": "Adolfo Lopez Mendez"
                    },
                    {
                        "name": "Kaiwei Huang"
                    },
                    {
                        "name": "Kun Duan"
                    },
                    {
                        "name": "Haoshuo Huang"
                    },
                    {
                        "name": "Rui Qian"
                    },
                    {
                        "name": "Stefano Ligas"
                    },
                    {
                        "name": "Jordan Huffaker"
                    },
                    {
                        "name": "Dongxu Li"
                    },
                    {
                        "name": "Bailin Wang"
                    },
                    {
                        "name": "Nanzhu Wang"
                    },
                    {
                        "name": "Anuva Agarwal"
                    },
                    {
                        "name": "Tait Madsen"
                    },
                    {
                        "name": "Josh Newnham"
                    },
                    {
                        "name": "Abhishek Sharma"
                    },
                    {
                        "name": "Zhile Ren"
                    },
                    {
                        "name": "Deepak Gopinath"
                    },
                    {
                        "name": "Erik Daxberger"
                    },
                    {
                        "name": "Saptarshi Guha"
                    },
                    {
                        "name": "Oron Levy"
                    },
                    {
                        "name": "Jing Lu"
                    },
                    {
                        "name": "Nan Dun"
                    },
                    {
                        "name": "Marc Kirchner"
                    },
                    {
                        "name": "Yinfei Yang"
                    },
                    {
                        "name": "Manjot Bilkhu"
                    },
                    {
                        "name": "Dave Nelson"
                    },
                    {
                        "name": "Anthony Spalvieri-Kruse"
                    },
                    {
                        "name": "Juan Lao Tebar"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Phani Mutyala"
                    },
                    {
                        "name": "Gabriel Jacoby-Cooper"
                    },
                    {
                        "name": "Yingbo Wang"
                    },
                    {
                        "name": "Karla Vega"
                    },
                    {
                        "name": "Vishaal Mahtani"
                    },
                    {
                        "name": "Darren Botten"
                    },
                    {
                        "name": "Eric Wang"
                    },
                    {
                        "name": "Hanli Li"
                    },
                    {
                        "name": "Matthias Paulik"
                    },
                    {
                        "name": "Haoran Yan"
                    },
                    {
                        "name": "Navid Shiee"
                    },
                    {
                        "name": "Yihao Qian"
                    },
                    {
                        "name": "Bugu Wu"
                    },
                    {
                        "name": "Qi Zhu"
                    },
                    {
                        "name": "Ob Adaranijo"
                    },
                    {
                        "name": "Bhuwan Dhingra"
                    },
                    {
                        "name": "Zhe Gan"
                    },
                    {
                        "name": "Nicholas Seidl"
                    },
                    {
                        "name": "Grace Duanmu"
                    },
                    {
                        "name": "Rong Situ"
                    },
                    {
                        "name": "Yiping Ma"
                    },
                    {
                        "name": "Yin Xia"
                    },
                    {
                        "name": "David Riazati"
                    },
                    {
                        "name": "Vasileios Saveris"
                    },
                    {
                        "name": "Anh Nguyen"
                    },
                    {
                        "name": "Michael"
                    },
                    {
                        "name": "Lee"
                    },
                    {
                        "name": "Patrick Sonnenberg"
                    },
                    {
                        "name": "Chinguun Erdenebileg"
                    },
                    {
                        "name": "Yanghao Li"
                    },
                    {
                        "name": "Vivian Ma"
                    },
                    {
                        "name": "James Chou"
                    },
                    {
                        "name": "Isha Garg"
                    },
                    {
                        "name": "Mark Lee"
                    },
                    {
                        "name": "Keen You"
                    },
                    {
                        "name": "Yuhong Li"
                    },
                    {
                        "name": "Ransen Niu"
                    },
                    {
                        "name": "Nandhitha Raghuram"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    },
                    {
                        "name": "Henry Mason"
                    },
                    {
                        "name": "Sumeet Singh"
                    },
                    {
                        "name": "Keyu He"
                    },
                    {
                        "name": "Hong-You Chen"
                    },
                    {
                        "name": "Lucas Guibert"
                    },
                    {
                        "name": "Shiyu Li"
                    },
                    {
                        "name": "Varsha Paidi"
                    },
                    {
                        "name": "Narendran Raghavan"
                    },
                    {
                        "name": "Mingze Xu"
                    },
                    {
                        "name": "Yuli Yang"
                    },
                    {
                        "name": "Sergiu Sima"
                    },
                    {
                        "name": "Irina Belousova"
                    },
                    {
                        "name": "Sprite Chu"
                    },
                    {
                        "name": "Afshin Dehghan"
                    },
                    {
                        "name": "Philipp Dufter"
                    },
                    {
                        "name": "David Haldimann"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Margit Bowler"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Ying-Chang Cheng"
                    },
                    {
                        "name": "Vivek Rathod"
                    },
                    {
                        "name": "Syd Evans"
                    },
                    {
                        "name": "Wilson Tsao"
                    },
                    {
                        "name": "Dustin Withers"
                    },
                    {
                        "name": "Haitian Sun"
                    },
                    {
                        "name": "Biyao Wang"
                    },
                    {
                        "name": "Peter Grasch"
                    },
                    {
                        "name": "Walker Cheng"
                    },
                    {
                        "name": "Yihao Feng"
                    },
                    {
                        "name": "Vivek Kumar"
                    },
                    {
                        "name": "Frank Chu"
                    },
                    {
                        "name": "Victoria MnchJuan Haladjian"
                    },
                    {
                        "name": "Doug Kang"
                    },
                    {
                        "name": "Jiarui Lu"
                    },
                    {
                        "name": "Ciro Sannino"
                    },
                    {
                        "name": "Max Lam"
                    },
                    {
                        "name": "Floris Weers"
                    },
                    {
                        "name": "Bowen Pan"
                    },
                    {
                        "name": "Kenneth Jung"
                    },
                    {
                        "name": "Dhaval Doshi"
                    },
                    {
                        "name": "Fangping Shi"
                    },
                    {
                        "name": "Olli Saarikivi"
                    },
                    {
                        "name": "Alp Aygar"
                    },
                    {
                        "name": "Josh Elman"
                    },
                    {
                        "name": "Cheng Leong"
                    },
                    {
                        "name": "Eshan Verma"
                    },
                    {
                        "name": "Matthew Lei"
                    },
                    {
                        "name": "Jeff Nichols"
                    },
                    {
                        "name": "Jiulong Shan"
                    },
                    {
                        "name": "Donald Zhang"
                    },
                    {
                        "name": "Lawrence Zhou"
                    },
                    {
                        "name": "Stephen Murphy"
                    },
                    {
                        "name": "Xianzhi Du"
                    },
                    {
                        "name": "Chang Lan"
                    },
                    {
                        "name": "Ankur Jain"
                    },
                    {
                        "name": "Elmira Amirloo"
                    },
                    {
                        "name": "Marcin Eichner"
                    },
                    {
                        "name": "Naomy Sabo"
                    },
                    {
                        "name": "Anupama Mann Anupama"
                    },
                    {
                        "name": "David Qiu"
                    },
                    {
                        "name": "Zhao Meng"
                    },
                    {
                        "name": "Michael FitzMaurice"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Simon Yeung"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Marco Zuliani"
                    },
                    {
                        "name": "Andrew Hansen"
                    },
                    {
                        "name": "Yang Lu"
                    },
                    {
                        "name": "Brent Ramerth"
                    },
                    {
                        "name": "Ziyi Zhong"
                    },
                    {
                        "name": "Parsa Mazaheri"
                    },
                    {
                        "name": "Matthew Hopkins"
                    },
                    {
                        "name": "Mengyu Li"
                    },
                    {
                        "name": "Simon Wang"
                    },
                    {
                        "name": "David Chen"
                    },
                    {
                        "name": "Farzin Rasteh"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Josh Gardner"
                    },
                    {
                        "name": "Asaf Liberman"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Andrew Walkingshaw"
                    },
                    {
                        "name": "Xingyu Zhou"
                    },
                    {
                        "name": "Jinhao Lei"
                    },
                    {
                        "name": "Yan Meng"
                    },
                    {
                        "name": "Quentin Keunebroek"
                    },
                    {
                        "name": "Sam Wiseman"
                    },
                    {
                        "name": "Anders Boesen Lindbo Larsen"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Zaid Ahmed"
                    },
                    {
                        "name": "Haiming Gang"
                    },
                    {
                        "name": "Aaron Franklin"
                    },
                    {
                        "name": "Kelvin Zou"
                    },
                    {
                        "name": "Guillaume Seguin"
                    },
                    {
                        "name": "Jonathan Janke"
                    },
                    {
                        "name": "Rachel Burger"
                    },
                    {
                        "name": "Co Giang"
                    },
                    {
                        "name": "Cheng Shen"
                    },
                    {
                        "name": "Jen Liu"
                    },
                    {
                        "name": "Sanskruti Shah"
                    },
                    {
                        "name": "Xiang Kong"
                    },
                    {
                        "name": "Yiran Fei"
                    },
                    {
                        "name": "TJ Collins"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Zhiyun Lu"
                    },
                    {
                        "name": "Michael Booker"
                    },
                    {
                        "name": "Qin Ba"
                    },
                    {
                        "name": "Yasutaka Tanaka"
                    },
                    {
                        "name": "Andres Romero Mier Y Teran"
                    },
                    {
                        "name": "Federico Scozzafava"
                    },
                    {
                        "name": "Regan Poston"
                    },
                    {
                        "name": "Jane Li"
                    },
                    {
                        "name": "Eduardo Jimenez"
                    },
                    {
                        "name": "Bas Straathof"
                    },
                    {
                        "name": "Karanjeet Singh"
                    },
                    {
                        "name": "Lindsay Hislop"
                    },
                    {
                        "name": "Rajat Arora"
                    },
                    {
                        "name": "Deepa Seshadri"
                    },
                    {
                        "name": "Boyue Li"
                    },
                    {
                        "name": "Colorado Reed"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "TJ Lu"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Kaelen Haag"
                    },
                    {
                        "name": "Nicholas Lusskin"
                    },
                    {
                        "name": "Raunak Sinha"
                    },
                    {
                        "name": "Rahul Nair"
                    },
                    {
                        "name": "Eldon Schoop"
                    },
                    {
                        "name": "Mary Beth Kery"
                    },
                    {
                        "name": "Mehrdad Farajtbar"
                    },
                    {
                        "name": "Brenda Yang"
                    },
                    {
                        "name": "George Horrell"
                    },
                    {
                        "name": "Shiwen Zhao"
                    },
                    {
                        "name": "Dhruti Shah"
                    },
                    {
                        "name": "Cha Chen"
                    },
                    {
                        "name": "Bowen Zhang"
                    },
                    {
                        "name": "Chang Gao"
                    },
                    {
                        "name": "Devi Krishna"
                    },
                    {
                        "name": "Jennifer Mallalieu"
                    },
                    {
                        "name": "Javier Movellan"
                    },
                    {
                        "name": "Di Feng"
                    },
                    {
                        "name": "Emily Zhang"
                    },
                    {
                        "name": "Sam Xu"
                    },
                    {
                        "name": "Junting Pan"
                    },
                    {
                        "name": "Dominik Moritz"
                    },
                    {
                        "name": "Suma Jayaram"
                    },
                    {
                        "name": "Kevin Smith"
                    },
                    {
                        "name": "Dongseong Hwang"
                    },
                    {
                        "name": "Daniel Parilla"
                    },
                    {
                        "name": "Jiaming Hu"
                    },
                    {
                        "name": "You-Cyuan Jhang"
                    },
                    {
                        "name": "Emad Soroush"
                    },
                    {
                        "name": "Fred Hohman"
                    },
                    {
                        "name": "Nan Du"
                    },
                    {
                        "name": "Emma Wang"
                    },
                    {
                        "name": "Sam Dodge"
                    },
                    {
                        "name": "Pragnya Sridhar"
                    },
                    {
                        "name": "Joris Pelemans"
                    },
                    {
                        "name": "Wei Fang"
                    },
                    {
                        "name": "Nina Wenzel"
                    },
                    {
                        "name": "Joseph Yitan Cheng"
                    },
                    {
                        "name": "Hadas Kotek"
                    },
                    {
                        "name": "Chung-Cheng Chiu"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Haijing Fu"
                    },
                    {
                        "name": "Ruixuan Hou"
                    },
                    {
                        "name": "Ke Ye"
                    },
                    {
                        "name": "Diane Zhu"
                    },
                    {
                        "name": "Nikhil Bhendawade"
                    },
                    {
                        "name": "Joseph Astrauskas"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "Sai Aitharaju"
                    },
                    {
                        "name": "Wentao Wu"
                    },
                    {
                        "name": "Artsiom Peshko"
                    },
                    {
                        "name": "Hyunjik Kim"
                    },
                    {
                        "name": "Nilesh Shahdadpuri"
                    },
                    {
                        "name": "Andy De Wang"
                    },
                    {
                        "name": "Qi Shan"
                    },
                    {
                        "name": "Piotr Maj"
                    },
                    {
                        "name": "Raul Rea Menacho"
                    },
                    {
                        "name": "Justin Lazarow"
                    },
                    {
                        "name": "Eric Liang Yang"
                    },
                    {
                        "name": "Arsalan Farooq"
                    },
                    {
                        "name": "Donghan Yu"
                    },
                    {
                        "name": "David Gera"
                    },
                    {
                        "name": "Minsik Cho"
                    },
                    {
                        "name": "Kavya Nerella"
                    },
                    {
                        "name": "Yongqiang Wang"
                    },
                    {
                        "name": "Tao Jia"
                    },
                    {
                        "name": "John Park"
                    },
                    {
                        "name": "Jeff Lai"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Futang Peng"
                    },
                    {
                        "name": "Daniele Molinari"
                    },
                    {
                        "name": "Aparna Rajamani"
                    },
                    {
                        "name": "Tyler Johnson"
                    },
                    {
                        "name": "Lauren Gardiner"
                    },
                    {
                        "name": "Chao Jia"
                    },
                    {
                        "name": "Violet Yao"
                    },
                    {
                        "name": "Wojciech Kryscinski"
                    },
                    {
                        "name": "Xiujun Li"
                    },
                    {
                        "name": "Shang-Chen Wu"
                    }
                ],
                "author_detail": {
                    "name": "Shang-Chen Wu"
                },
                "arxiv_affiliation": "Taoyi",
                "author": "Shang-Chen Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13575v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13575v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04018v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04018v2",
                "updated": "2025-07-17T13:44:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    13,
                    44,
                    39,
                    3,
                    198,
                    0
                ],
                "published": "2025-02-06T12:19:34Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    19,
                    34,
                    3,
                    37,
                    0
                ],
                "title": "PINT: Physics-Informed Neural Time Series Models with Applications to\n  Long-term Inference on WeatherBench 2m-Temperature Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PINT: Physics-Informed Neural Time Series Models with Applications to\n  Long-term Inference on WeatherBench 2m-Temperature Data"
                },
                "summary": "This paper introduces PINT (Physics-Informed Neural Time Series Models), a\nframework that integrates physical constraints into neural time series models\nto improve their ability to capture complex dynamics. We apply PINT to the ERA5\nWeatherBench dataset, focusing on long-term forecasting of 2m-temperature data.\nPINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed\nprior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures.\nThis equation's analytical solutions (sine and cosine functions) facilitate\nrigorous evaluation of the benefits of incorporating physics-informed\nconstraints. By benchmarking against a linear regression baseline derived from\nits exact solutions, we quantify the impact of embedding physical principles in\ndata-driven models. Unlike traditional time series models that rely on future\nobservations, PINT is designed for practical forecasting. Using only the first\n90 days of observed data, it iteratively predicts the next two years,\naddressing challenges posed by limited real-time updates. Experiments on the\nWeatherBench dataset demonstrate PINT's ability to generalize, capture periodic\ntrends, and align with physical principles. This study highlights the potential\nof physics-informed neural models in bridging machine learning and\ninterpretable climate applications.\n  Our models and datasets are publicly available on GitHub:\nhttps://github.com/KV-Park.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces PINT (Physics-Informed Neural Time Series Models), a\nframework that integrates physical constraints into neural time series models\nto improve their ability to capture complex dynamics. We apply PINT to the ERA5\nWeatherBench dataset, focusing on long-term forecasting of 2m-temperature data.\nPINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed\nprior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures.\nThis equation's analytical solutions (sine and cosine functions) facilitate\nrigorous evaluation of the benefits of incorporating physics-informed\nconstraints. By benchmarking against a linear regression baseline derived from\nits exact solutions, we quantify the impact of embedding physical principles in\ndata-driven models. Unlike traditional time series models that rely on future\nobservations, PINT is designed for practical forecasting. Using only the first\n90 days of observed data, it iteratively predicts the next two years,\naddressing challenges posed by limited real-time updates. Experiments on the\nWeatherBench dataset demonstrate PINT's ability to generalize, capture periodic\ntrends, and align with physical principles. This study highlights the potential\nof physics-informed neural models in bridging machine learning and\ninterpretable climate applications.\n  Our models and datasets are publicly available on GitHub:\nhttps://github.com/KV-Park."
                },
                "authors": [
                    {
                        "name": "Keonvin Park"
                    },
                    {
                        "name": "Jisu Kim"
                    },
                    {
                        "name": "Jaemin Seo"
                    }
                ],
                "author_detail": {
                    "name": "Jaemin Seo"
                },
                "author": "Jaemin Seo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04018v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04018v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00929v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00929v4",
                "updated": "2025-07-17T09:55:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    9,
                    55,
                    43,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-01T16:36:23Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    16,
                    36,
                    23,
                    1,
                    182,
                    0
                ],
                "title": "Integrating nano- and micrometer-scale energy deposition models for\n  mechanistic prediction of radiation-induced DNA damage and cell survival",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating nano- and micrometer-scale energy deposition models for\n  mechanistic prediction of radiation-induced DNA damage and cell survival"
                },
                "summary": "We present an integrated modeling framework that combines the Generalized\nStochastic Microdosimetric Model (GSM2), used to predict cell survival\nfractions, with MINAS-TIRITH, a fast and efficient Geant4 DNA-based tool for\nsimulating radiation-induced DNA damage in cell populations. This approach\nenables the generation of spatially and structurally resolved double-strand\nbreak (DSB) distributions, capturing key features such as damage complexity and\nchromosome specificity. A novel application of the DBSCAN clustering algorithm\nis introduced to group DSBs at the micrometer scale. This allows the\nidentification of physical aggregates of DNA damage and their association with\nsubnuclear domains, providing a direct link to the cell survival probability as\npredicted by \\gsm.\n  The model was validated using experimental data from HUVEC cells irradiated\nwith 220 kV X-rays and H460 cells exposed to protons over a wide linear energy\ntransfer (LET) range, from approximately 4 keV/{\\mu}m to over 20 keV/{\\mu}m.\nResults show excellent agreement between simulations and experimental survival\nprobabilities, making this one of the first consistent multi-scale models to\nbridge nanodosimetric and microdosimetric representations of radiation with\nbiological outcomes such as cell survival.\n  By incorporating the inherent stochastic nature of radiation-matter\ninteractions, this framework effectively connects the physical properties of\nthe radiation field to the biological response at the cellular level. Its\naccuracy across various radiation types and energies supports its potential for\nuse in biologically optimized radiotherapy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an integrated modeling framework that combines the Generalized\nStochastic Microdosimetric Model (GSM2), used to predict cell survival\nfractions, with MINAS-TIRITH, a fast and efficient Geant4 DNA-based tool for\nsimulating radiation-induced DNA damage in cell populations. This approach\nenables the generation of spatially and structurally resolved double-strand\nbreak (DSB) distributions, capturing key features such as damage complexity and\nchromosome specificity. A novel application of the DBSCAN clustering algorithm\nis introduced to group DSBs at the micrometer scale. This allows the\nidentification of physical aggregates of DNA damage and their association with\nsubnuclear domains, providing a direct link to the cell survival probability as\npredicted by \\gsm.\n  The model was validated using experimental data from HUVEC cells irradiated\nwith 220 kV X-rays and H460 cells exposed to protons over a wide linear energy\ntransfer (LET) range, from approximately 4 keV/{\\mu}m to over 20 keV/{\\mu}m.\nResults show excellent agreement between simulations and experimental survival\nprobabilities, making this one of the first consistent multi-scale models to\nbridge nanodosimetric and microdosimetric representations of radiation with\nbiological outcomes such as cell survival.\n  By incorporating the inherent stochastic nature of radiation-matter\ninteractions, this framework effectively connects the physical properties of\nthe radiation field to the biological response at the cellular level. Its\naccuracy across various radiation types and energies supports its potential for\nuse in biologically optimized radiotherapy."
                },
                "authors": [
                    {
                        "name": "Giulio Bordieri"
                    },
                    {
                        "name": "Marta Missiaggia"
                    },
                    {
                        "name": "Gianluca Lattanzi"
                    },
                    {
                        "name": "Carmen Villagrasa"
                    },
                    {
                        "name": "Yann Perrot"
                    },
                    {
                        "name": "Francesco G. Cordoni"
                    }
                ],
                "author_detail": {
                    "name": "Francesco G. Cordoni"
                },
                "author": "Francesco G. Cordoni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00929v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00929v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11953v1",
                "updated": "2025-07-16T06:39:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    6,
                    39,
                    11,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T06:39:11Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    6,
                    39,
                    11,
                    2,
                    197,
                    0
                ],
                "title": "IAM: Efficient Inference through Attention Mapping between\n  Different-scale LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IAM: Efficient Inference through Attention Mapping between\n  Different-scale LLMs"
                },
                "summary": "LLMs encounter significant challenges in resource consumption nowadays,\nespecially with long contexts. Despite extensive efforts dedicate to enhancing\ninference efficiency, these methods primarily exploit internal sparsity within\nthe models, without leveraging external information for optimization. We\nidentify the high similarity of attention matrices across different-scale LLMs,\nwhich offers a novel perspective for optimization. We first conduct a\ncomprehensive analysis of how to measure similarity, how to select mapping\nLayers and whether mapping is consistency. Based on these insights, we\nintroduce the IAM framework, which achieves dual benefits of accelerated\nattention computation and reduced KV cache usage by performing attention\nmapping between small and large LLMs. Our experimental results demonstrate that\nIAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without\nappreciably sacrificing performance. Experiments on different series of models\nshow the generalizability of IAM. Importantly, it is also orthogonal to many\nexisting KV cache optimization methods, making it a versatile addition to the\ncurrent toolkit for enhancing LLM efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs encounter significant challenges in resource consumption nowadays,\nespecially with long contexts. Despite extensive efforts dedicate to enhancing\ninference efficiency, these methods primarily exploit internal sparsity within\nthe models, without leveraging external information for optimization. We\nidentify the high similarity of attention matrices across different-scale LLMs,\nwhich offers a novel perspective for optimization. We first conduct a\ncomprehensive analysis of how to measure similarity, how to select mapping\nLayers and whether mapping is consistency. Based on these insights, we\nintroduce the IAM framework, which achieves dual benefits of accelerated\nattention computation and reduced KV cache usage by performing attention\nmapping between small and large LLMs. Our experimental results demonstrate that\nIAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without\nappreciably sacrificing performance. Experiments on different series of models\nshow the generalizability of IAM. Importantly, it is also orthogonal to many\nexisting KV cache optimization methods, making it a versatile addition to the\ncurrent toolkit for enhancing LLM efficiency."
                },
                "authors": [
                    {
                        "name": "Yi Zhao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11539v1",
                "updated": "2025-07-15T17:59:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    59,
                    57,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T17:59:57Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    59,
                    57,
                    1,
                    196,
                    0
                ],
                "title": "Streaming 4D Visual Geometry Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming 4D Visual Geometry Transformer"
                },
                "summary": "Perceiving and reconstructing 4D spatial-temporal geometry from videos is a\nfundamental yet challenging computer vision task. To facilitate interactive and\nreal-time applications, we propose a streaming 4D visual geometry transformer\nthat shares a similar philosophy with autoregressive large language models. We\nexplore a simple and efficient design and employ a causal transformer\narchitecture to process the input sequence in an online manner. We use temporal\ncausal attention and cache the historical keys and values as implicit memory to\nenable efficient streaming long-term 4D reconstruction. This design can handle\nreal-time 4D reconstruction by incrementally integrating historical information\nwhile maintaining high-quality spatial consistency. For efficient training, we\npropose to distill knowledge from the dense bidirectional visual geometry\ngrounded transformer (VGGT) to our causal model. For inference, our model\nsupports the migration of optimized efficient attention operator (e.g.,\nFlashAttention) from the field of large language models. Extensive experiments\non various 4D geometry perception benchmarks demonstrate that our model\nincreases the inference speed in online scenarios while maintaining competitive\nperformance, paving the way for scalable and interactive 4D vision systems.\nCode is available at: https://github.com/wzzheng/StreamVGGT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perceiving and reconstructing 4D spatial-temporal geometry from videos is a\nfundamental yet challenging computer vision task. To facilitate interactive and\nreal-time applications, we propose a streaming 4D visual geometry transformer\nthat shares a similar philosophy with autoregressive large language models. We\nexplore a simple and efficient design and employ a causal transformer\narchitecture to process the input sequence in an online manner. We use temporal\ncausal attention and cache the historical keys and values as implicit memory to\nenable efficient streaming long-term 4D reconstruction. This design can handle\nreal-time 4D reconstruction by incrementally integrating historical information\nwhile maintaining high-quality spatial consistency. For efficient training, we\npropose to distill knowledge from the dense bidirectional visual geometry\ngrounded transformer (VGGT) to our causal model. For inference, our model\nsupports the migration of optimized efficient attention operator (e.g.,\nFlashAttention) from the field of large language models. Extensive experiments\non various 4D geometry perception benchmarks demonstrate that our model\nincreases the inference speed in online scenarios while maintaining competitive\nperformance, paving the way for scalable and interactive 4D vision systems.\nCode is available at: https://github.com/wzzheng/StreamVGGT."
                },
                "authors": [
                    {
                        "name": "Dong Zhuo"
                    },
                    {
                        "name": "Wenzhao Zheng"
                    },
                    {
                        "name": "Jiahe Guo"
                    },
                    {
                        "name": "Yuqi Wu"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Code is available at: https://github.com/wzzheng/StreamVGGT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11507v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11507v1",
                "updated": "2025-07-15T17:23:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    23,
                    22,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T17:23:22Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    23,
                    22,
                    1,
                    196,
                    0
                ],
                "title": "MIRAGE: KV Cache Optimization through Parameter Remapping for\n  Multi-tenant LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIRAGE: KV Cache Optimization through Parameter Remapping for\n  Multi-tenant LLM Serving"
                },
                "summary": "KV cache accelerates LLM inference by avoiding redundant computation, at the\nexpense of memory. To support larger KV caches, prior work extends GPU memory\nwith CPU memory via CPU-offloading. This involves swapping KV cache between GPU\nand CPU memory. However, because the cache updates dynamically, such swapping\nincurs high CPU memory traffic. We make a key observation that model parameters\nremain constant during runtime, unlike the dynamically updated KV cache.\nBuilding on this, we introduce MIRAGE, which avoids KV cache swapping by\nremapping, and thereby repurposing, the memory allocated to model parameters\nfor KV cache. This parameter remapping is especially beneficial in multi-tenant\nenvironments, where the memory used for the parameters of the inactive models\ncan be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth\noffered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we\nshow that MIRAGE significantly outperforms state-of-the-art solutions,\nachieving a reduction of 44.8%-82.5% in tail time-between-token latency,\n20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher\nthroughput compared to vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache accelerates LLM inference by avoiding redundant computation, at the\nexpense of memory. To support larger KV caches, prior work extends GPU memory\nwith CPU memory via CPU-offloading. This involves swapping KV cache between GPU\nand CPU memory. However, because the cache updates dynamically, such swapping\nincurs high CPU memory traffic. We make a key observation that model parameters\nremain constant during runtime, unlike the dynamically updated KV cache.\nBuilding on this, we introduce MIRAGE, which avoids KV cache swapping by\nremapping, and thereby repurposing, the memory allocated to model parameters\nfor KV cache. This parameter remapping is especially beneficial in multi-tenant\nenvironments, where the memory used for the parameters of the inactive models\ncan be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth\noffered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we\nshow that MIRAGE significantly outperforms state-of-the-art solutions,\nachieving a reduction of 44.8%-82.5% in tail time-between-token latency,\n20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher\nthroughput compared to vLLM."
                },
                "authors": [
                    {
                        "name": "Ruihao Li"
                    },
                    {
                        "name": "Shagnik Pal"
                    },
                    {
                        "name": "Vineeth Narayan Pullu"
                    },
                    {
                        "name": "Prasoon Sinha"
                    },
                    {
                        "name": "Jeeho Ryoo"
                    },
                    {
                        "name": "Lizy K. John"
                    },
                    {
                        "name": "Neeraja J. Yadwadkar"
                    }
                ],
                "author_detail": {
                    "name": "Neeraja J. Yadwadkar"
                },
                "author": "Neeraja J. Yadwadkar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11507v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11507v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22791v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22791v3",
                "updated": "2025-07-15T12:59:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    12,
                    59,
                    47,
                    1,
                    196,
                    0
                ],
                "published": "2025-06-28T07:25:12Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    7,
                    25,
                    12,
                    5,
                    179,
                    0
                ],
                "title": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in\n  Large Language Models"
                },
                "summary": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications."
                },
                "authors": [
                    {
                        "name": "Jianxin Yan"
                    },
                    {
                        "name": "Wangze Ni"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Zhan Qin"
                    },
                    {
                        "name": "Kui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Kui Ren"
                },
                "author": "Kui Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22791v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22791v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11273v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11273v1",
                "updated": "2025-07-15T12:52:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    12,
                    52,
                    12,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T12:52:12Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    12,
                    52,
                    12,
                    1,
                    196,
                    0
                ],
                "title": "KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware\n  Rotary Positional Embedding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware\n  Rotary Positional Embedding"
                },
                "summary": "Large language models (LLMs) based on Transformer Decoders have become the\npreferred choice for conversational generative AI. Despite the overall\nsuperiority of the Decoder architecture, the gradually increasing Key-Value\n(KV) cache during inference has emerged as a primary efficiency bottleneck,\nboth in aspects of memory consumption and data transfer bandwidth limitations.\nTo address these challenges, we propose a paradigm called KV-Latent. By\ndown-sampling the Key-Value vector dimensions into a latent space, we can\nsignificantly reduce the KV Cache footprint and improve inference speed, only\nwith a small amount of extra training, less than 1\\% of pre-training takes.\nBesides, we enhanced the stability of Rotary Positional Embedding applied on\nlower-dimensional vectors by modifying its frequency sampling mechanism,\navoiding noise introduced by higher frequencies while retaining position\nattenuation. Our experiments, including both models with Grouped Query\nAttention and those without, have yielded satisfactory results. Finally, we\nconducted comparative experiments to study the impact of separately reducing\nKey and Value components on model's performance. Our approach allows for the\nconstruction of more efficient language model systems, and opens the new\npossibility on KV Cache saving and efficient LLMs. Our code is available at\nhttps://github.com/ShiLuohe/KV-Latent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) based on Transformer Decoders have become the\npreferred choice for conversational generative AI. Despite the overall\nsuperiority of the Decoder architecture, the gradually increasing Key-Value\n(KV) cache during inference has emerged as a primary efficiency bottleneck,\nboth in aspects of memory consumption and data transfer bandwidth limitations.\nTo address these challenges, we propose a paradigm called KV-Latent. By\ndown-sampling the Key-Value vector dimensions into a latent space, we can\nsignificantly reduce the KV Cache footprint and improve inference speed, only\nwith a small amount of extra training, less than 1\\% of pre-training takes.\nBesides, we enhanced the stability of Rotary Positional Embedding applied on\nlower-dimensional vectors by modifying its frequency sampling mechanism,\navoiding noise introduced by higher frequencies while retaining position\nattenuation. Our experiments, including both models with Grouped Query\nAttention and those without, have yielded satisfactory results. Finally, we\nconducted comparative experiments to study the impact of separately reducing\nKey and Value components on model's performance. Our approach allows for the\nconstruction of more efficient language model systems, and opens the new\npossibility on KV Cache saving and efficient LLMs. Our code is available at\nhttps://github.com/ShiLuohe/KV-Latent."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Lefei Zhang"
                    },
                    {
                        "name": "Guoming Liu"
                    },
                    {
                        "name": "Baoyuan Qi"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "To be published in The 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11273v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11273v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17911v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17911v3",
                "updated": "2025-07-15T11:31:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    11,
                    31,
                    14,
                    1,
                    196,
                    0
                ],
                "published": "2025-03-23T03:16:50Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    16,
                    50,
                    6,
                    82,
                    0
                ],
                "title": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search"
                },
                "summary": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index. This paper introduces VSAG, an\nopen-source framework that aims to enhance the in production performance of\ngraph-based ANNS algorithms. VSAG has been deployed at scale in the services of\nAnt Group, and it incorporates three key optimizations: (i) efficient memory\naccess: it reduces L3 cache misses with pre-fetching and cache-friendly vector\norganization; (ii) automated parameter tuning: it automatically selects\nperformance-optimal parameters without requiring index rebuilding; (iii)\nefficient distance computation: it leverages modern hardware, scalar\nquantization, and smartly switches to low-precision representation to\ndramatically reduce the distance computation costs. We evaluate VSAG on\nreal-world datasets. The experimental results show that VSAG achieves the\nstate-of-the-art performance and provides up to 4x speedup over HNSWlib (an\nindustry-standard library) while ensuring the same accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index. This paper introduces VSAG, an\nopen-source framework that aims to enhance the in production performance of\ngraph-based ANNS algorithms. VSAG has been deployed at scale in the services of\nAnt Group, and it incorporates three key optimizations: (i) efficient memory\naccess: it reduces L3 cache misses with pre-fetching and cache-friendly vector\norganization; (ii) automated parameter tuning: it automatically selects\nperformance-optimal parameters without requiring index rebuilding; (iii)\nefficient distance computation: it leverages modern hardware, scalar\nquantization, and smartly switches to low-precision representation to\ndramatically reduce the distance computation costs. We evaluate VSAG on\nreal-world datasets. The experimental results show that VSAG achieves the\nstate-of-the-art performance and provides up to 4x speedup over HNSWlib (an\nindustry-standard library) while ensuring the same accuracy."
                },
                "authors": [
                    {
                        "name": "Xiaoyao Zhong"
                    },
                    {
                        "name": "Haotian Li"
                    },
                    {
                        "name": "Jiabao Jin"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Deming Chu"
                    },
                    {
                        "name": "Xiangyu Wang"
                    },
                    {
                        "name": "Zhitao Shen"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "George Gu"
                    },
                    {
                        "name": "Yi Xie"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Heng Tao Shen"
                    },
                    {
                        "name": "Jingkuan Song"
                    },
                    {
                        "name": "Peng Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Peng Cheng"
                },
                "author": "Peng Cheng",
                "arxiv_comment": "the report of open-source library VSAG\n  (https://github.com/antgroup/vsag) accepted by VLDB 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17911v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17911v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11121v1",
                "updated": "2025-07-15T09:15:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    9,
                    15,
                    18,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T09:15:18Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    9,
                    15,
                    18,
                    1,
                    196,
                    0
                ],
                "title": "Two-dimensional single-crystal photonic scintillator for enhanced X-ray\n  imaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-dimensional single-crystal photonic scintillator for enhanced X-ray\n  imaging"
                },
                "summary": "The evolution of X-ray detection technology has significantly enhanced\nsensitivity and spatial resolution in non-destructive imaging of internal\nstructure. However, the problem of low luminescence and transparency of\nscintillator materials restricts imaging with lower radiation doses and thicker\nmaterials. Here, we propose a two-dimensional photonic scintillator for single\ncrystal and demonstrate that the optical guiding effect emerging from the\nstructure reduces luminescence leakage and increases the signal intensity by\naround a factor of 2 from 200 to 450 kV. This approach has the potential to\nenhance the output rate by an order of magnitude. The photonic structure\nfeatures a fine array pitch and large-scale detection area with fast\nfabrication time. Our scheme paves the way for high sensitivity X-ray imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution of X-ray detection technology has significantly enhanced\nsensitivity and spatial resolution in non-destructive imaging of internal\nstructure. However, the problem of low luminescence and transparency of\nscintillator materials restricts imaging with lower radiation doses and thicker\nmaterials. Here, we propose a two-dimensional photonic scintillator for single\ncrystal and demonstrate that the optical guiding effect emerging from the\nstructure reduces luminescence leakage and increases the signal intensity by\naround a factor of 2 from 200 to 450 kV. This approach has the potential to\nenhance the output rate by an order of magnitude. The photonic structure\nfeatures a fine array pitch and large-scale detection area with fast\nfabrication time. Our scheme paves the way for high sensitivity X-ray imaging."
                },
                "authors": [
                    {
                        "name": "Tatsunori Shibuya"
                    },
                    {
                        "name": "Eichi Terasawa"
                    },
                    {
                        "name": "Hiromi Kimura"
                    },
                    {
                        "name": "Takeshi Fujiwara"
                    }
                ],
                "author_detail": {
                    "name": "Takeshi Fujiwara"
                },
                "author": "Takeshi Fujiwara",
                "arxiv_comment": "16 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11067v1",
                "updated": "2025-07-15T08:00:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    8,
                    0,
                    11,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T08:00:11Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    8,
                    0,
                    11,
                    1,
                    196,
                    0
                ],
                "title": "MMStencil: Optimizing High-order Stencils on Multicore CPU using Matrix\n  Unit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMStencil: Optimizing High-order Stencils on Multicore CPU using Matrix\n  Unit"
                },
                "summary": "Matrix-accelerated stencil computation is a hot research topic, yet its\napplication to three-dimensional (3D) high-order stencils and HPC remains\nunderexplored. With the emergence of matrix units on multicore CPUs, we analyze\nmatrix-based acceleration strategies and tailor an optimal approach for 3D\nhigh-order stencils. We introduce algorithmic optimizations based on SIMD and\nmatrix units to address strided memory accesses, alignment conflicts, and\nredundant accesses. We propose memory optimizations to boost on-package memory\nefficiency, and a novel multi-thread parallelism paradigm to overcome\ndata-sharing challenges caused by the absence of shared data caches. MMStencil\nsustains consistently high hardware utilization across diverse stencil shapes\nand dimensions. Our DMA-based inter-NUMA communication further mitigates NUMA\neffects and MPI limitations in hybrid parallelism. Combining all the\ninnovations, MMStencil outperforms state-of-the-art libraries on Nvidia A100\nGPGPU by up to 2.1x. Moreover, the performance improvements translate directly\nto real-world HPC applications and enable RTM applications to yield 1.8x\nspeedup versus a highly optimized industrial Nvidia A100 GPGPU version.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix-accelerated stencil computation is a hot research topic, yet its\napplication to three-dimensional (3D) high-order stencils and HPC remains\nunderexplored. With the emergence of matrix units on multicore CPUs, we analyze\nmatrix-based acceleration strategies and tailor an optimal approach for 3D\nhigh-order stencils. We introduce algorithmic optimizations based on SIMD and\nmatrix units to address strided memory accesses, alignment conflicts, and\nredundant accesses. We propose memory optimizations to boost on-package memory\nefficiency, and a novel multi-thread parallelism paradigm to overcome\ndata-sharing challenges caused by the absence of shared data caches. MMStencil\nsustains consistently high hardware utilization across diverse stencil shapes\nand dimensions. Our DMA-based inter-NUMA communication further mitigates NUMA\neffects and MPI limitations in hybrid parallelism. Combining all the\ninnovations, MMStencil outperforms state-of-the-art libraries on Nvidia A100\nGPGPU by up to 2.1x. Moreover, the performance improvements translate directly\nto real-world HPC applications and enable RTM applications to yield 1.8x\nspeedup versus a highly optimized industrial Nvidia A100 GPGPU version."
                },
                "authors": [
                    {
                        "name": "Yinuo Wang"
                    },
                    {
                        "name": "Tianqi Mao"
                    },
                    {
                        "name": "Lin Gan"
                    },
                    {
                        "name": "Wubing Wan"
                    },
                    {
                        "name": "Zeyu Song"
                    },
                    {
                        "name": "Jiayu Fu"
                    },
                    {
                        "name": "Lanke He"
                    },
                    {
                        "name": "Wenqiang Wang"
                    },
                    {
                        "name": "Zekun Yin"
                    },
                    {
                        "name": "Wei Xue"
                    },
                    {
                        "name": "Guangwen Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guangwen Yang"
                },
                "author": "Guangwen Yang",
                "arxiv_comment": "Yinuo Wang and Tianqi Mao contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18191v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18191v2",
                "updated": "2025-07-14T19:51:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    19,
                    51,
                    9,
                    0,
                    195,
                    0
                ],
                "published": "2025-03-23T20:18:16Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    20,
                    18,
                    16,
                    6,
                    82,
                    0
                ],
                "title": "Enabling the Write-Back Page Cache with Strong Consistency in\n  Distributed Userspace File Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling the Write-Back Page Cache with Strong Consistency in\n  Distributed Userspace File Systems"
                },
                "summary": "Cloud platforms host thousands of tenants that demand POSIX semantics, high\nthroughput, and rapid evolution from their storage layer. Kernel-native\ndistributed file systems supply raw speed, but their privileged code base\ncouples every release to the kernel, widens the blast radius of crashes, and\nslows innovation. FUSE-based distributed file systems flip those trade-offs:\nthey run in user space for fast deployment and strong fault isolation, yet the\nFUSE interface disables the kernel's write-back page cache whenever strong\nconsistency is required. Practitioners must therefore choose between (i) weak\nconsistency with fast write-back caching or (ii) strong consistency with slow\nwrite-through I/O, an limitation that has kept FUSE distributed file systems\nout of write-intensive cloud workloads.\n  To this end, We present DistFUSE, the first distributed FUSE file system that\ndelivers write-back kernel caching and strong consistency. DistFUSE achieves\nthis by offloading userspace consistency control to the kernel driver, allowing\ncoordinated access to the kernel's page cache across nodes. This design\neliminates blind local cache updates and ensures cluster-wide strong\nconsistency without compromising performance. In our evaluation, DistFUSE\nachieves up to 68.0% higher throughput and 40.4% lower latency than the\nexisting write-through design of FUSE-based distributed file system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud platforms host thousands of tenants that demand POSIX semantics, high\nthroughput, and rapid evolution from their storage layer. Kernel-native\ndistributed file systems supply raw speed, but their privileged code base\ncouples every release to the kernel, widens the blast radius of crashes, and\nslows innovation. FUSE-based distributed file systems flip those trade-offs:\nthey run in user space for fast deployment and strong fault isolation, yet the\nFUSE interface disables the kernel's write-back page cache whenever strong\nconsistency is required. Practitioners must therefore choose between (i) weak\nconsistency with fast write-back caching or (ii) strong consistency with slow\nwrite-through I/O, an limitation that has kept FUSE distributed file systems\nout of write-intensive cloud workloads.\n  To this end, We present DistFUSE, the first distributed FUSE file system that\ndelivers write-back kernel caching and strong consistency. DistFUSE achieves\nthis by offloading userspace consistency control to the kernel driver, allowing\ncoordinated access to the kernel's page cache across nodes. This design\neliminates blind local cache updates and ensures cluster-wide strong\nconsistency without compromising performance. In our evaluation, DistFUSE\nachieves up to 68.0% higher throughput and 40.4% lower latency than the\nexisting write-through design of FUSE-based distributed file system."
                },
                "authors": [
                    {
                        "name": "Haoyu Li"
                    },
                    {
                        "name": "Jingkai Fu"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Windsor Hsu"
                    },
                    {
                        "name": "Asaf Cidon"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Cidon"
                },
                "author": "Asaf Cidon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18191v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18191v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10757v1",
                "updated": "2025-07-14T19:31:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    19,
                    31,
                    6,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T19:31:06Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    19,
                    31,
                    6,
                    0,
                    195,
                    0
                ],
                "title": "FAFO: Over 1 million TPS on a single node running EVM while still\n  Merkleizing every block",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAFO: Over 1 million TPS on a single node running EVM while still\n  Merkleizing every block"
                },
                "summary": "Current blockchain execution throughput is limited by data contention,\nreducing execution layer parallelism. Fast Ahead-of-Formation Optimization\n(FAFO) is the first blockchain transaction scheduler to address this problem by\nreordering transactions before block formation for maximum concurrency. FAFO\nuses CPU-optimized cache-friendly Bloom filters to efficiently detect conflicts\nand schedule parallel transaction execution at high throughput and low\noverhead.\n  We integrate the Rust EVM client (REVM) into FAFO and achieve over 1.1\nmillion native ETH transfers per second and over half a million ERC20 transfers\nper second on a single node (Table 1), with 91% lower cost compared to\nstate-of-the-art sharded execution. Unlike many other existing high throughput\nblockchain execution clients, FAFO uses QMDB to Merkleize world state after\nevery block, enabling light clients and stateless validation for ZK-based\nvApps. FAFO scales with minimal synchronization overhead, scaling linearly with\nadditional CPU resources until it fully exploits the maximum parallelism of the\nunderlying transaction flow. FAFO proves that the high throughput necessary to\nsupport future decentralized applications can be achieved with a streamlined\nexecution layer and innovations in blockchain transaction scheduler design.\nFAFO is open-sourced at https://github.com/LayerZero-Labs/fafo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current blockchain execution throughput is limited by data contention,\nreducing execution layer parallelism. Fast Ahead-of-Formation Optimization\n(FAFO) is the first blockchain transaction scheduler to address this problem by\nreordering transactions before block formation for maximum concurrency. FAFO\nuses CPU-optimized cache-friendly Bloom filters to efficiently detect conflicts\nand schedule parallel transaction execution at high throughput and low\noverhead.\n  We integrate the Rust EVM client (REVM) into FAFO and achieve over 1.1\nmillion native ETH transfers per second and over half a million ERC20 transfers\nper second on a single node (Table 1), with 91% lower cost compared to\nstate-of-the-art sharded execution. Unlike many other existing high throughput\nblockchain execution clients, FAFO uses QMDB to Merkleize world state after\nevery block, enabling light clients and stateless validation for ZK-based\nvApps. FAFO scales with minimal synchronization overhead, scaling linearly with\nadditional CPU resources until it fully exploits the maximum parallelism of the\nunderlying transaction flow. FAFO proves that the high throughput necessary to\nsupport future decentralized applications can be achieved with a streamlined\nexecution layer and innovations in blockchain transaction scheduler design.\nFAFO is open-sourced at https://github.com/LayerZero-Labs/fafo."
                },
                "authors": [
                    {
                        "name": "Ryan Zarick"
                    },
                    {
                        "name": "Isaac Zhang"
                    },
                    {
                        "name": "Daniel Wong"
                    },
                    {
                        "name": "Thomas Kim"
                    },
                    {
                        "name": "Bryan Pellegrino"
                    },
                    {
                        "name": "Mignon Li"
                    },
                    {
                        "name": "Kelvin Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kelvin Wong"
                },
                "author": "Kelvin Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14204v1",
                "updated": "2025-07-14T19:09:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    19,
                    9,
                    57,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T19:09:57Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    19,
                    9,
                    57,
                    0,
                    195,
                    0
                ],
                "title": "LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of\n  Large Language Models"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have spurred interest in\nnumerous applications requiring robust long-range capabilities, essential for\nprocessing extensive input contexts and continuously generating extended\noutputs. As sequence lengths increase, the number of Key-Value (KV) pairs in\nLLMs escalates, creating a significant efficiency bottleneck. In this paper, we\npropose a new KV cache optimization paradigm called LaCache, a training-free\nmethod for efficient and accurate generative inference of LLMs. LaCache enables\nLLMs to simultaneously address both of the critical challenges in long-range\nmodeling: robust long-range capabilities and continuous generation without\nrunning out-of-memory (OOM). Specifically, LaCache integrates two key\ninnovations: (1) a ladder-shaped KV cache pattern that stores KV pairs not only\nsequentially (left-to-right within each layer) but also across layers (from\nshallow to deep), providing an extended span for capturing long-range\ndependencies under a fixed storage budget, thereby boosting long-range\ncapabilities; and (2) an iterative compaction mechanism that progressively\ncompresses older caches, freeing up space for new tokens within a fixed cache\nsize. This token distance-based dynamic compression enables more effective\ncontinuous generation under constrained cache budgets. Experiments across\nvarious tasks, benchmarks, and LLM models consistently validate LaCache's\neffectiveness in enhancing LLMs' long-range capabilities. Our code is available\nat https://github.com/GATECH-EIC/LaCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have spurred interest in\nnumerous applications requiring robust long-range capabilities, essential for\nprocessing extensive input contexts and continuously generating extended\noutputs. As sequence lengths increase, the number of Key-Value (KV) pairs in\nLLMs escalates, creating a significant efficiency bottleneck. In this paper, we\npropose a new KV cache optimization paradigm called LaCache, a training-free\nmethod for efficient and accurate generative inference of LLMs. LaCache enables\nLLMs to simultaneously address both of the critical challenges in long-range\nmodeling: robust long-range capabilities and continuous generation without\nrunning out-of-memory (OOM). Specifically, LaCache integrates two key\ninnovations: (1) a ladder-shaped KV cache pattern that stores KV pairs not only\nsequentially (left-to-right within each layer) but also across layers (from\nshallow to deep), providing an extended span for capturing long-range\ndependencies under a fixed storage budget, thereby boosting long-range\ncapabilities; and (2) an iterative compaction mechanism that progressively\ncompresses older caches, freeing up space for new tokens within a fixed cache\nsize. This token distance-based dynamic compression enables more effective\ncontinuous generation under constrained cache budgets. Experiments across\nvarious tasks, benchmarks, and LLM models consistently validate LaCache's\neffectiveness in enhancing LLMs' long-range capabilities. Our code is available\nat https://github.com/GATECH-EIC/LaCache."
                },
                "authors": [
                    {
                        "name": "Dachuan Shi"
                    },
                    {
                        "name": "Yonggan Fu"
                    },
                    {
                        "name": "Xiangchi Yuan"
                    },
                    {
                        "name": "Zhongzhi Yu"
                    },
                    {
                        "name": "Haoran You"
                    },
                    {
                        "name": "Sixu Li"
                    },
                    {
                        "name": "Xin Dong"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Yingyan"
                    },
                    {
                        "name": "Lin"
                    }
                ],
                "author_detail": {
                    "name": "Lin"
                },
                "arxiv_affiliation": "Celine",
                "author": "Lin",
                "arxiv_comment": "ICML 2025. Code: https://github.com/GATECH-EIC/LaCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v4",
                "updated": "2025-07-14T18:22:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    18,
                    22,
                    53,
                    0,
                    195,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving"
                },
                "summary": "Compound AI systems, such as agentic systems, are an emerging trend in\nlarge-scale enterprise settings, with multiple LLMs specialized for different\nusers, tasks, and/or roles working together. In these scenarios, different\nmodels often process inputs that share the same context prefix. Although much\nwork was done in the past to enable the reuse of prefix KV caches across inputs\nfor a single model, how to enable one model to reuse the prefix KV caches of a\ndifferent model remains an open question.\n  We introduce DroidSpeak, the first distributed LLM inference system that\nenables KV cache reuse across distributed nodes running inference of different\nLLMs, so long as the LLMs have the same architecture. We present the first\nstudy that aims at understanding the impact of sharing KV caches across\ndifferent LLMs, and if/when such sharing affects quality. Inspired by the\nfindings, we present DroidSpeak, which selectively recomputes a few layers of\nthe KV cache produced by another LLM and reuses the remaining layers, with\nnegligible quality loss. Moreover, carefully pipelining the layer-wise\nre-computation and the loading of reused KV cache further improves the\ninference performance. Experiments on diverse datasets and model pairs\ndemonstrate that DroidSpeak achieves up to 4x throughput improvement and about\n3.1x faster prefill (time to first token), with negligible loss of quality in\nF1 scores, Rouge-L or code similarity score, compared to the baseline which\ndoes not allow any sharing across models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compound AI systems, such as agentic systems, are an emerging trend in\nlarge-scale enterprise settings, with multiple LLMs specialized for different\nusers, tasks, and/or roles working together. In these scenarios, different\nmodels often process inputs that share the same context prefix. Although much\nwork was done in the past to enable the reuse of prefix KV caches across inputs\nfor a single model, how to enable one model to reuse the prefix KV caches of a\ndifferent model remains an open question.\n  We introduce DroidSpeak, the first distributed LLM inference system that\nenables KV cache reuse across distributed nodes running inference of different\nLLMs, so long as the LLMs have the same architecture. We present the first\nstudy that aims at understanding the impact of sharing KV caches across\ndifferent LLMs, and if/when such sharing affects quality. Inspired by the\nfindings, we present DroidSpeak, which selectively recomputes a few layers of\nthe KV cache produced by another LLM and reuses the remaining layers, with\nnegligible quality loss. Moreover, carefully pipelining the layer-wise\nre-computation and the loading of reused KV cache further improves the\ninference performance. Experiments on diverse datasets and model pairs\ndemonstrate that DroidSpeak achieves up to 4x throughput improvement and about\n3.1x faster prefill (time to first token), with negligible loss of quality in\nF1 scores, Rouge-L or code similarity score, compared to the baseline which\ndoes not allow any sharing across models."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Shaoting Feng"
                    },
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Madan Musuvathi"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03409v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03409v3",
                "updated": "2025-07-14T16:14:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    14,
                    49,
                    0,
                    195,
                    0
                ],
                "published": "2024-12-04T15:48:59Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    48,
                    59,
                    2,
                    339,
                    0
                ],
                "title": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation"
                },
                "summary": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at https://github.com/THU-MIG/PrefixKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at https://github.com/THU-MIG/PrefixKV."
                },
                "authors": [
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Jiaxin Li"
                    },
                    {
                        "name": "Jianchao Tan"
                    },
                    {
                        "name": "Kefeng Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "12 pages, 5 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03409v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03409v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10367v1",
                "updated": "2025-07-14T15:09:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    9,
                    1,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T15:09:01Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    9,
                    1,
                    0,
                    195,
                    0
                ],
                "title": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline"
                },
                "summary": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year."
                },
                "authors": [
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Junbin Kang"
                    },
                    {
                        "name": "Mingkai Dong"
                    },
                    {
                        "name": "Mingyu Liu"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Shaohong Guo"
                    },
                    {
                        "name": "Ziyan Qiu"
                    },
                    {
                        "name": "Mingzhen You"
                    },
                    {
                        "name": "Ziyi Tian"
                    },
                    {
                        "name": "Anqi Yu"
                    },
                    {
                        "name": "Tianhong Ding"
                    },
                    {
                        "name": "Xinwei Hu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by NSDI'26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01465v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01465v2",
                "updated": "2025-07-14T09:45:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    9,
                    45,
                    34,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-02T08:24:50Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    24,
                    50,
                    2,
                    183,
                    0
                ],
                "title": "Pruning the Tree: Rethinking RPKI Architecture From The Ground Up",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pruning the Tree: Rethinking RPKI Architecture From The Ground Up"
                },
                "summary": "Resource Public Key Infrastructure (RPKI) is a critical security mechanism\nfor BGP, but the complexity of its architecture is a growing concern as its\nadoption scales. Current RPKI design heavily reuses legacy PKI components, such\nas X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols,\nwhich introduce excessive cryptographic validation, redundant metadata, and\ninefficiencies in both storage and processing. We show that these design\nchoices, although based on established standards, create significant\nperformance bottlenecks, increase the vulnerability surface, and hinder\nscalability for wide-scale Internet deployment.\n  In this paper, we perform the first systematic analysis of the root causes of\ncomplexity in RPKI's design and experimentally quantify their real-world\nimpact. We show that over 70\\% of validation time in RPKI relying parties is\nspent on certificate parsing and signature verification, much of it\nunnecessary. Building on this insight, we introduce the improved RPKI (iRPKI),\na backwards-compatible redesign that preserves all security guarantees while\nsubstantially reducing protocol overhead. iRPKI eliminates EE-certificates and\nROA signatures, merges revocation and integrity objects, replaces verbose\nencodings with Protobuf, and restructures repository metadata for more\nefficient access. We experimentally demonstrate that our implementation of\niRPKI in the Routinator validator achieves a 20x speed-up of processing time,\n18x improvement of bandwidth requirements and 8x reduction in cache memory\nfootprint, while also eliminating classes of vulnerabilities that have led to\nat least 10 vulnerabilities in RPKI software. iRPKI significantly increases the\nfeasibility of deploying RPKI at scale in the Internet, and especially in\nconstrained environments. Our design may be deployed incrementally without\nimpacting existing operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Public Key Infrastructure (RPKI) is a critical security mechanism\nfor BGP, but the complexity of its architecture is a growing concern as its\nadoption scales. Current RPKI design heavily reuses legacy PKI components, such\nas X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols,\nwhich introduce excessive cryptographic validation, redundant metadata, and\ninefficiencies in both storage and processing. We show that these design\nchoices, although based on established standards, create significant\nperformance bottlenecks, increase the vulnerability surface, and hinder\nscalability for wide-scale Internet deployment.\n  In this paper, we perform the first systematic analysis of the root causes of\ncomplexity in RPKI's design and experimentally quantify their real-world\nimpact. We show that over 70\\% of validation time in RPKI relying parties is\nspent on certificate parsing and signature verification, much of it\nunnecessary. Building on this insight, we introduce the improved RPKI (iRPKI),\na backwards-compatible redesign that preserves all security guarantees while\nsubstantially reducing protocol overhead. iRPKI eliminates EE-certificates and\nROA signatures, merges revocation and integrity objects, replaces verbose\nencodings with Protobuf, and restructures repository metadata for more\nefficient access. We experimentally demonstrate that our implementation of\niRPKI in the Routinator validator achieves a 20x speed-up of processing time,\n18x improvement of bandwidth requirements and 8x reduction in cache memory\nfootprint, while also eliminating classes of vulnerabilities that have led to\nat least 10 vulnerabilities in RPKI software. iRPKI significantly increases the\nfeasibility of deploying RPKI at scale in the Internet, and especially in\nconstrained environments. Our design may be deployed incrementally without\nimpacting existing operations."
                },
                "authors": [
                    {
                        "name": "Haya Schulmann"
                    },
                    {
                        "name": "Niklas Vogel"
                    }
                ],
                "author_detail": {
                    "name": "Niklas Vogel"
                },
                "author": "Niklas Vogel",
                "arxiv_comment": "Accepted for publication at NDSS2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01465v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01465v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10069v1",
                "updated": "2025-07-14T08:53:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    53,
                    48,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T08:53:48Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    53,
                    48,
                    0,
                    195,
                    0
                ],
                "title": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal\n  Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal\n  Parallelism"
                },
                "summary": "Multimodal large language models (MLLMs) extend LLMs to handle images,\nvideos, and audio by incorporating feature extractors and projection modules.\nHowever, these additional components -- combined with complex inference\npipelines and heterogeneous workloads -- introduce significant inference\noverhead. Therefore, efficiently serving MLLMs remains a major challenge.\nCurrent tightly coupled serving architectures struggle to distinguish between\nmixed request types or adapt parallelism strategies to different inference\nstages, leading to increased time-to-first-token (TTFT) latency and poor\nresource utilization. To address this, we propose Elastic Multimodal\nParallelism (EMP), a new serving paradigm that elastically adapts to resource\nheterogeneity across request types and inference stages. Building upon EMP, we\ndevelop ElasticMM, an MLLM serving system that (1) separates requests into\nindependent modality groups with dynamic resource allocation via a\nmodality-aware load balancer; (2) decouples inference stages and enables\nparallelism adjustment and adaptive scaling via elastic partition scheduling;\nand (3) improves inference efficiency through unified multimodal prefix caching\nand non-blocking encoding. Experiments on diverse real-world datasets show that\nElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by\nup to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level\nobjectives (SLOs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) extend LLMs to handle images,\nvideos, and audio by incorporating feature extractors and projection modules.\nHowever, these additional components -- combined with complex inference\npipelines and heterogeneous workloads -- introduce significant inference\noverhead. Therefore, efficiently serving MLLMs remains a major challenge.\nCurrent tightly coupled serving architectures struggle to distinguish between\nmixed request types or adapt parallelism strategies to different inference\nstages, leading to increased time-to-first-token (TTFT) latency and poor\nresource utilization. To address this, we propose Elastic Multimodal\nParallelism (EMP), a new serving paradigm that elastically adapts to resource\nheterogeneity across request types and inference stages. Building upon EMP, we\ndevelop ElasticMM, an MLLM serving system that (1) separates requests into\nindependent modality groups with dynamic resource allocation via a\nmodality-aware load balancer; (2) decouples inference stages and enables\nparallelism adjustment and adaptive scaling via elastic partition scheduling;\nand (3) improves inference efficiency through unified multimodal prefix caching\nand non-blocking encoding. Experiments on diverse real-world datasets show that\nElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by\nup to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level\nobjectives (SLOs)."
                },
                "authors": [
                    {
                        "name": "Zedong Liu"
                    },
                    {
                        "name": "Shenggan Cheng"
                    },
                    {
                        "name": "Guangming Tan"
                    },
                    {
                        "name": "Yang You"
                    },
                    {
                        "name": "Dingwen Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dingwen Tao"
                },
                "author": "Dingwen Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03940v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03940v3",
                "updated": "2025-07-14T07:05:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    7,
                    5,
                    28,
                    0,
                    195,
                    0
                ],
                "published": "2025-01-07T17:00:49Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "title": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection"
                },
                "summary": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages."
                },
                "authors": [
                    {
                        "name": "Pablo Miralles-Gonzlez"
                    },
                    {
                        "name": "Javier Huertas-Tato"
                    },
                    {
                        "name": "Alejandro Martn"
                    },
                    {
                        "name": "David Camacho"
                    }
                ],
                "author_detail": {
                    "name": "David Camacho"
                },
                "author": "David Camacho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03940v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03940v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02814v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02814v2",
                "updated": "2025-07-14T07:03:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    7,
                    3,
                    30,
                    0,
                    195,
                    0
                ],
                "published": "2024-11-05T05:22:14Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    22,
                    14,
                    1,
                    310,
                    0
                ],
                "title": "The Hitchhiker's Guide to Programming and Optimizing Cache Coherent\n  Heterogeneous Systems: CXL, NVLink-C2C, and AMD Infinity Fabric",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hitchhiker's Guide to Programming and Optimizing Cache Coherent\n  Heterogeneous Systems: CXL, NVLink-C2C, and AMD Infinity Fabric"
                },
                "summary": "We present a thorough analysis of the use of modern heterogeneous systems\ninterconnected by various cachecoherent links, including CXL, NVLink-C2C, and\nInfinity Fabric. We studied a wide range of server systems that combined CPUs\nfrom different vendors and various types of coherent memory devices, including\nCXL memory expander, CXL pool, CXL shared memory, GH200 GPU, and AMD MI300a\nHBM. For this study, we developed a heterogeneous memory benchmark suite,\nHeimdall, to profile the performance of such heterogeneous systems and present\na detailed performance comparison across systems. By leveraging H E I M DA L L\n, we unveiled the detailed architecture design in these systems, drew\nobservations on optimizing performance for workloads, and pointed out\ndirections for future development of cache coherent heterogeneous systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a thorough analysis of the use of modern heterogeneous systems\ninterconnected by various cachecoherent links, including CXL, NVLink-C2C, and\nInfinity Fabric. We studied a wide range of server systems that combined CPUs\nfrom different vendors and various types of coherent memory devices, including\nCXL memory expander, CXL pool, CXL shared memory, GH200 GPU, and AMD MI300a\nHBM. For this study, we developed a heterogeneous memory benchmark suite,\nHeimdall, to profile the performance of such heterogeneous systems and present\na detailed performance comparison across systems. By leveraging H E I M DA L L\n, we unveiled the detailed architecture design in these systems, drew\nobservations on optimizing performance for workloads, and pointed out\ndirections for future development of cache coherent heterogeneous systems."
                },
                "authors": [
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Suyash Mahar"
                    },
                    {
                        "name": "Luyi Li"
                    },
                    {
                        "name": "Jangseon Park"
                    },
                    {
                        "name": "Jinpyo Kim"
                    },
                    {
                        "name": "Theodore Michailidis"
                    },
                    {
                        "name": "Yue Pan"
                    },
                    {
                        "name": "Mingyao Shen"
                    },
                    {
                        "name": "Tajana Rosing"
                    },
                    {
                        "name": "Dean Tullsen"
                    },
                    {
                        "name": "Steven Swanson"
                    },
                    {
                        "name": "Jishen Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jishen Zhao"
                },
                "author": "Jishen Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02814v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02814v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13820v2",
                "updated": "2025-07-14T02:22:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    2,
                    22,
                    43,
                    0,
                    195,
                    0
                ],
                "published": "2024-11-21T03:52:41Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    52,
                    41,
                    3,
                    326,
                    0
                ],
                "title": "InstCache: A Predictive Cache for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstCache: A Predictive Cache for LLM Serving"
                },
                "summary": "The revolutionary capabilities of Large Language Models (LLMs) are attracting\nrapidly growing popularity and leading to soaring user requests to inference\nserving systems. Caching techniques, which leverage data reuse to reduce\ncomputation, offer opportunities to optimize the performance of LLM inference\nengines. On the one hand, the low-level key-value (KV) cache working at the\ntoken level is widely adopted, albeit it incurs significant overhead as request\nvolume grows. On the other hand, instruction-level caching, which stores full\ninstruction-response pairs, is expected to play an increasingly crucial role.\nHowever, the high variability in the content and length of instructions make it\nrare for identical instructions to recur within a short time window, presenting\nchallenges for effective caching instruction-response pairs. To address this\nchallenge, we propose InstCache, a predictive caching mechanism for LLM serving\nsystems. Leveraging the capability of LLMs, we can effectively reorder the\nrepresentation space of instruction texts and develop a sufficient level of\nspatial locality. Such spatial locality enables us to predict potential\ninstructions located in a compact region in the space, resulting in an\neffective caching system at runtime. Experimental results demonstrate that\nInstCache achieves a 2.3x higher hit rate compared to the upper bound of\ntraditional caching mechanisms on WildChat dataset and reduces the time per\noutput token of vLLM by up to 42.0% and 50.0% on LMSys and Moss datasets,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The revolutionary capabilities of Large Language Models (LLMs) are attracting\nrapidly growing popularity and leading to soaring user requests to inference\nserving systems. Caching techniques, which leverage data reuse to reduce\ncomputation, offer opportunities to optimize the performance of LLM inference\nengines. On the one hand, the low-level key-value (KV) cache working at the\ntoken level is widely adopted, albeit it incurs significant overhead as request\nvolume grows. On the other hand, instruction-level caching, which stores full\ninstruction-response pairs, is expected to play an increasingly crucial role.\nHowever, the high variability in the content and length of instructions make it\nrare for identical instructions to recur within a short time window, presenting\nchallenges for effective caching instruction-response pairs. To address this\nchallenge, we propose InstCache, a predictive caching mechanism for LLM serving\nsystems. Leveraging the capability of LLMs, we can effectively reorder the\nrepresentation space of instruction texts and develop a sufficient level of\nspatial locality. Such spatial locality enables us to predict potential\ninstructions located in a compact region in the space, resulting in an\neffective caching system at runtime. Experimental results demonstrate that\nInstCache achieves a 2.3x higher hit rate compared to the upper bound of\ntraditional caching mechanisms on WildChat dataset and reduces the time per\noutput token of vLLM by up to 42.0% and 50.0% on LMSys and Moss datasets,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Longwei Zou"
                    },
                    {
                        "name": "Yan Liu"
                    },
                    {
                        "name": "Jiamu Kang"
                    },
                    {
                        "name": "Tingfeng Liu"
                    },
                    {
                        "name": "Jiangang Kong"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09500v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09500v1",
                "updated": "2025-07-13T05:37:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    13,
                    5,
                    37,
                    33,
                    6,
                    194,
                    0
                ],
                "published": "2025-07-13T05:37:33Z",
                "published_parsed": [
                    2025,
                    7,
                    13,
                    5,
                    37,
                    33,
                    6,
                    194,
                    0
                ],
                "title": "Advancing Reliable Test-Time Adaptation of Vision-Language Models under\n  Visual Variations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Reliable Test-Time Adaptation of Vision-Language Models under\n  Visual Variations"
                },
                "summary": "Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but\nstruggle with distribution shifts in downstream tasks when labeled data is\nunavailable, which has motivated the development of Test-Time Adaptation (TTA)\nto improve VLMs' performance during inference without annotations. Among\nvarious TTA approaches, cache-based methods show promise by preserving\nhistorical knowledge from low-entropy samples in a dynamic cache and fostering\nefficient adaptation. However, these methods face two critical reliability\nchallenges: (1) entropy often becomes unreliable under distribution shifts,\ncausing error accumulation in the cache and degradation in adaptation\nperformance; (2) the final predictions may be unreliable due to inflexible\ndecision boundaries that fail to accommodate large downstream shifts. To\naddress these challenges, we propose a Reliable Test-time Adaptation (ReTA)\nmethod that integrates two complementary strategies to enhance reliability from\ntwo perspectives. First, to mitigate the unreliability of entropy as a sample\nselection criterion for cache construction, we introduce Consistency-aware\nEntropy Reweighting (CER), which incorporates consistency constraints to weight\nentropy during cache updating. While conventional approaches rely solely on low\nentropy for cache prioritization and risk introducing noise, our method\nleverages predictive consistency to maintain a high-quality cache and\nfacilitate more robust adaptation. Second, we present Diversity-driven\nDistribution Calibration (DDC), which models class-wise text embeddings as\nmultivariate Gaussian distributions, enabling adaptive decision boundaries for\nmore accurate predictions across visually diverse content. Extensive\nexperiments demonstrate that ReTA consistently outperforms state-of-the-art\nmethods, particularly under challenging real-world distribution shifts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but\nstruggle with distribution shifts in downstream tasks when labeled data is\nunavailable, which has motivated the development of Test-Time Adaptation (TTA)\nto improve VLMs' performance during inference without annotations. Among\nvarious TTA approaches, cache-based methods show promise by preserving\nhistorical knowledge from low-entropy samples in a dynamic cache and fostering\nefficient adaptation. However, these methods face two critical reliability\nchallenges: (1) entropy often becomes unreliable under distribution shifts,\ncausing error accumulation in the cache and degradation in adaptation\nperformance; (2) the final predictions may be unreliable due to inflexible\ndecision boundaries that fail to accommodate large downstream shifts. To\naddress these challenges, we propose a Reliable Test-time Adaptation (ReTA)\nmethod that integrates two complementary strategies to enhance reliability from\ntwo perspectives. First, to mitigate the unreliability of entropy as a sample\nselection criterion for cache construction, we introduce Consistency-aware\nEntropy Reweighting (CER), which incorporates consistency constraints to weight\nentropy during cache updating. While conventional approaches rely solely on low\nentropy for cache prioritization and risk introducing noise, our method\nleverages predictive consistency to maintain a high-quality cache and\nfacilitate more robust adaptation. Second, we present Diversity-driven\nDistribution Calibration (DDC), which models class-wise text embeddings as\nmultivariate Gaussian distributions, enabling adaptive decision boundaries for\nmore accurate predictions across visually diverse content. Extensive\nexperiments demonstrate that ReTA consistently outperforms state-of-the-art\nmethods, particularly under challenging real-world distribution shifts."
                },
                "authors": [
                    {
                        "name": "Yiwen Liang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Yizhe Xiong"
                    },
                    {
                        "name": "Zihan Zhou"
                    },
                    {
                        "name": "Mengyao Lyu"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Shuaicheng Niu"
                    },
                    {
                        "name": "Sicheng Zhao"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "Accepted at the 33rd ACM International Conference on Multimedia(ACM\n  MM 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09500v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09500v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07776v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07776v2",
                "updated": "2025-07-13T04:42:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    13,
                    4,
                    42,
                    28,
                    6,
                    194,
                    0
                ],
                "published": "2025-02-11T18:58:04Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    58,
                    4,
                    1,
                    42,
                    0
                ],
                "title": "Auditing Prompt Caching in Language Model APIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auditing Prompt Caching in Language Model APIs"
                },
                "summary": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known."
                },
                "authors": [
                    {
                        "name": "Chenchen Gu"
                    },
                    {
                        "name": "Xiang Lisa Li"
                    },
                    {
                        "name": "Rohith Kuditipudi"
                    },
                    {
                        "name": "Percy Liang"
                    },
                    {
                        "name": "Tatsunori Hashimoto"
                    }
                ],
                "author_detail": {
                    "name": "Tatsunori Hashimoto"
                },
                "author": "Tatsunori Hashimoto",
                "arxiv_comment": "Accepted at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07776v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07776v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19547v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19547v4",
                "updated": "2025-07-11T22:14:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    22,
                    14,
                    1,
                    4,
                    192,
                    0
                ],
                "published": "2024-07-28T17:46:15Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    17,
                    46,
                    15,
                    6,
                    210,
                    0
                ],
                "title": "Temporal Feature Matters: A Framework for Diffusion Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Feature Matters: A Framework for Diffusion Model Quantization"
                },
                "summary": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "Accepted by TPAMI 2025. arXiv admin note: substantial text overlap\n  with arXiv:2311.16503",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19547v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19547v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09202v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09202v3",
                "updated": "2025-07-11T19:57:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    19,
                    57,
                    51,
                    4,
                    192,
                    0
                ],
                "published": "2024-09-13T21:31:45Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    21,
                    31,
                    45,
                    4,
                    257,
                    0
                ],
                "title": "HotSwap: Enabling Live Dependency Sharing in Serverless Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HotSwap: Enabling Live Dependency Sharing in Serverless Computing"
                },
                "summary": "This work presents HotSwap, a novel provider-side cold-start optimization for\nserverless computing. This optimization reduces cold-start time when booting\nand loading dependencies at runtime inside a function container. Previous\nresearch has extensively focused on reducing cold-start latency for specific\nfunctions. However, little attention has been given to skewed production\nworkloads. In such cases, cross-function optimization becomes essential.\nWithout cross-function optimization, a cloud provider is left with two equally\npoor options: (i) Either the cloud provider gives up optimization for each\nfunction in the long tail (which is slow); or (ii) the cloud provider applies\nfunction-specific optimizations (e.g., cache function images) to every function\nin the long tail (which violates the vendor's cache constraints). HotSwap\ndemonstrates cross-function optimization using a novel pre-warming strategy. In\nthis strategy, a pre-initialized live dependency image is migrated to the new\nfunction instance. At the same time, HotSwap respects the provider's cache\nconstraints, because a single pre-warmed dependency image in the cache can be\nshared among all serverless functions that require that image. HotSwap has been\ntested on seven representative functions from FunctionBench. In those tests,\nHotSwap accelerates dependency loading for those serverless functions with\nlarge dependency requirements by a factor ranging from 2.2 to 3.2. Simulation\nexperiments using Azure traces indicate that HotSwap can save 88\\% of space,\ncompared with a previous function-specific method, PreBaking, when sharing a\ndependency image among ten different functions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents HotSwap, a novel provider-side cold-start optimization for\nserverless computing. This optimization reduces cold-start time when booting\nand loading dependencies at runtime inside a function container. Previous\nresearch has extensively focused on reducing cold-start latency for specific\nfunctions. However, little attention has been given to skewed production\nworkloads. In such cases, cross-function optimization becomes essential.\nWithout cross-function optimization, a cloud provider is left with two equally\npoor options: (i) Either the cloud provider gives up optimization for each\nfunction in the long tail (which is slow); or (ii) the cloud provider applies\nfunction-specific optimizations (e.g., cache function images) to every function\nin the long tail (which violates the vendor's cache constraints). HotSwap\ndemonstrates cross-function optimization using a novel pre-warming strategy. In\nthis strategy, a pre-initialized live dependency image is migrated to the new\nfunction instance. At the same time, HotSwap respects the provider's cache\nconstraints, because a single pre-warmed dependency image in the cache can be\nshared among all serverless functions that require that image. HotSwap has been\ntested on seven representative functions from FunctionBench. In those tests,\nHotSwap accelerates dependency loading for those serverless functions with\nlarge dependency requirements by a factor ranging from 2.2 to 3.2. Simulation\nexperiments using Azure traces indicate that HotSwap can save 88\\% of space,\ncompared with a previous function-specific method, PreBaking, when sharing a\ndependency image among ten different functions."
                },
                "authors": [
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Devesh Tiwari"
                    },
                    {
                        "name": "Gene Cooperman"
                    }
                ],
                "author_detail": {
                    "name": "Gene Cooperman"
                },
                "author": "Gene Cooperman",
                "arxiv_comment": "10 pages, 7 figures. This work was accepted at the IEEE International\n  Conference on Cloud Computing 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09202v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09202v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08799v1",
                "updated": "2025-07-11T17:59:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    59,
                    36,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T17:59:36Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    59,
                    36,
                    4,
                    192,
                    0
                ],
                "title": "KV Cache Steering for Inducing Reasoning in Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Steering for Inducing Reasoning in Small Language Models"
                },
                "summary": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach leverages\nGPT-4o-generated reasoning traces to construct steering vectors that shift\nmodel behavior toward more explicit, multi-step reasoning without fine-tuning\nor prompt modifications. Experimental evaluations on diverse reasoning\nbenchmarks demonstrate that cache steering improves both the qualitative\nstructure of model reasoning and quantitative task performance. Compared to\nprior activation steering techniques that require continuous interventions, our\none-shot cache steering offers substantial advantages in terms of\nhyperparameter stability, inference-time efficiency, and ease of integration,\nmaking it a more robust and practical solution for controlled generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach leverages\nGPT-4o-generated reasoning traces to construct steering vectors that shift\nmodel behavior toward more explicit, multi-step reasoning without fine-tuning\nor prompt modifications. Experimental evaluations on diverse reasoning\nbenchmarks demonstrate that cache steering improves both the qualitative\nstructure of model reasoning and quantitative task performance. Compared to\nprior activation steering techniques that require continuous interventions, our\none-shot cache steering offers substantial advantages in terms of\nhyperparameter stability, inference-time efficiency, and ease of integration,\nmaking it a more robust and practical solution for controlled generation."
                },
                "authors": [
                    {
                        "name": "Max Belitsky"
                    },
                    {
                        "name": "Dawid J. Kopiczko"
                    },
                    {
                        "name": "Michael Dorkenwald"
                    },
                    {
                        "name": "M. Jehanzeb Mirza"
                    },
                    {
                        "name": "Cees G. M. Snoek"
                    },
                    {
                        "name": "Yuki M. Asano"
                    }
                ],
                "author_detail": {
                    "name": "Yuki M. Asano"
                },
                "author": "Yuki M. Asano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08717v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08717v1",
                "updated": "2025-07-11T16:17:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    17,
                    46,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T16:17:46Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    17,
                    46,
                    4,
                    192,
                    0
                ],
                "title": "Knowledge Graph-Based approach for Sustainable 6G End-to-End System\n  Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graph-Based approach for Sustainable 6G End-to-End System\n  Design"
                },
                "summary": "Previous generations of cellular communication, such as 5G, have been\ndesigned with the objective of improving key performance indicators (KPIs) such\nas throughput, latency, etc. However, to meet the evolving KPI demands as well\nas the ambitious sustainability targets for the ICT industry, 6G will need to\nbe designed differently. Concretely, 6G will need to consider both the\nperformance and sustainability targets for the various use cases it will serve.\nMoreover, like previous generations, 6G will have various candidate\ntechnological enablers, making the design space of the system even more\ncomplex. Furthermore, given the subjective nature of the sustainability\nindicators, in particular social sustainability, there is a significant gap in\nliterature on how technical enablers and 6G System design can be linked to\nthem. Hence, in this article a novel method for 6G end-to-end (E2E) system\ndesign based on Knowledge graphs (KG) has been introduced. It considers as its\ninput: the use case KPIs, use case sustainability requirements expressed as Key\nValues (KV) and KV Indicators (KVIs), the ability of the technological enablers\nto satisfy these KPIs and KVIs, the 6G system design principles defined in\nHexa-X-II project, the maturity of a technological enabler and the dependencies\nbetween the various enablers. As part of the KG method, a novel approach for\ndetermining the key values a technological enabler addresses, has also been\nintroduced. The effectiveness of the KG method was demonstrated by its\napplication in designing the 6G E2E system for the cooperating mobile robot use\ncase defined in the Hexa-X-II project, where 82 enablers were selected. Lastly,\nresults from proof-of-concept demonstrations for a subset of the selected\nenablers have also been provided, which reinforce the efficacy of the KG method\nfor designing a sustainable 6G system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous generations of cellular communication, such as 5G, have been\ndesigned with the objective of improving key performance indicators (KPIs) such\nas throughput, latency, etc. However, to meet the evolving KPI demands as well\nas the ambitious sustainability targets for the ICT industry, 6G will need to\nbe designed differently. Concretely, 6G will need to consider both the\nperformance and sustainability targets for the various use cases it will serve.\nMoreover, like previous generations, 6G will have various candidate\ntechnological enablers, making the design space of the system even more\ncomplex. Furthermore, given the subjective nature of the sustainability\nindicators, in particular social sustainability, there is a significant gap in\nliterature on how technical enablers and 6G System design can be linked to\nthem. Hence, in this article a novel method for 6G end-to-end (E2E) system\ndesign based on Knowledge graphs (KG) has been introduced. It considers as its\ninput: the use case KPIs, use case sustainability requirements expressed as Key\nValues (KV) and KV Indicators (KVIs), the ability of the technological enablers\nto satisfy these KPIs and KVIs, the 6G system design principles defined in\nHexa-X-II project, the maturity of a technological enabler and the dependencies\nbetween the various enablers. As part of the KG method, a novel approach for\ndetermining the key values a technological enabler addresses, has also been\nintroduced. The effectiveness of the KG method was demonstrated by its\napplication in designing the 6G E2E system for the cooperating mobile robot use\ncase defined in the Hexa-X-II project, where 82 enablers were selected. Lastly,\nresults from proof-of-concept demonstrations for a subset of the selected\nenablers have also been provided, which reinforce the efficacy of the KG method\nfor designing a sustainable 6G system."
                },
                "authors": [
                    {
                        "name": "Akshay Jain"
                    },
                    {
                        "name": "Sylvaine Kerboeuf"
                    },
                    {
                        "name": "Sokratis Barmpounakis"
                    },
                    {
                        "name": "Cristbal Vinagre Z."
                    },
                    {
                        "name": "Stefan Wendt"
                    },
                    {
                        "name": "Dinh Thai Bui"
                    },
                    {
                        "name": "Pol Alemany"
                    },
                    {
                        "name": "Riccardo Nicolicchia"
                    },
                    {
                        "name": "Jos Mara Jorquera Valero"
                    },
                    {
                        "name": "Dani Korpi"
                    },
                    {
                        "name": "Mohammad Hossein Moghaddam"
                    },
                    {
                        "name": "Mikko A. Uusitalo"
                    },
                    {
                        "name": "Patrik Rugeland"
                    },
                    {
                        "name": "Abdelkader Outtagarts"
                    },
                    {
                        "name": "Karthik Upadhya"
                    },
                    {
                        "name": "Panagiotis Demestichas"
                    },
                    {
                        "name": "Raul Muoz"
                    },
                    {
                        "name": "Manuel Gil Prez"
                    },
                    {
                        "name": "Daniel Adanza"
                    },
                    {
                        "name": "Ricard Vilalta"
                    }
                ],
                "author_detail": {
                    "name": "Ricard Vilalta"
                },
                "author": "Ricard Vilalta",
                "arxiv_comment": "The paper is submitted to IEEE Open Journal of the Communications\n  Society (IEEE OJCOMS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08717v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "00",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v9",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v9",
                "updated": "2025-07-11T14:27:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    27,
                    25,
                    4,
                    192,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "arxiv_comment": "Added additional variations in appendix, at the request of\n  collaborators who want to prove various properties",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v9",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v9",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08607v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08607v1",
                "updated": "2025-07-11T14:02:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    2,
                    54,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T14:02:54Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    2,
                    54,
                    4,
                    192,
                    0
                ],
                "title": "BayesTTA: Continual-Temporal Test-Time Adaptation for Vision-Language\n  Models via Gaussian Discriminant Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BayesTTA: Continual-Temporal Test-Time Adaptation for Vision-Language\n  Models via Gaussian Discriminant Analysis"
                },
                "summary": "Vision-language models (VLMs) such as CLIP achieve strong zero-shot\nrecognition but degrade significantly under \\textit{temporally evolving\ndistribution shifts} common in real-world scenarios (e.g., gradual illumination\nor seasonal changes). Existing continual test-time adaptation (CTTA) methods\nare typically built around sudden and severe distribution shifts and neglect\ntemporal continuity, leading to three core defects: limited memory cache\nrestricts long-range distribution modeling, causing catastrophic forgetting;\nentropy-based confidence becomes unreliable under temporal drift, worsening\nerror accumulation; and static visual representations misalign with evolving\ninputs. We formalize this practical problem as \\textit{Continual-Temporal\nTest-Time Adaptation (CT-TTA)}, where test distributions evolve gradually over\ntime. To address it, we propose \\textit{BayesTTA}, a Bayesian adaptation\nframework that enforces temporally consistent predictions and dynamically\naligns visual representations. Specifically, BayesTTA incrementally estimates\nclass-conditional Gaussian mixture distributions without storing raw data,\nadaptively selects covariance structures through statistical hypothesis\ntesting, and performs calibrated inference using Gaussian discriminant analysis\n(GDA). These calibrated predictions supervise self-paced adaptation of\nnormalization layers, ensuring efficient and stable representation alignment.\nWe establish a comprehensive CT-TTA benchmark across four temporally evolving\ndatasets and further evaluate generalization on ten standard TTA datasets.\nExtensive experiments show that BayesTTA consistently outperforms\nstate-of-the-art methods, achieving significant gains while maintaining\nefficiency. Code is available at\n\\href{https://github.com/cuishuang99/BayesTTA}{https://github.com/cuishuang99/BayesTTA}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) such as CLIP achieve strong zero-shot\nrecognition but degrade significantly under \\textit{temporally evolving\ndistribution shifts} common in real-world scenarios (e.g., gradual illumination\nor seasonal changes). Existing continual test-time adaptation (CTTA) methods\nare typically built around sudden and severe distribution shifts and neglect\ntemporal continuity, leading to three core defects: limited memory cache\nrestricts long-range distribution modeling, causing catastrophic forgetting;\nentropy-based confidence becomes unreliable under temporal drift, worsening\nerror accumulation; and static visual representations misalign with evolving\ninputs. We formalize this practical problem as \\textit{Continual-Temporal\nTest-Time Adaptation (CT-TTA)}, where test distributions evolve gradually over\ntime. To address it, we propose \\textit{BayesTTA}, a Bayesian adaptation\nframework that enforces temporally consistent predictions and dynamically\naligns visual representations. Specifically, BayesTTA incrementally estimates\nclass-conditional Gaussian mixture distributions without storing raw data,\nadaptively selects covariance structures through statistical hypothesis\ntesting, and performs calibrated inference using Gaussian discriminant analysis\n(GDA). These calibrated predictions supervise self-paced adaptation of\nnormalization layers, ensuring efficient and stable representation alignment.\nWe establish a comprehensive CT-TTA benchmark across four temporally evolving\ndatasets and further evaluate generalization on ten standard TTA datasets.\nExtensive experiments show that BayesTTA consistently outperforms\nstate-of-the-art methods, achieving significant gains while maintaining\nefficiency. Code is available at\n\\href{https://github.com/cuishuang99/BayesTTA}{https://github.com/cuishuang99/BayesTTA}."
                },
                "authors": [
                    {
                        "name": "Shuang Cui"
                    },
                    {
                        "name": "Jinglin Xu"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Xiongxin Tang"
                    },
                    {
                        "name": "Jiangmeng Li"
                    },
                    {
                        "name": "Jiahuan Zhou"
                    },
                    {
                        "name": "Fanjiang Xu"
                    },
                    {
                        "name": "Fuchun Sun"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08607v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08607v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.23777v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23777v1",
                "updated": "2025-07-31T17:58:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    58,
                    30,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T17:58:30Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    58,
                    30,
                    3,
                    212,
                    0
                ],
                "title": "XSpecMesh: Quality-Preserving Auto-Regressive Mesh Generation\n  Acceleration via Multi-Head Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XSpecMesh: Quality-Preserving Auto-Regressive Mesh Generation\n  Acceleration via Multi-Head Speculative Decoding"
                },
                "summary": "Current auto-regressive models can generate high-quality, topologically\nprecise meshes; however, they necessitate thousands-or even tens of\nthousands-of next-token predictions during inference, resulting in substantial\nlatency. We introduce XSpecMesh, a quality-preserving acceleration method for\nauto-regressive mesh generation models. XSpecMesh employs a lightweight,\nmulti-head speculative decoding scheme to predict multiple tokens in parallel\nwithin a single forward pass, thereby accelerating inference. We further\npropose a verification and resampling strategy: the backbone model verifies\neach predicted token and resamples any tokens that do not meet the quality\ncriteria. In addition, we propose a distillation strategy that trains the\nlightweight decoding heads by distilling from the backbone model, encouraging\ntheir prediction distributions to align and improving the success rate of\nspeculative predictions. Extensive experiments demonstrate that our method\nachieves a 1.7x speedup without sacrificing generation quality. Our code will\nbe released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current auto-regressive models can generate high-quality, topologically\nprecise meshes; however, they necessitate thousands-or even tens of\nthousands-of next-token predictions during inference, resulting in substantial\nlatency. We introduce XSpecMesh, a quality-preserving acceleration method for\nauto-regressive mesh generation models. XSpecMesh employs a lightweight,\nmulti-head speculative decoding scheme to predict multiple tokens in parallel\nwithin a single forward pass, thereby accelerating inference. We further\npropose a verification and resampling strategy: the backbone model verifies\neach predicted token and resamples any tokens that do not meet the quality\ncriteria. In addition, we propose a distillation strategy that trains the\nlightweight decoding heads by distilling from the backbone model, encouraging\ntheir prediction distributions to align and improving the success rate of\nspeculative predictions. Extensive experiments demonstrate that our method\nachieves a 1.7x speedup without sacrificing generation quality. Our code will\nbe released."
                },
                "authors": [
                    {
                        "name": "Dian Chen"
                    },
                    {
                        "name": "Yansong Qu"
                    },
                    {
                        "name": "Xinyang Li"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Shengchuan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shengchuan Zhang"
                },
                "author": "Shengchuan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23777v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23777v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23776v1",
                "updated": "2025-07-31T17:58:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    58,
                    25,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T17:58:25Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    58,
                    25,
                    3,
                    212,
                    0
                ],
                "title": "Cascaded Information Disclosure for Generalized Evaluation of Problem\n  Solving Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cascaded Information Disclosure for Generalized Evaluation of Problem\n  Solving Capabilities"
                },
                "summary": "While question-answering~(QA) benchmark performance is an automatic and\nscalable method to compare LLMs, it is an indirect method of evaluating their\nunderlying problem-solving capabilities. Therefore, we propose a holistic and\ngeneralizable framework based on \\emph{cascaded question disclosure} that\nprovides a more accurate estimate of the models' problem-solving capabilities\nwhile maintaining the scalability and automation. This approach collects model\nresponses in a stagewise manner with each stage revealing partial information\nabout the question designed to elicit generalized reasoning in LLMs. We find\nthat our approach not only provides a better comparison between LLMs, but also\ninduces better intermediate traces in models compared to the standard QA\nparadigm. We empirically verify this behavior on diverse reasoning and\nknowledge-heavy QA datasets by comparing LLMs of varying sizes and families.\nOur approach narrows the performance gap observed in the standard QA evaluation\nsettings, indicating that the prevalent indirect QA paradigm of evaluation\noverestimates the differences in performance between models. We further\nvalidate our findings by extensive ablation studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While question-answering~(QA) benchmark performance is an automatic and\nscalable method to compare LLMs, it is an indirect method of evaluating their\nunderlying problem-solving capabilities. Therefore, we propose a holistic and\ngeneralizable framework based on \\emph{cascaded question disclosure} that\nprovides a more accurate estimate of the models' problem-solving capabilities\nwhile maintaining the scalability and automation. This approach collects model\nresponses in a stagewise manner with each stage revealing partial information\nabout the question designed to elicit generalized reasoning in LLMs. We find\nthat our approach not only provides a better comparison between LLMs, but also\ninduces better intermediate traces in models compared to the standard QA\nparadigm. We empirically verify this behavior on diverse reasoning and\nknowledge-heavy QA datasets by comparing LLMs of varying sizes and families.\nOur approach narrows the performance gap observed in the standard QA evaluation\nsettings, indicating that the prevalent indirect QA paradigm of evaluation\noverestimates the differences in performance between models. We further\nvalidate our findings by extensive ablation studies."
                },
                "authors": [
                    {
                        "name": "Yunxiang Yan"
                    },
                    {
                        "name": "Tomohiro Sawada"
                    },
                    {
                        "name": "Kartik Goyal"
                    }
                ],
                "author_detail": {
                    "name": "Kartik Goyal"
                },
                "author": "Kartik Goyal",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23773v1",
                "updated": "2025-07-31T17:57:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    57,
                    20,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T17:57:20Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    57,
                    20,
                    3,
                    212,
                    0
                ],
                "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning\n  Architecture with LLM-Based World Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning\n  Architecture with LLM-Based World Model"
                },
                "summary": "AI agents built on large language models (LLMs) hold enormous promise, but\ncurrent practice focuses on a one-task-one-agent approach, which not only falls\nshort of scalability and generality, but also suffers from the fundamental\nlimitations of autoregressive LLMs. On the other hand, humans are general\nagents who reason by mentally simulating the outcomes of their actions and\nplans. Moving towards a more general and powerful AI agent, we introduce\nSimuRA, a goal-oriented architecture for generalized agentic reasoning. Based\non a principled formulation of optimal agent in any environment, \\modelname\novercomes the limitations of autoregressive reasoning by introducing a world\nmodel for planning via simulation. The generalized world model is implemented\nusing LLM, which can flexibly plan in a wide range of environments using the\nconcept-rich latent space of natural language. Experiments on difficult web\nbrowsing tasks show that \\modelname improves the success of flight search from\n0\\% to 32.2\\%. World-model-based planning, in particular, shows consistent\nadvantage of up to 124\\% over autoregressive planning, demonstrating the\nadvantage of world model simulation as a reasoning paradigm. We are excited\nabout the possibility for training a single, general agent model based on LLMs\nthat can act superintelligently in all environments. To start, we make SimuRA,\na web-browsing agent built on \\modelname with pretrained LLMs, available as a\nresearch demo for public testing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI agents built on large language models (LLMs) hold enormous promise, but\ncurrent practice focuses on a one-task-one-agent approach, which not only falls\nshort of scalability and generality, but also suffers from the fundamental\nlimitations of autoregressive LLMs. On the other hand, humans are general\nagents who reason by mentally simulating the outcomes of their actions and\nplans. Moving towards a more general and powerful AI agent, we introduce\nSimuRA, a goal-oriented architecture for generalized agentic reasoning. Based\non a principled formulation of optimal agent in any environment, \\modelname\novercomes the limitations of autoregressive reasoning by introducing a world\nmodel for planning via simulation. The generalized world model is implemented\nusing LLM, which can flexibly plan in a wide range of environments using the\nconcept-rich latent space of natural language. Experiments on difficult web\nbrowsing tasks show that \\modelname improves the success of flight search from\n0\\% to 32.2\\%. World-model-based planning, in particular, shows consistent\nadvantage of up to 124\\% over autoregressive planning, demonstrating the\nadvantage of world model simulation as a reasoning paradigm. We are excited\nabout the possibility for training a single, general agent model based on LLMs\nthat can act superintelligently in all environments. To start, we make SimuRA,\na web-browsing agent built on \\modelname with pretrained LLMs, available as a\nresearch demo for public testing."
                },
                "authors": [
                    {
                        "name": "Mingkai Deng"
                    },
                    {
                        "name": "Jinyu Hou"
                    },
                    {
                        "name": "Yilin Shen"
                    },
                    {
                        "name": "Hongxia Jin"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Zhiting Hu"
                    },
                    {
                        "name": "Eric Xing"
                    }
                ],
                "author_detail": {
                    "name": "Eric Xing"
                },
                "author": "Eric Xing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21035v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21035v2",
                "updated": "2025-07-31T17:57:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    57,
                    18,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-28T17:55:08Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    55,
                    8,
                    0,
                    209,
                    0
                ],
                "title": "GenoMAS: A Multi-Agent Framework for Scientific Discovery via\n  Code-Driven Gene Expression Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenoMAS: A Multi-Agent Framework for Scientific Discovery via\n  Code-Driven Gene Expression Analysis"
                },
                "summary": "Gene expression analysis holds the key to many biomedical discoveries, yet\nextracting insights from raw transcriptomic data remains formidable due to the\ncomplexity of multiple large, semi-structured files and the need for extensive\ndomain expertise. Current automation approaches are often limited by either\ninflexible workflows that break down in edge cases or by fully autonomous\nagents that lack the necessary precision for rigorous scientific inquiry.\nGenoMAS charts a different course by presenting a team of LLM-based scientists\nthat integrates the reliability of structured workflows with the adaptability\nof autonomous agents. GenoMAS orchestrates six specialized LLM agents through\ntyped message-passing protocols, each contributing complementary strengths to a\nshared analytic canvas. At the heart of GenoMAS lies a guided-planning\nframework: programming agents unfold high-level task guidelines into Action\nUnits and, at each juncture, elect to advance, revise, bypass, or backtrack,\nthereby maintaining logical coherence while bending gracefully to the\nidiosyncrasies of genomic data.\n  On the GenoTEX benchmark, GenoMAS reaches a Composite Similarity Correlation\nof 89.13% for data preprocessing and an F$_1$ of 60.48% for gene\nidentification, surpassing the best prior art by 10.61% and 16.85%\nrespectively. Beyond metrics, GenoMAS surfaces biologically plausible\ngene-phenotype associations corroborated by the literature, all while adjusting\nfor latent confounders. Code is available at https://github.com/Liu-Hy/GenoMAS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gene expression analysis holds the key to many biomedical discoveries, yet\nextracting insights from raw transcriptomic data remains formidable due to the\ncomplexity of multiple large, semi-structured files and the need for extensive\ndomain expertise. Current automation approaches are often limited by either\ninflexible workflows that break down in edge cases or by fully autonomous\nagents that lack the necessary precision for rigorous scientific inquiry.\nGenoMAS charts a different course by presenting a team of LLM-based scientists\nthat integrates the reliability of structured workflows with the adaptability\nof autonomous agents. GenoMAS orchestrates six specialized LLM agents through\ntyped message-passing protocols, each contributing complementary strengths to a\nshared analytic canvas. At the heart of GenoMAS lies a guided-planning\nframework: programming agents unfold high-level task guidelines into Action\nUnits and, at each juncture, elect to advance, revise, bypass, or backtrack,\nthereby maintaining logical coherence while bending gracefully to the\nidiosyncrasies of genomic data.\n  On the GenoTEX benchmark, GenoMAS reaches a Composite Similarity Correlation\nof 89.13% for data preprocessing and an F$_1$ of 60.48% for gene\nidentification, surpassing the best prior art by 10.61% and 16.85%\nrespectively. Beyond metrics, GenoMAS surfaces biologically plausible\ngene-phenotype associations corroborated by the literature, all while adjusting\nfor latent confounders. Code is available at https://github.com/Liu-Hy/GenoMAS."
                },
                "authors": [
                    {
                        "name": "Haoyang Liu"
                    },
                    {
                        "name": "Yijiang Li"
                    },
                    {
                        "name": "Haohan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haohan Wang"
                },
                "author": "Haohan Wang",
                "arxiv_comment": "51 pages (13 pages for the main text, 9 pages for references, and 29\n  pages for the appendix)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21035v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21035v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23771v1",
                "updated": "2025-07-31T17:56:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    56,
                    28,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T17:56:28Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    56,
                    28,
                    3,
                    212,
                    0
                ],
                "title": "Consensus-Driven Active Model Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consensus-Driven Active Model Selection"
                },
                "summary": "The widespread availability of off-the-shelf machine learning models poses a\nchallenge: which model, of the many available candidates, should be chosen for\na given data analysis task? This question of model selection is traditionally\nanswered by collecting and annotating a validation dataset -- a costly and\ntime-intensive process. We propose a method for active model selection, using\npredictions from candidate models to prioritize the labeling of test data\npoints that efficiently differentiate the best candidate. Our method, CODA,\nperforms consensus-driven active model selection by modeling relationships\nbetween classifiers, categories, and data points within a probabilistic\nframework. The framework uses the consensus and disagreement between models in\nthe candidate pool to guide the label acquisition process, and Bayesian\ninference to update beliefs about which model is best as more information is\ncollected. We validate our approach by curating a collection of 26 benchmark\ntasks capturing a range of model selection scenarios. CODA outperforms existing\nmethods for active model selection significantly, reducing the annotation\neffort required to discover the best model by upwards of 70% compared to the\nprevious state-of-the-art. Code and data are available at\nhttps://github.com/justinkay/coda.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread availability of off-the-shelf machine learning models poses a\nchallenge: which model, of the many available candidates, should be chosen for\na given data analysis task? This question of model selection is traditionally\nanswered by collecting and annotating a validation dataset -- a costly and\ntime-intensive process. We propose a method for active model selection, using\npredictions from candidate models to prioritize the labeling of test data\npoints that efficiently differentiate the best candidate. Our method, CODA,\nperforms consensus-driven active model selection by modeling relationships\nbetween classifiers, categories, and data points within a probabilistic\nframework. The framework uses the consensus and disagreement between models in\nthe candidate pool to guide the label acquisition process, and Bayesian\ninference to update beliefs about which model is best as more information is\ncollected. We validate our approach by curating a collection of 26 benchmark\ntasks capturing a range of model selection scenarios. CODA outperforms existing\nmethods for active model selection significantly, reducing the annotation\neffort required to discover the best model by upwards of 70% compared to the\nprevious state-of-the-art. Code and data are available at\nhttps://github.com/justinkay/coda."
                },
                "authors": [
                    {
                        "name": "Justin Kay"
                    },
                    {
                        "name": "Grant Van Horn"
                    },
                    {
                        "name": "Subhransu Maji"
                    },
                    {
                        "name": "Daniel Sheldon"
                    },
                    {
                        "name": "Sara Beery"
                    }
                ],
                "author_detail": {
                    "name": "Sara Beery"
                },
                "author": "Sara Beery",
                "arxiv_comment": "ICCV 2025 Highlight. 16 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17788v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17788v2",
                "updated": "2025-07-31T17:55:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    55,
                    56,
                    3,
                    212,
                    0
                ],
                "published": "2025-03-22T14:42:27Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    14,
                    42,
                    27,
                    5,
                    81,
                    0
                ],
                "title": "Learning to Align and Refine: A Foundation-to-Diffusion Framework for\n  Occlusion-Robust Two-Hand Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Align and Refine: A Foundation-to-Diffusion Framework for\n  Occlusion-Robust Two-Hand Reconstruction"
                },
                "summary": "Two-hand reconstruction from monocular images faces persistent challenges due\nto complex and dynamic hand postures and occlusions, causing significant\ndifficulty in achieving plausible interaction alignment. Existing approaches\nstruggle with such alignment issues, often resulting in misalignment and\npenetration artifacts. To tackle this, we propose a dual-stage\nFoundation-to-Diffusion framework that precisely align 2D prior guidance from\nvision foundation models and diffusion-based generative 3D interaction\nrefinement to achieve occlusion-robust two-hand reconstruction. First, we\nintroduce a lightweight fusion alignment encoder that aligns fused multimodal\n2D priors like key points, segmentation maps, and depth cues from vision\nfoundation models during training. This provides robust structured guidance,\nfurther enabling efficient inference without heavy foundation model encoders at\ntest time while maintaining high reconstruction accuracy. Second, we implement\na two-hand diffusion model explicitly trained to convert interpenetrated 3D\nposes into plausible, penetration-free counterparts. Through collision\ngradient-guided denoising, the model rectifies artifacts while preserving\nnatural spatial relationships between hands. Extensive evaluations demonstrate\nthat our method achieves state-of-the-art performance on InterHand2.6M, HIC,\nand FreiHAND datasets, significantly advancing occlusion handling and\ninteraction robustness. Our code will be publicly released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-hand reconstruction from monocular images faces persistent challenges due\nto complex and dynamic hand postures and occlusions, causing significant\ndifficulty in achieving plausible interaction alignment. Existing approaches\nstruggle with such alignment issues, often resulting in misalignment and\npenetration artifacts. To tackle this, we propose a dual-stage\nFoundation-to-Diffusion framework that precisely align 2D prior guidance from\nvision foundation models and diffusion-based generative 3D interaction\nrefinement to achieve occlusion-robust two-hand reconstruction. First, we\nintroduce a lightweight fusion alignment encoder that aligns fused multimodal\n2D priors like key points, segmentation maps, and depth cues from vision\nfoundation models during training. This provides robust structured guidance,\nfurther enabling efficient inference without heavy foundation model encoders at\ntest time while maintaining high reconstruction accuracy. Second, we implement\na two-hand diffusion model explicitly trained to convert interpenetrated 3D\nposes into plausible, penetration-free counterparts. Through collision\ngradient-guided denoising, the model rectifies artifacts while preserving\nnatural spatial relationships between hands. Extensive evaluations demonstrate\nthat our method achieves state-of-the-art performance on InterHand2.6M, HIC,\nand FreiHAND datasets, significantly advancing occlusion handling and\ninteraction robustness. Our code will be publicly released."
                },
                "authors": [
                    {
                        "name": "Gaoge Han"
                    },
                    {
                        "name": "Yongkang Cheng"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Shaoli Huang"
                    },
                    {
                        "name": "Tongliang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Tongliang Liu"
                },
                "author": "Tongliang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17788v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17788v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06448v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06448v3",
                "updated": "2025-07-31T17:54:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    54,
                    47,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-08T23:22:34Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    23,
                    22,
                    34,
                    1,
                    189,
                    0
                ],
                "title": "Perception-Aware Policy Optimization for Multimodal Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perception-Aware Policy Optimization for Multimodal Reasoning"
                },
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be a\nhighly effective strategy for endowing Large Language Models (LLMs) with robust\nmulti-step reasoning abilities. However, its design and optimizations remain\ntailored to purely textual domains, resulting in suboptimal performance when\napplied to multimodal reasoning tasks. In particular, we observe that a major\nsource of error in current multimodal reasoning lies in the perception of\nvisual inputs. To address this bottleneck, we propose PAPO, a novel policy\ngradient algorithm that encourages the model to learn to perceive while\nlearning to reason. Specifically, we introduce the Implicit Perception Loss in\nthe form of a KL divergence term, which can be seamlessly plugged into\nmainstream RLVR algorithms such as GRPO and DAPO. Notably, PAPO does not rely\non additional data curation, reward models, or stronger teacher models. To\nfurther enhance the training stability of PAPO, we introduce the Double Entropy\nLoss, which effectively regularizes the new KL objective without compromising\nperformance. Despite its simplicity, PAPO yields significant overall\nimprovements of 4.4%-17.5% on diverse multimodal benchmarks. The improvements\nare more pronounced, approaching 8.0%-19.1%, on tasks with high vision\ndependency. We also observe a substantial reduction of 30.5% in perception\nerrors, indicating improved perceptual capabilities with PAPO. Overall, our\nwork introduces a deeper integration of perception-aware supervision into core\nlearning objectives and lays the groundwork for a new RL framework that\nencourages visually grounded reasoning. Code and data will be made publicly\navailable for research purposes. Project page:\nhttps://mikewangwzhl.github.io/PAPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be a\nhighly effective strategy for endowing Large Language Models (LLMs) with robust\nmulti-step reasoning abilities. However, its design and optimizations remain\ntailored to purely textual domains, resulting in suboptimal performance when\napplied to multimodal reasoning tasks. In particular, we observe that a major\nsource of error in current multimodal reasoning lies in the perception of\nvisual inputs. To address this bottleneck, we propose PAPO, a novel policy\ngradient algorithm that encourages the model to learn to perceive while\nlearning to reason. Specifically, we introduce the Implicit Perception Loss in\nthe form of a KL divergence term, which can be seamlessly plugged into\nmainstream RLVR algorithms such as GRPO and DAPO. Notably, PAPO does not rely\non additional data curation, reward models, or stronger teacher models. To\nfurther enhance the training stability of PAPO, we introduce the Double Entropy\nLoss, which effectively regularizes the new KL objective without compromising\nperformance. Despite its simplicity, PAPO yields significant overall\nimprovements of 4.4%-17.5% on diverse multimodal benchmarks. The improvements\nare more pronounced, approaching 8.0%-19.1%, on tasks with high vision\ndependency. We also observe a substantial reduction of 30.5% in perception\nerrors, indicating improved perceptual capabilities with PAPO. Overall, our\nwork introduces a deeper integration of perception-aware supervision into core\nlearning objectives and lays the groundwork for a new RL framework that\nencourages visually grounded reasoning. Code and data will be made publicly\navailable for research purposes. Project page:\nhttps://mikewangwzhl.github.io/PAPO."
                },
                "authors": [
                    {
                        "name": "Zhenhailong Wang"
                    },
                    {
                        "name": "Xuehang Guo"
                    },
                    {
                        "name": "Sofia Stoica"
                    },
                    {
                        "name": "Haiyang Xu"
                    },
                    {
                        "name": "Hongru Wang"
                    },
                    {
                        "name": "Hyeonjeong Ha"
                    },
                    {
                        "name": "Xiusi Chen"
                    },
                    {
                        "name": "Yangyi Chen"
                    },
                    {
                        "name": "Ming Yan"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Heng Ji"
                    }
                ],
                "author_detail": {
                    "name": "Heng Ji"
                },
                "author": "Heng Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06448v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06448v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23765v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23765v1",
                "updated": "2025-07-31T17:52:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    52,
                    3,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T17:52:03Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    52,
                    3,
                    3,
                    212,
                    0
                ],
                "title": "Intrinsic Heralding and Optimal Decoders for Non-Abelian Topological\n  Order",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intrinsic Heralding and Optimal Decoders for Non-Abelian Topological\n  Order"
                },
                "summary": "Topological order (TO) provides a natural platform for storing and\nmanipulating quantum information. However, its stability to noise has only been\nsystematically understood for Abelian TOs. In this work, we exploit the\nnon-deterministic fusion of non-Abelian anyons to inform active error\ncorrection and design decoders where the fusion products, instead of flag\nqubits, herald the noise. This intrinsic heralding enhances thresholds over\nthose of Abelian counterparts when noise is dominated by a single non-Abelian\nanyon type. Furthermore, we present an approach for determining the optimal\nthreshold for non-Abelian TOs with perfect anyon syndromes for any noise model,\nformulated as a statistical mechanics model using Bayesian inference. We\nnumerically illustrate these results for $D_4 \\cong \\mathbb Z_4 \\rtimes \\mathbb\nZ_2$ TO. In particular, for non-Abelian charge noise and perfect syndrome\nmeasurement, we find an optimal threshold $p_c=0.218(1)$, whereas an\nintrinsically heralded minimal-weight perfect-matching (MWPM) decoder already\ngives $p_c=0.20842(2)$, outperforming standard MWPM with $p_c = 0.15860(1)$.\nOur work highlights how non-Abelian properties can enhance stability, rather\nthan reduce it, and discusses potential generalizations for achieving fault\ntolerance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topological order (TO) provides a natural platform for storing and\nmanipulating quantum information. However, its stability to noise has only been\nsystematically understood for Abelian TOs. In this work, we exploit the\nnon-deterministic fusion of non-Abelian anyons to inform active error\ncorrection and design decoders where the fusion products, instead of flag\nqubits, herald the noise. This intrinsic heralding enhances thresholds over\nthose of Abelian counterparts when noise is dominated by a single non-Abelian\nanyon type. Furthermore, we present an approach for determining the optimal\nthreshold for non-Abelian TOs with perfect anyon syndromes for any noise model,\nformulated as a statistical mechanics model using Bayesian inference. We\nnumerically illustrate these results for $D_4 \\cong \\mathbb Z_4 \\rtimes \\mathbb\nZ_2$ TO. In particular, for non-Abelian charge noise and perfect syndrome\nmeasurement, we find an optimal threshold $p_c=0.218(1)$, whereas an\nintrinsically heralded minimal-weight perfect-matching (MWPM) decoder already\ngives $p_c=0.20842(2)$, outperforming standard MWPM with $p_c = 0.15860(1)$.\nOur work highlights how non-Abelian properties can enhance stability, rather\nthan reduce it, and discusses potential generalizations for achieving fault\ntolerance."
                },
                "authors": [
                    {
                        "name": "Dian Jing"
                    },
                    {
                        "name": "Pablo Sala"
                    },
                    {
                        "name": "Liang Jiang"
                    },
                    {
                        "name": "Ruben Verresen"
                    }
                ],
                "author_detail": {
                    "name": "Ruben Verresen"
                },
                "author": "Ruben Verresen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23765v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23751v1",
                "updated": "2025-07-31T17:38:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    38,
                    50,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T17:38:50Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    38,
                    50,
                    3,
                    212,
                    0
                ],
                "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning\n  and non-reasoning tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning\n  and non-reasoning tasks"
                },
                "summary": "We propose CoT-Self-Instruct, a synthetic data generation method that\ninstructs LLMs to first reason and plan via Chain-of-Thought (CoT) based on the\ngiven seed tasks, and then to generate a new synthetic prompt of similar\nquality and complexity for use in LLM training, followed by filtering for\nhigh-quality data with automatic metrics. In verifiable reasoning, our\nsynthetic data significantly outperforms existing training datasets, such as\ns1k and OpenMathReasoning, across MATH500, AMC23, AIME24 and GPQA-Diamond. For\nnon-verifiable instruction-following tasks, our method surpasses the\nperformance of human or standard self-instruct prompts on both AlpacaEval 2.0\nand Arena-Hard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose CoT-Self-Instruct, a synthetic data generation method that\ninstructs LLMs to first reason and plan via Chain-of-Thought (CoT) based on the\ngiven seed tasks, and then to generate a new synthetic prompt of similar\nquality and complexity for use in LLM training, followed by filtering for\nhigh-quality data with automatic metrics. In verifiable reasoning, our\nsynthetic data significantly outperforms existing training datasets, such as\ns1k and OpenMathReasoning, across MATH500, AMC23, AIME24 and GPQA-Diamond. For\nnon-verifiable instruction-following tasks, our method surpasses the\nperformance of human or standard self-instruct prompts on both AlpacaEval 2.0\nand Arena-Hard."
                },
                "authors": [
                    {
                        "name": "Ping Yu"
                    },
                    {
                        "name": "Jack Lanchantin"
                    },
                    {
                        "name": "Tianlu Wang"
                    },
                    {
                        "name": "Weizhe Yuan"
                    },
                    {
                        "name": "Olga Golovneva"
                    },
                    {
                        "name": "Ilia Kulikov"
                    },
                    {
                        "name": "Sainbayar Sukhbaatar"
                    },
                    {
                        "name": "Jason Weston"
                    },
                    {
                        "name": "Jing Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jing Xu"
                },
                "author": "Jing Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22174v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22174v2",
                "updated": "2025-07-31T17:34:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    34,
                    18,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-27T08:00:43Z",
                "published_parsed": [
                    2025,
                    7,
                    27,
                    8,
                    0,
                    43,
                    6,
                    208,
                    0
                ],
                "title": "Spatial-Temporal Reinforcement Learning for Network Routing with\n  Non-Markovian Traffic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial-Temporal Reinforcement Learning for Network Routing with\n  Non-Markovian Traffic"
                },
                "summary": "Reinforcement Learning (RL) has been widely used for packet routing in\ncommunication networks, but traditional RL methods rely on the Markov\nassumption that the current state contains all necessary information for\ndecision-making. In reality, internet traffic is non-Markovian, and past states\ndo influence routing performance. Moreover, common deep RL approaches use\nfunction approximators, such as neural networks, that do not model the spatial\nstructure in network topologies. To address these shortcomings, we design a\nnetwork environment with non-Markovian traffic and introduce a spatial-temporal\nRL (STRL) framework for packet routing. Our approach outperforms traditional\nbaselines by more than 19% during training and 7% for inference despite a\nchange in network topology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning (RL) has been widely used for packet routing in\ncommunication networks, but traditional RL methods rely on the Markov\nassumption that the current state contains all necessary information for\ndecision-making. In reality, internet traffic is non-Markovian, and past states\ndo influence routing performance. Moreover, common deep RL approaches use\nfunction approximators, such as neural networks, that do not model the spatial\nstructure in network topologies. To address these shortcomings, we design a\nnetwork environment with non-Markovian traffic and introduce a spatial-temporal\nRL (STRL) framework for packet routing. Our approach outperforms traditional\nbaselines by more than 19% during training and 7% for inference despite a\nchange in network topology."
                },
                "authors": [
                    {
                        "name": "Molly Wang"
                    },
                    {
                        "name": "Kin. K Leung"
                    }
                ],
                "author_detail": {
                    "name": "Kin. K Leung"
                },
                "author": "Kin. K Leung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22174v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22174v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23743v1",
                "updated": "2025-07-31T17:29:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    29,
                    20,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T17:29:20Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    29,
                    20,
                    3,
                    212,
                    0
                ],
                "title": "Relative Bias Under Imperfect Identification in Observational Causal\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relative Bias Under Imperfect Identification in Observational Causal\n  Inference"
                },
                "summary": "To conduct causal inference in observational settings, researchers must rely\non certain identifying assumptions. In practice, these assumptions are unlikely\nto hold exactly. This paper considers the bias of selection-on-observables,\ninstrumental variables, and proximal inference estimates under violations of\ntheir identifying assumptions. We develop bias expressions for IV and proximal\ninference that show how violations of their respective assumptions are\namplified by any unmeasured confounding in the outcome variable. We propose a\nset of sensitivity tools that quantify the sensitivity of different\nidentification strategies, and an augmented bias contour plot visualizes the\nrelationship between these strategies. We argue that the act of choosing an\nidentification strategy implicitly expresses a belief about the degree of\nviolations that must be present in alternative identification strategies. Even\nwhen researchers intend to conduct an IV or proximal analysis, a sensitivity\nanalysis comparing different identification strategies can help to better\nunderstand the implications of each set of assumptions. Throughout, we compare\nthe different approaches on a re-analysis of the impact of state surveillance\non the incidence of protest in Communist Poland.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To conduct causal inference in observational settings, researchers must rely\non certain identifying assumptions. In practice, these assumptions are unlikely\nto hold exactly. This paper considers the bias of selection-on-observables,\ninstrumental variables, and proximal inference estimates under violations of\ntheir identifying assumptions. We develop bias expressions for IV and proximal\ninference that show how violations of their respective assumptions are\namplified by any unmeasured confounding in the outcome variable. We propose a\nset of sensitivity tools that quantify the sensitivity of different\nidentification strategies, and an augmented bias contour plot visualizes the\nrelationship between these strategies. We argue that the act of choosing an\nidentification strategy implicitly expresses a belief about the degree of\nviolations that must be present in alternative identification strategies. Even\nwhen researchers intend to conduct an IV or proximal analysis, a sensitivity\nanalysis comparing different identification strategies can help to better\nunderstand the implications of each set of assumptions. Throughout, we compare\nthe different approaches on a re-analysis of the impact of state surveillance\non the incidence of protest in Communist Poland."
                },
                "authors": [
                    {
                        "name": "Melody Huang"
                    },
                    {
                        "name": "Cory McCartan"
                    }
                ],
                "author_detail": {
                    "name": "Cory McCartan"
                },
                "author": "Cory McCartan",
                "arxiv_comment": "19 pages, 2 figures, plus references and appendices",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23740v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23740v1",
                "updated": "2025-07-31T17:24:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    24,
                    4,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T17:24:04Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    24,
                    4,
                    3,
                    212,
                    0
                ],
                "title": "Rule2Text: Natural Language Explanation of Logical Rules in Knowledge\n  Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rule2Text: Natural Language Explanation of Logical Rules in Knowledge\n  Graphs"
                },
                "summary": "Knowledge graphs (KGs) often contain sufficient information to support the\ninference of new facts. Identifying logical rules not only improves the\ncompleteness of a knowledge graph but also enables the detection of potential\nerrors, reveals subtle data patterns, and enhances the overall capacity for\nreasoning and interpretation. However, the complexity of such rules, combined\nwith the unique labeling conventions of each KG, can make them difficult for\nhumans to understand. In this paper, we explore the potential of large language\nmodels to generate natural language explanations for logical rules.\nSpecifically, we extract logical rules using the AMIE 3.5.1 rule discovery\nalgorithm from the benchmark dataset FB15k-237 and two large-scale datasets,\nFB-CVT-REV and FB+CVT-REV. We examine various prompting strategies, including\nzero- and few-shot prompting, including variable entity types, and\nchain-of-thought reasoning. We conduct a comprehensive human evaluation of the\ngenerated explanations based on correctness, clarity, and hallucination, and\nalso assess the use of large language models as automatic judges. Our results\ndemonstrate promising performance in terms of explanation correctness and\nclarity, although several challenges remain for future research. All scripts\nand data used in this study are publicly available at\nhttps://github.com/idirlab/KGRule2NL}{https://github.com/idirlab/KGRule2NL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge graphs (KGs) often contain sufficient information to support the\ninference of new facts. Identifying logical rules not only improves the\ncompleteness of a knowledge graph but also enables the detection of potential\nerrors, reveals subtle data patterns, and enhances the overall capacity for\nreasoning and interpretation. However, the complexity of such rules, combined\nwith the unique labeling conventions of each KG, can make them difficult for\nhumans to understand. In this paper, we explore the potential of large language\nmodels to generate natural language explanations for logical rules.\nSpecifically, we extract logical rules using the AMIE 3.5.1 rule discovery\nalgorithm from the benchmark dataset FB15k-237 and two large-scale datasets,\nFB-CVT-REV and FB+CVT-REV. We examine various prompting strategies, including\nzero- and few-shot prompting, including variable entity types, and\nchain-of-thought reasoning. We conduct a comprehensive human evaluation of the\ngenerated explanations based on correctness, clarity, and hallucination, and\nalso assess the use of large language models as automatic judges. Our results\ndemonstrate promising performance in terms of explanation correctness and\nclarity, although several challenges remain for future research. All scripts\nand data used in this study are publicly available at\nhttps://github.com/idirlab/KGRule2NL}{https://github.com/idirlab/KGRule2NL."
                },
                "authors": [
                    {
                        "name": "Nasim Shirvani-Mahdavi"
                    },
                    {
                        "name": "Devin Wingfield"
                    },
                    {
                        "name": "Amin Ghasemi"
                    },
                    {
                        "name": "Chengkai Li"
                    }
                ],
                "author_detail": {
                    "name": "Chengkai Li"
                },
                "author": "Chengkai Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23740v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23740v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.13481v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.13481v3",
                "updated": "2025-07-31T17:19:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    19,
                    39,
                    3,
                    212,
                    0
                ],
                "published": "2024-01-24T14:29:39Z",
                "published_parsed": [
                    2024,
                    1,
                    24,
                    14,
                    29,
                    39,
                    2,
                    24,
                    0
                ],
                "title": "How AI Ideas Affect the Creativity, Diversity, and Evolution of Human\n  Ideas: Evidence From a Large, Dynamic Experiment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How AI Ideas Affect the Creativity, Diversity, and Evolution of Human\n  Ideas: Evidence From a Large, Dynamic Experiment"
                },
                "summary": "Exposure to large language model output is rapidly increasing. How will\nseeing AI-generated ideas affect human ideas? We conducted an experiment (800+\nparticipants, 40+ countries) where participants viewed creative ideas that were\nfrom ChatGPT or prior experimental participants and then brainstormed their own\nidea. We varied the number of AI-generated examples (none, low, or high\nexposure) and if the examples were labeled as 'AI' (disclosure). Our dynamic\nexperiment design -- ideas from prior participants in an experimental condition\nare used as stimuli for future participants in the same experimental condition\n-- speaks to the interdependent process of cultural creation: creative ideas\nare built upon prior ideas. Hence, we capture the compounding effects of having\nLLMs 'in the culture loop'. We find that high AI exposure (but not low AI\nexposure) did not affect the creativity of individual ideas but did increase\nthe average amount and rate of change of collective idea diversity. AI made\nideas different, not better. There were no main effects of disclosure. We also\nfound that self-reported creative people were less influenced by knowing an\nidea was from AI and that participants may knowingly adopt AI ideas when the\ntask is difficult. Our findings suggest that introducing AI ideas may increase\ncollective diversity but not individual creativity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposure to large language model output is rapidly increasing. How will\nseeing AI-generated ideas affect human ideas? We conducted an experiment (800+\nparticipants, 40+ countries) where participants viewed creative ideas that were\nfrom ChatGPT or prior experimental participants and then brainstormed their own\nidea. We varied the number of AI-generated examples (none, low, or high\nexposure) and if the examples were labeled as 'AI' (disclosure). Our dynamic\nexperiment design -- ideas from prior participants in an experimental condition\nare used as stimuli for future participants in the same experimental condition\n-- speaks to the interdependent process of cultural creation: creative ideas\nare built upon prior ideas. Hence, we capture the compounding effects of having\nLLMs 'in the culture loop'. We find that high AI exposure (but not low AI\nexposure) did not affect the creativity of individual ideas but did increase\nthe average amount and rate of change of collective idea diversity. AI made\nideas different, not better. There were no main effects of disclosure. We also\nfound that self-reported creative people were less influenced by knowing an\nidea was from AI and that participants may knowingly adopt AI ideas when the\ntask is difficult. Our findings suggest that introducing AI ideas may increase\ncollective diversity but not individual creativity."
                },
                "authors": [
                    {
                        "name": "Joshua Ashkinaze"
                    },
                    {
                        "name": "Julia Mendelsohn"
                    },
                    {
                        "name": "Li Qiwei"
                    },
                    {
                        "name": "Ceren Budak"
                    },
                    {
                        "name": "Eric Gilbert"
                    }
                ],
                "author_detail": {
                    "name": "Eric Gilbert"
                },
                "author": "Eric Gilbert",
                "arxiv_doi": "10.1145/3715928.3737481",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3715928.3737481",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.13481v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.13481v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at ACM Collective Intelligence 2025. Originally posted 2024",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23736v1",
                "updated": "2025-07-31T17:19:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    19,
                    38,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T17:19:38Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    19,
                    38,
                    3,
                    212,
                    0
                ],
                "title": "DICOM De-Identification via Hybrid AI and Rule-Based Framework for\n  Scalable, Uncertainty-Aware Redaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DICOM De-Identification via Hybrid AI and Rule-Based Framework for\n  Scalable, Uncertainty-Aware Redaction"
                },
                "summary": "Access to medical imaging and associated text data has the potential to drive\nmajor advances in healthcare research and patient outcomes. However, the\npresence of Protected Health Information (PHI) and Personally Identifiable\nInformation (PII) in Digital Imaging and Communications in Medicine (DICOM)\nfiles presents a significant barrier to the ethical and secure sharing of\nimaging datasets. This paper presents a hybrid de-identification framework\ndeveloped by Impact Business Information Solutions (IBIS) that combines\nrule-based and AI-driven techniques, and rigorous uncertainty quantification\nfor comprehensive PHI/PII removal from both metadata and pixel data.\n  Our approach begins with a two-tiered rule-based system targeting explicit\nand inferred metadata elements, further augmented by a large language model\n(LLM) fine-tuned for Named Entity Recognition (NER), and trained on a suite of\nsynthetic datasets simulating realistic clinical PHI/PII. For pixel data, we\nemploy an uncertainty-aware Faster R-CNN model to localize embedded text,\nextract candidate PHI via Optical Character Recognition (OCR), and apply the\nNER pipeline for final redaction. Crucially, uncertainty quantification\nprovides confidence measures for AI-based detections to enhance automation\nreliability and enable informed human-in-the-loop verification to manage\nresidual risks.\n  This uncertainty-aware deidentification framework achieves robust performance\nacross benchmark datasets and regulatory standards, including DICOM, HIPAA, and\nTCIA compliance metrics. By combining scalable automation, uncertainty\nquantification, and rigorous quality assurance, our solution addresses critical\nchallenges in medical data de-identification and supports the secure, ethical,\nand trustworthy release of imaging data for research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Access to medical imaging and associated text data has the potential to drive\nmajor advances in healthcare research and patient outcomes. However, the\npresence of Protected Health Information (PHI) and Personally Identifiable\nInformation (PII) in Digital Imaging and Communications in Medicine (DICOM)\nfiles presents a significant barrier to the ethical and secure sharing of\nimaging datasets. This paper presents a hybrid de-identification framework\ndeveloped by Impact Business Information Solutions (IBIS) that combines\nrule-based and AI-driven techniques, and rigorous uncertainty quantification\nfor comprehensive PHI/PII removal from both metadata and pixel data.\n  Our approach begins with a two-tiered rule-based system targeting explicit\nand inferred metadata elements, further augmented by a large language model\n(LLM) fine-tuned for Named Entity Recognition (NER), and trained on a suite of\nsynthetic datasets simulating realistic clinical PHI/PII. For pixel data, we\nemploy an uncertainty-aware Faster R-CNN model to localize embedded text,\nextract candidate PHI via Optical Character Recognition (OCR), and apply the\nNER pipeline for final redaction. Crucially, uncertainty quantification\nprovides confidence measures for AI-based detections to enhance automation\nreliability and enable informed human-in-the-loop verification to manage\nresidual risks.\n  This uncertainty-aware deidentification framework achieves robust performance\nacross benchmark datasets and regulatory standards, including DICOM, HIPAA, and\nTCIA compliance metrics. By combining scalable automation, uncertainty\nquantification, and rigorous quality assurance, our solution addresses critical\nchallenges in medical data de-identification and supports the secure, ethical,\nand trustworthy release of imaging data for research."
                },
                "authors": [
                    {
                        "name": "Kyle Naddeo"
                    },
                    {
                        "name": "Nikolas Koutsoubis"
                    },
                    {
                        "name": "Rahul Krish"
                    },
                    {
                        "name": "Ghulam Rasool"
                    },
                    {
                        "name": "Nidhal Bouaynaya"
                    },
                    {
                        "name": "Tony OSullivan"
                    },
                    {
                        "name": "Raj Krish"
                    }
                ],
                "author_detail": {
                    "name": "Raj Krish"
                },
                "author": "Raj Krish",
                "arxiv_comment": "15 pages, 6 figures,",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03837v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03837v2",
                "updated": "2025-07-31T17:13:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    13,
                    50,
                    3,
                    212,
                    0
                ],
                "published": "2024-09-05T18:02:30Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    18,
                    2,
                    30,
                    3,
                    249,
                    0
                ],
                "title": "STAR NRE: Solving supernova selection effects with set-based truncated\n  auto-regressive neural ratio estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STAR NRE: Solving supernova selection effects with set-based truncated\n  auto-regressive neural ratio estimation"
                },
                "summary": "Accounting for selection effects in supernova type Ia (SN Ia) cosmology is\ncrucial for unbiased cosmological parameter inference -- even more so for the\nnext generation of large, mostly photometric-only surveys. The conventional\n\"bias correction\" procedure has a built-in systematic bias towards the fiducial\nmodel used to derive it and fails to account for the additional Eddington bias\nthat arises in the presence of significant redshift uncertainty. On the other\nhand, likelihood-based analyses within a Bayesian hierarchical model, e.g.\nusing MCMC, scale poorly with the data set size and require explicit\nassumptions for the selection function that may be inaccurate or contrived.\n  To address these limitations, we introduce STAR NRE, a simulation-based\napproach that makes use of a conditioned deep set neural network and combines\nefficient high-dimensional global inference with subsampling-based truncation\nin order to scale to very large survey sizes while training on sets with\nvarying cardinality. Applying it to a simplified SN Ia model consisting of\nstandardised brightnesses and redshifts with Gaussian uncertainties and a\nselection procedure based on the expected LSST sensitivity, we demonstrate\nprecise and unbiased inference of cosmological parameters and the redshift\nevolution of the volumetric SN Ia rate from ~100 000 mock SNae Ia. Our\ninference procedure can incorporate arbitrarily complex selection criteria,\nincluding transient classification, in the forward simulator and be applied to\ncomplex data like light curves. We outline these and other steps aimed at\nintegrating STAR NRE into an end-to-end simulation-based pipeline for the\nanalysis of future photometric-only SN Ia data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accounting for selection effects in supernova type Ia (SN Ia) cosmology is\ncrucial for unbiased cosmological parameter inference -- even more so for the\nnext generation of large, mostly photometric-only surveys. The conventional\n\"bias correction\" procedure has a built-in systematic bias towards the fiducial\nmodel used to derive it and fails to account for the additional Eddington bias\nthat arises in the presence of significant redshift uncertainty. On the other\nhand, likelihood-based analyses within a Bayesian hierarchical model, e.g.\nusing MCMC, scale poorly with the data set size and require explicit\nassumptions for the selection function that may be inaccurate or contrived.\n  To address these limitations, we introduce STAR NRE, a simulation-based\napproach that makes use of a conditioned deep set neural network and combines\nefficient high-dimensional global inference with subsampling-based truncation\nin order to scale to very large survey sizes while training on sets with\nvarying cardinality. Applying it to a simplified SN Ia model consisting of\nstandardised brightnesses and redshifts with Gaussian uncertainties and a\nselection procedure based on the expected LSST sensitivity, we demonstrate\nprecise and unbiased inference of cosmological parameters and the redshift\nevolution of the volumetric SN Ia rate from ~100 000 mock SNae Ia. Our\ninference procedure can incorporate arbitrarily complex selection criteria,\nincluding transient classification, in the forward simulator and be applied to\ncomplex data like light curves. We outline these and other steps aimed at\nintegrating STAR NRE into an end-to-end simulation-based pipeline for the\nanalysis of future photometric-only SN Ia data."
                },
                "authors": [
                    {
                        "name": "Konstantin Karchev"
                    },
                    {
                        "name": "Roberto Trotta"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Trotta"
                },
                "author": "Roberto Trotta",
                "arxiv_doi": "10.1088/1475-7516/2025/07/031",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1088/1475-7516/2025/07/031",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.03837v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03837v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "published in JCAP; 25 pages, 6 figures (+ appendices)",
                "arxiv_journal_ref": "JCAP 07 (2025) 031",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09844v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09844v2",
                "updated": "2025-07-31T17:02:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    2,
                    29,
                    3,
                    212,
                    0
                ],
                "published": "2025-05-14T23:03:34Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    23,
                    3,
                    34,
                    2,
                    134,
                    0
                ],
                "title": "Inference for Dispersion and Curvature of Random Objects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference for Dispersion and Curvature of Random Objects"
                },
                "summary": "There are many open questions pertaining to the statistical analysis of\nrandom objects, which are increasingly encountered. A major challenge is the\nabsence of linear operations in such spaces. A basic statistical task is to\nquantify statistical dispersion or spread. For two measures of dispersion for\ndata objects in geodesic metric spaces, Fr\\'echet variance and metric variance,\nwe derive a central limit theorem (CLT) for their joint distribution. This\nanalysis reveals that the Alexandrov curvature of the geodesic space determines\nthe relationship between these two dispersion measures. This suggests a novel\ntest for inferring the curvature of a space based on the asymptotic\ndistribution of the dispersion measures. We demonstrate how this test can be\nemployed to detect the intrinsic curvature of an unknown underlying space,\nwhich emerges as a joint property of the space and the underlying probability\nmeasure that generates the random objects. We investigate the asymptotic\nproperties of the test and its finite-sample behavior for various data types,\nincluding distributional data and point cloud data. We illustrate the proposed\ninference for intrinsic curvature of random objects using gait synchronization\ndata represented as symmetric positive definite matrices and energy\ncompositional data on the sphere.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There are many open questions pertaining to the statistical analysis of\nrandom objects, which are increasingly encountered. A major challenge is the\nabsence of linear operations in such spaces. A basic statistical task is to\nquantify statistical dispersion or spread. For two measures of dispersion for\ndata objects in geodesic metric spaces, Fr\\'echet variance and metric variance,\nwe derive a central limit theorem (CLT) for their joint distribution. This\nanalysis reveals that the Alexandrov curvature of the geodesic space determines\nthe relationship between these two dispersion measures. This suggests a novel\ntest for inferring the curvature of a space based on the asymptotic\ndistribution of the dispersion measures. We demonstrate how this test can be\nemployed to detect the intrinsic curvature of an unknown underlying space,\nwhich emerges as a joint property of the space and the underlying probability\nmeasure that generates the random objects. We investigate the asymptotic\nproperties of the test and its finite-sample behavior for various data types,\nincluding distributional data and point cloud data. We illustrate the proposed\ninference for intrinsic curvature of random objects using gait synchronization\ndata represented as symmetric positive definite matrices and energy\ncompositional data on the sphere."
                },
                "authors": [
                    {
                        "name": "Wookyeong Song"
                    },
                    {
                        "name": "Hans-Georg Mller"
                    }
                ],
                "author_detail": {
                    "name": "Hans-Georg Mller"
                },
                "author": "Hans-Georg Mller",
                "arxiv_comment": "87 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09844v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09844v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23726v1",
                "updated": "2025-07-31T17:00:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    0,
                    30,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T17:00:30Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    0,
                    30,
                    3,
                    212,
                    0
                ],
                "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving"
                },
                "summary": "LLMs have demonstrated strong mathematical reasoning abilities by leveraging\nreinforcement learning with long chain-of-thought, yet they continue to\nstruggle with theorem proving due to the lack of clear supervision signals when\nsolely using natural language. Dedicated domain-specific languages like Lean\nprovide clear supervision via formal verification of proofs, enabling effective\ntraining through reinforcement learning. In this work, we propose\n\\textbf{Seed-Prover}, a lemma-style whole-proof reasoning model. Seed-Prover\ncan iteratively refine its proof based on Lean feedback, proved lemmas, and\nself-summarization. To solve IMO-level contest problems, we design three\ntest-time inference strategies that enable both deep and broad reasoning.\nSeed-Prover proves $78.1\\%$ of formalized past IMO problems, saturates MiniF2F,\nand achieves over 50\\% on PutnamBench, outperforming the previous\nstate-of-the-art by a large margin. To address the lack of geometry support in\nLean, we introduce a geometry reasoning engine \\textbf{Seed-Geometry}, which\noutperforms previous formal geometry engines. We use these two systems to\nparticipate in IMO 2025 and fully prove 5 out of 6 problems. This work\nrepresents a significant advancement in automated mathematical reasoning,\ndemonstrating the effectiveness of formal verification with long\nchain-of-thought reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have demonstrated strong mathematical reasoning abilities by leveraging\nreinforcement learning with long chain-of-thought, yet they continue to\nstruggle with theorem proving due to the lack of clear supervision signals when\nsolely using natural language. Dedicated domain-specific languages like Lean\nprovide clear supervision via formal verification of proofs, enabling effective\ntraining through reinforcement learning. In this work, we propose\n\\textbf{Seed-Prover}, a lemma-style whole-proof reasoning model. Seed-Prover\ncan iteratively refine its proof based on Lean feedback, proved lemmas, and\nself-summarization. To solve IMO-level contest problems, we design three\ntest-time inference strategies that enable both deep and broad reasoning.\nSeed-Prover proves $78.1\\%$ of formalized past IMO problems, saturates MiniF2F,\nand achieves over 50\\% on PutnamBench, outperforming the previous\nstate-of-the-art by a large margin. To address the lack of geometry support in\nLean, we introduce a geometry reasoning engine \\textbf{Seed-Geometry}, which\noutperforms previous formal geometry engines. We use these two systems to\nparticipate in IMO 2025 and fully prove 5 out of 6 problems. This work\nrepresents a significant advancement in automated mathematical reasoning,\ndemonstrating the effectiveness of formal verification with long\nchain-of-thought reasoning."
                },
                "authors": [
                    {
                        "name": "Luoxin Chen"
                    },
                    {
                        "name": "Jinming Gu"
                    },
                    {
                        "name": "Liankai Huang"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Zhicheng Jiang"
                    },
                    {
                        "name": "Allan Jie"
                    },
                    {
                        "name": "Xiaoran Jin"
                    },
                    {
                        "name": "Xing Jin"
                    },
                    {
                        "name": "Chenggang Li"
                    },
                    {
                        "name": "Kaijing Ma"
                    },
                    {
                        "name": "Cheng Ren"
                    },
                    {
                        "name": "Jiawei Shen"
                    },
                    {
                        "name": "Wenlei Shi"
                    },
                    {
                        "name": "Tong Sun"
                    },
                    {
                        "name": "He Sun"
                    },
                    {
                        "name": "Jiahui Wang"
                    },
                    {
                        "name": "Siran Wang"
                    },
                    {
                        "name": "Zhihong Wang"
                    },
                    {
                        "name": "Chenrui Wei"
                    },
                    {
                        "name": "Shufa Wei"
                    },
                    {
                        "name": "Yonghui Wu"
                    },
                    {
                        "name": "Yuchen Wu"
                    },
                    {
                        "name": "Yihang Xia"
                    },
                    {
                        "name": "Huajian Xin"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Huaiyuan Ying"
                    },
                    {
                        "name": "Hongyi Yuan"
                    },
                    {
                        "name": "Zheng Yuan"
                    },
                    {
                        "name": "Tianyang Zhan"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Tianyun Zhao"
                    },
                    {
                        "name": "Jianqiu Zhao"
                    },
                    {
                        "name": "Yichi Zhou"
                    },
                    {
                        "name": "Thomas Hanwen Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Hanwen Zhu"
                },
                "author": "Thomas Hanwen Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23723v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23723v1",
                "updated": "2025-07-31T16:57:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    16,
                    57,
                    8,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T16:57:08Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    16,
                    57,
                    8,
                    3,
                    212,
                    0
                ],
                "title": "Search for $t\\bar tt\\bar tW$ Production at $\\sqrt{s} = 13$ TeV Using a\n  Modified Graph Neural Network at the LHC",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search for $t\\bar tt\\bar tW$ Production at $\\sqrt{s} = 13$ TeV Using a\n  Modified Graph Neural Network at the LHC"
                },
                "summary": "The simultaneous production of four top quarks in association with a ($W$)\nboson at $(\\sqrt{s} = 13)$ TeV is an rare SM process with a\nnext-to-leading-order (NLO) cross-section of $(6.6^{+2.4}_{-2.6}\n{ab})$\\cite{saiel}. Identifying this process in the fully hadronic decay\nchannel is particularly challenging due to overwhelming backgrounds from\n$t\\bar{t}, t\\bar{t}W, t\\bar{t}Z$, and triple-top production processes. This\nstudy introduces a modified physics informed Neural Network, a hybrid graph\nneural network (GNN) enhancing event classification. The proposed model\nintegrates Graph layers for particle-level features, a custom Multi Layer\nPerceptron(MLP) based global stream with a quantum circuit and cross-attention\nfusion to combine local and global representations. Physics-informed Loss\nfunction enforce jet multiplicity constraints, derived from event decay\ndynamics. Benchmarked against conventional methods, the GNN achieves a signal\nsignificance $(S/\\sqrt{S+B})$ of $0.174$ and ROC-AUC of 0.974, surpassing BDT's\nsignificance of $0.148$ and ROC of $0.913$, while Xgboost achieves a\nsignificance of $0.149$ and ROC of $0.920$. The classification models are\ntrained on Monte Carlo (MC) simulations, with events normalized using\ncross-section-based reweighting to reflect their expected contributions in a\ndataset corresponding to $350\\;$fb$^{-1}$ of integrated luminosity. This\nenhanced approach offers a framework for precision event selection at the LHC,\nleveraging high dimensional statistical learning and physics informed inference\nto tackle fundamental HEP challenges, aligning with ML developments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The simultaneous production of four top quarks in association with a ($W$)\nboson at $(\\sqrt{s} = 13)$ TeV is an rare SM process with a\nnext-to-leading-order (NLO) cross-section of $(6.6^{+2.4}_{-2.6}\n{ab})$\\cite{saiel}. Identifying this process in the fully hadronic decay\nchannel is particularly challenging due to overwhelming backgrounds from\n$t\\bar{t}, t\\bar{t}W, t\\bar{t}Z$, and triple-top production processes. This\nstudy introduces a modified physics informed Neural Network, a hybrid graph\nneural network (GNN) enhancing event classification. The proposed model\nintegrates Graph layers for particle-level features, a custom Multi Layer\nPerceptron(MLP) based global stream with a quantum circuit and cross-attention\nfusion to combine local and global representations. Physics-informed Loss\nfunction enforce jet multiplicity constraints, derived from event decay\ndynamics. Benchmarked against conventional methods, the GNN achieves a signal\nsignificance $(S/\\sqrt{S+B})$ of $0.174$ and ROC-AUC of 0.974, surpassing BDT's\nsignificance of $0.148$ and ROC of $0.913$, while Xgboost achieves a\nsignificance of $0.149$ and ROC of $0.920$. The classification models are\ntrained on Monte Carlo (MC) simulations, with events normalized using\ncross-section-based reweighting to reflect their expected contributions in a\ndataset corresponding to $350\\;$fb$^{-1}$ of integrated luminosity. This\nenhanced approach offers a framework for precision event selection at the LHC,\nleveraging high dimensional statistical learning and physics informed inference\nto tackle fundamental HEP challenges, aligning with ML developments."
                },
                "authors": [
                    {
                        "name": "Syed Haider Ali"
                    },
                    {
                        "name": "Ashfaq Ahmad"
                    },
                    {
                        "name": "Muhammad Saiel"
                    },
                    {
                        "name": "Nadeem Shaukat"
                    }
                ],
                "author_detail": {
                    "name": "Nadeem Shaukat"
                },
                "author": "Nadeem Shaukat",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23723v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23723v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22879v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22879v2",
                "updated": "2025-07-31T16:54:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    16,
                    54,
                    43,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-30T17:55:06Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    17,
                    55,
                    6,
                    2,
                    211,
                    0
                ],
                "title": "RecGPT Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RecGPT Technical Report"
                },
                "summary": "Recommender systems are among the most impactful applications of artificial\nintelligence, serving as critical infrastructure connecting users, merchants,\nand platforms. However, most current industrial systems remain heavily reliant\non historical co-occurrence patterns and log-fitting objectives, i.e.,\noptimizing for past user interactions without explicitly modeling user intent.\nThis log-fitting approach often leads to overfitting to narrow historical\npreferences, failing to capture users' evolving and latent interests. As a\nresult, it reinforces filter bubbles and long-tail phenomena, ultimately\nharming user experience and threatening the sustainability of the whole\nrecommendation ecosystem.\n  To address these challenges, we rethink the overall design paradigm of\nrecommender systems and propose RecGPT, a next-generation framework that places\nuser intent at the center of the recommendation pipeline. By integrating large\nlanguage models (LLMs) into key stages of user interest mining, item retrieval,\nand explanation generation, RecGPT transforms log-fitting recommendation into\nan intent-centric process. To effectively align general-purpose LLMs to the\nabove domain-specific recommendation tasks at scale, RecGPT incorporates a\nmulti-stage training paradigm, which integrates reasoning-enhanced\npre-alignment and self-training evolution, guided by a Human-LLM cooperative\njudge system. Currently, RecGPT has been fully deployed on the Taobao App.\nOnline experiments demonstrate that RecGPT achieves consistent performance\ngains across stakeholders: users benefit from increased content diversity and\nsatisfaction, merchants and the platform gain greater exposure and conversions.\nThese comprehensive improvement results across all stakeholders validates that\nLLM-driven, intent-centric design can foster a more sustainable and mutually\nbeneficial recommendation ecosystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems are among the most impactful applications of artificial\nintelligence, serving as critical infrastructure connecting users, merchants,\nand platforms. However, most current industrial systems remain heavily reliant\non historical co-occurrence patterns and log-fitting objectives, i.e.,\noptimizing for past user interactions without explicitly modeling user intent.\nThis log-fitting approach often leads to overfitting to narrow historical\npreferences, failing to capture users' evolving and latent interests. As a\nresult, it reinforces filter bubbles and long-tail phenomena, ultimately\nharming user experience and threatening the sustainability of the whole\nrecommendation ecosystem.\n  To address these challenges, we rethink the overall design paradigm of\nrecommender systems and propose RecGPT, a next-generation framework that places\nuser intent at the center of the recommendation pipeline. By integrating large\nlanguage models (LLMs) into key stages of user interest mining, item retrieval,\nand explanation generation, RecGPT transforms log-fitting recommendation into\nan intent-centric process. To effectively align general-purpose LLMs to the\nabove domain-specific recommendation tasks at scale, RecGPT incorporates a\nmulti-stage training paradigm, which integrates reasoning-enhanced\npre-alignment and self-training evolution, guided by a Human-LLM cooperative\njudge system. Currently, RecGPT has been fully deployed on the Taobao App.\nOnline experiments demonstrate that RecGPT achieves consistent performance\ngains across stakeholders: users benefit from increased content diversity and\nsatisfaction, merchants and the platform gain greater exposure and conversions.\nThese comprehensive improvement results across all stakeholders validates that\nLLM-driven, intent-centric design can foster a more sustainable and mutually\nbeneficial recommendation ecosystem."
                },
                "authors": [
                    {
                        "name": "Chao Yi"
                    },
                    {
                        "name": "Dian Chen"
                    },
                    {
                        "name": "Gaoyang Guo"
                    },
                    {
                        "name": "Jiakai Tang"
                    },
                    {
                        "name": "Jian Wu"
                    },
                    {
                        "name": "Jing Yu"
                    },
                    {
                        "name": "Mao Zhang"
                    },
                    {
                        "name": "Sunhao Dai"
                    },
                    {
                        "name": "Wen Chen"
                    },
                    {
                        "name": "Wenjun Yang"
                    },
                    {
                        "name": "Yuning Jiang"
                    },
                    {
                        "name": "Zhujin Gao"
                    },
                    {
                        "name": "Bo Zheng"
                    },
                    {
                        "name": "Chi Li"
                    },
                    {
                        "name": "Dimin Wang"
                    },
                    {
                        "name": "Dixuan Wang"
                    },
                    {
                        "name": "Fan Li"
                    },
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "Haibin Chen"
                    },
                    {
                        "name": "Haozhuang Liu"
                    },
                    {
                        "name": "Jialin Zhu"
                    },
                    {
                        "name": "Jiamang Wang"
                    },
                    {
                        "name": "Jiawei Wu"
                    },
                    {
                        "name": "Jin Cui"
                    },
                    {
                        "name": "Ju Huang"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Kan Liu"
                    },
                    {
                        "name": "Lang Tian"
                    },
                    {
                        "name": "Liang Rao"
                    },
                    {
                        "name": "Longbin Li"
                    },
                    {
                        "name": "Lulu Zhao"
                    },
                    {
                        "name": "Na He"
                    },
                    {
                        "name": "Peiyang Wang"
                    },
                    {
                        "name": "Qiqi Huang"
                    },
                    {
                        "name": "Tao Luo"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Xiaoxiao He"
                    },
                    {
                        "name": "Xin Tong"
                    },
                    {
                        "name": "Xu Chen"
                    },
                    {
                        "name": "Xunke Xi"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Yaxuan Wu"
                    },
                    {
                        "name": "Yeqiu Yang"
                    },
                    {
                        "name": "Yi Hu"
                    },
                    {
                        "name": "Yinnan Song"
                    },
                    {
                        "name": "Yuchen Li"
                    },
                    {
                        "name": "Yujie Luo"
                    },
                    {
                        "name": "Yujin Yuan"
                    },
                    {
                        "name": "Yuliang Yan"
                    },
                    {
                        "name": "Zhengyang Wang"
                    },
                    {
                        "name": "Zhibo Xiao"
                    },
                    {
                        "name": "Zhixin Ma"
                    },
                    {
                        "name": "Zile Zhou"
                    },
                    {
                        "name": "Ziqi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ziqi Zhang"
                },
                "author": "Ziqi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22879v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22879v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08184v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08184v3",
                "updated": "2025-07-31T16:45:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    16,
                    45,
                    51,
                    3,
                    212,
                    0
                ],
                "published": "2025-06-09T19:49:11Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    19,
                    49,
                    11,
                    0,
                    160,
                    0
                ],
                "title": "Unable to Forget: Proactive Interference Reveals Working Memory Limits\n  in LLMs Beyond Context Length",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unable to Forget: Proactive Interference Reveals Working Memory Limits\n  in LLMs Beyond Context Length"
                },
                "summary": "Information retrieval in Large Language Models (LLMs) is increasingly\nrecognized as intertwined with generation capabilities rather than mere lookup.\nWhile longer contexts are often assumed to improve retrieval, the effects of\nintra-context interference remain understudied. To address this, we adapt the\nproactive interference (PI) paradigm from cognitive science, where earlier\ninformation disrupts recall of newer updates. In humans, susceptibility to such\ninterference is inversely linked to working memory capacity. We introduce\nPI-LLM, an evaluation that sequentially streams semantically related key-value\nupdates and queries only the final values. Although these final values are\nclearly positioned just before the query, LLM retrieval accuracy declines\nlog-linearly toward zero as interference accumulates; errors arise from\nretrieving previously overwritten values. Attempts to mitigate interference via\nprompt engineering (e.g., instructing models to ignore earlier input) yield\nlimited success. These findings reveal a fundamental constraint on LLMs'\nability to disentangle interference and flexibly manipulate information,\nsuggesting a working memory bottleneck beyond mere context access. This calls\nfor approaches that strengthen models' ability to suppress irrelevant content\nduring retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information retrieval in Large Language Models (LLMs) is increasingly\nrecognized as intertwined with generation capabilities rather than mere lookup.\nWhile longer contexts are often assumed to improve retrieval, the effects of\nintra-context interference remain understudied. To address this, we adapt the\nproactive interference (PI) paradigm from cognitive science, where earlier\ninformation disrupts recall of newer updates. In humans, susceptibility to such\ninterference is inversely linked to working memory capacity. We introduce\nPI-LLM, an evaluation that sequentially streams semantically related key-value\nupdates and queries only the final values. Although these final values are\nclearly positioned just before the query, LLM retrieval accuracy declines\nlog-linearly toward zero as interference accumulates; errors arise from\nretrieving previously overwritten values. Attempts to mitigate interference via\nprompt engineering (e.g., instructing models to ignore earlier input) yield\nlimited success. These findings reveal a fundamental constraint on LLMs'\nability to disentangle interference and flexibly manipulate information,\nsuggesting a working memory bottleneck beyond mere context access. This calls\nfor approaches that strengthen models' ability to suppress irrelevant content\nduring retrieval."
                },
                "authors": [
                    {
                        "name": "Chupei Wang"
                    },
                    {
                        "name": "Jiaqiu Vince Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqiu Vince Sun"
                },
                "arxiv_affiliation": "New York University",
                "author": "Jiaqiu Vince Sun",
                "arxiv_comment": "Accepted at ICML 2025 Workshop on Long Context Foundation Models\n  (ICFM). Code: https://github.com/zhuangziGiantfish/Unable-to-Forget",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08184v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08184v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14820v2",
                "updated": "2025-07-31T16:31:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    16,
                    31,
                    47,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-20T04:35:31Z",
                "published_parsed": [
                    2025,
                    7,
                    20,
                    4,
                    35,
                    31,
                    6,
                    201,
                    0
                ],
                "title": "KGN-Pro: Keypoint-Based Grasp Prediction through Probabilistic 2D-3D\n  Correspondence Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KGN-Pro: Keypoint-Based Grasp Prediction through Probabilistic 2D-3D\n  Correspondence Learning"
                },
                "summary": "High-level robotic manipulation tasks demand flexible 6-DoF grasp estimation\nto serve as a basic function. Previous approaches either directly generate\ngrasps from point-cloud data, suffering from challenges with small objects and\nsensor noise, or infer 3D information from RGB images, which introduces\nexpensive annotation requirements and discretization issues. Recent methods\nmitigate some challenges by retaining a 2D representation to estimate grasp\nkeypoints and applying Perspective-n-Point (PnP) algorithms to compute 6-DoF\nposes. However, these methods are limited by their non-differentiable nature\nand reliance solely on 2D supervision, which hinders the full exploitation of\nrich 3D information. In this work, we present KGN-Pro, a novel grasping network\nthat preserves the efficiency and fine-grained object grasping of previous KGNs\nwhile integrating direct 3D optimization through probabilistic PnP layers.\nKGN-Pro encodes paired RGB-D images to generate Keypoint Map, and further\noutputs a 2D confidence map to weight keypoint contributions during\nre-projection error minimization. By modeling the weighted sum of squared\nre-projection errors probabilistically, the network effectively transmits 3D\nsupervision to its 2D keypoint predictions, enabling end-to-end learning.\nExperiments on both simulated and real-world platforms demonstrate that KGN-Pro\noutperforms existing methods in terms of grasp cover rate and success rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-level robotic manipulation tasks demand flexible 6-DoF grasp estimation\nto serve as a basic function. Previous approaches either directly generate\ngrasps from point-cloud data, suffering from challenges with small objects and\nsensor noise, or infer 3D information from RGB images, which introduces\nexpensive annotation requirements and discretization issues. Recent methods\nmitigate some challenges by retaining a 2D representation to estimate grasp\nkeypoints and applying Perspective-n-Point (PnP) algorithms to compute 6-DoF\nposes. However, these methods are limited by their non-differentiable nature\nand reliance solely on 2D supervision, which hinders the full exploitation of\nrich 3D information. In this work, we present KGN-Pro, a novel grasping network\nthat preserves the efficiency and fine-grained object grasping of previous KGNs\nwhile integrating direct 3D optimization through probabilistic PnP layers.\nKGN-Pro encodes paired RGB-D images to generate Keypoint Map, and further\noutputs a 2D confidence map to weight keypoint contributions during\nre-projection error minimization. By modeling the weighted sum of squared\nre-projection errors probabilistically, the network effectively transmits 3D\nsupervision to its 2D keypoint predictions, enabling end-to-end learning.\nExperiments on both simulated and real-world platforms demonstrate that KGN-Pro\noutperforms existing methods in terms of grasp cover rate and success rate."
                },
                "authors": [
                    {
                        "name": "Bingran Chen"
                    },
                    {
                        "name": "Baorun Li"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Guangyao Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Guangyao Zhai"
                },
                "author": "Guangyao Zhai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23709v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23709v1",
                "updated": "2025-07-31T16:30:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    16,
                    30,
                    50,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T16:30:50Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    16,
                    30,
                    50,
                    3,
                    212,
                    0
                ],
                "title": "Explainable Image Classification with Reduced Overconfidence for Tissue\n  Characterisation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable Image Classification with Reduced Overconfidence for Tissue\n  Characterisation"
                },
                "summary": "The deployment of Machine Learning models intraoperatively for tissue\ncharacterisation can assist decision making and guide safe tumour resections.\nFor image classification models, pixel attribution methods are popular to infer\nexplainability. However, overconfidence in deep learning model's predictions\ntranslates to overconfidence in pixel attribution. In this paper, we propose\nthe first approach which incorporates risk estimation into a pixel attribution\nmethod for improved image classification explainability. The proposed method\niteratively applies a classification model with a pixel attribution method to\ncreate a volume of PA maps. This volume is used for the first time, to generate\na pixel-wise distribution of PA values. We introduce a method to generate an\nenhanced PA map by estimating the expectation values of the pixel-wise\ndistributions. In addition, the coefficient of variation (CV) is used to\nestimate pixel-wise risk of this enhanced PA map. Hence, the proposed method\nnot only provides an improved PA map but also produces an estimation of risk on\nthe output PA values. Performance evaluation on probe-based Confocal Laser\nEndomicroscopy (pCLE) data and ImageNet verifies that our improved\nexplainability method outperforms the state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Machine Learning models intraoperatively for tissue\ncharacterisation can assist decision making and guide safe tumour resections.\nFor image classification models, pixel attribution methods are popular to infer\nexplainability. However, overconfidence in deep learning model's predictions\ntranslates to overconfidence in pixel attribution. In this paper, we propose\nthe first approach which incorporates risk estimation into a pixel attribution\nmethod for improved image classification explainability. The proposed method\niteratively applies a classification model with a pixel attribution method to\ncreate a volume of PA maps. This volume is used for the first time, to generate\na pixel-wise distribution of PA values. We introduce a method to generate an\nenhanced PA map by estimating the expectation values of the pixel-wise\ndistributions. In addition, the coefficient of variation (CV) is used to\nestimate pixel-wise risk of this enhanced PA map. Hence, the proposed method\nnot only provides an improved PA map but also produces an estimation of risk on\nthe output PA values. Performance evaluation on probe-based Confocal Laser\nEndomicroscopy (pCLE) data and ImageNet verifies that our improved\nexplainability method outperforms the state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Alfie Roddan"
                    },
                    {
                        "name": "Chi Xu"
                    },
                    {
                        "name": "Serine Ajlouni"
                    },
                    {
                        "name": "Irini Kakaletri"
                    },
                    {
                        "name": "Patra Charalampaki"
                    },
                    {
                        "name": "Stamatia Giannarou"
                    }
                ],
                "author_detail": {
                    "name": "Stamatia Giannarou"
                },
                "author": "Stamatia Giannarou",
                "arxiv_doi": "10.1007/978-3-031-43895-0_54",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-43895-0_54",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.23709v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23709v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "(2023). In: Greenspan, H., et al. (eds) Medical Image Computing\n  and Computer Assisted Intervention MICCAI 2023. Lecture Notes in Computer\n  Science, vol 14221. Springer, Cham",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23701v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23701v1",
                "updated": "2025-07-31T16:22:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    16,
                    22,
                    55,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T16:22:55Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    16,
                    22,
                    55,
                    3,
                    212,
                    0
                ],
                "title": "TextQuests: How Good are LLMs at Text-Based Video Games?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TextQuests: How Good are LLMs at Text-Based Video Games?"
                },
                "summary": "Evaluating AI agents within complex, interactive environments that mirror\nreal-world challenges is critical for understanding their practical\ncapabilities. While existing agent benchmarks effectively assess skills like\ntool use or performance on structured tasks, they often do not fully capture an\nagent's ability to operate autonomously in exploratory environments that demand\nsustained, self-directed reasoning over a long and growing context. To spur the\ndevelopment of agents capable of more robust intrinsic reasoning over long\nhorizons, we introduce TextQuests, a benchmark based on the Infocom suite of\ninteractive fiction games. These text-based adventures, which can take human\nplayers over 30 hours and require hundreds of precise actions to solve, serve\nas an effective proxy for evaluating AI agents on focused, stateful tasks. The\nbenchmark is specifically designed to assess an LLM agent's capacity for\nself-contained problem-solving by precluding the use of external tools, thereby\nfocusing on intrinsic long-context reasoning capabilities in an exploratory\nenvironment characterized by the need for trial-and-error learning and\nsustained problem-solving within a single interactive session. We release\nTextQuests at https://textquests.ai.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating AI agents within complex, interactive environments that mirror\nreal-world challenges is critical for understanding their practical\ncapabilities. While existing agent benchmarks effectively assess skills like\ntool use or performance on structured tasks, they often do not fully capture an\nagent's ability to operate autonomously in exploratory environments that demand\nsustained, self-directed reasoning over a long and growing context. To spur the\ndevelopment of agents capable of more robust intrinsic reasoning over long\nhorizons, we introduce TextQuests, a benchmark based on the Infocom suite of\ninteractive fiction games. These text-based adventures, which can take human\nplayers over 30 hours and require hundreds of precise actions to solve, serve\nas an effective proxy for evaluating AI agents on focused, stateful tasks. The\nbenchmark is specifically designed to assess an LLM agent's capacity for\nself-contained problem-solving by precluding the use of external tools, thereby\nfocusing on intrinsic long-context reasoning capabilities in an exploratory\nenvironment characterized by the need for trial-and-error learning and\nsustained problem-solving within a single interactive session. We release\nTextQuests at https://textquests.ai."
                },
                "authors": [
                    {
                        "name": "Long Phan"
                    },
                    {
                        "name": "Mantas Mazeika"
                    },
                    {
                        "name": "Andy Zou"
                    },
                    {
                        "name": "Dan Hendrycks"
                    }
                ],
                "author_detail": {
                    "name": "Dan Hendrycks"
                },
                "author": "Dan Hendrycks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23701v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23701v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23694v1",
                "updated": "2025-07-31T16:12:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    16,
                    12,
                    22,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T16:12:22Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    16,
                    12,
                    22,
                    3,
                    212,
                    0
                ],
                "title": "A survey of multi-agent geosimulation methodologies: from ABM to LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A survey of multi-agent geosimulation methodologies: from ABM to LLM"
                },
                "summary": "We provide a comprehensive examination of agent-based approaches that codify\nthe principles and linkages underlying multi-agent systems, simulations, and\ninformation systems. Based on two decades of study, this paper confirms a\nframework intended as a formal specification for geosimulation platforms. Our\nfindings show that large language models (LLMs) can be effectively incorporated\nas agent components if they follow a structured architecture specific to\nfundamental agent activities such as perception, memory, planning, and action.\nThis integration is precisely consistent with the architecture that we\nformalize, providing a solid platform for next-generation geosimulation\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We provide a comprehensive examination of agent-based approaches that codify\nthe principles and linkages underlying multi-agent systems, simulations, and\ninformation systems. Based on two decades of study, this paper confirms a\nframework intended as a formal specification for geosimulation platforms. Our\nfindings show that large language models (LLMs) can be effectively incorporated\nas agent components if they follow a structured architecture specific to\nfundamental agent activities such as perception, memory, planning, and action.\nThis integration is precisely consistent with the architecture that we\nformalize, providing a solid platform for next-generation geosimulation\nsystems."
                },
                "authors": [
                    {
                        "name": "Virginia Padilla"
                    },
                    {
                        "name": "Jacinto Dvila"
                    }
                ],
                "author_detail": {
                    "name": "Jacinto Dvila"
                },
                "author": "Jacinto Dvila",
                "arxiv_comment": "20 pages, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T42",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23693v1",
                "updated": "2025-07-31T16:11:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    16,
                    11,
                    26,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T16:11:26Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    16,
                    11,
                    26,
                    3,
                    212,
                    0
                ],
                "title": "CFDagent: A Language-Guided, Zero-Shot Multi-Agent System for Complex\n  Flow Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CFDagent: A Language-Guided, Zero-Shot Multi-Agent System for Complex\n  Flow Simulation"
                },
                "summary": "We introduce CFDagent, a zero-shot, multi-agent system that enables fully\nautonomous computational fluid dynamics (CFD) simulations from natural language\nprompts. CFDagent integrates three specialized LLM-driven agents: (i) the\nPreprocessing Agent that generates 3D geometries from textual or visual inputs\nusing a hybrid text-to-3D diffusion model (Point-E) and automatically meshes\nthe geometries; (ii) the Solver Agent that configures and executes an immersed\nboundary flow solver; and (iii) the Postprocessing Agent that analyzes and\nvisualizes the results, including multimodal renderings. These agents are\ninteractively guided by GPT-4o via conversational prompts, enabling intuitive\nand user-friendly interaction. We validate CFDagent by reproducing canonical\nsphere flows at Reynolds numbers of 100 and 300 using three distinct inputs: a\nsimple text prompt (i.e., \"sphere\"), an image-based input, and a standard\nsphere model. The computed drag and lift coefficients from meshes produced by\neach input approach closely match available data. The proposed system enables\nsynthesization of flow simulations and photorealistic visualizations for\ncomplex geometries. Through extensive tests on canonical and realistic\nscenarios, we demonstrate the robustness, versatility, and practical\napplicability of CFDagent. By bridging generative AI with high-fidelity\nsimulations, CFDagent significantly lowers barriers to expert-level CFD,\nunlocking broad opportunities in education, scientific research, and practical\nengineering applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce CFDagent, a zero-shot, multi-agent system that enables fully\nautonomous computational fluid dynamics (CFD) simulations from natural language\nprompts. CFDagent integrates three specialized LLM-driven agents: (i) the\nPreprocessing Agent that generates 3D geometries from textual or visual inputs\nusing a hybrid text-to-3D diffusion model (Point-E) and automatically meshes\nthe geometries; (ii) the Solver Agent that configures and executes an immersed\nboundary flow solver; and (iii) the Postprocessing Agent that analyzes and\nvisualizes the results, including multimodal renderings. These agents are\ninteractively guided by GPT-4o via conversational prompts, enabling intuitive\nand user-friendly interaction. We validate CFDagent by reproducing canonical\nsphere flows at Reynolds numbers of 100 and 300 using three distinct inputs: a\nsimple text prompt (i.e., \"sphere\"), an image-based input, and a standard\nsphere model. The computed drag and lift coefficients from meshes produced by\neach input approach closely match available data. The proposed system enables\nsynthesization of flow simulations and photorealistic visualizations for\ncomplex geometries. Through extensive tests on canonical and realistic\nscenarios, we demonstrate the robustness, versatility, and practical\napplicability of CFDagent. By bridging generative AI with high-fidelity\nsimulations, CFDagent significantly lowers barriers to expert-level CFD,\nunlocking broad opportunities in education, scientific research, and practical\nengineering applications."
                },
                "authors": [
                    {
                        "name": "Zhaoyue Xu"
                    },
                    {
                        "name": "Long Wang"
                    },
                    {
                        "name": "Chunyu Wang"
                    },
                    {
                        "name": "Yixin Chen"
                    },
                    {
                        "name": "Qingyong Luo"
                    },
                    {
                        "name": "Hua-Dong Yao"
                    },
                    {
                        "name": "Shizhao Wang"
                    },
                    {
                        "name": "Guowei He"
                    }
                ],
                "author_detail": {
                    "name": "Guowei He"
                },
                "author": "Guowei He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23692v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23692v1",
                "updated": "2025-07-31T16:09:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    16,
                    9,
                    33,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T16:09:33Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    16,
                    9,
                    33,
                    3,
                    212,
                    0
                ],
                "title": "High-resolution eikonal imaging and uncertainty quantification of the\n  Kilauea caldera",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-resolution eikonal imaging and uncertainty quantification of the\n  Kilauea caldera"
                },
                "summary": "Images of the Earth's interior can provide us with insight into the\nunderlying properties of the Earth, such as how seismic activity might emerge\nand the interplay between seismic and volcanic activity. Understanding these\nsystems requires reliable high-resolution images to understand mechanisms and\nestimate physical quantities. However, reliable images are often difficult to\nobtain due to the non-linear nature of seismic wave propagation and the\nill-posedness of the related inverse problem. Reconstructions rely on good\ninitial estimates as well as hand-crafted priors, which can ultimately bias\nsolutions. In our work, we present a 3D reconstruction of Kilauea's magmatic\nsystem at a previously unattained resolution. Our eikonal tomography procedure\nimproves upon prior imaging results of Kilauea through increased resolution and\nper-pixel uncertainties estimated through variational inference. In particular,\nsolving eikonal imaging using variational inference with stochastic gradient\ndescent enables stable inversion and uncertainty quantification in the absence\nof strong prior knowledge of the velocity structure. Our work makes two key\ncontributions: developing a stochastic eikonal tomography scheme with\nuncertainty quantification and illuminating the structure and melt quantity of\nthe magmatic system that underlies Kilauea.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Images of the Earth's interior can provide us with insight into the\nunderlying properties of the Earth, such as how seismic activity might emerge\nand the interplay between seismic and volcanic activity. Understanding these\nsystems requires reliable high-resolution images to understand mechanisms and\nestimate physical quantities. However, reliable images are often difficult to\nobtain due to the non-linear nature of seismic wave propagation and the\nill-posedness of the related inverse problem. Reconstructions rely on good\ninitial estimates as well as hand-crafted priors, which can ultimately bias\nsolutions. In our work, we present a 3D reconstruction of Kilauea's magmatic\nsystem at a previously unattained resolution. Our eikonal tomography procedure\nimproves upon prior imaging results of Kilauea through increased resolution and\nper-pixel uncertainties estimated through variational inference. In particular,\nsolving eikonal imaging using variational inference with stochastic gradient\ndescent enables stable inversion and uncertainty quantification in the absence\nof strong prior knowledge of the velocity structure. Our work makes two key\ncontributions: developing a stochastic eikonal tomography scheme with\nuncertainty quantification and illuminating the structure and melt quantity of\nthe magmatic system that underlies Kilauea."
                },
                "authors": [
                    {
                        "name": "Angela F. Gao"
                    },
                    {
                        "name": "John D. Wilding"
                    },
                    {
                        "name": "Ettore Biondi"
                    },
                    {
                        "name": "Katherine L. Bouman"
                    },
                    {
                        "name": "Zachary E. Ross"
                    }
                ],
                "author_detail": {
                    "name": "Zachary E. Ross"
                },
                "author": "Zachary E. Ross",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23692v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23692v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23689v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23689v1",
                "updated": "2025-07-31T16:06:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    16,
                    6,
                    45,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T16:06:45Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    16,
                    6,
                    45,
                    3,
                    212,
                    0
                ],
                "title": "Probing graph topology from local quantum measurements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing graph topology from local quantum measurements"
                },
                "summary": "We show that global properties of an unknown quantum network, such as the\naverage degree, hub density, and the number of closed paths of fixed length,\ncan be inferred from strictly local quantum measurements. In particular, we\ndemonstrate that a malicious agent with access to only a small subset of nodes\ncan initialize quantum states locally and, through repeated short-time\nmeasurements, extract sensitive structural information about the entire\nnetwork. The intrusion strategy is inspired by extreme learning and quantum\nreservoir computing and combines short-time quantum evolution with a\nnon-iterative linear readout with trainable weights. These results suggest new\nstrategies for intrusion detection and structural diagnostics in future quantum\nInternet infrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We show that global properties of an unknown quantum network, such as the\naverage degree, hub density, and the number of closed paths of fixed length,\ncan be inferred from strictly local quantum measurements. In particular, we\ndemonstrate that a malicious agent with access to only a small subset of nodes\ncan initialize quantum states locally and, through repeated short-time\nmeasurements, extract sensitive structural information about the entire\nnetwork. The intrusion strategy is inspired by extreme learning and quantum\nreservoir computing and combines short-time quantum evolution with a\nnon-iterative linear readout with trainable weights. These results suggest new\nstrategies for intrusion detection and structural diagnostics in future quantum\nInternet infrastructures."
                },
                "authors": [
                    {
                        "name": "F. Romeo"
                    },
                    {
                        "name": "J. Settino"
                    }
                ],
                "author_detail": {
                    "name": "J. Settino"
                },
                "author": "J. Settino",
                "arxiv_comment": "6 pages, 1 figure, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23689v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23689v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16178v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16178v3",
                "updated": "2025-07-31T15:52:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    52,
                    39,
                    3,
                    212,
                    0
                ],
                "published": "2024-09-24T15:22:04Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    15,
                    22,
                    4,
                    1,
                    268,
                    0
                ],
                "title": "SDFit: 3D Object Pose and Shape by Fitting a Morphable SDF to a Single\n  Image",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SDFit: 3D Object Pose and Shape by Fitting a Morphable SDF to a Single\n  Image"
                },
                "summary": "Recovering 3D object pose and shape from a single image is a challenging and\nill-posed problem. This is due to strong (self-)occlusions, depth ambiguities,\nthe vast intra- and inter-class shape variance, and the lack of 3D ground truth\nfor natural images. Existing deep-network methods are trained on synthetic\ndatasets to predict 3D shapes, so they often struggle generalizing to\nreal-world images. Moreover, they lack an explicit feedback loop for refining\nnoisy estimates, and primarily focus on geometry without directly considering\npixel alignment. To tackle these limitations, we develop a novel\nrender-and-compare optimization framework, called SDFit. This has three key\ninnovations: First, it uses a learned category-specific and morphable\nsigned-distance-function (mSDF) model, and fits this to an image by iteratively\nrefining both 3D pose and shape. The mSDF robustifies inference by constraining\nthe search on the manifold of valid shapes, while allowing for arbitrary shape\ntopologies. Second, SDFit retrieves an initial 3D shape that likely matches the\nimage, by exploiting foundational models for efficient look-up into 3D shape\ndatabases. Third, SDFit initializes pose by establishing rich 2D-3D\ncorrespondences between the image and the mSDF through foundational features.\nWe evaluate SDFit on three image datasets, i.e., Pix3D, Pascal3D+, and COMIC.\nSDFit performs on par with SotA feed-forward networks for unoccluded images and\ncommon poses, but is uniquely robust to occlusions and uncommon poses.\nMoreover, it requires no retraining for unseen images. Thus, SDFit contributes\nnew insights for generalizing in the wild. Code is available at\nhttps://anticdimi.github.io/sdfit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recovering 3D object pose and shape from a single image is a challenging and\nill-posed problem. This is due to strong (self-)occlusions, depth ambiguities,\nthe vast intra- and inter-class shape variance, and the lack of 3D ground truth\nfor natural images. Existing deep-network methods are trained on synthetic\ndatasets to predict 3D shapes, so they often struggle generalizing to\nreal-world images. Moreover, they lack an explicit feedback loop for refining\nnoisy estimates, and primarily focus on geometry without directly considering\npixel alignment. To tackle these limitations, we develop a novel\nrender-and-compare optimization framework, called SDFit. This has three key\ninnovations: First, it uses a learned category-specific and morphable\nsigned-distance-function (mSDF) model, and fits this to an image by iteratively\nrefining both 3D pose and shape. The mSDF robustifies inference by constraining\nthe search on the manifold of valid shapes, while allowing for arbitrary shape\ntopologies. Second, SDFit retrieves an initial 3D shape that likely matches the\nimage, by exploiting foundational models for efficient look-up into 3D shape\ndatabases. Third, SDFit initializes pose by establishing rich 2D-3D\ncorrespondences between the image and the mSDF through foundational features.\nWe evaluate SDFit on three image datasets, i.e., Pix3D, Pascal3D+, and COMIC.\nSDFit performs on par with SotA feed-forward networks for unoccluded images and\ncommon poses, but is uniquely robust to occlusions and uncommon poses.\nMoreover, it requires no retraining for unseen images. Thus, SDFit contributes\nnew insights for generalizing in the wild. Code is available at\nhttps://anticdimi.github.io/sdfit."
                },
                "authors": [
                    {
                        "name": "Dimitrije Anti"
                    },
                    {
                        "name": "Georgios Paschalidis"
                    },
                    {
                        "name": "Shashank Tripathi"
                    },
                    {
                        "name": "Theo Gevers"
                    },
                    {
                        "name": "Sai Kumar Dwivedi"
                    },
                    {
                        "name": "Dimitrios Tzionas"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Tzionas"
                },
                "author": "Dimitrios Tzionas",
                "arxiv_comment": "ICCV'25 Camera Ready; 12 pages, 11 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16178v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16178v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23676v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23676v1",
                "updated": "2025-07-31T15:51:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    51,
                    41,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T15:51:41Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    51,
                    41,
                    3,
                    212,
                    0
                ],
                "title": "DepMicroDiff: Diffusion-Based Dependency-Aware Multimodal Imputation for\n  Microbiome Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DepMicroDiff: Diffusion-Based Dependency-Aware Multimodal Imputation for\n  Microbiome Data"
                },
                "summary": "Microbiome data analysis is essential for understanding host health and\ndisease, yet its inherent sparsity and noise pose major challenges for accurate\nimputation, hindering downstream tasks such as biomarker discovery. Existing\nimputation methods, including recent diffusion-based models, often fail to\ncapture the complex interdependencies between microbial taxa and overlook\ncontextual metadata that can inform imputation. We introduce DepMicroDiff, a\nnovel framework that combines diffusion-based generative modeling with a\nDependency-Aware Transformer (DAT) to explicitly capture both mutual pairwise\ndependencies and autoregressive relationships. DepMicroDiff is further enhanced\nby VAE-based pretraining across diverse cancer datasets and conditioning on\npatient metadata encoded via a large language model (LLM). Experiments on TCGA\nmicrobiome datasets show that DepMicroDiff substantially outperforms\nstate-of-the-art baselines, achieving higher Pearson correlation (up to 0.712),\ncosine similarity (up to 0.812), and lower RMSE and MAE across multiple cancer\ntypes, demonstrating its robustness and generalizability for microbiome\nimputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microbiome data analysis is essential for understanding host health and\ndisease, yet its inherent sparsity and noise pose major challenges for accurate\nimputation, hindering downstream tasks such as biomarker discovery. Existing\nimputation methods, including recent diffusion-based models, often fail to\ncapture the complex interdependencies between microbial taxa and overlook\ncontextual metadata that can inform imputation. We introduce DepMicroDiff, a\nnovel framework that combines diffusion-based generative modeling with a\nDependency-Aware Transformer (DAT) to explicitly capture both mutual pairwise\ndependencies and autoregressive relationships. DepMicroDiff is further enhanced\nby VAE-based pretraining across diverse cancer datasets and conditioning on\npatient metadata encoded via a large language model (LLM). Experiments on TCGA\nmicrobiome datasets show that DepMicroDiff substantially outperforms\nstate-of-the-art baselines, achieving higher Pearson correlation (up to 0.712),\ncosine similarity (up to 0.812), and lower RMSE and MAE across multiple cancer\ntypes, demonstrating its robustness and generalizability for microbiome\nimputation."
                },
                "authors": [
                    {
                        "name": "Rabeya Tus Sadia"
                    },
                    {
                        "name": "Qiang Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Cheng"
                },
                "author": "Qiang Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23676v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23676v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06946v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06946v3",
                "updated": "2025-07-31T15:51:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    51,
                    12,
                    3,
                    212,
                    0
                ],
                "published": "2024-12-09T19:45:30Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    19,
                    45,
                    30,
                    0,
                    344,
                    0
                ],
                "title": "A Deep Learning Powered Numerical Relativity Surrogate for Binary Black\n  Hole Waveforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Deep Learning Powered Numerical Relativity Surrogate for Binary Black\n  Hole Waveforms"
                },
                "summary": "Gravitational-wave approximants are essential for gravitational-wave\nastronomy, allowing the coverage binary black hole parameter space for\ninference or match filtering without costly numerical relativity (NR)\nsimulations, but generally trading some accuracy for computational efficiency.\nTo reduce this trade-off, NR surrogate models can be constructed using\ninterpolation within NR waveform space. We present a 2-stage training approach\nfor neural network-based NR surrogate models. Initially trained on\napproximant-generated waveforms and then fine-tuned with NR data, these\ndual-stage artificial neural surrogate (\\texttt{DANSur}) models offer rapid and\ncompetitively accurate waveform generation, generating millions in under 20ms\non a GPU while keeping mean mismatches with NR around $10^{-4}$. Implemented in\nthe \\textsc{bilby} framework, we show they can be used for parameter estimation\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gravitational-wave approximants are essential for gravitational-wave\nastronomy, allowing the coverage binary black hole parameter space for\ninference or match filtering without costly numerical relativity (NR)\nsimulations, but generally trading some accuracy for computational efficiency.\nTo reduce this trade-off, NR surrogate models can be constructed using\ninterpolation within NR waveform space. We present a 2-stage training approach\nfor neural network-based NR surrogate models. Initially trained on\napproximant-generated waveforms and then fine-tuned with NR data, these\ndual-stage artificial neural surrogate (\\texttt{DANSur}) models offer rapid and\ncompetitively accurate waveform generation, generating millions in under 20ms\non a GPU while keeping mean mismatches with NR around $10^{-4}$. Implemented in\nthe \\textsc{bilby} framework, we show they can be used for parameter estimation\ntasks."
                },
                "authors": [
                    {
                        "name": "Osvaldo Gramaxo Freitas"
                    },
                    {
                        "name": "Anastasios Theodoropoulos"
                    },
                    {
                        "name": "Nino Villanueva"
                    },
                    {
                        "name": "Tiago Fernandes"
                    },
                    {
                        "name": "Solange Nunes"
                    },
                    {
                        "name": "Jos A. Font"
                    },
                    {
                        "name": "Antonio Onofre"
                    },
                    {
                        "name": "Alejandro Torres-Forn"
                    },
                    {
                        "name": "Jos D. Martin-Guerrero"
                    }
                ],
                "author_detail": {
                    "name": "Jos D. Martin-Guerrero"
                },
                "author": "Jos D. Martin-Guerrero",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06946v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06946v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23675v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23675v1",
                "updated": "2025-07-31T15:51:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    51,
                    10,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T15:51:10Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    51,
                    10,
                    3,
                    212,
                    0
                ],
                "title": "One-Step Flow Policy Mirror Descent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One-Step Flow Policy Mirror Descent"
                },
                "summary": "Diffusion policies have achieved great success in online reinforcement\nlearning (RL) due to their strong expressive capacity. However, the inference\nof diffusion policy models relies on a slow iterative sampling process, which\nlimits their responsiveness. To overcome this limitation, we propose Flow\nPolicy Mirror Descent (FPMD), an online RL algorithm that enables 1-step\nsampling during policy inference. Our approach exploits a theoretical\nconnection between the distribution variance and the discretization error of\nsingle-step sampling in straight interpolation flow matching models, and\nrequires no extra distillation or consistency training. We present two\nalgorithm variants based on flow policy and MeanFlow policy parametrizations,\nrespectively. Extensive empirical evaluations on MuJoCo benchmarks demonstrate\nthat our algorithms show strong performance comparable to diffusion policy\nbaselines while requiring hundreds of times fewer function evaluations during\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion policies have achieved great success in online reinforcement\nlearning (RL) due to their strong expressive capacity. However, the inference\nof diffusion policy models relies on a slow iterative sampling process, which\nlimits their responsiveness. To overcome this limitation, we propose Flow\nPolicy Mirror Descent (FPMD), an online RL algorithm that enables 1-step\nsampling during policy inference. Our approach exploits a theoretical\nconnection between the distribution variance and the discretization error of\nsingle-step sampling in straight interpolation flow matching models, and\nrequires no extra distillation or consistency training. We present two\nalgorithm variants based on flow policy and MeanFlow policy parametrizations,\nrespectively. Extensive empirical evaluations on MuJoCo benchmarks demonstrate\nthat our algorithms show strong performance comparable to diffusion policy\nbaselines while requiring hundreds of times fewer function evaluations during\ninference."
                },
                "authors": [
                    {
                        "name": "Tianyi Chen"
                    },
                    {
                        "name": "Haitong Ma"
                    },
                    {
                        "name": "Na Li"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Bo Dai"
                    }
                ],
                "author_detail": {
                    "name": "Bo Dai"
                },
                "author": "Bo Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23675v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23675v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23674v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23674v1",
                "updated": "2025-07-31T15:50:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    50,
                    57,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T15:50:57Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    50,
                    57,
                    3,
                    212,
                    0
                ],
                "title": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses"
                },
                "summary": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience."
                },
                "authors": [
                    {
                        "name": "Muhammad Taha Cheema"
                    },
                    {
                        "name": "Abeer Aamir"
                    },
                    {
                        "name": "Khawaja Gul Muhammad"
                    },
                    {
                        "name": "Naveed Anwar Bhatti"
                    },
                    {
                        "name": "Ihsan Ayyub Qazi"
                    },
                    {
                        "name": "Zafar Ayyub Qazi"
                    }
                ],
                "author_detail": {
                    "name": "Zafar Ayyub Qazi"
                },
                "author": "Zafar Ayyub Qazi",
                "arxiv_comment": "13 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23674v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08540v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08540v2",
                "updated": "2025-07-31T15:49:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    49,
                    27,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-11T12:39:25Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    39,
                    25,
                    4,
                    192,
                    0
                ],
                "title": "White-Basilisk: A Hybrid Model for Code Vulnerability Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "White-Basilisk: A Hybrid Model for Code Vulnerability Detection"
                },
                "summary": "The proliferation of software vulnerabilities presents a significant\nchallenge to cybersecurity, necessitating more effective detection\nmethodologies. We introduce White-Basilisk, a novel approach to vulnerability\ndetection that demonstrates superior performance while challenging prevailing\nassumptions in AI model scaling. Utilizing an innovative architecture that\nintegrates Mamba layers, linear self-attention, and a Mixture of Experts\nframework, White-Basilisk achieves state-of-the-art results in vulnerability\ndetection tasks with a parameter count of only 200M. The model's capacity to\nprocess sequences of unprecedented length enables comprehensive analysis of\nextensive codebases in a single pass, surpassing the context limitations of\ncurrent Large Language Models (LLMs). White-Basilisk exhibits robust\nperformance on imbalanced, real-world datasets, while maintaining computational\nefficiency that facilitates deployment across diverse organizational scales.\nThis research not only establishes new benchmarks in code security but also\nprovides empirical evidence that compact, efficiently designed models can\noutperform larger counterparts in specialized tasks, potentially redefining\noptimization strategies in AI development for domain-specific applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of software vulnerabilities presents a significant\nchallenge to cybersecurity, necessitating more effective detection\nmethodologies. We introduce White-Basilisk, a novel approach to vulnerability\ndetection that demonstrates superior performance while challenging prevailing\nassumptions in AI model scaling. Utilizing an innovative architecture that\nintegrates Mamba layers, linear self-attention, and a Mixture of Experts\nframework, White-Basilisk achieves state-of-the-art results in vulnerability\ndetection tasks with a parameter count of only 200M. The model's capacity to\nprocess sequences of unprecedented length enables comprehensive analysis of\nextensive codebases in a single pass, surpassing the context limitations of\ncurrent Large Language Models (LLMs). White-Basilisk exhibits robust\nperformance on imbalanced, real-world datasets, while maintaining computational\nefficiency that facilitates deployment across diverse organizational scales.\nThis research not only establishes new benchmarks in code security but also\nprovides empirical evidence that compact, efficiently designed models can\noutperform larger counterparts in specialized tasks, potentially redefining\noptimization strategies in AI development for domain-specific applications."
                },
                "authors": [
                    {
                        "name": "Ioannis Lamprou"
                    },
                    {
                        "name": "Alexander Shevtsov"
                    },
                    {
                        "name": "Ioannis Arapakis"
                    },
                    {
                        "name": "Sotiris Ioannidis"
                    }
                ],
                "author_detail": {
                    "name": "Sotiris Ioannidis"
                },
                "author": "Sotiris Ioannidis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08540v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08540v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02380v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02380v3",
                "updated": "2025-07-31T15:48:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    48,
                    12,
                    3,
                    212,
                    0
                ],
                "published": "2024-11-04T18:48:18Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    48,
                    18,
                    0,
                    309,
                    0
                ],
                "title": "An approach to robust Bayesian regression in astronomy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An approach to robust Bayesian regression in astronomy"
                },
                "summary": "Model mis-specification (e.g. the presence of outliers) is commonly\nencountered in astronomical analyses, often requiring the use of ad hoc\nalgorithms which are sensitive to arbitrary thresholds (e.g. sigma-clipping).\nFor any given dataset, the optimal approach will be to develop a bespoke\nstatistical model of the data generation and measurement processes, but these\ncome with a development cost; there is hence utility in having generic\nmodelling approaches that are both principled and robust to model\nmis-specification. Here we develop and implement a generic Bayesian approach to\nlinear regression, based on Student's t-distributions, that is robust to\noutliers and mis-specification of the noise model. Our method is validated\nusing simulated datasets with various degrees of model mis-specification; the\nderived constraints are shown to be systematically less biased than those from\na similar model using normal distributions. We demonstrate that, for a dataset\nwithout outliers, a worst-case inference using t-distributions would give\nunbiased results with $\\lesssim\\!10$ per cent increase in the reported\nparameter uncertainties. We also compare with existing analyses of real-world\ndatasets, finding qualitatively different results where normal distributions\nhave been used and agreement where more robust methods have been applied. A\nPython implementation of this model, t-cup, is made available for others to\nuse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model mis-specification (e.g. the presence of outliers) is commonly\nencountered in astronomical analyses, often requiring the use of ad hoc\nalgorithms which are sensitive to arbitrary thresholds (e.g. sigma-clipping).\nFor any given dataset, the optimal approach will be to develop a bespoke\nstatistical model of the data generation and measurement processes, but these\ncome with a development cost; there is hence utility in having generic\nmodelling approaches that are both principled and robust to model\nmis-specification. Here we develop and implement a generic Bayesian approach to\nlinear regression, based on Student's t-distributions, that is robust to\noutliers and mis-specification of the noise model. Our method is validated\nusing simulated datasets with various degrees of model mis-specification; the\nderived constraints are shown to be systematically less biased than those from\na similar model using normal distributions. We demonstrate that, for a dataset\nwithout outliers, a worst-case inference using t-distributions would give\nunbiased results with $\\lesssim\\!10$ per cent increase in the reported\nparameter uncertainties. We also compare with existing analyses of real-world\ndatasets, finding qualitatively different results where normal distributions\nhave been used and agreement where more robust methods have been applied. A\nPython implementation of this model, t-cup, is made available for others to\nuse."
                },
                "authors": [
                    {
                        "name": "William Martin"
                    },
                    {
                        "name": "Daniel J. Mortlock"
                    }
                ],
                "author_detail": {
                    "name": "Daniel J. Mortlock"
                },
                "author": "Daniel J. Mortlock",
                "arxiv_comment": "16 pages, 21 figures; accepted for publication in RASTI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02380v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02380v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21903v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21903v2",
                "updated": "2025-07-31T15:33:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    33,
                    32,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-29T15:14:39Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    15,
                    14,
                    39,
                    1,
                    210,
                    0
                ],
                "title": "Who's important? -- SUnSET: Synergistic Understanding of Stakeholder,\n  Events and Time for Timeline Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Who's important? -- SUnSET: Synergistic Understanding of Stakeholder,\n  Events and Time for Timeline Generation"
                },
                "summary": "As news reporting becomes increasingly global and decentralized online,\ntracking related events across multiple sources presents significant\nchallenges. Existing news summarization methods typically utilizes Large\nLanguage Models and Graphical methods on article-based summaries. However, this\nis not effective since it only considers the textual content of similarly dated\narticles to understand the gist of the event. To counteract the lack of\nanalysis on the parties involved, it is essential to come up with a novel\nframework to gauge the importance of stakeholders and the connection of related\nevents through the relevant entities involved. Therefore, we present SUnSET:\nSynergistic Understanding of Stakeholder, Events and Time for the task of\nTimeline Summarization (TLS). We leverage powerful Large Language Models (LLMs)\nto build SET triplets and introduced the use of stakeholder-based ranking to\nconstruct a $Relevancy$ metric, which can be extended into general situations.\nOur experimental results outperform all prior baselines and emerged as the new\nState-of-the-Art, highlighting the impact of stakeholder information within\nnews article.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As news reporting becomes increasingly global and decentralized online,\ntracking related events across multiple sources presents significant\nchallenges. Existing news summarization methods typically utilizes Large\nLanguage Models and Graphical methods on article-based summaries. However, this\nis not effective since it only considers the textual content of similarly dated\narticles to understand the gist of the event. To counteract the lack of\nanalysis on the parties involved, it is essential to come up with a novel\nframework to gauge the importance of stakeholders and the connection of related\nevents through the relevant entities involved. Therefore, we present SUnSET:\nSynergistic Understanding of Stakeholder, Events and Time for the task of\nTimeline Summarization (TLS). We leverage powerful Large Language Models (LLMs)\nto build SET triplets and introduced the use of stakeholder-based ranking to\nconstruct a $Relevancy$ metric, which can be extended into general situations.\nOur experimental results outperform all prior baselines and emerged as the new\nState-of-the-Art, highlighting the impact of stakeholder information within\nnews article."
                },
                "authors": [
                    {
                        "name": "Tiviatis Sim"
                    },
                    {
                        "name": "Kaiwen Yang"
                    },
                    {
                        "name": "Shen Xin"
                    },
                    {
                        "name": "Kenji Kawaguchi"
                    }
                ],
                "author_detail": {
                    "name": "Kenji Kawaguchi"
                },
                "author": "Kenji Kawaguchi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21903v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21903v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18102v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18102v2",
                "updated": "2025-07-31T15:25:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    25,
                    9,
                    3,
                    212,
                    0
                ],
                "published": "2025-05-23T16:57:34Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    16,
                    57,
                    34,
                    4,
                    143,
                    0
                ],
                "title": "How Can I Publish My LLM Benchmark Without Giving the True Answers Away?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Can I Publish My LLM Benchmark Without Giving the True Answers Away?"
                },
                "summary": "Publishing a large language model (LLM) benchmark on the Internet risks\ncontaminating future LLMs: the benchmark may be unintentionally (or\nintentionally) used to train or select a model. A common mitigation is to keep\nthe benchmark private and let participants submit their models or predictions\nto the organizers. However, this strategy will require trust in a single\norganization and still permits test-set overfitting through repeated queries.\nTo overcome this issue, we propose a way to publish benchmarks without\ncompletely disclosing the ground-truth answers to the questions, while still\nmaintaining the ability to openly evaluate LLMs. Our main idea is to inject\nrandomness to the answers by preparing several logically correct answers, and\nonly include one of them as the solution in the benchmark. This reduces the\nbest possible accuracy, i.e., Bayes accuracy, of the benchmark. Not only is\nthis helpful to keep us from disclosing the ground truth, but this approach\nalso offers a test for detecting data contamination. In principle, even fully\ncapable models should not surpass the Bayes accuracy. If a model surpasses this\nceiling despite this expectation, this is a strong signal of data\ncontamination. We present experimental evidence that our method can detect data\ncontamination accurately on a wide range of benchmarks, models, and training\nmethodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Publishing a large language model (LLM) benchmark on the Internet risks\ncontaminating future LLMs: the benchmark may be unintentionally (or\nintentionally) used to train or select a model. A common mitigation is to keep\nthe benchmark private and let participants submit their models or predictions\nto the organizers. However, this strategy will require trust in a single\norganization and still permits test-set overfitting through repeated queries.\nTo overcome this issue, we propose a way to publish benchmarks without\ncompletely disclosing the ground-truth answers to the questions, while still\nmaintaining the ability to openly evaluate LLMs. Our main idea is to inject\nrandomness to the answers by preparing several logically correct answers, and\nonly include one of them as the solution in the benchmark. This reduces the\nbest possible accuracy, i.e., Bayes accuracy, of the benchmark. Not only is\nthis helpful to keep us from disclosing the ground truth, but this approach\nalso offers a test for detecting data contamination. In principle, even fully\ncapable models should not surpass the Bayes accuracy. If a model surpasses this\nceiling despite this expectation, this is a strong signal of data\ncontamination. We present experimental evidence that our method can detect data\ncontamination accurately on a wide range of benchmarks, models, and training\nmethodologies."
                },
                "authors": [
                    {
                        "name": "Takashi Ishida"
                    },
                    {
                        "name": "Thanawat Lodkaew"
                    },
                    {
                        "name": "Ikko Yamane"
                    }
                ],
                "author_detail": {
                    "name": "Ikko Yamane"
                },
                "author": "Ikko Yamane",
                "arxiv_comment": "Extended version of the paper presented as an Oral at the ICML 2025\n  Workshop on the Impact of Memorization on Trustworthy Foundation Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18102v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18102v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23633v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23633v1",
                "updated": "2025-07-31T15:11:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    11,
                    38,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T15:11:38Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    11,
                    38,
                    3,
                    212,
                    0
                ],
                "title": "MemoCue: Empowering LLM-Based Agents for Human Memory Recall via\n  Strategy-Guided Querying",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemoCue: Empowering LLM-Based Agents for Human Memory Recall via\n  Strategy-Guided Querying"
                },
                "summary": "Agent-assisted memory recall is one critical research problem in the field of\nhuman-computer interaction. In conventional methods, the agent can retrieve\ninformation from its equipped memory module to help the person recall\nincomplete or vague memories. The limited size of memory module hinders the\nacquisition of complete memories and impacts the memory recall performance in\npractice. Memory theories suggest that the person's relevant memory can be\nproactively activated through some effective cues. Inspired by this, we propose\na novel strategy-guided agent-assisted memory recall method, allowing the agent\nto transform an original query into a cue-rich one via the judiciously designed\nstrategy to help the person recall memories. To this end, there are two key\nchallenges. (1) How to choose the appropriate recall strategy for diverse\nforgetting scenarios with distinct memory-recall characteristics? (2) How to\nobtain the high-quality responses leveraging recall strategies, given only\nabstract and sparsely annotated strategy patterns? To address the challenges,\nwe propose a Recall Router framework. Specifically, we design a 5W Recall Map\nto classify memory queries into five typical scenarios and define fifteen\nrecall strategy patterns across the corresponding scenarios. We then propose a\nhierarchical recall tree combined with the Monte Carlo Tree Search algorithm to\noptimize the selection of strategy and the generation of strategy responses. We\nconstruct an instruction tuning dataset and fine-tune multiple open-source\nlarge language models (LLMs) to develop MemoCue, an agent that excels in\nproviding memory-inspired responses. Experiments on three representative\ndatasets show that MemoCue surpasses LLM-based methods by 17.74% in recall\ninspiration. Further human evaluation highlights its advantages in\nmemory-recall applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent-assisted memory recall is one critical research problem in the field of\nhuman-computer interaction. In conventional methods, the agent can retrieve\ninformation from its equipped memory module to help the person recall\nincomplete or vague memories. The limited size of memory module hinders the\nacquisition of complete memories and impacts the memory recall performance in\npractice. Memory theories suggest that the person's relevant memory can be\nproactively activated through some effective cues. Inspired by this, we propose\na novel strategy-guided agent-assisted memory recall method, allowing the agent\nto transform an original query into a cue-rich one via the judiciously designed\nstrategy to help the person recall memories. To this end, there are two key\nchallenges. (1) How to choose the appropriate recall strategy for diverse\nforgetting scenarios with distinct memory-recall characteristics? (2) How to\nobtain the high-quality responses leveraging recall strategies, given only\nabstract and sparsely annotated strategy patterns? To address the challenges,\nwe propose a Recall Router framework. Specifically, we design a 5W Recall Map\nto classify memory queries into five typical scenarios and define fifteen\nrecall strategy patterns across the corresponding scenarios. We then propose a\nhierarchical recall tree combined with the Monte Carlo Tree Search algorithm to\noptimize the selection of strategy and the generation of strategy responses. We\nconstruct an instruction tuning dataset and fine-tune multiple open-source\nlarge language models (LLMs) to develop MemoCue, an agent that excels in\nproviding memory-inspired responses. Experiments on three representative\ndatasets show that MemoCue surpasses LLM-based methods by 17.74% in recall\ninspiration. Further human evaluation highlights its advantages in\nmemory-recall applications."
                },
                "authors": [
                    {
                        "name": "Qian Zhao"
                    },
                    {
                        "name": "Zhuo Sun"
                    },
                    {
                        "name": "Bin Guo"
                    },
                    {
                        "name": "Zhiwen Yu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiwen Yu"
                },
                "author": "Zhiwen Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23633v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23633v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23611v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23611v1",
                "updated": "2025-07-31T14:49:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    14,
                    49,
                    3,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T14:49:03Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    14,
                    49,
                    3,
                    3,
                    212,
                    0
                ],
                "title": "LLM-Based Identification of Infostealer Infection Vectors from\n  Screenshots: The Case of Aurora",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Based Identification of Infostealer Infection Vectors from\n  Screenshots: The Case of Aurora"
                },
                "summary": "Infostealers exfiltrate credentials, session cookies, and sensitive data from\ninfected systems. With over 29 million stealer logs reported in 2024, manual\nanalysis and mitigation at scale are virtually unfeasible/unpractical. While\nmost research focuses on proactive malware detection, a significant gap remains\nin leveraging reactive analysis of stealer logs and their associated artifacts.\nSpecifically, infection artifacts such as screenshots, image captured at the\npoint of compromise, are largely overlooked by the current literature. This\npaper introduces a novel approach leveraging Large Language Models (LLMs), more\nspecifically gpt-4o-mini, to analyze infection screenshots to extract potential\nIndicators of Compromise (IoCs), map infection vectors, and track campaigns.\nFocusing on the Aurora infostealer, we demonstrate how LLMs can process\nscreenshots to identify infection vectors, such as malicious URLs, installer\nfiles, and exploited software themes. Our method extracted 337 actionable URLs\nand 246 relevant files from 1000 screenshots, revealing key malware\ndistribution methods and social engineering tactics. By correlating extracted\nfilenames, URLs, and infection themes, we identified three distinct malware\ncampaigns, demonstrating the potential of LLM-driven analysis for uncovering\ninfection workflows and enhancing threat intelligence. By shifting malware\nanalysis from traditional log-based detection methods to a reactive,\nartifact-driven approach that leverages infection screenshots, this research\npresents a scalable method for identifying infection vectors and enabling early\nintervention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Infostealers exfiltrate credentials, session cookies, and sensitive data from\ninfected systems. With over 29 million stealer logs reported in 2024, manual\nanalysis and mitigation at scale are virtually unfeasible/unpractical. While\nmost research focuses on proactive malware detection, a significant gap remains\nin leveraging reactive analysis of stealer logs and their associated artifacts.\nSpecifically, infection artifacts such as screenshots, image captured at the\npoint of compromise, are largely overlooked by the current literature. This\npaper introduces a novel approach leveraging Large Language Models (LLMs), more\nspecifically gpt-4o-mini, to analyze infection screenshots to extract potential\nIndicators of Compromise (IoCs), map infection vectors, and track campaigns.\nFocusing on the Aurora infostealer, we demonstrate how LLMs can process\nscreenshots to identify infection vectors, such as malicious URLs, installer\nfiles, and exploited software themes. Our method extracted 337 actionable URLs\nand 246 relevant files from 1000 screenshots, revealing key malware\ndistribution methods and social engineering tactics. By correlating extracted\nfilenames, URLs, and infection themes, we identified three distinct malware\ncampaigns, demonstrating the potential of LLM-driven analysis for uncovering\ninfection workflows and enhancing threat intelligence. By shifting malware\nanalysis from traditional log-based detection methods to a reactive,\nartifact-driven approach that leverages infection screenshots, this research\npresents a scalable method for identifying infection vectors and enabling early\nintervention."
                },
                "authors": [
                    {
                        "name": "Estelle Ruellan"
                    },
                    {
                        "name": "Eric Clay"
                    },
                    {
                        "name": "Nicholas Ascoli"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas Ascoli"
                },
                "author": "Nicholas Ascoli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23611v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23611v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23599v1",
                "updated": "2025-07-31T14:39:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    14,
                    39,
                    31,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T14:39:31Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    14,
                    39,
                    31,
                    3,
                    212,
                    0
                ],
                "title": "DA-Occ: Efficient 3D Voxel Occupancy Prediction via Directional 2D for\n  Geometric Structure Preservation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DA-Occ: Efficient 3D Voxel Occupancy Prediction via Directional 2D for\n  Geometric Structure Preservation"
                },
                "summary": "Efficient and high-accuracy 3D occupancy prediction is crucial for ensuring\nthe performance of autonomous driving (AD) systems. However, many current\nmethods focus on high accuracy at the expense of real-time processing needs. To\naddress this challenge of balancing accuracy and inference speed, we propose a\ndirectional pure 2D approach. Our method involves slicing 3D voxel features to\npreserve complete vertical geometric information. This strategy compensates for\nthe loss of height cues in Bird's-Eye View (BEV) representations, thereby\nmaintaining the integrity of the 3D geometric structure. By employing a\ndirectional attention mechanism, we efficiently extract geometric features from\ndifferent orientations, striking a balance between accuracy and computational\nefficiency. Experimental results highlight the significant advantages of our\napproach for autonomous driving. On the Occ3D-nuScenes, the proposed method\nachieves an mIoU of 39.3% and an inference speed of 27.7 FPS, effectively\nbalancing accuracy and efficiency. In simulations on edge devices, the\ninference speed reaches 14.8 FPS, further demonstrating the method's\napplicability for real-time deployment in resource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and high-accuracy 3D occupancy prediction is crucial for ensuring\nthe performance of autonomous driving (AD) systems. However, many current\nmethods focus on high accuracy at the expense of real-time processing needs. To\naddress this challenge of balancing accuracy and inference speed, we propose a\ndirectional pure 2D approach. Our method involves slicing 3D voxel features to\npreserve complete vertical geometric information. This strategy compensates for\nthe loss of height cues in Bird's-Eye View (BEV) representations, thereby\nmaintaining the integrity of the 3D geometric structure. By employing a\ndirectional attention mechanism, we efficiently extract geometric features from\ndifferent orientations, striking a balance between accuracy and computational\nefficiency. Experimental results highlight the significant advantages of our\napproach for autonomous driving. On the Occ3D-nuScenes, the proposed method\nachieves an mIoU of 39.3% and an inference speed of 27.7 FPS, effectively\nbalancing accuracy and efficiency. In simulations on edge devices, the\ninference speed reaches 14.8 FPS, further demonstrating the method's\napplicability for real-time deployment in resource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Yuchen Zhou"
                    },
                    {
                        "name": "Yan Luo"
                    },
                    {
                        "name": "Xiangang Wang"
                    },
                    {
                        "name": "Xingjian Gu"
                    },
                    {
                        "name": "Mingzhou Lu"
                    }
                ],
                "author_detail": {
                    "name": "Mingzhou Lu"
                },
                "author": "Mingzhou Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15299v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15299v3",
                "updated": "2025-07-31T14:38:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    14,
                    38,
                    33,
                    3,
                    212,
                    0
                ],
                "published": "2025-03-19T15:21:48Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    21,
                    48,
                    2,
                    78,
                    0
                ],
                "title": "Inside-Out: Hidden Factual Knowledge in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inside-Out: Hidden Factual Knowledge in LLMs"
                },
                "summary": "This work presents a framework for assessing whether large language models\n(LLMs) encode more factual knowledge in their parameters than what they express\nin their outputs. While a few studies hint at this possibility, none has\nclearly defined or demonstrated this phenomenon. We first propose a formal\ndefinition of knowledge, quantifying it for a given question as the fraction of\ncorrect-incorrect answer pairs where the correct one is ranked higher. This\ngives rise to external and internal knowledge, depending on the information\nused to score individual answer candidates: either the model's observable\ntoken-level probabilities or its intermediate computations. Hidden knowledge\narises when internal knowledge exceeds external knowledge. We then present a\ncase study, applying this framework to three popular open-weights LLMs in a\nclosed-book QA setup. Our results indicate that: (1) LLMs consistently encode\nmore factual knowledge internally than what they express externally, with an\naverage relative gap of 40%. (2) Surprisingly, some knowledge is so deeply\nhidden that a model can internally know an answer perfectly, yet fail to\ngenerate it even once, despite large-scale repeated sampling of 1,000 answers.\nThis reveals fundamental limitations in the generation capabilities of LLMs,\nwhich (3) put a practical constraint on scaling test-time compute via repeated\nanswer sampling in closed-book QA: significant performance improvements remain\ninaccessible because some answers are practically never sampled, yet if they\nwere, we would be guaranteed to rank them first.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a framework for assessing whether large language models\n(LLMs) encode more factual knowledge in their parameters than what they express\nin their outputs. While a few studies hint at this possibility, none has\nclearly defined or demonstrated this phenomenon. We first propose a formal\ndefinition of knowledge, quantifying it for a given question as the fraction of\ncorrect-incorrect answer pairs where the correct one is ranked higher. This\ngives rise to external and internal knowledge, depending on the information\nused to score individual answer candidates: either the model's observable\ntoken-level probabilities or its intermediate computations. Hidden knowledge\narises when internal knowledge exceeds external knowledge. We then present a\ncase study, applying this framework to three popular open-weights LLMs in a\nclosed-book QA setup. Our results indicate that: (1) LLMs consistently encode\nmore factual knowledge internally than what they express externally, with an\naverage relative gap of 40%. (2) Surprisingly, some knowledge is so deeply\nhidden that a model can internally know an answer perfectly, yet fail to\ngenerate it even once, despite large-scale repeated sampling of 1,000 answers.\nThis reveals fundamental limitations in the generation capabilities of LLMs,\nwhich (3) put a practical constraint on scaling test-time compute via repeated\nanswer sampling in closed-book QA: significant performance improvements remain\ninaccessible because some answers are practically never sampled, yet if they\nwere, we would be guaranteed to rank them first."
                },
                "authors": [
                    {
                        "name": "Zorik Gekhman"
                    },
                    {
                        "name": "Eyal Ben David"
                    },
                    {
                        "name": "Hadas Orgad"
                    },
                    {
                        "name": "Eran Ofek"
                    },
                    {
                        "name": "Yonatan Belinkov"
                    },
                    {
                        "name": "Idan Szpektor"
                    },
                    {
                        "name": "Jonathan Herzig"
                    },
                    {
                        "name": "Roi Reichart"
                    }
                ],
                "author_detail": {
                    "name": "Roi Reichart"
                },
                "author": "Roi Reichart",
                "arxiv_comment": "Accepted to COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15299v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15299v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20451v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20451v2",
                "updated": "2025-07-31T14:36:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    14,
                    36,
                    30,
                    3,
                    212,
                    0
                ],
                "published": "2024-12-29T12:24:31Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    12,
                    24,
                    31,
                    6,
                    364,
                    0
                ],
                "title": "CoA-VLA: Improving Vision-Language-Action Models via Visual-Textual\n  Chain-of-Affordance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoA-VLA: Improving Vision-Language-Action Models via Visual-Textual\n  Chain-of-Affordance"
                },
                "summary": "Robot foundation models, particularly Vision-Language-Action (VLA) models,\nhave garnered significant attention for their ability to enhance robot policy\nlearning, greatly improving robot's generalization and robustness. OpenAI's\nrecent model, O1, showcased impressive capabilities in solving complex problems\nby utilizing extensive reasoning chains. This prompts an important question:\ncan robot models achieve better performance in multi-task , complex\nenvironments by reviewing prior observations and then providing task-specific\nreasoning to guide action prediction? In this paper, we introduce\nChain-of-Affordance (CoA-VLA) , a novel approach to scaling robot models by\nincorporating reasoning in the format of sequential robot affordances to\nfacilitate task completion. Specifically, we prompt the model to consider the\nfollowing four types of affordances before taking action: (1) object affordance\n- what object to manipulate and where it is ; (2) grasp affordance - the\nspecific object part to grasp ; (3) spatial affordance - the optimal space to\nplace the object ; and (4) movement affordance-the collision - free path for\nmovement. We further transform each affordance into two prompting formats:\nvisual affordance and textual affordance. We introduce a novel vision-language\nco-injection module that integrates this knowledge into the policy network.\nThis allows the robot to leverage essential contextual information during\naction inference, resulting in improved precision and robustness. Our\nexperiments demonstrate that CoA-VLA outperforms state-of-the-art robot\nfoundation models, including OpenVLA and Octo, on a variety of tasks.\nFurthermore, CoA-VLA exhibits strong generalization capabilities, including\nrecognizing unseen object poses, identifying free space, and avoiding obstacles\nin novel environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robot foundation models, particularly Vision-Language-Action (VLA) models,\nhave garnered significant attention for their ability to enhance robot policy\nlearning, greatly improving robot's generalization and robustness. OpenAI's\nrecent model, O1, showcased impressive capabilities in solving complex problems\nby utilizing extensive reasoning chains. This prompts an important question:\ncan robot models achieve better performance in multi-task , complex\nenvironments by reviewing prior observations and then providing task-specific\nreasoning to guide action prediction? In this paper, we introduce\nChain-of-Affordance (CoA-VLA) , a novel approach to scaling robot models by\nincorporating reasoning in the format of sequential robot affordances to\nfacilitate task completion. Specifically, we prompt the model to consider the\nfollowing four types of affordances before taking action: (1) object affordance\n- what object to manipulate and where it is ; (2) grasp affordance - the\nspecific object part to grasp ; (3) spatial affordance - the optimal space to\nplace the object ; and (4) movement affordance-the collision - free path for\nmovement. We further transform each affordance into two prompting formats:\nvisual affordance and textual affordance. We introduce a novel vision-language\nco-injection module that integrates this knowledge into the policy network.\nThis allows the robot to leverage essential contextual information during\naction inference, resulting in improved precision and robustness. Our\nexperiments demonstrate that CoA-VLA outperforms state-of-the-art robot\nfoundation models, including OpenVLA and Octo, on a variety of tasks.\nFurthermore, CoA-VLA exhibits strong generalization capabilities, including\nrecognizing unseen object poses, identifying free space, and avoiding obstacles\nin novel environments."
                },
                "authors": [
                    {
                        "name": "Jinming Li"
                    },
                    {
                        "name": "Yichen Zhu"
                    },
                    {
                        "name": "Zhibin Tang"
                    },
                    {
                        "name": "Junjie Wen"
                    },
                    {
                        "name": "Minjie Zhu"
                    },
                    {
                        "name": "Xiaoyu Liu"
                    },
                    {
                        "name": "Chengmeng Li"
                    },
                    {
                        "name": "Ran Cheng"
                    },
                    {
                        "name": "Yaxin Peng"
                    },
                    {
                        "name": "Yan Peng"
                    },
                    {
                        "name": "Feifei Feng"
                    }
                ],
                "author_detail": {
                    "name": "Feifei Feng"
                },
                "author": "Feifei Feng",
                "arxiv_comment": "Project webpage is available at https://chain-of-affordance.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20451v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20451v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23597v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23597v1",
                "updated": "2025-07-31T14:36:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    14,
                    36,
                    24,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T14:36:24Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    14,
                    36,
                    24,
                    3,
                    212,
                    0
                ],
                "title": "MoGA: 3D Generative Avatar Prior for Monocular Gaussian Avatar\n  Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoGA: 3D Generative Avatar Prior for Monocular Gaussian Avatar\n  Reconstruction"
                },
                "summary": "We present MoGA, a novel method to reconstruct high-fidelity 3D Gaussian\navatars from a single-view image. The main challenge lies in inferring unseen\nappearance and geometric details while ensuring 3D consistency and realism.\nMost previous methods rely on 2D diffusion models to synthesize unseen views;\nhowever, these generated views are sparse and inconsistent, resulting in\nunrealistic 3D artifacts and blurred appearance. To address these limitations,\nwe leverage a generative avatar model, that can generate diverse 3D avatars by\nsampling deformed Gaussians from a learned prior distribution. Due to the\nlimited amount of 3D training data such a 3D model alone cannot capture all\nimage details of unseen identities. Consequently, we integrate it as a prior,\nensuring 3D consistency by projecting input images into its latent space and\nenforcing additional 3D appearance and geometric constraints. Our novel\napproach formulates Gaussian avatar creation as a model inversion process by\nfitting the generative avatar to synthetic views from 2D diffusion models. The\ngenerative avatar provides a meaningful initialization for model fitting,\nenforces 3D regularization, and helps in refining pose estimation. Experiments\nshow that our method surpasses state-of-the-art techniques and generalizes well\nto real-world scenarios. Our Gaussian avatars are also inherently animatable",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present MoGA, a novel method to reconstruct high-fidelity 3D Gaussian\navatars from a single-view image. The main challenge lies in inferring unseen\nappearance and geometric details while ensuring 3D consistency and realism.\nMost previous methods rely on 2D diffusion models to synthesize unseen views;\nhowever, these generated views are sparse and inconsistent, resulting in\nunrealistic 3D artifacts and blurred appearance. To address these limitations,\nwe leverage a generative avatar model, that can generate diverse 3D avatars by\nsampling deformed Gaussians from a learned prior distribution. Due to the\nlimited amount of 3D training data such a 3D model alone cannot capture all\nimage details of unseen identities. Consequently, we integrate it as a prior,\nensuring 3D consistency by projecting input images into its latent space and\nenforcing additional 3D appearance and geometric constraints. Our novel\napproach formulates Gaussian avatar creation as a model inversion process by\nfitting the generative avatar to synthetic views from 2D diffusion models. The\ngenerative avatar provides a meaningful initialization for model fitting,\nenforces 3D regularization, and helps in refining pose estimation. Experiments\nshow that our method surpasses state-of-the-art techniques and generalizes well\nto real-world scenarios. Our Gaussian avatars are also inherently animatable"
                },
                "authors": [
                    {
                        "name": "Zijian Dong"
                    },
                    {
                        "name": "Longteng Duan"
                    },
                    {
                        "name": "Jie Song"
                    },
                    {
                        "name": "Michael J. Black"
                    },
                    {
                        "name": "Andreas Geiger"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Geiger"
                },
                "author": "Andreas Geiger",
                "arxiv_comment": "ICCV 2025 (Highlight), Project Page: https://zj-dong.github.io/MoGA/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23597v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23597v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19115v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19115v2",
                "updated": "2025-07-31T14:34:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    14,
                    34,
                    0,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-25T09:50:48Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    9,
                    50,
                    48,
                    4,
                    206,
                    0
                ],
                "title": "Automated Code Review Using Large Language Models at Ericsson: An\n  Experience Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Code Review Using Large Language Models at Ericsson: An\n  Experience Report"
                },
                "summary": "Code review is one of the primary means of assuring the quality of released\nsoftware along with testing and static analysis. However, code review requires\nexperienced developers who may not always have the time to perform an in-depth\nreview of code. Thus, automating code review can help alleviate the cognitive\nburden on experienced software developers allowing them to focus on their\nprimary activities of writing code to add new features and fix bugs. In this\npaper, we describe our experience in using Large Language Models towards\nautomating the code review process in Ericsson. We describe the development of\na lightweight tool using LLMs and static program analysis. We then describe our\npreliminary experiments with experienced developers in evaluating our code\nreview tool and the encouraging results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code review is one of the primary means of assuring the quality of released\nsoftware along with testing and static analysis. However, code review requires\nexperienced developers who may not always have the time to perform an in-depth\nreview of code. Thus, automating code review can help alleviate the cognitive\nburden on experienced software developers allowing them to focus on their\nprimary activities of writing code to add new features and fix bugs. In this\npaper, we describe our experience in using Large Language Models towards\nautomating the code review process in Ericsson. We describe the development of\na lightweight tool using LLMs and static program analysis. We then describe our\npreliminary experiments with experienced developers in evaluating our code\nreview tool and the encouraging results."
                },
                "authors": [
                    {
                        "name": "Shweta Ramesh"
                    },
                    {
                        "name": "Joy Bose"
                    },
                    {
                        "name": "Hamender Singh"
                    },
                    {
                        "name": "A K Raghavan"
                    },
                    {
                        "name": "Sujoy Roychowdhury"
                    },
                    {
                        "name": "Giriprasad Sridhara"
                    },
                    {
                        "name": "Nishrith Saini"
                    },
                    {
                        "name": "Ricardo Britto"
                    }
                ],
                "author_detail": {
                    "name": "Ricardo Britto"
                },
                "author": "Ricardo Britto",
                "arxiv_comment": "6 pages, 4 figures, 1 table. Accepted in ICSME 2025 conference in\n  Auckland",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19115v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19115v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23589v1",
                "updated": "2025-07-31T14:25:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    14,
                    25,
                    54,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T14:25:54Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    14,
                    25,
                    54,
                    3,
                    212,
                    0
                ],
                "title": "Can LLM-Reasoning Models Replace Classical Planning? A Benchmark Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLM-Reasoning Models Replace Classical Planning? A Benchmark Study"
                },
                "summary": "Recent advancements in Large Language Models have sparked interest in their\npotential for robotic task planning. While these models demonstrate strong\ngenerative capabilities, their effectiveness in producing structured and\nexecutable plans remains uncertain. This paper presents a systematic evaluation\nof a broad spectrum of current state of the art language models, each directly\nprompted using Planning Domain Definition Language domain and problem files,\nand compares their planning performance with the Fast Downward planner across a\nvariety of benchmarks. In addition to measuring success rates, we assess how\nfaithfully the generated plans translate into sequences of actions that can\nactually be executed, identifying both strengths and limitations of using these\nmodels in this setting. Our findings show that while the models perform well on\nsimpler planning tasks, they continue to struggle with more complex scenarios\nthat require precise resource management, consistent state tracking, and strict\nconstraint compliance. These results underscore fundamental challenges in\napplying language models to robotic planning in real world environments. By\noutlining the gaps that emerge during execution, we aim to guide future\nresearch toward combined approaches that integrate language models with\nclassical planners in order to enhance the reliability and scalability of\nplanning in autonomous robotics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models have sparked interest in their\npotential for robotic task planning. While these models demonstrate strong\ngenerative capabilities, their effectiveness in producing structured and\nexecutable plans remains uncertain. This paper presents a systematic evaluation\nof a broad spectrum of current state of the art language models, each directly\nprompted using Planning Domain Definition Language domain and problem files,\nand compares their planning performance with the Fast Downward planner across a\nvariety of benchmarks. In addition to measuring success rates, we assess how\nfaithfully the generated plans translate into sequences of actions that can\nactually be executed, identifying both strengths and limitations of using these\nmodels in this setting. Our findings show that while the models perform well on\nsimpler planning tasks, they continue to struggle with more complex scenarios\nthat require precise resource management, consistent state tracking, and strict\nconstraint compliance. These results underscore fundamental challenges in\napplying language models to robotic planning in real world environments. By\noutlining the gaps that emerge during execution, we aim to guide future\nresearch toward combined approaches that integrate language models with\nclassical planners in order to enhance the reliability and scalability of\nplanning in autonomous robotics."
                },
                "authors": [
                    {
                        "name": "Kai Goebel"
                    },
                    {
                        "name": "Patrik Zips"
                    }
                ],
                "author_detail": {
                    "name": "Patrik Zips"
                },
                "author": "Patrik Zips",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23581v1",
                "updated": "2025-07-31T14:11:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    14,
                    11,
                    16,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T14:11:16Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    14,
                    11,
                    16,
                    3,
                    212,
                    0
                ],
                "title": "GraphRAG-R1: Graph Retrieval-Augmented Generation with\n  Process-Constrained Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphRAG-R1: Graph Retrieval-Augmented Generation with\n  Process-Constrained Reinforcement Learning"
                },
                "summary": "Graph Retrieval-Augmented Generation (GraphRAG) has shown great effectiveness\nin enhancing the reasoning abilities of LLMs by leveraging graph structures for\nknowledge representation and modeling complex real-world relationships.\nHowever, existing GraphRAG methods still face significant bottlenecks when\nhandling complex problems that require multi-hop reasoning, as their query and\nretrieval phases are largely based on pre-defined heuristics and do not fully\nutilize the reasoning potentials of LLMs. To address this problem, we propose\nGraphRAG-R1, an adaptive GraphRAG framework by training LLMs with\nprocess-constrained outcome-based reinforcement learning (RL) to enhance the\nmulti-hop reasoning ability. Our method can decompose complex problems,\nautonomously invoke retrieval tools to acquire necessary information, and\nperform effective reasoning. Specifically, we utilize a modified version of\nGroup Relative Policy Optimization (GRPO) that supports rollout-with-thinking\ncapability. Next, we design two process-constrained reward functions. To handle\nthe shallow retrieval problem, we design a Progressive Retrieval Attenuation\n(PRA) reward to encourage essential retrievals. Then, to handle the\nover-thinking problem, we design Cost-Aware F1 (CAF) reward to balance the\nmodel performance with computational costs. We further design a phase-dependent\ntraining strategy, containing three training stages corresponding to cold start\nand these two rewards. Lastly, our method adopts a hybrid graph-textual\nretrieval to improve the reasoning capacity. Extensive experimental results\ndemonstrate that GraphRAG-R1 boosts LLM capabilities in solving complex\nreasoning problems compared to state-of-the-art GraphRAG methods on both\nin-domain and out-of-domain datasets. Furthermore, our framework can be\nflexibly integrated with various existing retrieval methods, consistently\ndelivering performance improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Retrieval-Augmented Generation (GraphRAG) has shown great effectiveness\nin enhancing the reasoning abilities of LLMs by leveraging graph structures for\nknowledge representation and modeling complex real-world relationships.\nHowever, existing GraphRAG methods still face significant bottlenecks when\nhandling complex problems that require multi-hop reasoning, as their query and\nretrieval phases are largely based on pre-defined heuristics and do not fully\nutilize the reasoning potentials of LLMs. To address this problem, we propose\nGraphRAG-R1, an adaptive GraphRAG framework by training LLMs with\nprocess-constrained outcome-based reinforcement learning (RL) to enhance the\nmulti-hop reasoning ability. Our method can decompose complex problems,\nautonomously invoke retrieval tools to acquire necessary information, and\nperform effective reasoning. Specifically, we utilize a modified version of\nGroup Relative Policy Optimization (GRPO) that supports rollout-with-thinking\ncapability. Next, we design two process-constrained reward functions. To handle\nthe shallow retrieval problem, we design a Progressive Retrieval Attenuation\n(PRA) reward to encourage essential retrievals. Then, to handle the\nover-thinking problem, we design Cost-Aware F1 (CAF) reward to balance the\nmodel performance with computational costs. We further design a phase-dependent\ntraining strategy, containing three training stages corresponding to cold start\nand these two rewards. Lastly, our method adopts a hybrid graph-textual\nretrieval to improve the reasoning capacity. Extensive experimental results\ndemonstrate that GraphRAG-R1 boosts LLM capabilities in solving complex\nreasoning problems compared to state-of-the-art GraphRAG methods on both\nin-domain and out-of-domain datasets. Furthermore, our framework can be\nflexibly integrated with various existing retrieval methods, consistently\ndelivering performance improvements."
                },
                "authors": [
                    {
                        "name": "Chuanyue Yu"
                    },
                    {
                        "name": "Kuo Zhao"
                    },
                    {
                        "name": "Yuhan Li"
                    },
                    {
                        "name": "Heng Chang"
                    },
                    {
                        "name": "Mingjian Feng"
                    },
                    {
                        "name": "Xiangzhe Jiang"
                    },
                    {
                        "name": "Yufei Sun"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Yuzhi Zhang"
                    },
                    {
                        "name": "Jianxin Li"
                    },
                    {
                        "name": "Ziwei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ziwei Zhang"
                },
                "author": "Ziwei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02744v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02744v3",
                "updated": "2025-07-31T14:02:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    14,
                    2,
                    13,
                    3,
                    212,
                    0
                ],
                "published": "2024-10-03T17:55:17Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    55,
                    17,
                    3,
                    277,
                    0
                ],
                "title": "Neutral Residues: Revisiting Adapters for Model Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neutral Residues: Revisiting Adapters for Model Extension"
                },
                "summary": "We address the problem of extending a pretrained large language model to a\nnew domain that was not seen during training. Standard techniques, such as\nfinetuning or low-rank adaptation (LoRA) are successful at domain adaptation,\nbut do not formally add capacity to the model. This often leads to a trade-off,\nbetween performing well on the new domain vs. degrading performance on the\noriginal domain. Here, we revisit and improve adapters to extend LLMs from\nthree angles: data, architecture and training procedure, which are\nadvantageously considered jointly. The resulting method, called neutral\nresidues, modifies adapters in a way that leads each new residual block to\noutput near-zeros on the original domain. This solution leads to strong results\nwhen adapting a state-of-the-art model originally trained on English to a new\nlanguage. Neutral residues significantly outperform competing approaches such\nas finetuning, LoRA or vanilla adapters in terms of the trade-off between\nlearning the new language and not forgetting English.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the problem of extending a pretrained large language model to a\nnew domain that was not seen during training. Standard techniques, such as\nfinetuning or low-rank adaptation (LoRA) are successful at domain adaptation,\nbut do not formally add capacity to the model. This often leads to a trade-off,\nbetween performing well on the new domain vs. degrading performance on the\noriginal domain. Here, we revisit and improve adapters to extend LLMs from\nthree angles: data, architecture and training procedure, which are\nadvantageously considered jointly. The resulting method, called neutral\nresidues, modifies adapters in a way that leads each new residual block to\noutput near-zeros on the original domain. This solution leads to strong results\nwhen adapting a state-of-the-art model originally trained on English to a new\nlanguage. Neutral residues significantly outperform competing approaches such\nas finetuning, LoRA or vanilla adapters in terms of the trade-off between\nlearning the new language and not forgetting English."
                },
                "authors": [
                    {
                        "name": "Franck Signe Talla"
                    },
                    {
                        "name": "Edouard Grave"
                    },
                    {
                        "name": "Herv Jgou"
                    }
                ],
                "author_detail": {
                    "name": "Herv Jgou"
                },
                "author": "Herv Jgou",
                "arxiv_comment": "Accepted at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02744v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02744v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07426v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07426v3",
                "updated": "2025-07-31T13:57:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    13,
                    57,
                    25,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-10T04:39:55Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    4,
                    39,
                    55,
                    3,
                    191,
                    0
                ],
                "title": "DrugMCTS: a drug repurposing framework combining multi-agent, RAG and\n  Monte Carlo Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DrugMCTS: a drug repurposing framework combining multi-agent, RAG and\n  Monte Carlo Tree Search"
                },
                "summary": "Recent advances in large language models have demonstrated considerable\npotential in scientific domains such as drug repositioning. However, their\neffectiveness remains constrained when reasoning extends beyond the knowledge\nacquired during pretraining. Conventional approaches, such as fine-tuning or\nretrieval-augmented generation, face limitations in either imposing high\ncomputational overhead or failing to fully exploit structured scientific data.\nTo overcome these challenges, we propose DrugMCTS, a novel framework that\nsynergistically integrates RAG, multi-agent collaboration, and Monte Carlo Tree\nSearch for drug repositioning. The framework employs five specialized agents\ntasked with retrieving and analyzing molecular and protein information, thereby\nenabling structured and iterative reasoning. Extensive experiments on the\nDrugBank and KIBA datasets demonstrate that DrugMCTS achieves substantially\nhigher recall and robustness compared to both general-purpose LLMs and deep\nlearning baselines. Our results highlight the importance of structured\nreasoning, agent-based collaboration, and feedback-driven search mechanisms in\nadvancing LLM applications for drug repositioning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models have demonstrated considerable\npotential in scientific domains such as drug repositioning. However, their\neffectiveness remains constrained when reasoning extends beyond the knowledge\nacquired during pretraining. Conventional approaches, such as fine-tuning or\nretrieval-augmented generation, face limitations in either imposing high\ncomputational overhead or failing to fully exploit structured scientific data.\nTo overcome these challenges, we propose DrugMCTS, a novel framework that\nsynergistically integrates RAG, multi-agent collaboration, and Monte Carlo Tree\nSearch for drug repositioning. The framework employs five specialized agents\ntasked with retrieving and analyzing molecular and protein information, thereby\nenabling structured and iterative reasoning. Extensive experiments on the\nDrugBank and KIBA datasets demonstrate that DrugMCTS achieves substantially\nhigher recall and robustness compared to both general-purpose LLMs and deep\nlearning baselines. Our results highlight the importance of structured\nreasoning, agent-based collaboration, and feedback-driven search mechanisms in\nadvancing LLM applications for drug repositioning."
                },
                "authors": [
                    {
                        "name": "Zerui Yang"
                    },
                    {
                        "name": "Yuwei Wan"
                    },
                    {
                        "name": "Siyu Yan"
                    },
                    {
                        "name": "Yudai Matsuda"
                    },
                    {
                        "name": "Tong Xie"
                    },
                    {
                        "name": "Bram Hoex"
                    },
                    {
                        "name": "Linqi Song"
                    }
                ],
                "author_detail": {
                    "name": "Linqi Song"
                },
                "author": "Linqi Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07426v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07426v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23562v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23562v1",
                "updated": "2025-07-31T13:49:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    13,
                    49,
                    44,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T13:49:44Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    13,
                    49,
                    44,
                    3,
                    212,
                    0
                ],
                "title": "Hardware-Aware Fine-Tuning of Spiking Q-Networks on the SpiNNaker2\n  Neuromorphic Platform",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hardware-Aware Fine-Tuning of Spiking Q-Networks on the SpiNNaker2\n  Neuromorphic Platform"
                },
                "summary": "Spiking Neural Networks (SNNs) promise orders-of-magnitude lower power\nconsumption and low-latency inference on neuromorphic hardware for a wide range\nof robotic tasks. In this work, we present an energy-efficient implementation\nof a reinforcement learning (RL) algorithm using quantized SNNs to solve two\nclassical control tasks. The network is trained using the Q-learning algorithm,\nthen fine-tuned and quantized to low-bit (8-bit) precision for embedded\ndeployment on the SpiNNaker2 neuromorphic chip. To evaluate the comparative\nadvantage of SpiNNaker2 over conventional computing platforms, we analyze\ninference latency, dynamic power consumption, and energy cost per inference for\nour SNN models, comparing performance against a GTX 1650 GPU baseline. Our\nresults demonstrate SpiNNaker2's strong potential for scalable, low-energy\nneuromorphic computing, achieving up to 32x reduction in energy consumption.\nInference latency remains on par with GPU-based execution, with improvements\nobserved in certain task settings, reinforcing SpiNNaker2's viability for\nreal-time neuromorphic control and making the neuromorphic approach a\ncompelling direction for efficient deep Q-learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Neural Networks (SNNs) promise orders-of-magnitude lower power\nconsumption and low-latency inference on neuromorphic hardware for a wide range\nof robotic tasks. In this work, we present an energy-efficient implementation\nof a reinforcement learning (RL) algorithm using quantized SNNs to solve two\nclassical control tasks. The network is trained using the Q-learning algorithm,\nthen fine-tuned and quantized to low-bit (8-bit) precision for embedded\ndeployment on the SpiNNaker2 neuromorphic chip. To evaluate the comparative\nadvantage of SpiNNaker2 over conventional computing platforms, we analyze\ninference latency, dynamic power consumption, and energy cost per inference for\nour SNN models, comparing performance against a GTX 1650 GPU baseline. Our\nresults demonstrate SpiNNaker2's strong potential for scalable, low-energy\nneuromorphic computing, achieving up to 32x reduction in energy consumption.\nInference latency remains on par with GPU-based execution, with improvements\nobserved in certain task settings, reinforcing SpiNNaker2's viability for\nreal-time neuromorphic control and making the neuromorphic approach a\ncompelling direction for efficient deep Q-learning."
                },
                "authors": [
                    {
                        "name": "Sirine Arfa"
                    },
                    {
                        "name": "Bernhard Vogginger"
                    },
                    {
                        "name": "Christian Mayr"
                    }
                ],
                "author_detail": {
                    "name": "Christian Mayr"
                },
                "author": "Christian Mayr",
                "arxiv_comment": "8 pages, 5 figures, 3 tables",
                "arxiv_journal_ref": "ACM ICONS 2025 - International Conference on Neuromorphic Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23562v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23562v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23554v1",
                "updated": "2025-07-31T13:42:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    13,
                    42,
                    14,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T13:42:14Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    13,
                    42,
                    14,
                    3,
                    212,
                    0
                ],
                "title": "DICE: Dynamic In-Context Example Selection in LLM Agents via Efficient\n  Knowledge Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DICE: Dynamic In-Context Example Selection in LLM Agents via Efficient\n  Knowledge Transfer"
                },
                "summary": "Large language model-based agents, empowered by in-context learning (ICL),\nhave demonstrated strong capabilities in complex reasoning and tool-use tasks.\nHowever, existing works have shown that the effectiveness of ICL is highly\nsensitive to the choice of demonstrations, with suboptimal examples often\nleading to unstable or degraded performance. While prior work has explored\nexample selection, including in some agentic or multi-step settings, existing\napproaches typically rely on heuristics or task-specific designs and lack a\ngeneral, theoretically grounded criterion for what constitutes an effective\ndemonstration across reasoning steps. Therefore, it is non-trivial to develop a\nprincipled, general-purpose method for selecting demonstrations that\nconsistently benefit agent performance. In this paper, we address this\nchallenge with DICE, Dynamic In-Context Example Selection for LLM Agents, a\ntheoretically grounded ICL framework for agentic tasks that selects the most\nrelevant demonstrations at each step of reasoning. Our approach decomposes\ndemonstration knowledge into transferable and non-transferable components\nthrough a causal lens, showing how the latter can introduce spurious\ndependencies that impair generalization. We further propose a stepwise\nselection criterion with a formal guarantee of improved agent performance.\nImportantly, DICE is a general, framework-agnostic solution that can be\nintegrated as a plug-in module into existing agentic frameworks without any\nadditional training cost. Extensive experiments across diverse domains\ndemonstrate our method's effectiveness and generality, highlighting the\nimportance of principled, context-aware demo selection for robust and efficient\nLLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model-based agents, empowered by in-context learning (ICL),\nhave demonstrated strong capabilities in complex reasoning and tool-use tasks.\nHowever, existing works have shown that the effectiveness of ICL is highly\nsensitive to the choice of demonstrations, with suboptimal examples often\nleading to unstable or degraded performance. While prior work has explored\nexample selection, including in some agentic or multi-step settings, existing\napproaches typically rely on heuristics or task-specific designs and lack a\ngeneral, theoretically grounded criterion for what constitutes an effective\ndemonstration across reasoning steps. Therefore, it is non-trivial to develop a\nprincipled, general-purpose method for selecting demonstrations that\nconsistently benefit agent performance. In this paper, we address this\nchallenge with DICE, Dynamic In-Context Example Selection for LLM Agents, a\ntheoretically grounded ICL framework for agentic tasks that selects the most\nrelevant demonstrations at each step of reasoning. Our approach decomposes\ndemonstration knowledge into transferable and non-transferable components\nthrough a causal lens, showing how the latter can introduce spurious\ndependencies that impair generalization. We further propose a stepwise\nselection criterion with a formal guarantee of improved agent performance.\nImportantly, DICE is a general, framework-agnostic solution that can be\nintegrated as a plug-in module into existing agentic frameworks without any\nadditional training cost. Extensive experiments across diverse domains\ndemonstrate our method's effectiveness and generality, highlighting the\nimportance of principled, context-aware demo selection for robust and efficient\nLLM agents."
                },
                "authors": [
                    {
                        "name": "Ruoyu Wang"
                    },
                    {
                        "name": "Junda Wu"
                    },
                    {
                        "name": "Yu Xia"
                    },
                    {
                        "name": "Tong Yu"
                    },
                    {
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "name": "Julian McAuley"
                    },
                    {
                        "name": "Lina Yao"
                    }
                ],
                "author_detail": {
                    "name": "Lina Yao"
                },
                "author": "Lina Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04502v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04502v2",
                "updated": "2025-07-31T13:39:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    13,
                    39,
                    20,
                    3,
                    212,
                    0
                ],
                "published": "2024-12-02T15:37:37Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    15,
                    37,
                    37,
                    0,
                    337,
                    0
                ],
                "title": "Physics-informed Gaussian Processes as Linear Model Predictive\n  Controller",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physics-informed Gaussian Processes as Linear Model Predictive\n  Controller"
                },
                "summary": "We introduce a novel algorithm for controlling linear time invariant systems\nin a tracking problem. The controller is based on a Gaussian Process (GP) whose\nrealizations satisfy a system of linear ordinary differential equations with\nconstant coefficients. Control inputs for tracking are determined by\nconditioning the prior GP on the setpoints, i.e. control as inference. The\nresulting Model Predictive Control scheme incorporates pointwise soft\nconstraints by introducing virtual setpoints to the posterior Gaussian process.\nWe show theoretically that our controller satisfies open-loop stability for the\noptimal control problem by leveraging general results from Bayesian inference\nand demonstrate this result in a numerical example.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel algorithm for controlling linear time invariant systems\nin a tracking problem. The controller is based on a Gaussian Process (GP) whose\nrealizations satisfy a system of linear ordinary differential equations with\nconstant coefficients. Control inputs for tracking are determined by\nconditioning the prior GP on the setpoints, i.e. control as inference. The\nresulting Model Predictive Control scheme incorporates pointwise soft\nconstraints by introducing virtual setpoints to the posterior Gaussian process.\nWe show theoretically that our controller satisfies open-loop stability for the\noptimal control problem by leveraging general results from Bayesian inference\nand demonstrate this result in a numerical example."
                },
                "authors": [
                    {
                        "name": "Jrn Tebbe"
                    },
                    {
                        "name": "Andreas Besginow"
                    },
                    {
                        "name": "Markus Lange-Hegermann"
                    }
                ],
                "author_detail": {
                    "name": "Markus Lange-Hegermann"
                },
                "author": "Markus Lange-Hegermann",
                "arxiv_comment": "Accepted at L4DC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04502v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04502v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18337v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18337v4",
                "updated": "2025-07-31T13:39:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    13,
                    39,
                    18,
                    3,
                    212,
                    0
                ],
                "published": "2024-11-27T13:35:32Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    13,
                    35,
                    32,
                    2,
                    332,
                    0
                ],
                "title": "Can LLMs assist with Ambiguity? A Quantitative Evaluation of various\n  Large Language Models on Word Sense Disambiguation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs assist with Ambiguity? A Quantitative Evaluation of various\n  Large Language Models on Word Sense Disambiguation"
                },
                "summary": "Ambiguous words are often found in modern digital communications. Lexical\nambiguity challenges traditional Word Sense Disambiguation (WSD) methods, due\nto limited data. Consequently, the efficiency of translation, information\nretrieval, and question-answering systems is hindered by these limitations.\nThis study investigates the use of Large Language Models (LLMs) to improve WSD\nusing a novel approach combining a systematic prompt augmentation mechanism\nwith a knowledge base (KB) consisting of different sense interpretations. The\nproposed method incorporates a human-in-loop approach for prompt augmentation\nwhere prompt is supported by Part-of-Speech (POS) tagging, synonyms of\nambiguous words, aspect-based sense filtering and few-shot prompting to guide\nthe LLM. By utilizing a few-shot Chain of Thought (COT) prompting-based\napproach, this work demonstrates a substantial improvement in performance. The\nevaluation was conducted using FEWS test data and sense tags. This research\nadvances accurate word interpretation in social media and digital\ncommunication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ambiguous words are often found in modern digital communications. Lexical\nambiguity challenges traditional Word Sense Disambiguation (WSD) methods, due\nto limited data. Consequently, the efficiency of translation, information\nretrieval, and question-answering systems is hindered by these limitations.\nThis study investigates the use of Large Language Models (LLMs) to improve WSD\nusing a novel approach combining a systematic prompt augmentation mechanism\nwith a knowledge base (KB) consisting of different sense interpretations. The\nproposed method incorporates a human-in-loop approach for prompt augmentation\nwhere prompt is supported by Part-of-Speech (POS) tagging, synonyms of\nambiguous words, aspect-based sense filtering and few-shot prompting to guide\nthe LLM. By utilizing a few-shot Chain of Thought (COT) prompting-based\napproach, this work demonstrates a substantial improvement in performance. The\nevaluation was conducted using FEWS test data and sense tags. This research\nadvances accurate word interpretation in social media and digital\ncommunication."
                },
                "authors": [
                    {
                        "name": "T. G. D. K. Sumanathilaka"
                    },
                    {
                        "name": "Nicholas Micallef"
                    },
                    {
                        "name": "Julian Hough"
                    }
                ],
                "author_detail": {
                    "name": "Julian Hough"
                },
                "author": "Julian Hough",
                "arxiv_comment": "12 pages,6 tables, 1 figure, Proceedings of the 1st International\n  Conference on NLP & AI for Cyber Security",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18337v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18337v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23543v1",
                "updated": "2025-07-31T13:34:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    13,
                    34,
                    6,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T13:34:06Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    13,
                    34,
                    6,
                    3,
                    212,
                    0
                ],
                "title": "ART: Adaptive Relation Tuning for Generalized Relation Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ART: Adaptive Relation Tuning for Generalized Relation Prediction"
                },
                "summary": "Visual relation detection (VRD) is the task of identifying the relationships\nbetween objects in a scene. VRD models trained solely on relation detection\ndata struggle to generalize beyond the relations on which they are trained.\nWhile prompt tuning has been used to adapt vision-language models (VLMs) for\nVRD, it uses handcrafted prompts and struggles with novel or complex relations.\nWe argue that instruction tuning offers a more effective solution by\nfine-tuning VLMs on diverse instructional data. We thus introduce ART, an\nAdaptive Relation Tuning framework that adapts VLMs for VRD through instruction\ntuning and strategic instance selection. By converting VRD datasets into an\ninstruction tuning format and employing an adaptive sampling algorithm, ART\ndirects the VLM to focus on informative relations while maintaining\ngeneralizability. Specifically, we focus on the relation classification, where\nsubject-object boxes are given and the model predicts the predicate between\nthem. We tune on a held-in set and evaluate across multiple held-out datasets\nof varying complexity. Our approach strongly improves over its baselines and\ncan infer unseen relation concepts, a capability absent in mainstream VRD\nmethods. We demonstrate ART's practical value by using the predicted relations\nfor segmenting complex scenes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual relation detection (VRD) is the task of identifying the relationships\nbetween objects in a scene. VRD models trained solely on relation detection\ndata struggle to generalize beyond the relations on which they are trained.\nWhile prompt tuning has been used to adapt vision-language models (VLMs) for\nVRD, it uses handcrafted prompts and struggles with novel or complex relations.\nWe argue that instruction tuning offers a more effective solution by\nfine-tuning VLMs on diverse instructional data. We thus introduce ART, an\nAdaptive Relation Tuning framework that adapts VLMs for VRD through instruction\ntuning and strategic instance selection. By converting VRD datasets into an\ninstruction tuning format and employing an adaptive sampling algorithm, ART\ndirects the VLM to focus on informative relations while maintaining\ngeneralizability. Specifically, we focus on the relation classification, where\nsubject-object boxes are given and the model predicts the predicate between\nthem. We tune on a held-in set and evaluate across multiple held-out datasets\nof varying complexity. Our approach strongly improves over its baselines and\ncan infer unseen relation concepts, a capability absent in mainstream VRD\nmethods. We demonstrate ART's practical value by using the predicted relations\nfor segmenting complex scenes."
                },
                "authors": [
                    {
                        "name": "Gopika Sudhakaran"
                    },
                    {
                        "name": "Hikaru Shindo"
                    },
                    {
                        "name": "Patrick Schramowski"
                    },
                    {
                        "name": "Simone Schaub-Meyer"
                    },
                    {
                        "name": "Kristian Kersting"
                    },
                    {
                        "name": "Stefan Roth"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Roth"
                },
                "author": "Stefan Roth",
                "arxiv_comment": "Accepted for publication in ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23541v1",
                "updated": "2025-07-31T13:31:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    13,
                    31,
                    1,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T13:31:01Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    13,
                    31,
                    1,
                    3,
                    212,
                    0
                ],
                "title": "Med-R$^3$: Enhancing Medical Retrieval-Augmented Reasoning of LLMs via\n  Progressive Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Med-R$^3$: Enhancing Medical Retrieval-Augmented Reasoning of LLMs via\n  Progressive Reinforcement Learning"
                },
                "summary": "In medical scenarios, effectively retrieving external knowledge and\nleveraging it for rigorous logical reasoning is of significant importance.\nDespite their potential, existing work has predominantly focused on enhancing\neither retrieval or reasoning capabilities of the models in isolation, with\nlittle attention given to their joint optimization, which leads to limited\ncoordination between the two processes. Additionally, current methods rely\nheavily on supervised fine-tuning (SFT), which can cause models to memorize\nexisting problem-solving pathways, thereby restricting their generalization\nability when confronted with novel problem contexts. Furthermore, while some\nstudies have explored to improve retrieval-augmented reasoning in general\ndomains via reinforcement learning, their reward function designs do not\nadequately capture the specific demands of the medical domain. To address these\nchallenges, we introduce **Med-R$^3$**, a **Med**ical **R**etrieval-augmented\n**R**easoning framework driven by progressive **R**einforcement learning. In\nthis framework, we first develop the model's ability to perform logical\nreasoning over medical problems. Subsequently, on the basis of this foundation,\nwe adaptively optimize the retrieval capability to better align with the\ncharacteristics of knowledge corpus and external information utilization\nthroughout the reasoning process. Finally, we conduct joint optimization of the\nmodel's retrieval and reasoning coordination. Extensive experiments indicate\nthat **Med-R$^3$** could achieve state-of-the-art performances, with\nLLaMA3.1-8B-Instruct + Med-R$^3$ surpassing closed-sourced GPT-4o-mini by\n3.93\\% at a comparable parameter scale, while Qwen2.5-14B augmented with\nMed-R$^3$ shows a more substantial gain of 13.53\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In medical scenarios, effectively retrieving external knowledge and\nleveraging it for rigorous logical reasoning is of significant importance.\nDespite their potential, existing work has predominantly focused on enhancing\neither retrieval or reasoning capabilities of the models in isolation, with\nlittle attention given to their joint optimization, which leads to limited\ncoordination between the two processes. Additionally, current methods rely\nheavily on supervised fine-tuning (SFT), which can cause models to memorize\nexisting problem-solving pathways, thereby restricting their generalization\nability when confronted with novel problem contexts. Furthermore, while some\nstudies have explored to improve retrieval-augmented reasoning in general\ndomains via reinforcement learning, their reward function designs do not\nadequately capture the specific demands of the medical domain. To address these\nchallenges, we introduce **Med-R$^3$**, a **Med**ical **R**etrieval-augmented\n**R**easoning framework driven by progressive **R**einforcement learning. In\nthis framework, we first develop the model's ability to perform logical\nreasoning over medical problems. Subsequently, on the basis of this foundation,\nwe adaptively optimize the retrieval capability to better align with the\ncharacteristics of knowledge corpus and external information utilization\nthroughout the reasoning process. Finally, we conduct joint optimization of the\nmodel's retrieval and reasoning coordination. Extensive experiments indicate\nthat **Med-R$^3$** could achieve state-of-the-art performances, with\nLLaMA3.1-8B-Instruct + Med-R$^3$ surpassing closed-sourced GPT-4o-mini by\n3.93\\% at a comparable parameter scale, while Qwen2.5-14B augmented with\nMed-R$^3$ shows a more substantial gain of 13.53\\%."
                },
                "authors": [
                    {
                        "name": "Keer Lu"
                    },
                    {
                        "name": "Zheng Liang"
                    },
                    {
                        "name": "Youquan Li"
                    },
                    {
                        "name": "Jiejun Tan"
                    },
                    {
                        "name": "Da Pan"
                    },
                    {
                        "name": "Shusen Zhang"
                    },
                    {
                        "name": "Guosheng Dong"
                    },
                    {
                        "name": "Huang Leng"
                    }
                ],
                "author_detail": {
                    "name": "Huang Leng"
                },
                "author": "Huang Leng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23540v1",
                "updated": "2025-07-31T13:30:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    13,
                    30,
                    47,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T13:30:47Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    13,
                    30,
                    47,
                    3,
                    212,
                    0
                ],
                "title": "A Unified Perception-Language-Action Framework for Adaptive Autonomous\n  Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Perception-Language-Action Framework for Adaptive Autonomous\n  Driving"
                },
                "summary": "Autonomous driving systems face significant challenges in achieving\nhuman-like adaptability, robustness, and interpretability in complex,\nopen-world environments. These challenges stem from fragmented architectures,\nlimited generalization to novel scenarios, and insufficient semantic extraction\nfrom perception. To address these limitations, we propose a unified\nPerception-Language-Action (PLA) framework that integrates multi-sensor fusion\n(cameras, LiDAR, radar) with a large language model (LLM)-augmented\nVision-Language-Action (VLA) architecture, specifically a GPT-4.1-powered\nreasoning core. This framework unifies low-level sensory processing with\nhigh-level contextual reasoning, tightly coupling perception with natural\nlanguage-based semantic understanding and decision-making to enable\ncontext-aware, explainable, and safety-bounded autonomous driving. Evaluations\non an urban intersection scenario with a construction zone demonstrate superior\nperformance in trajectory tracking, speed prediction, and adaptive planning.\nThe results highlight the potential of language-augmented cognitive frameworks\nfor advancing the safety, interpretability, and scalability of autonomous\ndriving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous driving systems face significant challenges in achieving\nhuman-like adaptability, robustness, and interpretability in complex,\nopen-world environments. These challenges stem from fragmented architectures,\nlimited generalization to novel scenarios, and insufficient semantic extraction\nfrom perception. To address these limitations, we propose a unified\nPerception-Language-Action (PLA) framework that integrates multi-sensor fusion\n(cameras, LiDAR, radar) with a large language model (LLM)-augmented\nVision-Language-Action (VLA) architecture, specifically a GPT-4.1-powered\nreasoning core. This framework unifies low-level sensory processing with\nhigh-level contextual reasoning, tightly coupling perception with natural\nlanguage-based semantic understanding and decision-making to enable\ncontext-aware, explainable, and safety-bounded autonomous driving. Evaluations\non an urban intersection scenario with a construction zone demonstrate superior\nperformance in trajectory tracking, speed prediction, and adaptive planning.\nThe results highlight the potential of language-augmented cognitive frameworks\nfor advancing the safety, interpretability, and scalability of autonomous\ndriving systems."
                },
                "authors": [
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Erik Leo Ha"
                    },
                    {
                        "name": "Kuo-Yi Chao"
                    },
                    {
                        "name": "Nenad Petrovic"
                    },
                    {
                        "name": "Yinglei Song"
                    },
                    {
                        "name": "Chengdong Wu"
                    },
                    {
                        "name": "Alois Knoll"
                    }
                ],
                "author_detail": {
                    "name": "Alois Knoll"
                },
                "author": "Alois Knoll",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23539v1",
                "updated": "2025-07-31T13:29:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    13,
                    29,
                    43,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T13:29:43Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    13,
                    29,
                    43,
                    3,
                    212,
                    0
                ],
                "title": "Improved Algorithms for Kernel Matrix-Vector Multiplication Under\n  Sparsity Assumptions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improved Algorithms for Kernel Matrix-Vector Multiplication Under\n  Sparsity Assumptions"
                },
                "summary": "Motivated by the problem of fast processing of attention matrices, we study\nfast algorithms for computing matrix-vector products for asymmetric Gaussian\nKernel matrices $K\\in \\mathbb{R}^{n\\times n}$. $K$'s columns are indexed by a\nset of $n$ keys $k_1,k_2\\ldots, k_n\\in \\mathbb{R}^d$, rows by a set of $n$\nqueries $q_1,q_2,\\ldots,q_n\\in \\mathbb{R}^d $, and its $i,j$ entry is $K_{ij} =\ne^{-\\|q_i-k_j\\|_2^2/2\\sigma^2}$ for some bandwidth parameter $\\sigma>0$. Given\na vector $x\\in \\mathbb{R}^n$ and error parameter $\\epsilon>0$, our task is to\noutput a $y\\in \\mathbb{R}^n$ such that $\\|Kx-y\\|_2\\leq \\epsilon \\|x\\|_2$ in\ntime subquadratic in $n$ and linear in $d$. Our algorithms rely on the\nfollowing modelling assumption about the matrices $K$: the sum of the entries\nof $K$ scales linearly in $n$, as opposed to worst case quadratic growth. We\nvalidate this assumption experimentally, for Gaussian kernel matrices\nencountered in various settings such as fast attention computation in LLMs. We\nobtain the first subquadratic-time algorithm that works under this assumption,\nfor unrestricted vectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by the problem of fast processing of attention matrices, we study\nfast algorithms for computing matrix-vector products for asymmetric Gaussian\nKernel matrices $K\\in \\mathbb{R}^{n\\times n}$. $K$'s columns are indexed by a\nset of $n$ keys $k_1,k_2\\ldots, k_n\\in \\mathbb{R}^d$, rows by a set of $n$\nqueries $q_1,q_2,\\ldots,q_n\\in \\mathbb{R}^d $, and its $i,j$ entry is $K_{ij} =\ne^{-\\|q_i-k_j\\|_2^2/2\\sigma^2}$ for some bandwidth parameter $\\sigma>0$. Given\na vector $x\\in \\mathbb{R}^n$ and error parameter $\\epsilon>0$, our task is to\noutput a $y\\in \\mathbb{R}^n$ such that $\\|Kx-y\\|_2\\leq \\epsilon \\|x\\|_2$ in\ntime subquadratic in $n$ and linear in $d$. Our algorithms rely on the\nfollowing modelling assumption about the matrices $K$: the sum of the entries\nof $K$ scales linearly in $n$, as opposed to worst case quadratic growth. We\nvalidate this assumption experimentally, for Gaussian kernel matrices\nencountered in various settings such as fast attention computation in LLMs. We\nobtain the first subquadratic-time algorithm that works under this assumption,\nfor unrestricted vectors."
                },
                "authors": [
                    {
                        "name": "Piotr Indyk"
                    },
                    {
                        "name": "Michael Kapralov"
                    },
                    {
                        "name": "Kshiteej Sheth"
                    },
                    {
                        "name": "Tal Wagner"
                    }
                ],
                "author_detail": {
                    "name": "Tal Wagner"
                },
                "author": "Tal Wagner",
                "arxiv_comment": "Published in ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23536v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23536v1",
                "updated": "2025-07-31T13:23:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    13,
                    23,
                    21,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T13:23:21Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    13,
                    23,
                    21,
                    3,
                    212,
                    0
                ],
                "title": "From LLMs to Edge: Parameter-Efficient Fine-Tuning on Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From LLMs to Edge: Parameter-Efficient Fine-Tuning on Edge Devices"
                },
                "summary": "Parameter-efficient fine-tuning (PEFT) methods reduce the computational costs\nof updating deep learning models by minimizing the number of additional\nparameters used to adapt a model to a down- stream task. While extensively\nresearched in large language models (LLMs), their application to smaller models\nused on edge devices, such as convolutional neural networks, remains\nunderexplored. This paper benchmarks and analyzes popular PEFT methods on\nconvolutional architectures typically deployed in resource-constrained edge\nenvironments. We evaluate LoRA, DoRA, and GaLore for updating standard and\ndepthwise convolutional architectures to handle distribution shifts and\naccommodate unseen classes. We utilize recently proposed PyTorch profilers to\ncompare the updated model performance and computational costs of these PEFT\nmethods with traditional fine-tuning approaches. With resource efficiency in\nmind, we investigate their update behavior across different rank dimensions. We\nfind that the evaluated PEFT methods are only half as memory-efficient when\napplied to depthwise-separable convolution architectures, compared to their\nefficiency with LLMs. Conversely, when targeting convolu- tional architectures\noptimized for edge deployment, adapter-based PEFT methods can reduce floating\npoint operations (FLOPs) during model updates by up to 95%. These insights\noffer valuable guidance for selecting PEFT methods based on hardware\nconstraints, performance requirements, and application needs. Our code is\nonline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-efficient fine-tuning (PEFT) methods reduce the computational costs\nof updating deep learning models by minimizing the number of additional\nparameters used to adapt a model to a down- stream task. While extensively\nresearched in large language models (LLMs), their application to smaller models\nused on edge devices, such as convolutional neural networks, remains\nunderexplored. This paper benchmarks and analyzes popular PEFT methods on\nconvolutional architectures typically deployed in resource-constrained edge\nenvironments. We evaluate LoRA, DoRA, and GaLore for updating standard and\ndepthwise convolutional architectures to handle distribution shifts and\naccommodate unseen classes. We utilize recently proposed PyTorch profilers to\ncompare the updated model performance and computational costs of these PEFT\nmethods with traditional fine-tuning approaches. With resource efficiency in\nmind, we investigate their update behavior across different rank dimensions. We\nfind that the evaluated PEFT methods are only half as memory-efficient when\napplied to depthwise-separable convolution architectures, compared to their\nefficiency with LLMs. Conversely, when targeting convolu- tional architectures\noptimized for edge deployment, adapter-based PEFT methods can reduce floating\npoint operations (FLOPs) during model updates by up to 95%. These insights\noffer valuable guidance for selecting PEFT methods based on hardware\nconstraints, performance requirements, and application needs. Our code is\nonline."
                },
                "authors": [
                    {
                        "name": "Georg Slamanig"
                    },
                    {
                        "name": "Francesco Corti"
                    },
                    {
                        "name": "Olga Saukh"
                    }
                ],
                "author_detail": {
                    "name": "Olga Saukh"
                },
                "author": "Olga Saukh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23536v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23536v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23590v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23590v2",
                "updated": "2025-07-31T12:57:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    12,
                    57,
                    49,
                    3,
                    212,
                    0
                ],
                "published": "2024-10-31T03:01:35Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    3,
                    1,
                    35,
                    3,
                    305,
                    0
                ],
                "title": "The Nudge Average Treatment Effect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Nudge Average Treatment Effect"
                },
                "summary": "The instrumental variable method is a prominent approach to recover under\ncertain conditions, valid inference about a treatment causal effect even when\nunmeasured confounding might be present. In a groundbreaking paper, Imbens and\nAngrist (1994) established that a valid instrument nonparametrically identifies\nthe average causal effect among compliers, also known as the local average\ntreatment effect under a certain monotonicity assumption which rules out the\nexistence of so-called defiers. An often-cited attractive property of\nmonotonicity is that it facilitates a causal interpretation of the instrumental\nvariable estimand without restricting the degree of heterogeneity of the\ntreatment causal effect. In this paper, we introduce an alternative equally\nstraightforward and interpretable condition for identification, which\naccommodates both the presence of defiers and heterogenous treatment effects.\nMainly, we show that under our new conditions, the instrumental variable\nestimand recovers the average causal effect for the subgroup of units for whom\nthe treatment is manipulable by the instrument, a subgroup which may consist of\nboth defiers and compliers, therefore recovering an effect estimand we aptly\ncall the Nudge Average Treatment Effect.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The instrumental variable method is a prominent approach to recover under\ncertain conditions, valid inference about a treatment causal effect even when\nunmeasured confounding might be present. In a groundbreaking paper, Imbens and\nAngrist (1994) established that a valid instrument nonparametrically identifies\nthe average causal effect among compliers, also known as the local average\ntreatment effect under a certain monotonicity assumption which rules out the\nexistence of so-called defiers. An often-cited attractive property of\nmonotonicity is that it facilitates a causal interpretation of the instrumental\nvariable estimand without restricting the degree of heterogeneity of the\ntreatment causal effect. In this paper, we introduce an alternative equally\nstraightforward and interpretable condition for identification, which\naccommodates both the presence of defiers and heterogenous treatment effects.\nMainly, we show that under our new conditions, the instrumental variable\nestimand recovers the average causal effect for the subgroup of units for whom\nthe treatment is manipulable by the instrument, a subgroup which may consist of\nboth defiers and compliers, therefore recovering an effect estimand we aptly\ncall the Nudge Average Treatment Effect."
                },
                "authors": [
                    {
                        "name": "Eric J Tchetgen Tchetgen"
                    }
                ],
                "author_detail": {
                    "name": "Eric J Tchetgen Tchetgen"
                },
                "author": "Eric J Tchetgen Tchetgen",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23590v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23590v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23506v1",
                "updated": "2025-07-31T12:43:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    12,
                    43,
                    53,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T12:43:53Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    12,
                    43,
                    53,
                    3,
                    212,
                    0
                ],
                "title": "Neural Posterior Estimation of Neutron Star Equations of State",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Posterior Estimation of Neutron Star Equations of State"
                },
                "summary": "We present a simulation-based inference (SBI) framework to constrain the\nneutron star (NS) equation of state (EoS) from astrophysical observations of\nmasses, radii and tidal deformabilities, using Neural posterior estimation\n(NPE) with Conditional Normalising Flows (CNF). To ensure that the model\nconforms with reality, physics-informed constraints are embedded directly into\nthe training loss. This enables efficient, likelihood-free inference of full\nposterior distributions for key thermodynamic quantities-including pressure,\nsquared speed of sound, and the trace anomaly-conditioned on observational\ndata. Our models are trained on synthetic datasets generated from two agnostic\nEoS priors: polytropic parametrizations (PT) and gaussian process (GP)\nreconstructions. These datasets span various scenarios, including the presence\nor absence of tidal deformability information and observational noise. Across\nall settings, the method produces accurate and well-calibrated posteriors, with\nuncertainties reduced when tidal deformability constraints are included.\nFurthermore, we find that the behavior of normalized predictive dispersions is\nstrongly correlated with the maximum central density inside NSs, suggesting\nthat the model can indirectly infer this physically meaningful quantity. The\napproach generalizes well across EoS families and accurately reconstructs\nderivative quantities such as the polytropic index, demonstrating its\nrobustness and potential for probing dense matter in NS cores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a simulation-based inference (SBI) framework to constrain the\nneutron star (NS) equation of state (EoS) from astrophysical observations of\nmasses, radii and tidal deformabilities, using Neural posterior estimation\n(NPE) with Conditional Normalising Flows (CNF). To ensure that the model\nconforms with reality, physics-informed constraints are embedded directly into\nthe training loss. This enables efficient, likelihood-free inference of full\nposterior distributions for key thermodynamic quantities-including pressure,\nsquared speed of sound, and the trace anomaly-conditioned on observational\ndata. Our models are trained on synthetic datasets generated from two agnostic\nEoS priors: polytropic parametrizations (PT) and gaussian process (GP)\nreconstructions. These datasets span various scenarios, including the presence\nor absence of tidal deformability information and observational noise. Across\nall settings, the method produces accurate and well-calibrated posteriors, with\nuncertainties reduced when tidal deformability constraints are included.\nFurthermore, we find that the behavior of normalized predictive dispersions is\nstrongly correlated with the maximum central density inside NSs, suggesting\nthat the model can indirectly infer this physically meaningful quantity. The\napproach generalizes well across EoS families and accurately reconstructs\nderivative quantities such as the polytropic index, demonstrating its\nrobustness and potential for probing dense matter in NS cores."
                },
                "authors": [
                    {
                        "name": "Valria Carvalho"
                    },
                    {
                        "name": "Mrcio Ferreira"
                    },
                    {
                        "name": "Micha Bejger"
                    },
                    {
                        "name": "Constana Providncia"
                    }
                ],
                "author_detail": {
                    "name": "Constana Providncia"
                },
                "author": "Constana Providncia",
                "arxiv_comment": "18 pages, 18 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "nucl-th",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15621v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15621v2",
                "updated": "2025-07-31T12:41:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    12,
                    41,
                    25,
                    3,
                    212,
                    0
                ],
                "published": "2025-03-19T18:10:12Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    18,
                    10,
                    12,
                    2,
                    78,
                    0
                ],
                "title": "LLaVA-MORE: A Comparative Study of LLMs and Visual Backbones for\n  Enhanced Visual Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaVA-MORE: A Comparative Study of LLMs and Visual Backbones for\n  Enhanced Visual Instruction Tuning"
                },
                "summary": "Recent progress in Multimodal Large Language Models (MLLMs) has highlighted\nthe critical roles of both the visual backbone and the underlying language\nmodel. While prior work has primarily focused on scaling these components to\nbillions of parameters, the trade-offs between model size, architecture, and\nperformance remain underexplored. Additionally, inconsistencies in training\ndata and evaluation protocols have hindered direct comparisons, making it\ndifficult to derive optimal design choices. In this paper, we introduce\nLLaVA-MORE, a new family of MLLMs that integrates recent language models with\ndiverse visual backbones. To ensure fair comparisons, we employ a unified\ntraining protocol applied consistently across all architectures. Our analysis\nsystematically explores both small- and medium-scale LLMs -- including Phi-4,\nLLaMA-3.1, and Gemma-2 -- to evaluate multimodal reasoning, generation, and\ninstruction following, while examining the relationship between model size and\nperformance. Beyond evaluating the LLM impact on final results, we conduct a\ncomprehensive study of various visual encoders, ranging from CLIP-based\narchitectures to alternatives such as DINOv2, SigLIP, and SigLIP2. Additional\nexperiments investigate the effects of increased image resolution and\nvariations in pre-training datasets. Overall, our results provide insights into\nthe design of more effective MLLMs, offering a reproducible evaluation\nframework that facilitates direct comparisons and can guide future model\ndevelopment. Our source code and trained models are publicly available at:\nhttps://github.com/aimagelab/LLaVA-MORE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in Multimodal Large Language Models (MLLMs) has highlighted\nthe critical roles of both the visual backbone and the underlying language\nmodel. While prior work has primarily focused on scaling these components to\nbillions of parameters, the trade-offs between model size, architecture, and\nperformance remain underexplored. Additionally, inconsistencies in training\ndata and evaluation protocols have hindered direct comparisons, making it\ndifficult to derive optimal design choices. In this paper, we introduce\nLLaVA-MORE, a new family of MLLMs that integrates recent language models with\ndiverse visual backbones. To ensure fair comparisons, we employ a unified\ntraining protocol applied consistently across all architectures. Our analysis\nsystematically explores both small- and medium-scale LLMs -- including Phi-4,\nLLaMA-3.1, and Gemma-2 -- to evaluate multimodal reasoning, generation, and\ninstruction following, while examining the relationship between model size and\nperformance. Beyond evaluating the LLM impact on final results, we conduct a\ncomprehensive study of various visual encoders, ranging from CLIP-based\narchitectures to alternatives such as DINOv2, SigLIP, and SigLIP2. Additional\nexperiments investigate the effects of increased image resolution and\nvariations in pre-training datasets. Overall, our results provide insights into\nthe design of more effective MLLMs, offering a reproducible evaluation\nframework that facilitates direct comparisons and can guide future model\ndevelopment. Our source code and trained models are publicly available at:\nhttps://github.com/aimagelab/LLaVA-MORE."
                },
                "authors": [
                    {
                        "name": "Federico Cocchi"
                    },
                    {
                        "name": "Nicholas Moratelli"
                    },
                    {
                        "name": "Davide Caffagni"
                    },
                    {
                        "name": "Sara Sarto"
                    },
                    {
                        "name": "Lorenzo Baraldi"
                    },
                    {
                        "name": "Marcella Cornia"
                    },
                    {
                        "name": "Rita Cucchiara"
                    }
                ],
                "author_detail": {
                    "name": "Rita Cucchiara"
                },
                "author": "Rita Cucchiara",
                "arxiv_comment": "ICCV 2025 Workshop on What is Next in Multimodal Foundation Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15621v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15621v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01694v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01694v2",
                "updated": "2025-07-31T12:30:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    12,
                    30,
                    18,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-02T13:20:52Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    13,
                    20,
                    52,
                    2,
                    183,
                    0
                ],
                "title": "Graph Representation-based Model Poisoning on Federated Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Representation-based Model Poisoning on Federated Large Language\n  Models"
                },
                "summary": "Federated large language models (FedLLMs) enable powerful generative\ncapabilities within wireless networks while preserving data privacy.\nNonetheless, FedLLMs remain vulnerable to model poisoning attacks. This article\nfirst reviews recent advancements in model poisoning techniques and existing\ndefense mechanisms for FedLLMs, underscoring critical limitations, especially\nwhen dealing with non-IID textual data distributions. Current defense\nstrategies predominantly employ distance or similarity-based outlier detection\nmechanisms, relying on the assumption that malicious updates markedly differ\nfrom benign statistical patterns. However, this assumption becomes inadequate\nagainst adaptive adversaries targeting billion-parameter LLMs. The article\nfurther investigates graph representation-based model poisoning (GRMP), an\nemerging attack paradigm that exploits higher-order correlations among benign\nclient gradients to craft malicious updates indistinguishable from legitimate\nones. GRMP can effectively circumvent advanced defense systems, causing\nsubstantial degradation in model accuracy and overall performance. Moreover,\nthe article outlines a forward-looking research roadmap that emphasizes the\nnecessity of graph-aware secure aggregation methods, specialized vulnerability\nmetrics tailored for FedLLMs, and evaluation frameworks to enhance the\nrobustness of federated language model deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated large language models (FedLLMs) enable powerful generative\ncapabilities within wireless networks while preserving data privacy.\nNonetheless, FedLLMs remain vulnerable to model poisoning attacks. This article\nfirst reviews recent advancements in model poisoning techniques and existing\ndefense mechanisms for FedLLMs, underscoring critical limitations, especially\nwhen dealing with non-IID textual data distributions. Current defense\nstrategies predominantly employ distance or similarity-based outlier detection\nmechanisms, relying on the assumption that malicious updates markedly differ\nfrom benign statistical patterns. However, this assumption becomes inadequate\nagainst adaptive adversaries targeting billion-parameter LLMs. The article\nfurther investigates graph representation-based model poisoning (GRMP), an\nemerging attack paradigm that exploits higher-order correlations among benign\nclient gradients to craft malicious updates indistinguishable from legitimate\nones. GRMP can effectively circumvent advanced defense systems, causing\nsubstantial degradation in model accuracy and overall performance. Moreover,\nthe article outlines a forward-looking research roadmap that emphasizes the\nnecessity of graph-aware secure aggregation methods, specialized vulnerability\nmetrics tailored for FedLLMs, and evaluation frameworks to enhance the\nrobustness of federated language model deployments."
                },
                "authors": [
                    {
                        "name": "Hanlin Cai"
                    },
                    {
                        "name": "Haofan Dong"
                    },
                    {
                        "name": "Houtianfu Wang"
                    },
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Ozgur B. Akan"
                    }
                ],
                "author_detail": {
                    "name": "Ozgur B. Akan"
                },
                "author": "Ozgur B. Akan",
                "arxiv_comment": "7 pages, 5 figures (Submitted to IEEE Communication Magazine)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01694v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01694v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23495v1",
                "updated": "2025-07-31T12:29:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    12,
                    29,
                    49,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T12:29:49Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    12,
                    29,
                    49,
                    3,
                    212,
                    0
                ],
                "title": "Incorporating structural uncertainty in causal decision making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incorporating structural uncertainty in causal decision making"
                },
                "summary": "Practitioners making decisions based on causal effects typically ignore\nstructural uncertainty. We analyze when this uncertainty is consequential\nenough to warrant methodological solutions (Bayesian model averaging over\ncompeting causal structures). Focusing on bivariate relationships ($X\n\\rightarrow Y$ vs. $X \\leftarrow Y$), we establish that model averaging is\nbeneficial when: (1) structural uncertainty is moderate to high, (2) causal\neffects differ substantially between structures, and (3) loss functions are\nsufficiently sensitive to the size of the causal effect. We prove optimality\nresults of our suggested methodological solution under regularity conditions\nand demonstrate through simulations that modern causal discovery methods can\nprovide, within limits, the necessary quantification. Our framework complements\nexisting robust causal inference approaches by addressing a distinct source of\nuncertainty typically overlooked in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practitioners making decisions based on causal effects typically ignore\nstructural uncertainty. We analyze when this uncertainty is consequential\nenough to warrant methodological solutions (Bayesian model averaging over\ncompeting causal structures). Focusing on bivariate relationships ($X\n\\rightarrow Y$ vs. $X \\leftarrow Y$), we establish that model averaging is\nbeneficial when: (1) structural uncertainty is moderate to high, (2) causal\neffects differ substantially between structures, and (3) loss functions are\nsufficiently sensitive to the size of the causal effect. We prove optimality\nresults of our suggested methodological solution under regularity conditions\nand demonstrate through simulations that modern causal discovery methods can\nprovide, within limits, the necessary quantification. Our framework complements\nexisting robust causal inference approaches by addressing a distinct source of\nuncertainty typically overlooked in practice."
                },
                "authors": [
                    {
                        "name": "Maurits Kaptein"
                    }
                ],
                "author_detail": {
                    "name": "Maurits Kaptein"
                },
                "author": "Maurits Kaptein",
                "arxiv_comment": "This work is under review at the Journal of Causal Inference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.04613v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.04613v2",
                "updated": "2025-07-31T12:24:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    12,
                    24,
                    11,
                    3,
                    212,
                    0
                ],
                "published": "2024-03-07T15:58:25Z",
                "published_parsed": [
                    2024,
                    3,
                    7,
                    15,
                    58,
                    25,
                    3,
                    67,
                    0
                ],
                "title": "Conditional Predictive Inference for Missing Outcomes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conditional Predictive Inference for Missing Outcomes"
                },
                "summary": "We study the problem of conditional predictive inference on multiple outcomes\nmissing at random (MAR) -- or equivalently, under covariate shift. While the\nweighted conformal prediction offers a tool for inference under covariate shift\nwith a marginal coverage guarantee, procedures with conditional coverage\nguarantees are often desired in many applications to ensure reliable inference\nfor a specific group of individuals. A standard approach to overcoming the\nfundamental limitation of distribution-free conditional predictive inference is\nto relax the target and instead aim to control coverage conditional on a local\narea, subset, or bin in the feature space. However, when the missingness\npattern depends on the features, this relaxation remains challenging due to the\nviolation of the MAR assumption with respect to the bins. To address this\nissue, we propose a propensity score $\\epsilon$-discretization, a carefully\ndesigned binning strategy based on the propensity score, which enables valid\nconditional inference. Based on this strategy, we develop a procedure -- termed\npro-CP -- that enables simultaneous conditional predictive inference for\nmultiple missing outcomes. We show that pro-CP controls the bin-conditional\ncoverage rate in a distribution-free manner when the propensity score is either\nknown exactly or estimated with sufficient accuracy. Furthermore, we provide a\ntheoretical bound on the coverage rate when the propensity score is unknown and\nmust be estimated. Notably, the error bound remains constant and depends only\non the estimation quality, not on the sample size or the number of outcomes\nunder consideration. In extensive empirical experiments on simulated data and\non a job search intervention dataset, we illustrate that our procedures provide\ninformative prediction sets with valid conditional coverage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of conditional predictive inference on multiple outcomes\nmissing at random (MAR) -- or equivalently, under covariate shift. While the\nweighted conformal prediction offers a tool for inference under covariate shift\nwith a marginal coverage guarantee, procedures with conditional coverage\nguarantees are often desired in many applications to ensure reliable inference\nfor a specific group of individuals. A standard approach to overcoming the\nfundamental limitation of distribution-free conditional predictive inference is\nto relax the target and instead aim to control coverage conditional on a local\narea, subset, or bin in the feature space. However, when the missingness\npattern depends on the features, this relaxation remains challenging due to the\nviolation of the MAR assumption with respect to the bins. To address this\nissue, we propose a propensity score $\\epsilon$-discretization, a carefully\ndesigned binning strategy based on the propensity score, which enables valid\nconditional inference. Based on this strategy, we develop a procedure -- termed\npro-CP -- that enables simultaneous conditional predictive inference for\nmultiple missing outcomes. We show that pro-CP controls the bin-conditional\ncoverage rate in a distribution-free manner when the propensity score is either\nknown exactly or estimated with sufficient accuracy. Furthermore, we provide a\ntheoretical bound on the coverage rate when the propensity score is unknown and\nmust be estimated. Notably, the error bound remains constant and depends only\non the estimation quality, not on the sample size or the number of outcomes\nunder consideration. In extensive empirical experiments on simulated data and\non a job search intervention dataset, we illustrate that our procedures provide\ninformative prediction sets with valid conditional coverage."
                },
                "authors": [
                    {
                        "name": "Yonghoon Lee"
                    },
                    {
                        "name": "Edgar Dobriban"
                    },
                    {
                        "name": "Eric Tchetgen Tchetgen"
                    }
                ],
                "author_detail": {
                    "name": "Eric Tchetgen Tchetgen"
                },
                "author": "Eric Tchetgen Tchetgen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.04613v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.04613v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23488v1",
                "updated": "2025-07-31T12:10:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    12,
                    10,
                    27,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T12:10:27Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    12,
                    10,
                    27,
                    3,
                    212,
                    0
                ],
                "title": "Causal Reasoning in Pieces: Modular In-Context Learning for Causal\n  Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Reasoning in Pieces: Modular In-Context Learning for Causal\n  Discovery"
                },
                "summary": "Causal inference remains a fundamental challenge for large language models.\nRecent advances in internal reasoning with large language models have sparked\ninterest in whether state-of-the-art reasoning models can robustly perform\ncausal discovery-a task where conventional models often suffer from severe\noverfitting and near-random performance under data perturbations. We study\ncausal discovery on the Corr2Cause benchmark using the emergent OpenAI's\no-series and DeepSeek-R model families and find that these reasoning-first\narchitectures achieve significantly greater native gains than prior approaches.\nTo capitalize on these strengths, we introduce a modular in-context pipeline\ninspired by the Tree-of-Thoughts and Chain-of-Thoughts methodologies, yielding\nnearly three-fold improvements over conventional baselines. We further probe\nthe pipeline's impact by analyzing reasoning chain length, complexity, and\nconducting qualitative and quantitative comparisons between conventional and\nreasoning models. Our findings suggest that while advanced reasoning models\nrepresent a substantial leap forward, carefully structured in-context\nframeworks are essential to maximize their capabilities and offer a\ngeneralizable blueprint for causal discovery across diverse domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal inference remains a fundamental challenge for large language models.\nRecent advances in internal reasoning with large language models have sparked\ninterest in whether state-of-the-art reasoning models can robustly perform\ncausal discovery-a task where conventional models often suffer from severe\noverfitting and near-random performance under data perturbations. We study\ncausal discovery on the Corr2Cause benchmark using the emergent OpenAI's\no-series and DeepSeek-R model families and find that these reasoning-first\narchitectures achieve significantly greater native gains than prior approaches.\nTo capitalize on these strengths, we introduce a modular in-context pipeline\ninspired by the Tree-of-Thoughts and Chain-of-Thoughts methodologies, yielding\nnearly three-fold improvements over conventional baselines. We further probe\nthe pipeline's impact by analyzing reasoning chain length, complexity, and\nconducting qualitative and quantitative comparisons between conventional and\nreasoning models. Our findings suggest that while advanced reasoning models\nrepresent a substantial leap forward, carefully structured in-context\nframeworks are essential to maximize their capabilities and offer a\ngeneralizable blueprint for causal discovery across diverse domains."
                },
                "authors": [
                    {
                        "name": "Kacper Kadziolka"
                    },
                    {
                        "name": "Saber Salehkaleybar"
                    }
                ],
                "author_detail": {
                    "name": "Saber Salehkaleybar"
                },
                "author": "Saber Salehkaleybar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23486v1",
                "updated": "2025-07-31T12:10:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    12,
                    10,
                    0,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T12:10:00Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    12,
                    10,
                    0,
                    3,
                    212,
                    0
                ],
                "title": "A Novel Evaluation Benchmark for Medical LLMs: Illuminating Safety and\n  Effectiveness in Clinical Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Evaluation Benchmark for Medical LLMs: Illuminating Safety and\n  Effectiveness in Clinical Domains"
                },
                "summary": "Large language models (LLMs) hold promise in clinical decision support but\nface major challenges in safety evaluation and effectiveness validation. We\ndeveloped the Clinical Safety-Effectiveness Dual-Track Benchmark (CSEDB), a\nmultidimensional framework built on clinical expert consensus, encompassing 30\ncriteria covering critical areas like critical illness recognition, guideline\nadherence, and medication safety, with weighted consequence measures.\nThirty-two specialist physicians developed and reviewed 2,069 open-ended Q&A\nitems aligned with these criteria, spanning 26 clinical departments to simulate\nreal-world scenarios. Benchmark testing of six LLMs revealed moderate overall\nperformance (average total score 57.2%, safety 54.7%, effectiveness 62.3%),\nwith a significant 13.3% performance drop in high-risk scenarios (p < 0.0001).\nDomain-specific medical LLMs showed consistent performance advantages over\ngeneral-purpose models, with relatively higher top scores in safety (0.912) and\neffectiveness (0.861). The findings of this study not only provide a\nstandardized metric for evaluating the clinical application of medical LLMs,\nfacilitating comparative analyses, risk exposure identification, and\nimprovement directions across different scenarios, but also hold the potential\nto promote safer and more effective deployment of large language models in\nhealthcare environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) hold promise in clinical decision support but\nface major challenges in safety evaluation and effectiveness validation. We\ndeveloped the Clinical Safety-Effectiveness Dual-Track Benchmark (CSEDB), a\nmultidimensional framework built on clinical expert consensus, encompassing 30\ncriteria covering critical areas like critical illness recognition, guideline\nadherence, and medication safety, with weighted consequence measures.\nThirty-two specialist physicians developed and reviewed 2,069 open-ended Q&A\nitems aligned with these criteria, spanning 26 clinical departments to simulate\nreal-world scenarios. Benchmark testing of six LLMs revealed moderate overall\nperformance (average total score 57.2%, safety 54.7%, effectiveness 62.3%),\nwith a significant 13.3% performance drop in high-risk scenarios (p < 0.0001).\nDomain-specific medical LLMs showed consistent performance advantages over\ngeneral-purpose models, with relatively higher top scores in safety (0.912) and\neffectiveness (0.861). The findings of this study not only provide a\nstandardized metric for evaluating the clinical application of medical LLMs,\nfacilitating comparative analyses, risk exposure identification, and\nimprovement directions across different scenarios, but also hold the potential\nto promote safer and more effective deployment of large language models in\nhealthcare environments."
                },
                "authors": [
                    {
                        "name": "Shirui Wang"
                    },
                    {
                        "name": "Zhihui Tang"
                    },
                    {
                        "name": "Huaxia Yang"
                    },
                    {
                        "name": "Qiuhong Gong"
                    },
                    {
                        "name": "Tiantian Gu"
                    },
                    {
                        "name": "Hongyang Ma"
                    },
                    {
                        "name": "Yongxin Wang"
                    },
                    {
                        "name": "Wubin Sun"
                    },
                    {
                        "name": "Zeliang Lian"
                    },
                    {
                        "name": "Kehang Mao"
                    },
                    {
                        "name": "Yinan Jiang"
                    },
                    {
                        "name": "Zhicheng Huang"
                    },
                    {
                        "name": "Lingyun Ma"
                    },
                    {
                        "name": "Wenjie Shen"
                    },
                    {
                        "name": "Yajie Ji"
                    },
                    {
                        "name": "Yunhui Tan"
                    },
                    {
                        "name": "Chunbo Wang"
                    },
                    {
                        "name": "Yunlu Gao"
                    },
                    {
                        "name": "Qianling Ye"
                    },
                    {
                        "name": "Rui Lin"
                    },
                    {
                        "name": "Mingyu Chen"
                    },
                    {
                        "name": "Lijuan Niu"
                    },
                    {
                        "name": "Zhihao Wang"
                    },
                    {
                        "name": "Peng Yu"
                    },
                    {
                        "name": "Mengran Lang"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Huimin Zhang"
                    },
                    {
                        "name": "Haitao Shen"
                    },
                    {
                        "name": "Long Chen"
                    },
                    {
                        "name": "Qiguang Zhao"
                    },
                    {
                        "name": "Si-Xuan Liu"
                    },
                    {
                        "name": "Lina Zhou"
                    },
                    {
                        "name": "Hua Gao"
                    },
                    {
                        "name": "Dongqiang Ye"
                    },
                    {
                        "name": "Lingmin Meng"
                    },
                    {
                        "name": "Youtao Yu"
                    },
                    {
                        "name": "Naixin Liang"
                    },
                    {
                        "name": "Jianxiong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Jianxiong Wu"
                },
                "author": "Jianxiong Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23480v1",
                "updated": "2025-07-31T12:02:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    12,
                    2,
                    40,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T12:02:40Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    12,
                    2,
                    40,
                    3,
                    212,
                    0
                ],
                "title": "FastPoint: Accelerating 3D Point Cloud Model Inference via Sample Point\n  Distance Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastPoint: Accelerating 3D Point Cloud Model Inference via Sample Point\n  Distance Prediction"
                },
                "summary": "Deep neural networks have revolutionized 3D point cloud processing, yet\nefficiently handling large and irregular point clouds remains challenging. To\ntackle this problem, we introduce FastPoint, a novel software-based\nacceleration technique that leverages the predictable distance trend between\nsampled points during farthest point sampling. By predicting the distance\ncurve, we can efficiently identify subsequent sample points without\nexhaustively computing all pairwise distances. Our proposal substantially\naccelerates farthest point sampling and neighbor search operations while\npreserving sampling quality and model performance. By integrating FastPoint\ninto state-of-the-art 3D point cloud models, we achieve 2.55x end-to-end\nspeedup on NVIDIA RTX 3090 GPU without sacrificing accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep neural networks have revolutionized 3D point cloud processing, yet\nefficiently handling large and irregular point clouds remains challenging. To\ntackle this problem, we introduce FastPoint, a novel software-based\nacceleration technique that leverages the predictable distance trend between\nsampled points during farthest point sampling. By predicting the distance\ncurve, we can efficiently identify subsequent sample points without\nexhaustively computing all pairwise distances. Our proposal substantially\naccelerates farthest point sampling and neighbor search operations while\npreserving sampling quality and model performance. By integrating FastPoint\ninto state-of-the-art 3D point cloud models, we achieve 2.55x end-to-end\nspeedup on NVIDIA RTX 3090 GPU without sacrificing accuracy."
                },
                "authors": [
                    {
                        "name": "Donghyun Lee"
                    },
                    {
                        "name": "Dawoon Jeong"
                    },
                    {
                        "name": "Jae W. Lee"
                    },
                    {
                        "name": "Hongil Yoon"
                    }
                ],
                "author_detail": {
                    "name": "Hongil Yoon"
                },
                "author": "Hongil Yoon",
                "arxiv_comment": "Accepted to ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23474v1",
                "updated": "2025-07-31T11:55:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    11,
                    55,
                    2,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T11:55:02Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    11,
                    55,
                    2,
                    3,
                    212,
                    0
                ],
                "title": "Finger Force Decoding from Motor Units Activity on Neuromorphic Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finger Force Decoding from Motor Units Activity on Neuromorphic Hardware"
                },
                "summary": "Accurate finger force estimation is critical for next-generation\nhuman-machine interfaces. Traditional electromyography (EMG)-based decoding\nmethods using deep learning require large datasets and high computational\nresources, limiting their use in real-time, embedded systems. Here, we propose\na novel approach that performs finger force regression using spike trains from\nindividual motor neurons, extracted from high-density EMG. These biologically\ngrounded signals drive a spiking neural network implemented on a mixed-signal\nneuromorphic processor. Unlike prior work that encodes EMG into events, our\nmethod exploits spike timing on motor units to perform low-power, real-time\ninference. This is the first demonstration of motor neuron-based continuous\nregression computed directly on neuromorphic hardware. Our results confirm\naccurate finger-specific force prediction with minimal energy use, opening new\npossibilities for embedded decoding in prosthetics and wearable\nneurotechnology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate finger force estimation is critical for next-generation\nhuman-machine interfaces. Traditional electromyography (EMG)-based decoding\nmethods using deep learning require large datasets and high computational\nresources, limiting their use in real-time, embedded systems. Here, we propose\na novel approach that performs finger force regression using spike trains from\nindividual motor neurons, extracted from high-density EMG. These biologically\ngrounded signals drive a spiking neural network implemented on a mixed-signal\nneuromorphic processor. Unlike prior work that encodes EMG into events, our\nmethod exploits spike timing on motor units to perform low-power, real-time\ninference. This is the first demonstration of motor neuron-based continuous\nregression computed directly on neuromorphic hardware. Our results confirm\naccurate finger-specific force prediction with minimal energy use, opening new\npossibilities for embedded decoding in prosthetics and wearable\nneurotechnology."
                },
                "authors": [
                    {
                        "name": "Farah Baracat"
                    },
                    {
                        "name": "Giacomo Indiveri"
                    },
                    {
                        "name": "Elisa Donati"
                    }
                ],
                "author_detail": {
                    "name": "Elisa Donati"
                },
                "author": "Elisa Donati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23472v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23472v1",
                "updated": "2025-07-31T11:53:15Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    11,
                    53,
                    15,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T11:53:15Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    11,
                    53,
                    15,
                    3,
                    212,
                    0
                ],
                "title": "Kilo-scale point-source inference using Parametric Cataloging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kilo-scale point-source inference using Parametric Cataloging"
                },
                "summary": "The estimation of the number of point-sources in the sky is one the oldest\nproblems in astronomy, yet an easy and efficient method for estimating the\nuncertainty on these counts is still an open problem. Probabilistic cataloging\nsolves the general point-source inference problem, but the trans-dimensional\nnature of the inference method requires a bespoke approach that is difficult to\nscale. Here it is shown that probabilistic cataloging can be performed in a\nfixed-dimensional framework called Parametric Cataloging under mild assumptions\non some of the priors. The method requires only a simple reparameterization of\nthe flux coordinates, yielding an accessible method that can be implemented in\nmost probabilistic programming environments. As the parameter space is\nfixed-dimensional, off the shelf gradient based samplers can be employed which\nallows the method to scale to tens of thousands of sources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The estimation of the number of point-sources in the sky is one the oldest\nproblems in astronomy, yet an easy and efficient method for estimating the\nuncertainty on these counts is still an open problem. Probabilistic cataloging\nsolves the general point-source inference problem, but the trans-dimensional\nnature of the inference method requires a bespoke approach that is difficult to\nscale. Here it is shown that probabilistic cataloging can be performed in a\nfixed-dimensional framework called Parametric Cataloging under mild assumptions\non some of the priors. The method requires only a simple reparameterization of\nthe flux coordinates, yielding an accessible method that can be implemented in\nmost probabilistic programming environments. As the parameter space is\nfixed-dimensional, off the shelf gradient based samplers can be employed which\nallows the method to scale to tens of thousands of sources."
                },
                "authors": [
                    {
                        "name": "Gabriel H. Collin"
                    }
                ],
                "author_detail": {
                    "name": "Gabriel H. Collin"
                },
                "author": "Gabriel H. Collin",
                "arxiv_comment": "7 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23472v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23472v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23470v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23470v1",
                "updated": "2025-07-31T11:49:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    11,
                    49,
                    1,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T11:49:01Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    11,
                    49,
                    1,
                    3,
                    212,
                    0
                ],
                "title": "Automated Feedback on Student-Generated UML and ER Diagrams Using Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Feedback on Student-Generated UML and ER Diagrams Using Large\n  Language Models"
                },
                "summary": "UML and ER diagrams are foundational in computer science education but come\nwith challenges for learners due to the need for abstract thinking, contextual\nunderstanding, and mastery of both syntax and semantics. These complexities are\ndifficult to address through traditional teaching methods, which often struggle\nto provide scalable, personalized feedback, especially in large classes. We\nintroduce DUET (Diagrammatic UML & ER Tutor), a prototype of an LLM-based tool,\nwhich converts a reference diagram and a student-submitted diagram into a\ntextual representation and provides structured feedback based on the\ndifferences. It uses a multi-stage LLM pipeline to compare diagrams and\ngenerate reflective feedback. Furthermore, the tool enables analytical insights\nfor educators, aiming to foster self-directed learning and inform instructional\nstrategies. We evaluated DUET through semi-structured interviews with six\nparticipants, including two educators and four teaching assistants. They\nidentified strengths such as accessibility, scalability, and learning support\nalongside limitations, including reliability and potential misuse. Participants\nalso suggested potential improvements, such as bulk upload functionality and\ninteractive clarification features. DUET presents a promising direction for\nintegrating LLMs into modeling education and offers a foundation for future\nclassroom integration and empirical evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UML and ER diagrams are foundational in computer science education but come\nwith challenges for learners due to the need for abstract thinking, contextual\nunderstanding, and mastery of both syntax and semantics. These complexities are\ndifficult to address through traditional teaching methods, which often struggle\nto provide scalable, personalized feedback, especially in large classes. We\nintroduce DUET (Diagrammatic UML & ER Tutor), a prototype of an LLM-based tool,\nwhich converts a reference diagram and a student-submitted diagram into a\ntextual representation and provides structured feedback based on the\ndifferences. It uses a multi-stage LLM pipeline to compare diagrams and\ngenerate reflective feedback. Furthermore, the tool enables analytical insights\nfor educators, aiming to foster self-directed learning and inform instructional\nstrategies. We evaluated DUET through semi-structured interviews with six\nparticipants, including two educators and four teaching assistants. They\nidentified strengths such as accessibility, scalability, and learning support\nalongside limitations, including reliability and potential misuse. Participants\nalso suggested potential improvements, such as bulk upload functionality and\ninteractive clarification features. DUET presents a promising direction for\nintegrating LLMs into modeling education and offers a foundation for future\nclassroom integration and empirical evaluation."
                },
                "authors": [
                    {
                        "name": "Sebastian Grtl"
                    },
                    {
                        "name": "Gloria Schimetta"
                    },
                    {
                        "name": "David Kerschbaumer"
                    },
                    {
                        "name": "Michael Liut"
                    },
                    {
                        "name": "Alexander Steinmaurer"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Steinmaurer"
                },
                "author": "Alexander Steinmaurer",
                "arxiv_comment": "Learnersourcing: Student-generated Content @ Scale Workshop at L@S\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23470v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23470v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06254v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06254v2",
                "updated": "2025-07-31T11:48:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    11,
                    48,
                    39,
                    3,
                    212,
                    0
                ],
                "published": "2024-11-09T19:03:56Z",
                "published_parsed": [
                    2024,
                    11,
                    9,
                    19,
                    3,
                    56,
                    5,
                    314,
                    0
                ],
                "title": "KeyB2: Selecting Key Blocks is Also Important for Long Document Ranking\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KeyB2: Selecting Key Blocks is Also Important for Long Document Ranking\n  with Large Language Models"
                },
                "summary": "The emergence of large language models (LLMs) such as Llama has significantly\nadvanced neural information retrieval (IR). However, applying LLMs to long\ndocument reranking remains computationally expensive and may be ineffective.\nMoreover, the internal behavior of LLMs during document relevance judgment is\nstill underexplored. In this paper, we begin with an in-depth analysis of\ndecoder-only LLM attention patterns and find that several attention heads\nconsistently align with relevance signals, yet this alignment deteriorates as\nirrelevant content increases. Motivated by this observation, we revisit and\nextend the block selection paradigm, introducing KeyB2, a scalable reranking\nframework that combines block pre-selection with powerful decoder-only LLMs.\nKeyB2 generalizes the selection stage to support BM25, cross-encoder, and\nbi-encoder, and adapts LLM to compute fine-grained relevance scores. We further\nintroduce a new bi-encoder strategy that performs strongly and efficiently.\nExtensive experiments on TREC DL 2019/2023 document task, Robust04, and MLDR-zh\ndemonstrate that KeyB2 outperforms baselines including RankLLaMA,\nRankLLaMA-MaxP/AvgP, and KeyB, achieving new state-of-the-art (SOTA) results on\nTREC DL 2019 document reranking task. In addition, KeyB2 reduces reranking\nlatency compared with RankLLaMA by over 83% and memory usage by over 74%,\npositioning it as a practical and effective solution for long document ranking\nwith LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models (LLMs) such as Llama has significantly\nadvanced neural information retrieval (IR). However, applying LLMs to long\ndocument reranking remains computationally expensive and may be ineffective.\nMoreover, the internal behavior of LLMs during document relevance judgment is\nstill underexplored. In this paper, we begin with an in-depth analysis of\ndecoder-only LLM attention patterns and find that several attention heads\nconsistently align with relevance signals, yet this alignment deteriorates as\nirrelevant content increases. Motivated by this observation, we revisit and\nextend the block selection paradigm, introducing KeyB2, a scalable reranking\nframework that combines block pre-selection with powerful decoder-only LLMs.\nKeyB2 generalizes the selection stage to support BM25, cross-encoder, and\nbi-encoder, and adapts LLM to compute fine-grained relevance scores. We further\nintroduce a new bi-encoder strategy that performs strongly and efficiently.\nExtensive experiments on TREC DL 2019/2023 document task, Robust04, and MLDR-zh\ndemonstrate that KeyB2 outperforms baselines including RankLLaMA,\nRankLLaMA-MaxP/AvgP, and KeyB, achieving new state-of-the-art (SOTA) results on\nTREC DL 2019 document reranking task. In addition, KeyB2 reduces reranking\nlatency compared with RankLLaMA by over 83% and memory usage by over 74%,\npositioning it as a practical and effective solution for long document ranking\nwith LLMs."
                },
                "authors": [
                    {
                        "name": "Minghan Li"
                    },
                    {
                        "name": "Eric Gaussier"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Guodong Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guodong Zhou"
                },
                "author": "Guodong Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06254v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06254v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23465v1",
                "updated": "2025-07-31T11:41:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    11,
                    41,
                    4,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T11:41:04Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    11,
                    41,
                    4,
                    3,
                    212,
                    0
                ],
                "title": "Role-Aware Language Models for Secure and Contextualized Access Control\n  in Organizations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role-Aware Language Models for Secure and Contextualized Access Control\n  in Organizations"
                },
                "summary": "As large language models (LLMs) are increasingly deployed in enterprise\nsettings, controlling model behavior based on user roles becomes an essential\nrequirement. Existing safety methods typically assume uniform access and focus\non preventing harmful or toxic outputs, without addressing role-specific access\nconstraints. In this work, we investigate whether LLMs can be fine-tuned to\ngenerate responses that reflect the access privileges associated with different\norganizational roles. We explore three modeling strategies: a BERT-based\nclassifier, an LLM-based classifier, and role-conditioned generation. To\nevaluate these approaches, we construct two complementary datasets. The first\nis adapted from existing instruction-tuning corpora through clustering and role\nlabeling, while the second is synthetically generated to reflect realistic,\nrole-sensitive enterprise scenarios. We assess model performance across varying\norganizational structures and analyze robustness to prompt injection, role\nmismatch, and jailbreak attempts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are increasingly deployed in enterprise\nsettings, controlling model behavior based on user roles becomes an essential\nrequirement. Existing safety methods typically assume uniform access and focus\non preventing harmful or toxic outputs, without addressing role-specific access\nconstraints. In this work, we investigate whether LLMs can be fine-tuned to\ngenerate responses that reflect the access privileges associated with different\norganizational roles. We explore three modeling strategies: a BERT-based\nclassifier, an LLM-based classifier, and role-conditioned generation. To\nevaluate these approaches, we construct two complementary datasets. The first\nis adapted from existing instruction-tuning corpora through clustering and role\nlabeling, while the second is synthetically generated to reflect realistic,\nrole-sensitive enterprise scenarios. We assess model performance across varying\norganizational structures and analyze robustness to prompt injection, role\nmismatch, and jailbreak attempts."
                },
                "authors": [
                    {
                        "name": "Saeed Almheiri"
                    },
                    {
                        "name": "Yerulan Kongrat"
                    },
                    {
                        "name": "Adrian Santosh"
                    },
                    {
                        "name": "Ruslan Tasmukhanov"
                    },
                    {
                        "name": "Josemaria Vera"
                    },
                    {
                        "name": "Muhammad Dehan Al Kautsar"
                    },
                    {
                        "name": "Fajri Koto"
                    }
                ],
                "author_detail": {
                    "name": "Fajri Koto"
                },
                "author": "Fajri Koto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23453v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23453v1",
                "updated": "2025-07-31T11:29:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    11,
                    29,
                    42,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T11:29:42Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    11,
                    29,
                    42,
                    3,
                    212,
                    0
                ],
                "title": "Counterfactual Evaluation for Blind Attack Detection in LLM-based\n  Evaluation Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterfactual Evaluation for Blind Attack Detection in LLM-based\n  Evaluation Systems"
                },
                "summary": "This paper investigates defenses for LLM-based evaluation systems against\nprompt injection. We formalize a class of threats called blind attacks, where a\ncandidate answer is crafted independently of the true answer to deceive the\nevaluator. To counter such attacks, we propose a framework that augments\nStandard Evaluation (SE) with Counterfactual Evaluation (CFE), which\nre-evaluates the submission against a deliberately false ground-truth answer.\nAn attack is detected if the system validates an answer under both standard and\ncounterfactual conditions. Experiments show that while standard evaluation is\nhighly vulnerable, our SE+CFE framework significantly improves security by\nboosting attack detection with minimal performance trade-offs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates defenses for LLM-based evaluation systems against\nprompt injection. We formalize a class of threats called blind attacks, where a\ncandidate answer is crafted independently of the true answer to deceive the\nevaluator. To counter such attacks, we propose a framework that augments\nStandard Evaluation (SE) with Counterfactual Evaluation (CFE), which\nre-evaluates the submission against a deliberately false ground-truth answer.\nAn attack is detected if the system validates an answer under both standard and\ncounterfactual conditions. Experiments show that while standard evaluation is\nhighly vulnerable, our SE+CFE framework significantly improves security by\nboosting attack detection with minimal performance trade-offs."
                },
                "authors": [
                    {
                        "name": "Lijia Liu"
                    },
                    {
                        "name": "Takumi Kondo"
                    },
                    {
                        "name": "Kyohei Atarashi"
                    },
                    {
                        "name": "Koh Takeuchi"
                    },
                    {
                        "name": "Jiyi Li"
                    },
                    {
                        "name": "Shigeru Saito"
                    },
                    {
                        "name": "Hisashi Kashima"
                    }
                ],
                "author_detail": {
                    "name": "Hisashi Kashima"
                },
                "author": "Hisashi Kashima",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23453v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23453v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23446v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23446v1",
                "updated": "2025-07-31T11:26:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    11,
                    26,
                    3,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T11:26:03Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    11,
                    26,
                    3,
                    3,
                    212,
                    0
                ],
                "title": "Miscellanea: \"Within-trial\" prognostic score adjustment is targeted\n  maximum likelihood estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Miscellanea: \"Within-trial\" prognostic score adjustment is targeted\n  maximum likelihood estimation"
                },
                "summary": "Adjustment for ``super'' or ``prognostic'' composite covariates has become\nmore popular in randomized trials recently. These prognostic covariates are\noften constructed from historical data by fitting a predictive model of the\noutcome on the raw covariates. A natural question that we have been asked by\napplied researchers is whether this can be done without the historical data:\ncan the prognostic covariate be constructed or derived from the trial data\nitself, possibly using different folds of the data, before adjusting for it?\nHere we clarify that such ``within-trial'' prognostic adjustment is nothing\nmore than a form of targeted maximum likelihood estimation (TMLE), a\nwell-studied procedure for optimal inference. We demonstrate the equivalence\nwith a simulation study and discuss the pros and cons of within-trial\nprognostic adjustment (standard efficient estimation) relative to standard TMLE\nand standard prognostic adjustment with historical data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adjustment for ``super'' or ``prognostic'' composite covariates has become\nmore popular in randomized trials recently. These prognostic covariates are\noften constructed from historical data by fitting a predictive model of the\noutcome on the raw covariates. A natural question that we have been asked by\napplied researchers is whether this can be done without the historical data:\ncan the prognostic covariate be constructed or derived from the trial data\nitself, possibly using different folds of the data, before adjusting for it?\nHere we clarify that such ``within-trial'' prognostic adjustment is nothing\nmore than a form of targeted maximum likelihood estimation (TMLE), a\nwell-studied procedure for optimal inference. We demonstrate the equivalence\nwith a simulation study and discuss the pros and cons of within-trial\nprognostic adjustment (standard efficient estimation) relative to standard TMLE\nand standard prognostic adjustment with historical data."
                },
                "authors": [
                    {
                        "name": "Emilie Hjbjerre-Frandsen"
                    },
                    {
                        "name": "Alejandro Schuler"
                    }
                ],
                "author_detail": {
                    "name": "Alejandro Schuler"
                },
                "author": "Alejandro Schuler",
                "arxiv_comment": "10 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23446v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23446v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23440v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23440v1",
                "updated": "2025-07-31T11:18:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    11,
                    18,
                    42,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T11:18:42Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    11,
                    18,
                    42,
                    3,
                    212,
                    0
                ],
                "title": "Self-Foveate: Enhancing Diversity and Difficulty of Synthesized\n  Instructions from Unsupervised Text via Multi-Level Foveation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Foveate: Enhancing Diversity and Difficulty of Synthesized\n  Instructions from Unsupervised Text via Multi-Level Foveation"
                },
                "summary": "Large language models (LLMs) with instruction following capabilities have\ndemonstrated impressive problem-solving abilities. While synthesizing\ninstructional data from unsupervised text has become a common approach for\ntraining such models, conventional methods rely heavily on human effort for\ndata annotation. Although existing automated synthesis paradigms have\nalleviated this constraint, they still exhibit significant limitations in\nensuring adequate diversity and difficulty of synthesized instructions. To\naddress these challenges, we propose Self-Foveate, an innovative LLM-driven\nmethod for instruction synthesis. This approach introduces a\n\"Micro-Scatter-Macro\" multi-level foveation methodology that effectively guides\nthe LLM to deeply excavate fine-grained information embedded in unsupervised\ntext, thereby enhancing both the diversity and difficulty of synthesized\ninstructions. Comprehensive experiments across multiple unsupervised corpora\nand diverse model architectures validate the effectiveness and superiority of\nour proposed method. We publicly release our data and codes:\nhttps://github.com/Mubuky/Self-Foveate",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with instruction following capabilities have\ndemonstrated impressive problem-solving abilities. While synthesizing\ninstructional data from unsupervised text has become a common approach for\ntraining such models, conventional methods rely heavily on human effort for\ndata annotation. Although existing automated synthesis paradigms have\nalleviated this constraint, they still exhibit significant limitations in\nensuring adequate diversity and difficulty of synthesized instructions. To\naddress these challenges, we propose Self-Foveate, an innovative LLM-driven\nmethod for instruction synthesis. This approach introduces a\n\"Micro-Scatter-Macro\" multi-level foveation methodology that effectively guides\nthe LLM to deeply excavate fine-grained information embedded in unsupervised\ntext, thereby enhancing both the diversity and difficulty of synthesized\ninstructions. Comprehensive experiments across multiple unsupervised corpora\nand diverse model architectures validate the effectiveness and superiority of\nour proposed method. We publicly release our data and codes:\nhttps://github.com/Mubuky/Self-Foveate"
                },
                "authors": [
                    {
                        "name": "Mingzhe Li"
                    },
                    {
                        "name": "Xin Lu"
                    },
                    {
                        "name": "Yanyan Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yanyan Zhao"
                },
                "author": "Yanyan Zhao",
                "arxiv_comment": "Accepted by Findings of ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23440v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23440v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14928v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14928v3",
                "updated": "2025-07-31T11:11:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    11,
                    11,
                    30,
                    3,
                    212,
                    0
                ],
                "published": "2025-04-21T07:48:20Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    7,
                    48,
                    20,
                    0,
                    111,
                    0
                ],
                "title": "EducationQ: Evaluating LLMs' Teaching Capabilities Through Multi-Agent\n  Dialogue Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EducationQ: Evaluating LLMs' Teaching Capabilities Through Multi-Agent\n  Dialogue Framework"
                },
                "summary": "Large language models (LLMs) increasingly serve as educational tools, yet\nevaluating their teaching capabilities remains challenging due to the\nresource-intensive, context-dependent, and methodologically complex nature of\nteacher-student interactions. We introduce EducationQ, a multi-agent dialogue\nframework that efficiently assesses teaching capabilities through simulated\ndynamic educational scenarios, featuring specialized agents for teaching,\nlearning, and evaluation. Testing 14 LLMs across major AI Organizations\n(OpenAI, Meta, Google, Anthropic, and others) on 1,498 questions spanning 13\ndisciplines and 10 difficulty levels reveals that teaching effectiveness does\nnot correlate linearly with model scale or general reasoning capabilities -\nwith some smaller open-source models outperforming larger commercial\ncounterparts in teaching contexts. This finding highlights a critical gap in\ncurrent evaluations that prioritize knowledge recall over interactive pedagogy.\nOur mixed-methods evaluation, combining quantitative metrics with qualitative\nanalysis and expert case studies, identifies distinct pedagogical strengths\nemployed by top-performing models (e.g., sophisticated questioning strategies,\nadaptive feedback mechanisms). Human expert evaluations show 78% agreement with\nour automated qualitative analysis of effective teaching behaviors, validating\nour methodology. EducationQ demonstrates that LLMs-as-teachers require\nspecialized optimization beyond simple scaling, suggesting next-generation\neducational AI prioritize targeted enhancement of specific pedagogical\neffectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) increasingly serve as educational tools, yet\nevaluating their teaching capabilities remains challenging due to the\nresource-intensive, context-dependent, and methodologically complex nature of\nteacher-student interactions. We introduce EducationQ, a multi-agent dialogue\nframework that efficiently assesses teaching capabilities through simulated\ndynamic educational scenarios, featuring specialized agents for teaching,\nlearning, and evaluation. Testing 14 LLMs across major AI Organizations\n(OpenAI, Meta, Google, Anthropic, and others) on 1,498 questions spanning 13\ndisciplines and 10 difficulty levels reveals that teaching effectiveness does\nnot correlate linearly with model scale or general reasoning capabilities -\nwith some smaller open-source models outperforming larger commercial\ncounterparts in teaching contexts. This finding highlights a critical gap in\ncurrent evaluations that prioritize knowledge recall over interactive pedagogy.\nOur mixed-methods evaluation, combining quantitative metrics with qualitative\nanalysis and expert case studies, identifies distinct pedagogical strengths\nemployed by top-performing models (e.g., sophisticated questioning strategies,\nadaptive feedback mechanisms). Human expert evaluations show 78% agreement with\nour automated qualitative analysis of effective teaching behaviors, validating\nour methodology. EducationQ demonstrates that LLMs-as-teachers require\nspecialized optimization beyond simple scaling, suggesting next-generation\neducational AI prioritize targeted enhancement of specific pedagogical\neffectiveness."
                },
                "authors": [
                    {
                        "name": "Yao Shi"
                    },
                    {
                        "name": "Rongkeng Liang"
                    },
                    {
                        "name": "Yong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Xu"
                },
                "author": "Yong Xu",
                "arxiv_doi": "10.18653/v1/2025.acl-long.1576",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2025.acl-long.1576",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.14928v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14928v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Paper URL: https://aclanthology.org/2025.acl-long.1576 ;Presentation\n  Video: https://www.youtube.com/watch?v=j63ooKE50I0",
                "arxiv_journal_ref": "Proceedings of the 63rd Annual Meeting of the Association for\n  Computational Linguistics (Volume 1: Long Papers) (2025) 32799-32828",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23429v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23429v1",
                "updated": "2025-07-31T11:09:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    11,
                    9,
                    50,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T11:09:50Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    11,
                    9,
                    50,
                    3,
                    212,
                    0
                ],
                "title": "Chatting with your ERP: A Recipe",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chatting with your ERP: A Recipe"
                },
                "summary": "This paper presents the design, implementation, and evaluation behind a Large\nLanguage Model (LLM) agent that chats with an industrial production-grade ERP\nsystem. The agent is capable of interpreting natural language queries and\ntranslating them into executable SQL statements, leveraging open-weight LLMs. A\nnovel dual-agent architecture combining reasoning and critique stages was\nproposed to improve query generation reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the design, implementation, and evaluation behind a Large\nLanguage Model (LLM) agent that chats with an industrial production-grade ERP\nsystem. The agent is capable of interpreting natural language queries and\ntranslating them into executable SQL statements, leveraging open-weight LLMs. A\nnovel dual-agent architecture combining reasoning and critique stages was\nproposed to improve query generation reliability."
                },
                "authors": [
                    {
                        "name": "Jorge Ruiz Gmez"
                    },
                    {
                        "name": "Lidia Andrs Susinos"
                    },
                    {
                        "name": "Jorge Alamo Oliv"
                    },
                    {
                        "name": "Sonia Rey Osorno"
                    },
                    {
                        "name": "Manuel Luis Gonzalez Hernndez"
                    }
                ],
                "author_detail": {
                    "name": "Manuel Luis Gonzalez Hernndez"
                },
                "author": "Manuel Luis Gonzalez Hernndez",
                "arxiv_comment": "11 pages, includes 3 tables summarizing schema and model performance.\n  Submitted on July 31, 2025. Targets integration of LLM agents with ERP\n  systems using open-weight models and Ollama deployment",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23429v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 68P20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.2.5; H.2.8; H.5.m",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03222v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03222v5",
                "updated": "2025-07-31T11:03:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    11,
                    3,
                    35,
                    3,
                    212,
                    0
                ],
                "published": "2025-03-05T06:32:49Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    6,
                    32,
                    49,
                    2,
                    64,
                    0
                ],
                "title": "Mocap-2-to-3: Multi-view Lifting for Monocular Motion Recovery with 2D\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mocap-2-to-3: Multi-view Lifting for Monocular Motion Recovery with 2D\n  Pretraining"
                },
                "summary": "Recovering absolute human motion from monocular inputs is challenging due to\ntwo main issues. First, existing methods depend on 3D training data collected\nfrom limited environments, constraining out-of-distribution generalization. The\nsecond issue is the difficulty of estimating metric-scale poses from monocular\ninput. To address these challenges, we introduce Mocap-2-to-3, a novel\nframework that performs multi-view lifting from monocular input by leveraging\n2D data pre-training, enabling the reconstruction of metrically accurate 3D\nmotions with absolute positions. To leverage abundant 2D data, we decompose\ncomplex 3D motion into multi-view syntheses. We first pretrain a single-view\ndiffusion model on extensive 2D datasets, then fine-tune a multi-view model\nusing public 3D data to enable view-consistent motion generation from monocular\ninput, allowing the model to acquire action priors and diversity through 2D\ndata. Furthermore, to recover absolute poses, we propose a novel human motion\nrepresentation that decouples the learning of local pose and global movements,\nwhile encoding geometric priors of the ground to accelerate convergence. This\nenables progressive recovery of motion in absolute space during inference.\nExperimental results on in-the-wild benchmarks demonstrate that our method\nsurpasses state-of-the-art approaches in both camera-space motion realism and\nworld-grounded human positioning, while exhibiting superior generalization\ncapability. Our code will be made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recovering absolute human motion from monocular inputs is challenging due to\ntwo main issues. First, existing methods depend on 3D training data collected\nfrom limited environments, constraining out-of-distribution generalization. The\nsecond issue is the difficulty of estimating metric-scale poses from monocular\ninput. To address these challenges, we introduce Mocap-2-to-3, a novel\nframework that performs multi-view lifting from monocular input by leveraging\n2D data pre-training, enabling the reconstruction of metrically accurate 3D\nmotions with absolute positions. To leverage abundant 2D data, we decompose\ncomplex 3D motion into multi-view syntheses. We first pretrain a single-view\ndiffusion model on extensive 2D datasets, then fine-tune a multi-view model\nusing public 3D data to enable view-consistent motion generation from monocular\ninput, allowing the model to acquire action priors and diversity through 2D\ndata. Furthermore, to recover absolute poses, we propose a novel human motion\nrepresentation that decouples the learning of local pose and global movements,\nwhile encoding geometric priors of the ground to accelerate convergence. This\nenables progressive recovery of motion in absolute space during inference.\nExperimental results on in-the-wild benchmarks demonstrate that our method\nsurpasses state-of-the-art approaches in both camera-space motion realism and\nworld-grounded human positioning, while exhibiting superior generalization\ncapability. Our code will be made publicly available."
                },
                "authors": [
                    {
                        "name": "Zhumei Wang"
                    },
                    {
                        "name": "Zechen Hu"
                    },
                    {
                        "name": "Ruoxi Guo"
                    },
                    {
                        "name": "Huaijin Pi"
                    },
                    {
                        "name": "Ziyong Feng"
                    },
                    {
                        "name": "Sida Peng"
                    },
                    {
                        "name": "Xiaowei Zhou"
                    },
                    {
                        "name": "Mingtao Pei"
                    },
                    {
                        "name": "Siyuan Huang"
                    }
                ],
                "author_detail": {
                    "name": "Siyuan Huang"
                },
                "author": "Siyuan Huang",
                "arxiv_comment": "Project page: https://wangzhumei.github.io/mocap-2-to-3/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03222v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03222v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18497v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18497v2",
                "updated": "2025-07-31T11:02:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    11,
                    2,
                    54,
                    3,
                    212,
                    0
                ],
                "published": "2025-05-24T04:24:59Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    4,
                    24,
                    59,
                    5,
                    144,
                    0
                ],
                "title": "The Pragmatic Mind of Machines: Tracing the Emergence of Pragmatic\n  Competence in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Pragmatic Mind of Machines: Tracing the Emergence of Pragmatic\n  Competence in Large Language Models"
                },
                "summary": "Current large language models (LLMs) have demonstrated emerging capabilities\nin social intelligence tasks, including implicature resolution and\ntheory-of-mind reasoning, both of which require substantial pragmatic\nunderstanding. However, how LLMs acquire this pragmatic competence throughout\nthe training process remains poorly understood. In this work, we introduce\nALTPRAG, a dataset grounded in the pragmatic concept of alternatives, to\nevaluate whether LLMs at different training stages can accurately infer nuanced\nspeaker intentions. Each instance pairs two equally plausible yet pragmatically\ndivergent continuations and requires the model to (i) infer the speaker's\nintended meaning and (ii) explain when and why a speaker would choose one\nutterance over its alternative, thus directly probing pragmatic competence\nthrough contrastive reasoning. We systematically evaluate 22 LLMs across 3 key\ntraining stages: after pre-training, supervised fine-tuning (SFT), and\npreference optimization, to examine the development of pragmatic competence.\nOur results show that even base models exhibit notable sensitivity to pragmatic\ncues, which improves consistently with increases in model and data scale.\nAdditionally, SFT and RLHF contribute further gains, particularly in\ncognitive-pragmatic scenarios. These findings highlight pragmatic competence as\nan emergent and compositional property of LLM training and offer new insights\nfor aligning models with human communicative norms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current large language models (LLMs) have demonstrated emerging capabilities\nin social intelligence tasks, including implicature resolution and\ntheory-of-mind reasoning, both of which require substantial pragmatic\nunderstanding. However, how LLMs acquire this pragmatic competence throughout\nthe training process remains poorly understood. In this work, we introduce\nALTPRAG, a dataset grounded in the pragmatic concept of alternatives, to\nevaluate whether LLMs at different training stages can accurately infer nuanced\nspeaker intentions. Each instance pairs two equally plausible yet pragmatically\ndivergent continuations and requires the model to (i) infer the speaker's\nintended meaning and (ii) explain when and why a speaker would choose one\nutterance over its alternative, thus directly probing pragmatic competence\nthrough contrastive reasoning. We systematically evaluate 22 LLMs across 3 key\ntraining stages: after pre-training, supervised fine-tuning (SFT), and\npreference optimization, to examine the development of pragmatic competence.\nOur results show that even base models exhibit notable sensitivity to pragmatic\ncues, which improves consistently with increases in model and data scale.\nAdditionally, SFT and RLHF contribute further gains, particularly in\ncognitive-pragmatic scenarios. These findings highlight pragmatic competence as\nan emergent and compositional property of LLM training and offer new insights\nfor aligning models with human communicative norms."
                },
                "authors": [
                    {
                        "name": "Kefan Yu"
                    },
                    {
                        "name": "Qingcheng Zeng"
                    },
                    {
                        "name": "Weihao Xuan"
                    },
                    {
                        "name": "Wanxin Li"
                    },
                    {
                        "name": "Jingyi Wu"
                    },
                    {
                        "name": "Rob Voigt"
                    }
                ],
                "author_detail": {
                    "name": "Rob Voigt"
                },
                "author": "Rob Voigt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18497v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18497v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22351v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22351v3",
                "updated": "2025-07-31T10:44:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    10,
                    44,
                    36,
                    3,
                    212,
                    0
                ],
                "published": "2025-03-28T11:46:50Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    11,
                    46,
                    50,
                    4,
                    87,
                    0
                ],
                "title": "One Look is Enough: Seamless Patchwise Refinement for Zero-Shot\n  Monocular Depth Estimation on High-Resolution Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One Look is Enough: Seamless Patchwise Refinement for Zero-Shot\n  Monocular Depth Estimation on High-Resolution Images"
                },
                "summary": "Zero-shot depth estimation (DE) models exhibit strong generalization\nperformance as they are trained on large-scale datasets. However, existing\nmodels struggle with high-resolution images due to the discrepancy in image\nresolutions of training (with smaller resolutions) and inference (for high\nresolutions). Processing them at full resolution leads to decreased estimation\naccuracy on depth with tremendous memory consumption, while downsampling to the\ntraining resolution results in blurred edges in the estimated depth images.\nPrevailing high-resolution depth estimation methods adopt a patch-based\napproach, which introduces depth discontinuity issues when reassembling the\nestimated depth patches, resulting in test-time inefficiency. Additionally, to\nobtain fine-grained depth details, these methods rely on synthetic datasets due\nto the real-world sparse ground truth depth, leading to poor generalizability.\nTo tackle these limitations, we propose Patch Refine Once (PRO), an efficient\nand generalizable tile-based framework. Our PRO consists of two key components:\n(i) Grouped Patch Consistency Training that enhances test-time efficiency while\nmitigating the depth discontinuity problem by jointly processing four\noverlapping patches and enforcing a consistency loss on their overlapping\nregions within a single backpropagation step, and (ii) Bias Free Masking that\nprevents the DE models from overfitting to dataset-specific biases, enabling\nbetter generalization to real-world datasets even after training on synthetic\ndata. Zero-shot evaluations on Booster, ETH3D, Middlebury 2014, and NuScenes\ndemonstrate that our PRO can be seamlessly integrated into existing depth\nestimation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot depth estimation (DE) models exhibit strong generalization\nperformance as they are trained on large-scale datasets. However, existing\nmodels struggle with high-resolution images due to the discrepancy in image\nresolutions of training (with smaller resolutions) and inference (for high\nresolutions). Processing them at full resolution leads to decreased estimation\naccuracy on depth with tremendous memory consumption, while downsampling to the\ntraining resolution results in blurred edges in the estimated depth images.\nPrevailing high-resolution depth estimation methods adopt a patch-based\napproach, which introduces depth discontinuity issues when reassembling the\nestimated depth patches, resulting in test-time inefficiency. Additionally, to\nobtain fine-grained depth details, these methods rely on synthetic datasets due\nto the real-world sparse ground truth depth, leading to poor generalizability.\nTo tackle these limitations, we propose Patch Refine Once (PRO), an efficient\nand generalizable tile-based framework. Our PRO consists of two key components:\n(i) Grouped Patch Consistency Training that enhances test-time efficiency while\nmitigating the depth discontinuity problem by jointly processing four\noverlapping patches and enforcing a consistency loss on their overlapping\nregions within a single backpropagation step, and (ii) Bias Free Masking that\nprevents the DE models from overfitting to dataset-specific biases, enabling\nbetter generalization to real-world datasets even after training on synthetic\ndata. Zero-shot evaluations on Booster, ETH3D, Middlebury 2014, and NuScenes\ndemonstrate that our PRO can be seamlessly integrated into existing depth\nestimation models."
                },
                "authors": [
                    {
                        "name": "Byeongjun Kwon"
                    },
                    {
                        "name": "Munchurl Kim"
                    }
                ],
                "author_detail": {
                    "name": "Munchurl Kim"
                },
                "author": "Munchurl Kim",
                "arxiv_comment": "ICCV 2025 (camera-ready version). [Project\n  page](https://kaist-viclab.github.io/One-Look-is-Enough_site)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22351v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22351v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10375v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10375v2",
                "updated": "2025-07-31T10:44:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    10,
                    44,
                    12,
                    3,
                    212,
                    0
                ],
                "published": "2024-06-14T19:07:03Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    19,
                    7,
                    3,
                    4,
                    166,
                    0
                ],
                "title": "Mokav: Execution-driven Differential Testing with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mokav: Execution-driven Differential Testing with LLMs"
                },
                "summary": "It is essential to detect functional differences between programs in various\nsoftware engineering tasks, such as automated program repair, mutation testing,\nand code refactoring. The problem of detecting functional differences between\ntwo programs can be reduced to searching for a difference exposing test (DET):\na test input that results in different outputs on the subject programs. In this\npaper, we propose Mokav, a novel execution-driven tool that leverages LLMs to\ngenerate DETs. Mokav takes two versions of a program (P and Q) and an example\ntest input. When successful, Mokav generates a valid DET, a test input that\nleads to provably different outputs on P and Q. Mokav iteratively prompts an\nLLM with a specialized prompt to generate new test inputs. At each iteration,\nMokav provides execution-based feedback from previously generated tests until\nthe LLM produces a DET. We evaluate Mokav on 1535 pairs of Python programs\ncollected from the Codeforces competition platform and 32 pairs of programs\nfrom the QuixBugs dataset. Our experiments show that Mokav outperforms the\nstate-of-the-art, Pynguin and Differential Prompting, by a large margin. Mokav\ncan generate DETs for 81.7% (1,255/1535) of the program pairs in our benchmark\n(versus 4.9% for Pynguin and 37.3% for Differential Prompting). We demonstrate\nthat the iterative and execution-driven feedback components of the system\ncontribute to its high effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is essential to detect functional differences between programs in various\nsoftware engineering tasks, such as automated program repair, mutation testing,\nand code refactoring. The problem of detecting functional differences between\ntwo programs can be reduced to searching for a difference exposing test (DET):\na test input that results in different outputs on the subject programs. In this\npaper, we propose Mokav, a novel execution-driven tool that leverages LLMs to\ngenerate DETs. Mokav takes two versions of a program (P and Q) and an example\ntest input. When successful, Mokav generates a valid DET, a test input that\nleads to provably different outputs on P and Q. Mokav iteratively prompts an\nLLM with a specialized prompt to generate new test inputs. At each iteration,\nMokav provides execution-based feedback from previously generated tests until\nthe LLM produces a DET. We evaluate Mokav on 1535 pairs of Python programs\ncollected from the Codeforces competition platform and 32 pairs of programs\nfrom the QuixBugs dataset. Our experiments show that Mokav outperforms the\nstate-of-the-art, Pynguin and Differential Prompting, by a large margin. Mokav\ncan generate DETs for 81.7% (1,255/1535) of the program pairs in our benchmark\n(versus 4.9% for Pynguin and 37.3% for Differential Prompting). We demonstrate\nthat the iterative and execution-driven feedback components of the system\ncontribute to its high effectiveness."
                },
                "authors": [
                    {
                        "name": "Khashayar Etemadi"
                    },
                    {
                        "name": "Bardia Mohammadi"
                    },
                    {
                        "name": "Zhendong Su"
                    },
                    {
                        "name": "Martin Monperrus"
                    }
                ],
                "author_detail": {
                    "name": "Martin Monperrus"
                },
                "author": "Martin Monperrus",
                "arxiv_doi": "10.1016/j.jss.2025.112571",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jss.2025.112571",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.10375v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10375v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23411v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23411v1",
                "updated": "2025-07-31T10:36:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    10,
                    36,
                    58,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T10:36:58Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    10,
                    36,
                    58,
                    3,
                    212,
                    0
                ],
                "title": "Out-of-Distribution Detection in Medical Imaging via Diffusion\n  Trajectories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-Distribution Detection in Medical Imaging via Diffusion\n  Trajectories"
                },
                "summary": "In medical imaging, unsupervised out-of-distribution (OOD) detection offers\nan attractive approach for identifying pathological cases with extremely low\nincidence rates. In contrast to supervised methods, OOD-based approaches\nfunction without labels and are inherently robust to data imbalances. Current\ngenerative approaches often rely on likelihood estimation or reconstruction\nerror, but these methods can be computationally expensive, unreliable, and\nrequire retraining if the inlier data changes. These limitations hinder their\nability to distinguish nominal from anomalous inputs efficiently, consistently,\nand robustly. We propose a reconstruction-free OOD detection method that\nleverages the forward diffusion trajectories of a Stein score-based denoising\ndiffusion model (SBDDM). By capturing trajectory curvature via the estimated\nStein score, our approach enables accurate anomaly scoring with only five\ndiffusion steps. A single SBDDM pre-trained on a large, semantically aligned\nmedical dataset generalizes effectively across multiple Near-OOD and Far-OOD\nbenchmarks, achieving state-of-the-art performance while drastically reducing\ncomputational cost during inference. Compared to existing methods, SBDDM\nachieves a relative improvement of up to 10.43% and 18.10% for Near-OOD and\nFar-OOD detection, making it a practical building block for real-time, reliable\ncomputer-aided diagnosis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In medical imaging, unsupervised out-of-distribution (OOD) detection offers\nan attractive approach for identifying pathological cases with extremely low\nincidence rates. In contrast to supervised methods, OOD-based approaches\nfunction without labels and are inherently robust to data imbalances. Current\ngenerative approaches often rely on likelihood estimation or reconstruction\nerror, but these methods can be computationally expensive, unreliable, and\nrequire retraining if the inlier data changes. These limitations hinder their\nability to distinguish nominal from anomalous inputs efficiently, consistently,\nand robustly. We propose a reconstruction-free OOD detection method that\nleverages the forward diffusion trajectories of a Stein score-based denoising\ndiffusion model (SBDDM). By capturing trajectory curvature via the estimated\nStein score, our approach enables accurate anomaly scoring with only five\ndiffusion steps. A single SBDDM pre-trained on a large, semantically aligned\nmedical dataset generalizes effectively across multiple Near-OOD and Far-OOD\nbenchmarks, achieving state-of-the-art performance while drastically reducing\ncomputational cost during inference. Compared to existing methods, SBDDM\nachieves a relative improvement of up to 10.43% and 18.10% for Near-OOD and\nFar-OOD detection, making it a practical building block for real-time, reliable\ncomputer-aided diagnosis."
                },
                "authors": [
                    {
                        "name": "Lemar Abdi"
                    },
                    {
                        "name": "Francisco Caetano"
                    },
                    {
                        "name": "Amaan Valiuddin"
                    },
                    {
                        "name": "Christiaan Viviers"
                    },
                    {
                        "name": "Hamdi Joudeh"
                    },
                    {
                        "name": "Fons van der Sommen"
                    }
                ],
                "author_detail": {
                    "name": "Fons van der Sommen"
                },
                "author": "Fons van der Sommen",
                "arxiv_comment": "Accepted at Uncertainty for Safe Utilization of Machine Learning in\n  Medical Imaging, MICCAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23411v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23411v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23410v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23410v1",
                "updated": "2025-07-31T10:33:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    10,
                    33,
                    47,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T10:33:47Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    10,
                    33,
                    47,
                    3,
                    212,
                    0
                ],
                "title": "Towards LLM-Enhanced Product Line Scoping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards LLM-Enhanced Product Line Scoping"
                },
                "summary": "The idea of product line scoping is to identify the set of features and\nconfigurations that a product line should include, i.e., offer for\nconfiguration purposes. In this context, a major scoping task is to find a\nbalance between commercial relevance and technical feasibility. Traditional\nproduct line scoping approaches rely on formal feature models and require a\nmanual analysis which can be quite time-consuming. In this paper, we sketch how\nLarge Language Models (LLMs) can be applied to support product line scoping\ntasks with a natural language interaction based scoping process. Using a\nworking example from the smarthome domain, we sketch how LLMs can be applied to\nevaluate different feature model alternatives. We discuss open research\nchallenges regarding the integration of LLMs with product line scoping.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The idea of product line scoping is to identify the set of features and\nconfigurations that a product line should include, i.e., offer for\nconfiguration purposes. In this context, a major scoping task is to find a\nbalance between commercial relevance and technical feasibility. Traditional\nproduct line scoping approaches rely on formal feature models and require a\nmanual analysis which can be quite time-consuming. In this paper, we sketch how\nLarge Language Models (LLMs) can be applied to support product line scoping\ntasks with a natural language interaction based scoping process. Using a\nworking example from the smarthome domain, we sketch how LLMs can be applied to\nevaluate different feature model alternatives. We discuss open research\nchallenges regarding the integration of LLMs with product line scoping."
                },
                "authors": [
                    {
                        "name": "Alexander Felfernig"
                    },
                    {
                        "name": "Damian Garber"
                    },
                    {
                        "name": "Viet-Man Le"
                    },
                    {
                        "name": "Sebastian Lubos"
                    },
                    {
                        "name": "Thi Ngoc Trang Tran"
                    }
                ],
                "author_detail": {
                    "name": "Thi Ngoc Trang Tran"
                },
                "author": "Thi Ngoc Trang Tran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23410v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23410v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23407v1",
                "updated": "2025-07-31T10:27:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    10,
                    27,
                    48,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T10:27:48Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    10,
                    27,
                    48,
                    3,
                    212,
                    0
                ],
                "title": "Beyond Passive Critical Thinking: Fostering Proactive Questioning to\n  Enhance Human-AI Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Passive Critical Thinking: Fostering Proactive Questioning to\n  Enhance Human-AI Collaboration"
                },
                "summary": "Critical thinking is essential for building robust AI systems, preventing\nthem from blindly accepting flawed data or biased reasoning. However, prior\nwork has primarily focused on passive critical thinking, where models simply\nreject problematic queries without taking constructive steps to address user\nrequests. In this work, we introduce proactive critical thinking, a paradigm\nwhere models actively seek missing or clarifying information from users to\nresolve their queries better. To evaluate this capability, we present GSM-MC\nand GSM-MCE, two novel benchmarks based on GSM8K for assessing mathematical\nreasoning under incomplete or misleading conditions. GSM-MC contains 1,368 math\nproblems with a key variable deliberately removed, requiring models to identify\nand request the missing information. GSM-MCE further increases the difficulty\nby introducing irrelevant details to test robustness against distractions.\nExperiments on Qwen3 and Llama series models show that, while these models\nexcel in traditional reasoning tasks due to extensive post-training and\ninference-time scaling, they struggle with proactive critical thinking,\nespecially smaller ones. However, we demonstrate that reinforcement learning\n(RL) can significantly improve this ability. Using our enhanced RL algorithm,\nwe achieve substantial gains, boosting the Qwen3-1.7B's accuracy from 0.15% to\n73.98% on GSM-MC. We hope this work advances models that collaborate more\neffectively with users in problem-solving through proactive critical thinking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critical thinking is essential for building robust AI systems, preventing\nthem from blindly accepting flawed data or biased reasoning. However, prior\nwork has primarily focused on passive critical thinking, where models simply\nreject problematic queries without taking constructive steps to address user\nrequests. In this work, we introduce proactive critical thinking, a paradigm\nwhere models actively seek missing or clarifying information from users to\nresolve their queries better. To evaluate this capability, we present GSM-MC\nand GSM-MCE, two novel benchmarks based on GSM8K for assessing mathematical\nreasoning under incomplete or misleading conditions. GSM-MC contains 1,368 math\nproblems with a key variable deliberately removed, requiring models to identify\nand request the missing information. GSM-MCE further increases the difficulty\nby introducing irrelevant details to test robustness against distractions.\nExperiments on Qwen3 and Llama series models show that, while these models\nexcel in traditional reasoning tasks due to extensive post-training and\ninference-time scaling, they struggle with proactive critical thinking,\nespecially smaller ones. However, we demonstrate that reinforcement learning\n(RL) can significantly improve this ability. Using our enhanced RL algorithm,\nwe achieve substantial gains, boosting the Qwen3-1.7B's accuracy from 0.15% to\n73.98% on GSM-MC. We hope this work advances models that collaborate more\neffectively with users in problem-solving through proactive critical thinking."
                },
                "authors": [
                    {
                        "name": "Ante Wang"
                    },
                    {
                        "name": "Yujie Lin"
                    },
                    {
                        "name": "Jingyao Liu"
                    },
                    {
                        "name": "Suhang Wu"
                    },
                    {
                        "name": "Hao Liu"
                    },
                    {
                        "name": "Xinyan Xiao"
                    },
                    {
                        "name": "Jinsong Su"
                    }
                ],
                "author_detail": {
                    "name": "Jinsong Su"
                },
                "author": "Jinsong Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06989v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06989v2",
                "updated": "2025-07-31T10:26:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    10,
                    26,
                    35,
                    3,
                    212,
                    0
                ],
                "published": "2025-03-10T07:10:38Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    7,
                    10,
                    38,
                    0,
                    69,
                    0
                ],
                "title": "Probabilistic Modeling of Jailbreak on Multimodal LLMs: From\n  Quantification to Application",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic Modeling of Jailbreak on Multimodal LLMs: From\n  Quantification to Application"
                },
                "summary": "Recently, Multimodal Large Language Models (MLLMs) have demonstrated their\nsuperior ability in understanding multimodal content. However, they remain\nvulnerable to jailbreak attacks, which exploit weaknesses in their safety\nalignment to generate harmful responses. Previous studies categorize jailbreaks\nas successful or failed based on whether responses contain malicious content.\nHowever, given the stochastic nature of MLLM responses, this binary\nclassification of an input's ability to jailbreak MLLMs is inappropriate.\nDerived from this viewpoint, we introduce jailbreak probability to quantify the\njailbreak potential of an input, which represents the likelihood that MLLMs\ngenerated a malicious response when prompted with this input. We approximate\nthis probability through multiple queries to MLLMs. After modeling the\nrelationship between input hidden states and their corresponding jailbreak\nprobability using Jailbreak Probability Prediction Network (JPPN), we use\ncontinuous jailbreak probability for optimization. Specifically, we propose\nJailbreak-Probability-based Attack (JPA) that optimizes adversarial\nperturbations on input image to maximize jailbreak probability, and further\nenhance it as Multimodal JPA (MJPA) by including monotonic text rephrasing. To\ncounteract attacks, we also propose Jailbreak-Probability-based Finetuning\n(JPF), which minimizes jailbreak probability through MLLM parameter updates.\nExtensive experiments show that (1) (M)JPA yields significant improvements when\nattacking a wide range of models under both white and black box settings. (2)\nJPF vastly reduces jailbreaks by at most over 60\\%. Both of the above results\ndemonstrate the significance of introducing jailbreak probability to make\nnuanced distinctions among input jailbreak abilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Multimodal Large Language Models (MLLMs) have demonstrated their\nsuperior ability in understanding multimodal content. However, they remain\nvulnerable to jailbreak attacks, which exploit weaknesses in their safety\nalignment to generate harmful responses. Previous studies categorize jailbreaks\nas successful or failed based on whether responses contain malicious content.\nHowever, given the stochastic nature of MLLM responses, this binary\nclassification of an input's ability to jailbreak MLLMs is inappropriate.\nDerived from this viewpoint, we introduce jailbreak probability to quantify the\njailbreak potential of an input, which represents the likelihood that MLLMs\ngenerated a malicious response when prompted with this input. We approximate\nthis probability through multiple queries to MLLMs. After modeling the\nrelationship between input hidden states and their corresponding jailbreak\nprobability using Jailbreak Probability Prediction Network (JPPN), we use\ncontinuous jailbreak probability for optimization. Specifically, we propose\nJailbreak-Probability-based Attack (JPA) that optimizes adversarial\nperturbations on input image to maximize jailbreak probability, and further\nenhance it as Multimodal JPA (MJPA) by including monotonic text rephrasing. To\ncounteract attacks, we also propose Jailbreak-Probability-based Finetuning\n(JPF), which minimizes jailbreak probability through MLLM parameter updates.\nExtensive experiments show that (1) (M)JPA yields significant improvements when\nattacking a wide range of models under both white and black box settings. (2)\nJPF vastly reduces jailbreaks by at most over 60\\%. Both of the above results\ndemonstrate the significance of introducing jailbreak probability to make\nnuanced distinctions among input jailbreak abilities."
                },
                "authors": [
                    {
                        "name": "Wenzhuo Xu"
                    },
                    {
                        "name": "Zhipeng Wei"
                    },
                    {
                        "name": "Xiongtao Sun"
                    },
                    {
                        "name": "Zonghao Ying"
                    },
                    {
                        "name": "Deyue Zhang"
                    },
                    {
                        "name": "Dongdong Yang"
                    },
                    {
                        "name": "Xiangzheng Zhang"
                    },
                    {
                        "name": "Quanchen Zou"
                    }
                ],
                "author_detail": {
                    "name": "Quanchen Zou"
                },
                "author": "Quanchen Zou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06989v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06989v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16725v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16725v2",
                "updated": "2025-07-31T10:20:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    10,
                    20,
                    56,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-22T16:08:12Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    16,
                    8,
                    12,
                    1,
                    203,
                    0
                ],
                "title": "RAVine: Reality-Aligned Evaluation for Agentic Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAVine: Reality-Aligned Evaluation for Agentic Search"
                },
                "summary": "Agentic search, as a more autonomous and adaptive paradigm of retrieval\naugmentation, is driving the evolution of intelligent search systems. However,\nexisting evaluation frameworks fail to align well with the goals of agentic\nsearch. First, the complex queries commonly used in current benchmarks often\ndeviate from realistic user search scenarios. Second, prior approaches tend to\nintroduce noise when extracting ground truth for end-to-end evaluations,\nleading to distorted assessments at a fine-grained level. Third, most current\nframeworks focus solely on the quality of final answers, neglecting the\nevaluation of the iterative process inherent to agentic search. To address\nthese limitations, we propose RAVine -- a Reality-Aligned eValuation framework\nfor agentic LLMs with search. RAVine targets multi-point queries and long-form\nanswers that better reflect user intents, and introduces an attributable ground\ntruth construction strategy to enhance the accuracy of fine-grained evaluation.\nMoreover, RAVine examines model's interaction with search tools throughout the\niterative process, and accounts for factors of efficiency. We benchmark a\nseries of models using RAVine and derive several insights, which we hope will\ncontribute to advancing the development of agentic search systems. The code and\ndatasets are available at https://github.com/SwordFaith/RAVine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic search, as a more autonomous and adaptive paradigm of retrieval\naugmentation, is driving the evolution of intelligent search systems. However,\nexisting evaluation frameworks fail to align well with the goals of agentic\nsearch. First, the complex queries commonly used in current benchmarks often\ndeviate from realistic user search scenarios. Second, prior approaches tend to\nintroduce noise when extracting ground truth for end-to-end evaluations,\nleading to distorted assessments at a fine-grained level. Third, most current\nframeworks focus solely on the quality of final answers, neglecting the\nevaluation of the iterative process inherent to agentic search. To address\nthese limitations, we propose RAVine -- a Reality-Aligned eValuation framework\nfor agentic LLMs with search. RAVine targets multi-point queries and long-form\nanswers that better reflect user intents, and introduces an attributable ground\ntruth construction strategy to enhance the accuracy of fine-grained evaluation.\nMoreover, RAVine examines model's interaction with search tools throughout the\niterative process, and accounts for factors of efficiency. We benchmark a\nseries of models using RAVine and derive several insights, which we hope will\ncontribute to advancing the development of agentic search systems. The code and\ndatasets are available at https://github.com/SwordFaith/RAVine."
                },
                "authors": [
                    {
                        "name": "Yilong Xu"
                    },
                    {
                        "name": "Xiang Long"
                    },
                    {
                        "name": "Zhi Zheng"
                    },
                    {
                        "name": "Jinhua Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jinhua Gao"
                },
                "author": "Jinhua Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16725v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16725v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23399v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23399v1",
                "updated": "2025-07-31T10:13:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    10,
                    13,
                    48,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T10:13:48Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    10,
                    13,
                    48,
                    3,
                    212,
                    0
                ],
                "title": "Beyond the Cloud: Assessing the Benefits and Drawbacks of Local LLM\n  Deployment for Translators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Cloud: Assessing the Benefits and Drawbacks of Local LLM\n  Deployment for Translators"
                },
                "summary": "The rapid proliferation of Large Language Models presents both opportunities\nand challenges for the translation field. While commercial, cloud-based AI\nchatbots have garnered significant attention in translation studies, concerns\nregarding data privacy, security, and equitable access necessitate exploration\nof alternative deployment models. This paper investigates the feasibility and\nperformance of locally deployable, free language models as a viable alternative\nto proprietary, cloud-based AI solutions. This study evaluates three\nopen-source models installed on CPU-based platforms and compared against\ncommercially available online chat-bots. The evaluation focuses on functional\nperformance rather than a comparative analysis of human-machine translation\nquality, an area already subject to extensive research. The platforms assessed\nwere chosen for their accessibility and ease of use across various operating\nsystems. While local deployment introduces its own challenges, the benefits of\nenhanced data control, improved privacy, and reduced dependency on cloud\nservices are compelling. The findings of this study contribute to a growing\nbody of knowledge concerning the democratization of AI technology and inform\nfuture research and development efforts aimed at making LLMs more accessible\nand practical for a wider range of users, specifically focusing on the needs of\nindividual translators and small businesses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid proliferation of Large Language Models presents both opportunities\nand challenges for the translation field. While commercial, cloud-based AI\nchatbots have garnered significant attention in translation studies, concerns\nregarding data privacy, security, and equitable access necessitate exploration\nof alternative deployment models. This paper investigates the feasibility and\nperformance of locally deployable, free language models as a viable alternative\nto proprietary, cloud-based AI solutions. This study evaluates three\nopen-source models installed on CPU-based platforms and compared against\ncommercially available online chat-bots. The evaluation focuses on functional\nperformance rather than a comparative analysis of human-machine translation\nquality, an area already subject to extensive research. The platforms assessed\nwere chosen for their accessibility and ease of use across various operating\nsystems. While local deployment introduces its own challenges, the benefits of\nenhanced data control, improved privacy, and reduced dependency on cloud\nservices are compelling. The findings of this study contribute to a growing\nbody of knowledge concerning the democratization of AI technology and inform\nfuture research and development efforts aimed at making LLMs more accessible\nand practical for a wider range of users, specifically focusing on the needs of\nindividual translators and small businesses."
                },
                "authors": [
                    {
                        "name": "Peter Sandrini"
                    }
                ],
                "author_detail": {
                    "name": "Peter Sandrini"
                },
                "author": "Peter Sandrini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23399v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23399v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; K.4.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23394v1",
                "updated": "2025-07-31T10:11:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    10,
                    11,
                    54,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T10:11:54Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    10,
                    11,
                    54,
                    3,
                    212,
                    0
                ],
                "title": "A Data-driven Heavy-Metal Scenario for Ultra-High-Energy Cosmic Rays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Data-driven Heavy-Metal Scenario for Ultra-High-Energy Cosmic Rays"
                },
                "summary": "The mass composition of ultra-high-energy cosmic rays (UHECRs) is usually\ninferred from the depth of the shower maximum ($X_{\\rm{max}}$) of cosmic-ray\nshowers, which is only ambiguously determined by modern hadronic interaction\nmodels. We present a data-driven interpretation of UHECRs, the heavy-metal\nscenario, which assumes pure iron nuclei above $10^{19.6}$ eV ($\\approx 40$\nEeV) as the heaviest observed mass composition and introduces a global shift in\nthe $X_{\\rm{max}}$ scale predicted by the two hadronic interaction models\nQGSJet II-04 and Sibyll 2.3d. We investigate the consequences of the proposed\nmass-composition model based on the obtained shifts in the $X_{\\rm{max}}$\nvalues, which naturally lead to a heavier mass composition of UHECRs than\nconventionally assumed. We explore the consequences of our model on the energy\nevolution of relative fractions of primary species, consequently decomposed\nenergy spectrum, hadronic-interaction studies and the arrival directions of\nUHECRs. We show that within this scenario, presented recently in Vicha et al\n2025 ApJL 986 L34, the cosmic-ray measurements can be interpreted in a more\nconsistent way.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The mass composition of ultra-high-energy cosmic rays (UHECRs) is usually\ninferred from the depth of the shower maximum ($X_{\\rm{max}}$) of cosmic-ray\nshowers, which is only ambiguously determined by modern hadronic interaction\nmodels. We present a data-driven interpretation of UHECRs, the heavy-metal\nscenario, which assumes pure iron nuclei above $10^{19.6}$ eV ($\\approx 40$\nEeV) as the heaviest observed mass composition and introduces a global shift in\nthe $X_{\\rm{max}}$ scale predicted by the two hadronic interaction models\nQGSJet II-04 and Sibyll 2.3d. We investigate the consequences of the proposed\nmass-composition model based on the obtained shifts in the $X_{\\rm{max}}$\nvalues, which naturally lead to a heavier mass composition of UHECRs than\nconventionally assumed. We explore the consequences of our model on the energy\nevolution of relative fractions of primary species, consequently decomposed\nenergy spectrum, hadronic-interaction studies and the arrival directions of\nUHECRs. We show that within this scenario, presented recently in Vicha et al\n2025 ApJL 986 L34, the cosmic-ray measurements can be interpreted in a more\nconsistent way."
                },
                "authors": [
                    {
                        "name": "Jakub Vcha"
                    },
                    {
                        "name": "Alena Bakalov"
                    },
                    {
                        "name": "Ana L. Mller"
                    },
                    {
                        "name": "Olena Tkachenko"
                    },
                    {
                        "name": "Maximilian K. Stadelmaier"
                    }
                ],
                "author_detail": {
                    "name": "Maximilian K. Stadelmaier"
                },
                "author": "Maximilian K. Stadelmaier",
                "arxiv_comment": "Presented at the 39th International Cosmic Ray Conference (ICRC\n  2025), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23386v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23386v1",
                "updated": "2025-07-31T10:01:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    10,
                    1,
                    11,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T10:01:11Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    10,
                    1,
                    11,
                    3,
                    212,
                    0
                ],
                "title": "Causal2Vec: Improving Decoder-only LLMs as Versatile Embedding Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal2Vec: Improving Decoder-only LLMs as Versatile Embedding Models"
                },
                "summary": "Decoder-only large language models (LLMs) are increasingly used to build\nembedding models that effectively encode the semantic information of natural\nlanguage texts into dense vector representations for various embedding tasks.\nHowever, many existing methods primarily focus on removing the causal attention\nmask in LLMs to enable bidirectional attention, potentially undermining the\nmodel's ability to extract semantic information acquired during pretraining.\nAdditionally, leading unidirectional approaches often rely on extra input text\nto overcome the inherent limitations of causal attention, inevitably increasing\ncomputational costs. In this work, we propose Causal2Vec, a general-purpose\nembedding model tailored to enhance the performance of decoder-only LLMs\nwithout altering their original architectures or introducing significant\ncomputational overhead. Specifically, we first employ a lightweight BERT-style\nmodel to pre-encode the input text into a single Contextual token, which is\nthen prepended to the LLM's input sequence, allowing each token to capture\ncontextualized information even without attending to future tokens.\nFurthermore, to mitigate the recency bias introduced by last-token pooling and\nhelp LLMs better leverage the semantic information encoded in the Contextual\ntoken, we concatenate the last hidden states of Contextual and EOS tokens as\nthe final text embedding. In practice, Causal2Vec achieves state-of-the-art\nperformance on the Massive Text Embeddings Benchmark (MTEB) among models\ntrained solely on publicly available retrieval datasets, while reducing the\nrequired sequence length by up to 85% and inference time by up to 82% compared\nto best-performing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoder-only large language models (LLMs) are increasingly used to build\nembedding models that effectively encode the semantic information of natural\nlanguage texts into dense vector representations for various embedding tasks.\nHowever, many existing methods primarily focus on removing the causal attention\nmask in LLMs to enable bidirectional attention, potentially undermining the\nmodel's ability to extract semantic information acquired during pretraining.\nAdditionally, leading unidirectional approaches often rely on extra input text\nto overcome the inherent limitations of causal attention, inevitably increasing\ncomputational costs. In this work, we propose Causal2Vec, a general-purpose\nembedding model tailored to enhance the performance of decoder-only LLMs\nwithout altering their original architectures or introducing significant\ncomputational overhead. Specifically, we first employ a lightweight BERT-style\nmodel to pre-encode the input text into a single Contextual token, which is\nthen prepended to the LLM's input sequence, allowing each token to capture\ncontextualized information even without attending to future tokens.\nFurthermore, to mitigate the recency bias introduced by last-token pooling and\nhelp LLMs better leverage the semantic information encoded in the Contextual\ntoken, we concatenate the last hidden states of Contextual and EOS tokens as\nthe final text embedding. In practice, Causal2Vec achieves state-of-the-art\nperformance on the Massive Text Embeddings Benchmark (MTEB) among models\ntrained solely on publicly available retrieval datasets, while reducing the\nrequired sequence length by up to 85% and inference time by up to 82% compared\nto best-performing methods."
                },
                "authors": [
                    {
                        "name": "Ailiang Lin"
                    },
                    {
                        "name": "Zhuoyun Li"
                    },
                    {
                        "name": "Kotaro Funakoshi"
                    }
                ],
                "author_detail": {
                    "name": "Kotaro Funakoshi"
                },
                "author": "Kotaro Funakoshi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23386v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23386v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14422v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14422v3",
                "updated": "2025-07-31T09:49:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    9,
                    49,
                    19,
                    3,
                    212,
                    0
                ],
                "published": "2025-05-20T14:31:53Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    31,
                    53,
                    1,
                    140,
                    0
                ],
                "title": "MindVote: When AI Meets the Wild West of Social Media Opinion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MindVote: When AI Meets the Wild West of Social Media Opinion"
                },
                "summary": "Large Language Models (LLMs) are increasingly used as scalable tools for\npilot testing, predicting public opinion distributions before deploying costly\nsurveys. To serve as effective pilot testing tools, the performance of these\nLLMs is typically benchmarked against their ability to reproduce the outcomes\nof past structured surveys. This evaluation paradigm, however, is misaligned\nwith the dynamic, context-rich social media environments where public opinion\nis increasingly formed and expressed. By design, surveys strip away the social,\ncultural, and temporal context that shapes public opinion, and LLM benchmarks\nbuilt on this paradigm inherit these critical limitations. To bridge this gap,\nwe introduce MindVote, the first benchmark for public opinion distribution\nprediction grounded in authentic social media discourse. MindVote is\nconstructed from 3,918 naturalistic polls sourced from Reddit and Weibo,\nspanning 23 topics and enriched with detailed annotations for platform,\ntopical, and temporal context. Using this benchmark, we conduct a comprehensive\nevaluation of 15 LLMs. MindVote provides a robust, ecologically valid framework\nto move beyond survey-based evaluations and advance the development of more\nsocially intelligent AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used as scalable tools for\npilot testing, predicting public opinion distributions before deploying costly\nsurveys. To serve as effective pilot testing tools, the performance of these\nLLMs is typically benchmarked against their ability to reproduce the outcomes\nof past structured surveys. This evaluation paradigm, however, is misaligned\nwith the dynamic, context-rich social media environments where public opinion\nis increasingly formed and expressed. By design, surveys strip away the social,\ncultural, and temporal context that shapes public opinion, and LLM benchmarks\nbuilt on this paradigm inherit these critical limitations. To bridge this gap,\nwe introduce MindVote, the first benchmark for public opinion distribution\nprediction grounded in authentic social media discourse. MindVote is\nconstructed from 3,918 naturalistic polls sourced from Reddit and Weibo,\nspanning 23 topics and enriched with detailed annotations for platform,\ntopical, and temporal context. Using this benchmark, we conduct a comprehensive\nevaluation of 15 LLMs. MindVote provides a robust, ecologically valid framework\nto move beyond survey-based evaluations and advance the development of more\nsocially intelligent AI systems."
                },
                "authors": [
                    {
                        "name": "Xutao Mao"
                    },
                    {
                        "name": "Ezra Xuanru Tao"
                    },
                    {
                        "name": "Leyao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Leyao Wang"
                },
                "author": "Leyao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14422v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14422v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23377v1",
                "updated": "2025-07-31T09:45:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    9,
                    45,
                    55,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T09:45:55Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    9,
                    45,
                    55,
                    3,
                    212,
                    0
                ],
                "title": "LLM4Rail: An LLM-Augmented Railway Service Consulting Platform",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4Rail: An LLM-Augmented Railway Service Consulting Platform"
                },
                "summary": "Large language models (LLMs) have significantly reshaped different walks of\nbusiness. To meet the increasing demands for individualized railway service, we\ndevelop LLM4Rail - a novel LLM-augmented railway service consulting platform.\nEmpowered by LLM, LLM4Rail can provide custom modules for ticketing, railway\nfood & drink recommendations, weather information, and chitchat. In LLM4Rail,\nwe propose the iterative \"Question-Thought-Action-Observation (QTAO)\" prompting\nframework. It meticulously integrates verbal reasoning with task-oriented\nactions, that is, reasoning to guide action selection, to effectively retrieve\nexternal observations relevant to railway operation and service to generate\naccurate responses. To provide personalized onboard dining services, we first\nconstruct the Chinese Railway Food and Drink (CRFD-25) - a publicly accessible\ntakeout dataset tailored for railway services. CRFD-25 covers a wide range of\nsignature dishes categorized by cities, cuisines, age groups, and spiciness\nlevels. We further introduce an LLM-based zero-shot conversational recommender\nfor railway catering. To address the unconstrained nature of open\nrecommendations, the feature similarity-based post-processing step is\nintroduced to ensure all the recommended items are aligned with CRFD-25\ndataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have significantly reshaped different walks of\nbusiness. To meet the increasing demands for individualized railway service, we\ndevelop LLM4Rail - a novel LLM-augmented railway service consulting platform.\nEmpowered by LLM, LLM4Rail can provide custom modules for ticketing, railway\nfood & drink recommendations, weather information, and chitchat. In LLM4Rail,\nwe propose the iterative \"Question-Thought-Action-Observation (QTAO)\" prompting\nframework. It meticulously integrates verbal reasoning with task-oriented\nactions, that is, reasoning to guide action selection, to effectively retrieve\nexternal observations relevant to railway operation and service to generate\naccurate responses. To provide personalized onboard dining services, we first\nconstruct the Chinese Railway Food and Drink (CRFD-25) - a publicly accessible\ntakeout dataset tailored for railway services. CRFD-25 covers a wide range of\nsignature dishes categorized by cities, cuisines, age groups, and spiciness\nlevels. We further introduce an LLM-based zero-shot conversational recommender\nfor railway catering. To address the unconstrained nature of open\nrecommendations, the feature similarity-based post-processing step is\nintroduced to ensure all the recommended items are aligned with CRFD-25\ndataset."
                },
                "authors": [
                    {
                        "name": "Zhuo Li"
                    },
                    {
                        "name": "Xianghuai Deng"
                    },
                    {
                        "name": "Chiwei Feng"
                    },
                    {
                        "name": "Hanmeng Li"
                    },
                    {
                        "name": "Shenjie Wang"
                    },
                    {
                        "name": "Haichao Zhang"
                    },
                    {
                        "name": "Teng Jia"
                    },
                    {
                        "name": "Conlin Chen"
                    },
                    {
                        "name": "Louis Linchun Wu"
                    },
                    {
                        "name": "Jia Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jia Wang"
                },
                "author": "Jia Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23371v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23371v1",
                "updated": "2025-07-31T09:39:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    9,
                    39,
                    16,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T09:39:16Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    9,
                    39,
                    16,
                    3,
                    212,
                    0
                ],
                "title": "VMatcher: State-Space Semi-Dense Local Feature Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VMatcher: State-Space Semi-Dense Local Feature Matching"
                },
                "summary": "This paper introduces VMatcher, a hybrid Mamba-Transformer network for\nsemi-dense feature matching between image pairs. Learning-based feature\nmatching methods, whether detector-based or detector-free, achieve\nstate-of-the-art performance but depend heavily on the Transformer's attention\nmechanism, which, while effective, incurs high computational costs due to its\nquadratic complexity. In contrast, Mamba introduces a Selective State-Space\nModel (SSM) that achieves comparable or superior performance with linear\ncomplexity, offering significant efficiency gains. VMatcher leverages a hybrid\napproach, integrating Mamba's highly efficient long-sequence processing with\nthe Transformer's attention mechanism. Multiple VMatcher configurations are\nproposed, including hierarchical architectures, demonstrating their\neffectiveness in setting new benchmarks efficiently while ensuring robustness\nand practicality for real-time applications where rapid inference is crucial.\nSource Code is available at: https://github.com/ayoussf/VMatcher",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces VMatcher, a hybrid Mamba-Transformer network for\nsemi-dense feature matching between image pairs. Learning-based feature\nmatching methods, whether detector-based or detector-free, achieve\nstate-of-the-art performance but depend heavily on the Transformer's attention\nmechanism, which, while effective, incurs high computational costs due to its\nquadratic complexity. In contrast, Mamba introduces a Selective State-Space\nModel (SSM) that achieves comparable or superior performance with linear\ncomplexity, offering significant efficiency gains. VMatcher leverages a hybrid\napproach, integrating Mamba's highly efficient long-sequence processing with\nthe Transformer's attention mechanism. Multiple VMatcher configurations are\nproposed, including hierarchical architectures, demonstrating their\neffectiveness in setting new benchmarks efficiently while ensuring robustness\nand practicality for real-time applications where rapid inference is crucial.\nSource Code is available at: https://github.com/ayoussf/VMatcher"
                },
                "authors": [
                    {
                        "name": "Ali Youssef"
                    }
                ],
                "author_detail": {
                    "name": "Ali Youssef"
                },
                "author": "Ali Youssef",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23371v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23371v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2206.05161v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2206.05161v3",
                "updated": "2025-07-31T09:38:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    9,
                    38,
                    34,
                    3,
                    212,
                    0
                ],
                "published": "2022-06-10T14:59:21Z",
                "published_parsed": [
                    2022,
                    6,
                    10,
                    14,
                    59,
                    21,
                    4,
                    161,
                    0
                ],
                "title": "Approximating optimal SMC proposal distributions in individual-based\n  epidemic models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximating optimal SMC proposal distributions in individual-based\n  epidemic models"
                },
                "summary": "Many epidemic models are naturally defined as individual-based models: where\nwe track the state of each individual within a susceptible population.\nInference for individual-based models is challenging due to the\nhigh-dimensional state-space of such models, which increases exponentially with\npopulation size. We consider sequential Monte Carlo algorithms for inference\nfor individual-based epidemic models where we make direct observations of the\nstate of a sample of individuals. Standard implementations, such as the\nbootstrap filter or the auxiliary particle filter are inefficient due to\nmismatch between the proposal distribution of the state and future\nobservations. We develop new efficient proposal distributions that take account\nof future observations, leveraging the properties that (i) we can analytically\ncalculate the optimal proposal distribution for a single individual given\nfuture observations and the future infection rate of that individual; and (ii)\nthe dynamics of individuals are independent if we condition on their infection\nrates. Thus we construct estimates of the future infection rate for each\nindividual, and then use an independent proposal for the state of each\nindividual given this estimate. Empirical results show order of magnitude\nimprovement in efficiency of the sequential Monte Carlo sampler for both SIS\nand SEIR models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many epidemic models are naturally defined as individual-based models: where\nwe track the state of each individual within a susceptible population.\nInference for individual-based models is challenging due to the\nhigh-dimensional state-space of such models, which increases exponentially with\npopulation size. We consider sequential Monte Carlo algorithms for inference\nfor individual-based epidemic models where we make direct observations of the\nstate of a sample of individuals. Standard implementations, such as the\nbootstrap filter or the auxiliary particle filter are inefficient due to\nmismatch between the proposal distribution of the state and future\nobservations. We develop new efficient proposal distributions that take account\nof future observations, leveraging the properties that (i) we can analytically\ncalculate the optimal proposal distribution for a single individual given\nfuture observations and the future infection rate of that individual; and (ii)\nthe dynamics of individuals are independent if we condition on their infection\nrates. Thus we construct estimates of the future infection rate for each\nindividual, and then use an independent proposal for the state of each\nindividual given this estimate. Empirical results show order of magnitude\nimprovement in efficiency of the sequential Monte Carlo sampler for both SIS\nand SEIR models."
                },
                "authors": [
                    {
                        "name": "Lorenzo Rimella"
                    },
                    {
                        "name": "Christopher Jewell"
                    },
                    {
                        "name": "Paul Fearnhead"
                    }
                ],
                "author_detail": {
                    "name": "Paul Fearnhead"
                },
                "author": "Paul Fearnhead",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2206.05161v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2206.05161v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23370v1",
                "updated": "2025-07-31T09:37:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    9,
                    37,
                    22,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T09:37:22Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    9,
                    37,
                    22,
                    3,
                    212,
                    0
                ],
                "title": "Trae Agent: An LLM-based Agent for Software Engineering with Test-time\n  Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trae Agent: An LLM-based Agent for Software Engineering with Test-time\n  Scaling"
                },
                "summary": "Software issue resolution is a critical challenge in software engineering and\nhas garnered increasing attention in recent years. With the rapid advancement\nof large language models (LLMs), substantial progress has been made in\naddressing real-world software engineering tasks. Recent studies have\nintroduced ensemble reasoning techniques to enhance the performance of\nLLM-based issue resolution. However, existing prompting-based methods still\nface limitations in effectively exploring large ensemble spaces and lack the\ncapacity for repository-level understanding, both of which constrain their\noverall effectiveness. In this paper, we propose Trae Agent, the first\nagent-based ensemble reasoning approach for repository-level issue resolution.\nTrae Agent formulates our goal as an optimal solution search problem and\naddresses two key challenges, i.e., large ensemble spaces and repository-level\nunderstanding, through modular agents for generation, pruning, and selection.\nWe conduct extensive experiments using three leading LLMs on the widely-adopted\nSWE-bench benchmark, comparing Trae Agent against four state-of-the-art\nensemble reasoning techniques. Experimental results demonstrate that Trae Agent\nconsistently achieves superior performance, with an average improvement of\n10.22% over all baselines in terms of Pass@1. Trae Agent has achieved first\nplace on the SWE-bench Verified leaderboard, with a notable Pass@1 score of\n75.20%. We are pleased to release Trae Agent as an open-source project to\nsupport the research community, with all resources available at\nhttps://github.com/bytedance/trae-agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software issue resolution is a critical challenge in software engineering and\nhas garnered increasing attention in recent years. With the rapid advancement\nof large language models (LLMs), substantial progress has been made in\naddressing real-world software engineering tasks. Recent studies have\nintroduced ensemble reasoning techniques to enhance the performance of\nLLM-based issue resolution. However, existing prompting-based methods still\nface limitations in effectively exploring large ensemble spaces and lack the\ncapacity for repository-level understanding, both of which constrain their\noverall effectiveness. In this paper, we propose Trae Agent, the first\nagent-based ensemble reasoning approach for repository-level issue resolution.\nTrae Agent formulates our goal as an optimal solution search problem and\naddresses two key challenges, i.e., large ensemble spaces and repository-level\nunderstanding, through modular agents for generation, pruning, and selection.\nWe conduct extensive experiments using three leading LLMs on the widely-adopted\nSWE-bench benchmark, comparing Trae Agent against four state-of-the-art\nensemble reasoning techniques. Experimental results demonstrate that Trae Agent\nconsistently achieves superior performance, with an average improvement of\n10.22% over all baselines in terms of Pass@1. Trae Agent has achieved first\nplace on the SWE-bench Verified leaderboard, with a notable Pass@1 score of\n75.20%. We are pleased to release Trae Agent as an open-source project to\nsupport the research community, with all resources available at\nhttps://github.com/bytedance/trae-agent."
                },
                "authors": [
                    {
                        "name": "Trae Research Team"
                    },
                    {
                        "name": "Pengfei Gao"
                    },
                    {
                        "name": "Zhao Tian"
                    },
                    {
                        "name": "Xiangxin Meng"
                    },
                    {
                        "name": "Xinchen Wang"
                    },
                    {
                        "name": "Ruida Hu"
                    },
                    {
                        "name": "Yuanan Xiao"
                    },
                    {
                        "name": "Yizhou Liu"
                    },
                    {
                        "name": "Zhao Zhang"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Cuiyun Gao"
                    },
                    {
                        "name": "Yun Lin"
                    },
                    {
                        "name": "Yingfei Xiong"
                    },
                    {
                        "name": "Chao Peng"
                    },
                    {
                        "name": "Xia Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xia Liu"
                },
                "author": "Xia Liu",
                "arxiv_comment": "Pengfei Gao and Zhao Tian contributed equally to this technical\n  report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00657v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00657v2",
                "updated": "2025-07-31T09:35:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    9,
                    35,
                    10,
                    3,
                    212,
                    0
                ],
                "published": "2025-05-01T16:59:36Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    16,
                    59,
                    36,
                    3,
                    121,
                    0
                ],
                "title": "Joint inference for gravitational wave signals and glitches using a\n  data-informed glitch model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint inference for gravitational wave signals and glitches using a\n  data-informed glitch model"
                },
                "summary": "Gravitational wave data are often contaminated by non-Gaussian noise\ntransients, glitches, which can bias the inference of astrophysical signal\nparameters. Traditional approaches either subtract glitches in a pre-processing\nstep, or a glitch model can be included from an agnostic wavelet basis (e.g.\nBayesWave). In this work, we introduce a machine-learning-based approach to\nbuild a parameterised model of glitches. We train a normalising flow on known\nglitches from the Gravity Spy catalogue, constructing an informative prior on\nthe glitch model. By incorporating this model into the Bayesian inference\nanalysis with Bilby, we estimate glitch and signal parameters simultaneously.\nWe demonstrate the performance of our method through bias reduction, glitch\nidentification and Bayesian model selection on real glitches. Our results show\nthat this approach effectively removes glitches from the data, significantly\nimproving source parameter estimation and reducing bias.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gravitational wave data are often contaminated by non-Gaussian noise\ntransients, glitches, which can bias the inference of astrophysical signal\nparameters. Traditional approaches either subtract glitches in a pre-processing\nstep, or a glitch model can be included from an agnostic wavelet basis (e.g.\nBayesWave). In this work, we introduce a machine-learning-based approach to\nbuild a parameterised model of glitches. We train a normalising flow on known\nglitches from the Gravity Spy catalogue, constructing an informative prior on\nthe glitch model. By incorporating this model into the Bayesian inference\nanalysis with Bilby, we estimate glitch and signal parameters simultaneously.\nWe demonstrate the performance of our method through bias reduction, glitch\nidentification and Bayesian model selection on real glitches. Our results show\nthat this approach effectively removes glitches from the data, significantly\nimproving source parameter estimation and reducing bias."
                },
                "authors": [
                    {
                        "name": "Ann-Kristin Malz"
                    },
                    {
                        "name": "John Veitch"
                    }
                ],
                "author_detail": {
                    "name": "John Veitch"
                },
                "author": "John Veitch",
                "arxiv_doi": "10.1103/fp4b-mvzx",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/fp4b-mvzx",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.00657v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00657v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Phys. Rev. D 112, 024071 (2025)",
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07106v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07106v2",
                "updated": "2025-07-31T09:33:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    9,
                    33,
                    35,
                    3,
                    212,
                    0
                ],
                "published": "2025-06-08T12:28:38Z",
                "published_parsed": [
                    2025,
                    6,
                    8,
                    12,
                    28,
                    38,
                    6,
                    159,
                    0
                ],
                "title": "Theorem-of-Thought: A Multi-Agent Framework for Abductive, Deductive,\n  and Inductive Reasoning in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theorem-of-Thought: A Multi-Agent Framework for Abductive, Deductive,\n  and Inductive Reasoning in Language Models"
                },
                "summary": "Large language models (LLMs) have shown strong performance across natural\nlanguage reasoning tasks, yet their reasoning processes remain brittle and\ndifficult to interpret. Prompting techniques like Chain-of-Thought (CoT)\nenhance reliability by eliciting intermediate reasoning steps or aggregating\nmultiple outputs. However, they lack mechanisms for enforcing logical structure\nand assessing internal coherence. We introduce Theorem-of-Thought (ToTh), a\nnovel framework that models reasoning as collaboration among three parallel\nagents, each simulating a distinct mode of inference: abductive, deductive, and\ninductive. Each agent produces a reasoning trace, which is structured into a\nformal reasoning graph. To evaluate consistency, we apply Bayesian belief\npropagation guided by natural language inference (NLI), assigning confidence\nscores to each step. The most coherent graph is selected to derive the final\nanswer. Experiments on symbolic (WebOfLies) and numerical (MultiArith)\nreasoning benchmarks show that ToTh consistently outperforms CoT,\nSelf-Consistency, and CoT-Decoding across multiple LLMs, while producing\ninterpretable and logically grounded reasoning chains. Our findings suggest a\npromising direction for building more robust and cognitively inspired LLM\nreasoning. The implementation is available at\nhttps://github.com/KurbanIntelligenceLab/theorem-of-thought.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown strong performance across natural\nlanguage reasoning tasks, yet their reasoning processes remain brittle and\ndifficult to interpret. Prompting techniques like Chain-of-Thought (CoT)\nenhance reliability by eliciting intermediate reasoning steps or aggregating\nmultiple outputs. However, they lack mechanisms for enforcing logical structure\nand assessing internal coherence. We introduce Theorem-of-Thought (ToTh), a\nnovel framework that models reasoning as collaboration among three parallel\nagents, each simulating a distinct mode of inference: abductive, deductive, and\ninductive. Each agent produces a reasoning trace, which is structured into a\nformal reasoning graph. To evaluate consistency, we apply Bayesian belief\npropagation guided by natural language inference (NLI), assigning confidence\nscores to each step. The most coherent graph is selected to derive the final\nanswer. Experiments on symbolic (WebOfLies) and numerical (MultiArith)\nreasoning benchmarks show that ToTh consistently outperforms CoT,\nSelf-Consistency, and CoT-Decoding across multiple LLMs, while producing\ninterpretable and logically grounded reasoning chains. Our findings suggest a\npromising direction for building more robust and cognitively inspired LLM\nreasoning. The implementation is available at\nhttps://github.com/KurbanIntelligenceLab/theorem-of-thought."
                },
                "authors": [
                    {
                        "name": "Samir Abdaljalil"
                    },
                    {
                        "name": "Hasan Kurban"
                    },
                    {
                        "name": "Khalid Qaraqe"
                    },
                    {
                        "name": "Erchin Serpedin"
                    }
                ],
                "author_detail": {
                    "name": "Erchin Serpedin"
                },
                "author": "Erchin Serpedin",
                "arxiv_comment": "ACL 2025 KnowFM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07106v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07106v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23365v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23365v1",
                "updated": "2025-07-31T09:25:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    9,
                    25,
                    55,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T09:25:55Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    9,
                    25,
                    55,
                    3,
                    212,
                    0
                ],
                "title": "\"I made this (sort of)\": Negotiating authorship, confronting\n  fraudulence, and exploring new musical spaces with prompt-based AI music\n  generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"I made this (sort of)\": Negotiating authorship, confronting\n  fraudulence, and exploring new musical spaces with prompt-based AI music\n  generation"
                },
                "summary": "I reflect on my experience creating two music albums centered on\nstate-of-the-art prompt-based AI music generation platforms. The first album\nexplicitly poses the question: What happens when I collide my junk mail with\nthese platforms? The second album is a direct response to the first, and toys\nwith the inability of state-of-the-art prompt-based AI music generation\nplatforms to generate music that is not ``practiced'', ``polished'', and\n``produced''. I seed a large language model (LLM) with information about these\nalbums and have it interview me, which results in the exploration of several\ndeeper questions: To what extent am I the author? Where am I in the resulting\nmusic? How is my musical identity changing as I am faced with machines that are\nin some ways far more talented than I? What new musical spaces does my work\nopen, for me or anyone/thing else? I conclude by reflecting on my reflections,\nas well as LLM-mediated self-reflection as method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I reflect on my experience creating two music albums centered on\nstate-of-the-art prompt-based AI music generation platforms. The first album\nexplicitly poses the question: What happens when I collide my junk mail with\nthese platforms? The second album is a direct response to the first, and toys\nwith the inability of state-of-the-art prompt-based AI music generation\nplatforms to generate music that is not ``practiced'', ``polished'', and\n``produced''. I seed a large language model (LLM) with information about these\nalbums and have it interview me, which results in the exploration of several\ndeeper questions: To what extent am I the author? Where am I in the resulting\nmusic? How is my musical identity changing as I am faced with machines that are\nin some ways far more talented than I? What new musical spaces does my work\nopen, for me or anyone/thing else? I conclude by reflecting on my reflections,\nas well as LLM-mediated self-reflection as method."
                },
                "authors": [
                    {
                        "name": "Bob L. T. Sturm"
                    }
                ],
                "author_detail": {
                    "name": "Bob L. T. Sturm"
                },
                "author": "Bob L. T. Sturm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23365v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; J.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21875v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21875v2",
                "updated": "2025-07-31T09:23:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    9,
                    23,
                    52,
                    3,
                    212,
                    0
                ],
                "published": "2025-06-27T03:18:45Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    3,
                    18,
                    45,
                    4,
                    178,
                    0
                ],
                "title": "WildSpeech-Bench: Benchmarking Audio LLMs in Natural Speech Conversation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildSpeech-Bench: Benchmarking Audio LLMs in Natural Speech Conversation"
                },
                "summary": "Recent multi-modal Large Language Models (LLMs) such as GPT-4o have\ndemonstrated strong capabilities of direct speech interaction. However, the\nlack of specialized and comprehensive benchmarks for end-to-end speech LLM\nevaluation hinders optimizing the user experience of Audio LLMs in real-world\napplications. Existing evaluation methods often adapt text-based benchmarks,\noverlooking speech's unique characteristics and challenges, including prosody,\nhomophones, stuttering, and differing user expectations. Here, we present a\nnovel approach to thoroughly evaluate LLMs in practical speech conversations.\nWe systematically curate real-world chat data relevant to spoken scenarios,\nintroduce diversity in speaker attributes and acoustic conditions, and augment\nthe dataset with speech-specific phenomena. We further design a query-aware\nevaluation method to use customized evaluation checklists and prompts to\nenhance the accuracy of automatic evaluation. We conduct comprehensive testing\nand detailed analysis of various mainstream speech models, revealing\nsignificant differences in model performance across different speech scenarios.\nThe use of query-aware evaluation further enables a finer-grained assessment\nunder various speech-specific scenarios. Our benchmark can provide valuable\ninsights for speech model development and evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent multi-modal Large Language Models (LLMs) such as GPT-4o have\ndemonstrated strong capabilities of direct speech interaction. However, the\nlack of specialized and comprehensive benchmarks for end-to-end speech LLM\nevaluation hinders optimizing the user experience of Audio LLMs in real-world\napplications. Existing evaluation methods often adapt text-based benchmarks,\noverlooking speech's unique characteristics and challenges, including prosody,\nhomophones, stuttering, and differing user expectations. Here, we present a\nnovel approach to thoroughly evaluate LLMs in practical speech conversations.\nWe systematically curate real-world chat data relevant to spoken scenarios,\nintroduce diversity in speaker attributes and acoustic conditions, and augment\nthe dataset with speech-specific phenomena. We further design a query-aware\nevaluation method to use customized evaluation checklists and prompts to\nenhance the accuracy of automatic evaluation. We conduct comprehensive testing\nand detailed analysis of various mainstream speech models, revealing\nsignificant differences in model performance across different speech scenarios.\nThe use of query-aware evaluation further enables a finer-grained assessment\nunder various speech-specific scenarios. Our benchmark can provide valuable\ninsights for speech model development and evaluation."
                },
                "authors": [
                    {
                        "name": "Jian Zhang"
                    },
                    {
                        "name": "Linhao Zhang"
                    },
                    {
                        "name": "Bokai Lei"
                    },
                    {
                        "name": "Chuhan Wu"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "Xiao Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Zhou"
                },
                "author": "Xiao Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21875v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21875v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11952v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11952v3",
                "updated": "2025-07-31T09:14:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    9,
                    14,
                    25,
                    3,
                    212,
                    0
                ],
                "published": "2025-04-16T10:29:30Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    10,
                    29,
                    30,
                    2,
                    106,
                    0
                ],
                "title": "Robust and Fine-Grained Detection of AI Generated Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust and Fine-Grained Detection of AI Generated Texts"
                },
                "summary": "An ideal detection system for machine generated content is supposed to work\nwell on any generator as many more advanced LLMs come into existence day by\nday. Existing systems often struggle with accurately identifying AI-generated\ncontent over shorter texts. Further, not all texts might be entirely authored\nby a human or LLM, hence we focused more over partial cases i.e human-LLM\nco-authored texts. Our paper introduces a set of models built for the task of\ntoken classification which are trained on an extensive collection of\nhuman-machine co-authored texts, which performed well over texts of unseen\ndomains, unseen generators, texts by non-native speakers and those with\nadversarial inputs. We also introduce a new dataset of over 2.4M such texts\nmostly co-authored by several popular proprietary LLMs over 23 languages. We\nalso present findings of our models' performance over each texts of each domain\nand generator. Additional findings include comparison of performance against\neach adversarial method, length of input texts and characteristics of generated\ntexts compared to the original human authored texts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An ideal detection system for machine generated content is supposed to work\nwell on any generator as many more advanced LLMs come into existence day by\nday. Existing systems often struggle with accurately identifying AI-generated\ncontent over shorter texts. Further, not all texts might be entirely authored\nby a human or LLM, hence we focused more over partial cases i.e human-LLM\nco-authored texts. Our paper introduces a set of models built for the task of\ntoken classification which are trained on an extensive collection of\nhuman-machine co-authored texts, which performed well over texts of unseen\ndomains, unseen generators, texts by non-native speakers and those with\nadversarial inputs. We also introduce a new dataset of over 2.4M such texts\nmostly co-authored by several popular proprietary LLMs over 23 languages. We\nalso present findings of our models' performance over each texts of each domain\nand generator. Additional findings include comparison of performance against\neach adversarial method, length of input texts and characteristics of generated\ntexts compared to the original human authored texts."
                },
                "authors": [
                    {
                        "name": "Ram Mohan Rao Kadiyala"
                    },
                    {
                        "name": "Siddartha Pullakhandam"
                    },
                    {
                        "name": "Kanwal Mehreen"
                    },
                    {
                        "name": "Drishti Sharma"
                    },
                    {
                        "name": "Siddhant Gupta"
                    },
                    {
                        "name": "Jebish Purbey"
                    },
                    {
                        "name": "Ashay Srivastava"
                    },
                    {
                        "name": "Subhasya TippaReddy"
                    },
                    {
                        "name": "Arvind Reddy Bobbili"
                    },
                    {
                        "name": "Suraj Telugara Chandrashekhar"
                    },
                    {
                        "name": "Modabbir Adeeb"
                    },
                    {
                        "name": "Srinadh Vura"
                    },
                    {
                        "name": "Suman Debnath"
                    },
                    {
                        "name": "Hamza Farooq"
                    }
                ],
                "author_detail": {
                    "name": "Hamza Farooq"
                },
                "author": "Hamza Farooq",
                "arxiv_comment": "18 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11952v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11952v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23361v1",
                "updated": "2025-07-31T09:13:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    9,
                    13,
                    42,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T09:13:42Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    9,
                    13,
                    42,
                    3,
                    212,
                    0
                ],
                "title": "SWE-Exp: Experience-Driven Software Issue Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWE-Exp: Experience-Driven Software Issue Resolution"
                },
                "summary": "Recent advances in large language model (LLM) agents have shown remarkable\nprogress in software issue resolution, leveraging advanced techniques such as\nmulti-agent collaboration and Monte Carlo Tree Search (MCTS). However, current\nagents act as memoryless explorers - treating each problem separately without\nretaining or reusing knowledge from previous repair experiences. This leads to\nredundant exploration of failed trajectories and missed chances to adapt\nsuccessful issue resolution methods to similar problems. To address this\nproblem, we introduce SWE-Exp, an experience - enhanced approach that distills\nconcise and actionable experience from prior agent trajectories, enabling\ncontinuous learning across issues. Our method introduces a multi-faceted\nexperience bank that captures both successful and failed repair attempts.\nSpecifically, it extracts reusable issue resolution knowledge at different\nlevels - from high-level problem comprehension to specific code changes.\nExperiments show that SWE-Exp achieves state-of-the-art resolution rate (41.6%\nPass@1) on SWE-bench-Verified under open-source agent frameworks. Our approach\nestablishes a new paradigm in which automated software engineering agents\nsystematically accumulate and leverage repair expertise, fundamentally shifting\nfrom trial-and-error exploration to strategic, experience-driven issue\nresolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language model (LLM) agents have shown remarkable\nprogress in software issue resolution, leveraging advanced techniques such as\nmulti-agent collaboration and Monte Carlo Tree Search (MCTS). However, current\nagents act as memoryless explorers - treating each problem separately without\nretaining or reusing knowledge from previous repair experiences. This leads to\nredundant exploration of failed trajectories and missed chances to adapt\nsuccessful issue resolution methods to similar problems. To address this\nproblem, we introduce SWE-Exp, an experience - enhanced approach that distills\nconcise and actionable experience from prior agent trajectories, enabling\ncontinuous learning across issues. Our method introduces a multi-faceted\nexperience bank that captures both successful and failed repair attempts.\nSpecifically, it extracts reusable issue resolution knowledge at different\nlevels - from high-level problem comprehension to specific code changes.\nExperiments show that SWE-Exp achieves state-of-the-art resolution rate (41.6%\nPass@1) on SWE-bench-Verified under open-source agent frameworks. Our approach\nestablishes a new paradigm in which automated software engineering agents\nsystematically accumulate and leverage repair expertise, fundamentally shifting\nfrom trial-and-error exploration to strategic, experience-driven issue\nresolution."
                },
                "authors": [
                    {
                        "name": "Silin Chen"
                    },
                    {
                        "name": "Shaoxin Lin"
                    },
                    {
                        "name": "Xiaodong Gu"
                    },
                    {
                        "name": "Yuling Shi"
                    },
                    {
                        "name": "Heng Lian"
                    },
                    {
                        "name": "Longfei Yun"
                    },
                    {
                        "name": "Dong Chen"
                    },
                    {
                        "name": "Weiguo Sun"
                    },
                    {
                        "name": "Lin Cao"
                    },
                    {
                        "name": "Qianxiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qianxiang Wang"
                },
                "author": "Qianxiang Wang",
                "arxiv_comment": "Our code and data are available at\n  https://github.com/YerbaPage/SWE-Exp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23358v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23358v1",
                "updated": "2025-07-31T09:08:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    9,
                    8,
                    59,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T09:08:59Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    9,
                    8,
                    59,
                    3,
                    212,
                    0
                ],
                "title": "Text-to-SQL Task-oriented Dialogue Ontology Construction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL Task-oriented Dialogue Ontology Construction"
                },
                "summary": "Large language models (LLMs) are widely used as general-purpose knowledge\nsources, but they rely on parametric knowledge, limiting explainability and\ntrustworthiness. In task-oriented dialogue (TOD) systems, this separation is\nexplicit, using an external database structured by an explicit ontology to\nensure explainability and controllability. However, building such ontologies\nrequires manual labels or supervised training. We introduce TeQoDO: a\nText-to-SQL task-oriented Dialogue Ontology construction method. Here, an LLM\nautonomously builds a TOD ontology from scratch without supervision using its\ninherent SQL programming capabilities combined with dialogue theory provided in\nthe prompt. We show that TeQoDO outperforms transfer learning approaches, and\nits constructed ontology is competitive on a downstream dialogue state tracking\ntask. Ablation studies demonstrate the key role of dialogue theory. TeQoDO also\nscales to allow construction of much larger ontologies, which we investigate on\na Wikipedia and ArXiv dataset. We view this as a step towards broader\napplication of ontologies to increase LLM explainability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely used as general-purpose knowledge\nsources, but they rely on parametric knowledge, limiting explainability and\ntrustworthiness. In task-oriented dialogue (TOD) systems, this separation is\nexplicit, using an external database structured by an explicit ontology to\nensure explainability and controllability. However, building such ontologies\nrequires manual labels or supervised training. We introduce TeQoDO: a\nText-to-SQL task-oriented Dialogue Ontology construction method. Here, an LLM\nautonomously builds a TOD ontology from scratch without supervision using its\ninherent SQL programming capabilities combined with dialogue theory provided in\nthe prompt. We show that TeQoDO outperforms transfer learning approaches, and\nits constructed ontology is competitive on a downstream dialogue state tracking\ntask. Ablation studies demonstrate the key role of dialogue theory. TeQoDO also\nscales to allow construction of much larger ontologies, which we investigate on\na Wikipedia and ArXiv dataset. We view this as a step towards broader\napplication of ontologies to increase LLM explainability."
                },
                "authors": [
                    {
                        "name": "Renato Vukovic"
                    },
                    {
                        "name": "Carel van Niekerk"
                    },
                    {
                        "name": "Michael Heck"
                    },
                    {
                        "name": "Benjamin Ruppik"
                    },
                    {
                        "name": "Hsien-Chin Lin"
                    },
                    {
                        "name": "Shutong Feng"
                    },
                    {
                        "name": "Nurul Lubis"
                    },
                    {
                        "name": "Milica Gasic"
                    }
                ],
                "author_detail": {
                    "name": "Milica Gasic"
                },
                "author": "Milica Gasic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23358v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23356v1",
                "updated": "2025-07-31T09:06:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    9,
                    6,
                    20,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T09:06:20Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    9,
                    6,
                    20,
                    3,
                    212,
                    0
                ],
                "title": "Quality Evaluation of COBOL to Java Code Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quality Evaluation of COBOL to Java Code Transformation"
                },
                "summary": "We present an automated evaluation system for assessing COBOL-to-Java code\ntranslation within IBM's watsonx Code Assistant for Z (WCA4Z). The system\naddresses key challenges in evaluating LLM-based translators, including model\nopacity and the complexity of translation quality assessment. Our approach\ncombines analytic checkers with LLM-as-a-judge (LaaJ) techniques to deliver\nscalable, multi-faceted evaluations. The system supports continuous integration\nworkflows, enables large-scale benchmarking, and reduces reliance on manual\nreview. We describe the system architecture, evaluation strategies, and\nreporting mechanisms that provide actionable insights for developers and\nproject managers, facilitating the evolution of high-quality, modernized\ncodebases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an automated evaluation system for assessing COBOL-to-Java code\ntranslation within IBM's watsonx Code Assistant for Z (WCA4Z). The system\naddresses key challenges in evaluating LLM-based translators, including model\nopacity and the complexity of translation quality assessment. Our approach\ncombines analytic checkers with LLM-as-a-judge (LaaJ) techniques to deliver\nscalable, multi-faceted evaluations. The system supports continuous integration\nworkflows, enables large-scale benchmarking, and reduces reliance on manual\nreview. We describe the system architecture, evaluation strategies, and\nreporting mechanisms that provide actionable insights for developers and\nproject managers, facilitating the evolution of high-quality, modernized\ncodebases."
                },
                "authors": [
                    {
                        "name": "Shmulik Froimovich"
                    },
                    {
                        "name": "Raviv Gal"
                    },
                    {
                        "name": "Wesam Ibraheem"
                    },
                    {
                        "name": "Avi Ziv"
                    }
                ],
                "author_detail": {
                    "name": "Avi Ziv"
                },
                "author": "Avi Ziv",
                "arxiv_comment": "Submitted to ASE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.23779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23779v1",
                "updated": "2025-07-31T17:59:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    59,
                    9,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T17:59:09Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    59,
                    9,
                    3,
                    212,
                    0
                ],
                "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding"
                },
                "summary": "With the development of multimodal reasoning models, Computer Use Agents\n(CUAs), akin to Jarvis from \\textit{\"Iron Man\"}, are becoming a reality. GUI\ngrounding is a core component for CUAs to execute actual actions, similar to\nmechanical control in robotics, and it directly leads to the success or failure\nof the system. It determines actions such as clicking and typing, as well as\nrelated parameters like the coordinates for clicks. Current end-to-end\ngrounding models still achieve less than 65\\% accuracy on challenging\nbenchmarks like ScreenSpot-pro and UI-Vision, indicating they are far from\nbeing ready for deployment. % , as a single misclick can result in unacceptable\nconsequences. In this work, we conduct an empirical study on the training of\ngrounding models, examining details from data collection to model training.\nUltimately, we developed the \\textbf{Phi-Ground} model family, which achieves\nstate-of-the-art performance across all five grounding benchmarks for models\nunder $10B$ parameters in agent settings. In the end-to-end model setting, our\nmodel still achieves SOTA results with scores of \\textit{\\textbf{43.2}} on\nScreenSpot-pro and \\textit{\\textbf{27.2}} on UI-Vision. We believe that the\nvarious details discussed in this paper, along with our successes and failures,\nnot only clarify the construction of grounding models but also benefit other\nperception tasks. Project homepage:\n\\href{https://zhangmiaosen2000.github.io/Phi-Ground/}{https://zhangmiaosen2000.github.io/Phi-Ground/}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of multimodal reasoning models, Computer Use Agents\n(CUAs), akin to Jarvis from \\textit{\"Iron Man\"}, are becoming a reality. GUI\ngrounding is a core component for CUAs to execute actual actions, similar to\nmechanical control in robotics, and it directly leads to the success or failure\nof the system. It determines actions such as clicking and typing, as well as\nrelated parameters like the coordinates for clicks. Current end-to-end\ngrounding models still achieve less than 65\\% accuracy on challenging\nbenchmarks like ScreenSpot-pro and UI-Vision, indicating they are far from\nbeing ready for deployment. % , as a single misclick can result in unacceptable\nconsequences. In this work, we conduct an empirical study on the training of\ngrounding models, examining details from data collection to model training.\nUltimately, we developed the \\textbf{Phi-Ground} model family, which achieves\nstate-of-the-art performance across all five grounding benchmarks for models\nunder $10B$ parameters in agent settings. In the end-to-end model setting, our\nmodel still achieves SOTA results with scores of \\textit{\\textbf{43.2}} on\nScreenSpot-pro and \\textit{\\textbf{27.2}} on UI-Vision. We believe that the\nvarious details discussed in this paper, along with our successes and failures,\nnot only clarify the construction of grounding models but also benefit other\nperception tasks. Project homepage:\n\\href{https://zhangmiaosen2000.github.io/Phi-Ground/}{https://zhangmiaosen2000.github.io/Phi-Ground/}"
                },
                "authors": [
                    {
                        "name": "Miaosen Zhang"
                    },
                    {
                        "name": "Ziqiang Xu"
                    },
                    {
                        "name": "Jialiang Zhu"
                    },
                    {
                        "name": "Qi Dai"
                    },
                    {
                        "name": "Kai Qiu"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Chong Luo"
                    },
                    {
                        "name": "Tianyi Chen"
                    },
                    {
                        "name": "Justin Wagle"
                    },
                    {
                        "name": "Tim Franklin"
                    },
                    {
                        "name": "Baining Guo"
                    }
                ],
                "author_detail": {
                    "name": "Baining Guo"
                },
                "author": "Baining Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23776v1",
                "updated": "2025-07-31T17:58:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    58,
                    25,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T17:58:25Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    58,
                    25,
                    3,
                    212,
                    0
                ],
                "title": "Cascaded Information Disclosure for Generalized Evaluation of Problem\n  Solving Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cascaded Information Disclosure for Generalized Evaluation of Problem\n  Solving Capabilities"
                },
                "summary": "While question-answering~(QA) benchmark performance is an automatic and\nscalable method to compare LLMs, it is an indirect method of evaluating their\nunderlying problem-solving capabilities. Therefore, we propose a holistic and\ngeneralizable framework based on \\emph{cascaded question disclosure} that\nprovides a more accurate estimate of the models' problem-solving capabilities\nwhile maintaining the scalability and automation. This approach collects model\nresponses in a stagewise manner with each stage revealing partial information\nabout the question designed to elicit generalized reasoning in LLMs. We find\nthat our approach not only provides a better comparison between LLMs, but also\ninduces better intermediate traces in models compared to the standard QA\nparadigm. We empirically verify this behavior on diverse reasoning and\nknowledge-heavy QA datasets by comparing LLMs of varying sizes and families.\nOur approach narrows the performance gap observed in the standard QA evaluation\nsettings, indicating that the prevalent indirect QA paradigm of evaluation\noverestimates the differences in performance between models. We further\nvalidate our findings by extensive ablation studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While question-answering~(QA) benchmark performance is an automatic and\nscalable method to compare LLMs, it is an indirect method of evaluating their\nunderlying problem-solving capabilities. Therefore, we propose a holistic and\ngeneralizable framework based on \\emph{cascaded question disclosure} that\nprovides a more accurate estimate of the models' problem-solving capabilities\nwhile maintaining the scalability and automation. This approach collects model\nresponses in a stagewise manner with each stage revealing partial information\nabout the question designed to elicit generalized reasoning in LLMs. We find\nthat our approach not only provides a better comparison between LLMs, but also\ninduces better intermediate traces in models compared to the standard QA\nparadigm. We empirically verify this behavior on diverse reasoning and\nknowledge-heavy QA datasets by comparing LLMs of varying sizes and families.\nOur approach narrows the performance gap observed in the standard QA evaluation\nsettings, indicating that the prevalent indirect QA paradigm of evaluation\noverestimates the differences in performance between models. We further\nvalidate our findings by extensive ablation studies."
                },
                "authors": [
                    {
                        "name": "Yunxiang Yan"
                    },
                    {
                        "name": "Tomohiro Sawada"
                    },
                    {
                        "name": "Kartik Goyal"
                    }
                ],
                "author_detail": {
                    "name": "Kartik Goyal"
                },
                "author": "Kartik Goyal",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23773v1",
                "updated": "2025-07-31T17:57:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    57,
                    20,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T17:57:20Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    57,
                    20,
                    3,
                    212,
                    0
                ],
                "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning\n  Architecture with LLM-Based World Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning\n  Architecture with LLM-Based World Model"
                },
                "summary": "AI agents built on large language models (LLMs) hold enormous promise, but\ncurrent practice focuses on a one-task-one-agent approach, which not only falls\nshort of scalability and generality, but also suffers from the fundamental\nlimitations of autoregressive LLMs. On the other hand, humans are general\nagents who reason by mentally simulating the outcomes of their actions and\nplans. Moving towards a more general and powerful AI agent, we introduce\nSimuRA, a goal-oriented architecture for generalized agentic reasoning. Based\non a principled formulation of optimal agent in any environment, \\modelname\novercomes the limitations of autoregressive reasoning by introducing a world\nmodel for planning via simulation. The generalized world model is implemented\nusing LLM, which can flexibly plan in a wide range of environments using the\nconcept-rich latent space of natural language. Experiments on difficult web\nbrowsing tasks show that \\modelname improves the success of flight search from\n0\\% to 32.2\\%. World-model-based planning, in particular, shows consistent\nadvantage of up to 124\\% over autoregressive planning, demonstrating the\nadvantage of world model simulation as a reasoning paradigm. We are excited\nabout the possibility for training a single, general agent model based on LLMs\nthat can act superintelligently in all environments. To start, we make SimuRA,\na web-browsing agent built on \\modelname with pretrained LLMs, available as a\nresearch demo for public testing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI agents built on large language models (LLMs) hold enormous promise, but\ncurrent practice focuses on a one-task-one-agent approach, which not only falls\nshort of scalability and generality, but also suffers from the fundamental\nlimitations of autoregressive LLMs. On the other hand, humans are general\nagents who reason by mentally simulating the outcomes of their actions and\nplans. Moving towards a more general and powerful AI agent, we introduce\nSimuRA, a goal-oriented architecture for generalized agentic reasoning. Based\non a principled formulation of optimal agent in any environment, \\modelname\novercomes the limitations of autoregressive reasoning by introducing a world\nmodel for planning via simulation. The generalized world model is implemented\nusing LLM, which can flexibly plan in a wide range of environments using the\nconcept-rich latent space of natural language. Experiments on difficult web\nbrowsing tasks show that \\modelname improves the success of flight search from\n0\\% to 32.2\\%. World-model-based planning, in particular, shows consistent\nadvantage of up to 124\\% over autoregressive planning, demonstrating the\nadvantage of world model simulation as a reasoning paradigm. We are excited\nabout the possibility for training a single, general agent model based on LLMs\nthat can act superintelligently in all environments. To start, we make SimuRA,\na web-browsing agent built on \\modelname with pretrained LLMs, available as a\nresearch demo for public testing."
                },
                "authors": [
                    {
                        "name": "Mingkai Deng"
                    },
                    {
                        "name": "Jinyu Hou"
                    },
                    {
                        "name": "Yilin Shen"
                    },
                    {
                        "name": "Hongxia Jin"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Zhiting Hu"
                    },
                    {
                        "name": "Eric Xing"
                    }
                ],
                "author_detail": {
                    "name": "Eric Xing"
                },
                "author": "Eric Xing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21035v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21035v2",
                "updated": "2025-07-31T17:57:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    57,
                    18,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-28T17:55:08Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    55,
                    8,
                    0,
                    209,
                    0
                ],
                "title": "GenoMAS: A Multi-Agent Framework for Scientific Discovery via\n  Code-Driven Gene Expression Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenoMAS: A Multi-Agent Framework for Scientific Discovery via\n  Code-Driven Gene Expression Analysis"
                },
                "summary": "Gene expression analysis holds the key to many biomedical discoveries, yet\nextracting insights from raw transcriptomic data remains formidable due to the\ncomplexity of multiple large, semi-structured files and the need for extensive\ndomain expertise. Current automation approaches are often limited by either\ninflexible workflows that break down in edge cases or by fully autonomous\nagents that lack the necessary precision for rigorous scientific inquiry.\nGenoMAS charts a different course by presenting a team of LLM-based scientists\nthat integrates the reliability of structured workflows with the adaptability\nof autonomous agents. GenoMAS orchestrates six specialized LLM agents through\ntyped message-passing protocols, each contributing complementary strengths to a\nshared analytic canvas. At the heart of GenoMAS lies a guided-planning\nframework: programming agents unfold high-level task guidelines into Action\nUnits and, at each juncture, elect to advance, revise, bypass, or backtrack,\nthereby maintaining logical coherence while bending gracefully to the\nidiosyncrasies of genomic data.\n  On the GenoTEX benchmark, GenoMAS reaches a Composite Similarity Correlation\nof 89.13% for data preprocessing and an F$_1$ of 60.48% for gene\nidentification, surpassing the best prior art by 10.61% and 16.85%\nrespectively. Beyond metrics, GenoMAS surfaces biologically plausible\ngene-phenotype associations corroborated by the literature, all while adjusting\nfor latent confounders. Code is available at https://github.com/Liu-Hy/GenoMAS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gene expression analysis holds the key to many biomedical discoveries, yet\nextracting insights from raw transcriptomic data remains formidable due to the\ncomplexity of multiple large, semi-structured files and the need for extensive\ndomain expertise. Current automation approaches are often limited by either\ninflexible workflows that break down in edge cases or by fully autonomous\nagents that lack the necessary precision for rigorous scientific inquiry.\nGenoMAS charts a different course by presenting a team of LLM-based scientists\nthat integrates the reliability of structured workflows with the adaptability\nof autonomous agents. GenoMAS orchestrates six specialized LLM agents through\ntyped message-passing protocols, each contributing complementary strengths to a\nshared analytic canvas. At the heart of GenoMAS lies a guided-planning\nframework: programming agents unfold high-level task guidelines into Action\nUnits and, at each juncture, elect to advance, revise, bypass, or backtrack,\nthereby maintaining logical coherence while bending gracefully to the\nidiosyncrasies of genomic data.\n  On the GenoTEX benchmark, GenoMAS reaches a Composite Similarity Correlation\nof 89.13% for data preprocessing and an F$_1$ of 60.48% for gene\nidentification, surpassing the best prior art by 10.61% and 16.85%\nrespectively. Beyond metrics, GenoMAS surfaces biologically plausible\ngene-phenotype associations corroborated by the literature, all while adjusting\nfor latent confounders. Code is available at https://github.com/Liu-Hy/GenoMAS."
                },
                "authors": [
                    {
                        "name": "Haoyang Liu"
                    },
                    {
                        "name": "Yijiang Li"
                    },
                    {
                        "name": "Haohan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haohan Wang"
                },
                "author": "Haohan Wang",
                "arxiv_comment": "51 pages (13 pages for the main text, 9 pages for references, and 29\n  pages for the appendix)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21035v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21035v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06448v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06448v3",
                "updated": "2025-07-31T17:54:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    54,
                    47,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-08T23:22:34Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    23,
                    22,
                    34,
                    1,
                    189,
                    0
                ],
                "title": "Perception-Aware Policy Optimization for Multimodal Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perception-Aware Policy Optimization for Multimodal Reasoning"
                },
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be a\nhighly effective strategy for endowing Large Language Models (LLMs) with robust\nmulti-step reasoning abilities. However, its design and optimizations remain\ntailored to purely textual domains, resulting in suboptimal performance when\napplied to multimodal reasoning tasks. In particular, we observe that a major\nsource of error in current multimodal reasoning lies in the perception of\nvisual inputs. To address this bottleneck, we propose PAPO, a novel policy\ngradient algorithm that encourages the model to learn to perceive while\nlearning to reason. Specifically, we introduce the Implicit Perception Loss in\nthe form of a KL divergence term, which can be seamlessly plugged into\nmainstream RLVR algorithms such as GRPO and DAPO. Notably, PAPO does not rely\non additional data curation, reward models, or stronger teacher models. To\nfurther enhance the training stability of PAPO, we introduce the Double Entropy\nLoss, which effectively regularizes the new KL objective without compromising\nperformance. Despite its simplicity, PAPO yields significant overall\nimprovements of 4.4%-17.5% on diverse multimodal benchmarks. The improvements\nare more pronounced, approaching 8.0%-19.1%, on tasks with high vision\ndependency. We also observe a substantial reduction of 30.5% in perception\nerrors, indicating improved perceptual capabilities with PAPO. Overall, our\nwork introduces a deeper integration of perception-aware supervision into core\nlearning objectives and lays the groundwork for a new RL framework that\nencourages visually grounded reasoning. Code and data will be made publicly\navailable for research purposes. Project page:\nhttps://mikewangwzhl.github.io/PAPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be a\nhighly effective strategy for endowing Large Language Models (LLMs) with robust\nmulti-step reasoning abilities. However, its design and optimizations remain\ntailored to purely textual domains, resulting in suboptimal performance when\napplied to multimodal reasoning tasks. In particular, we observe that a major\nsource of error in current multimodal reasoning lies in the perception of\nvisual inputs. To address this bottleneck, we propose PAPO, a novel policy\ngradient algorithm that encourages the model to learn to perceive while\nlearning to reason. Specifically, we introduce the Implicit Perception Loss in\nthe form of a KL divergence term, which can be seamlessly plugged into\nmainstream RLVR algorithms such as GRPO and DAPO. Notably, PAPO does not rely\non additional data curation, reward models, or stronger teacher models. To\nfurther enhance the training stability of PAPO, we introduce the Double Entropy\nLoss, which effectively regularizes the new KL objective without compromising\nperformance. Despite its simplicity, PAPO yields significant overall\nimprovements of 4.4%-17.5% on diverse multimodal benchmarks. The improvements\nare more pronounced, approaching 8.0%-19.1%, on tasks with high vision\ndependency. We also observe a substantial reduction of 30.5% in perception\nerrors, indicating improved perceptual capabilities with PAPO. Overall, our\nwork introduces a deeper integration of perception-aware supervision into core\nlearning objectives and lays the groundwork for a new RL framework that\nencourages visually grounded reasoning. Code and data will be made publicly\navailable for research purposes. Project page:\nhttps://mikewangwzhl.github.io/PAPO."
                },
                "authors": [
                    {
                        "name": "Zhenhailong Wang"
                    },
                    {
                        "name": "Xuehang Guo"
                    },
                    {
                        "name": "Sofia Stoica"
                    },
                    {
                        "name": "Haiyang Xu"
                    },
                    {
                        "name": "Hongru Wang"
                    },
                    {
                        "name": "Hyeonjeong Ha"
                    },
                    {
                        "name": "Xiusi Chen"
                    },
                    {
                        "name": "Yangyi Chen"
                    },
                    {
                        "name": "Ming Yan"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Heng Ji"
                    }
                ],
                "author_detail": {
                    "name": "Heng Ji"
                },
                "author": "Heng Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06448v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06448v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23751v1",
                "updated": "2025-07-31T17:38:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    38,
                    50,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T17:38:50Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    38,
                    50,
                    3,
                    212,
                    0
                ],
                "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning\n  and non-reasoning tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning\n  and non-reasoning tasks"
                },
                "summary": "We propose CoT-Self-Instruct, a synthetic data generation method that\ninstructs LLMs to first reason and plan via Chain-of-Thought (CoT) based on the\ngiven seed tasks, and then to generate a new synthetic prompt of similar\nquality and complexity for use in LLM training, followed by filtering for\nhigh-quality data with automatic metrics. In verifiable reasoning, our\nsynthetic data significantly outperforms existing training datasets, such as\ns1k and OpenMathReasoning, across MATH500, AMC23, AIME24 and GPQA-Diamond. For\nnon-verifiable instruction-following tasks, our method surpasses the\nperformance of human or standard self-instruct prompts on both AlpacaEval 2.0\nand Arena-Hard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose CoT-Self-Instruct, a synthetic data generation method that\ninstructs LLMs to first reason and plan via Chain-of-Thought (CoT) based on the\ngiven seed tasks, and then to generate a new synthetic prompt of similar\nquality and complexity for use in LLM training, followed by filtering for\nhigh-quality data with automatic metrics. In verifiable reasoning, our\nsynthetic data significantly outperforms existing training datasets, such as\ns1k and OpenMathReasoning, across MATH500, AMC23, AIME24 and GPQA-Diamond. For\nnon-verifiable instruction-following tasks, our method surpasses the\nperformance of human or standard self-instruct prompts on both AlpacaEval 2.0\nand Arena-Hard."
                },
                "authors": [
                    {
                        "name": "Ping Yu"
                    },
                    {
                        "name": "Jack Lanchantin"
                    },
                    {
                        "name": "Tianlu Wang"
                    },
                    {
                        "name": "Weizhe Yuan"
                    },
                    {
                        "name": "Olga Golovneva"
                    },
                    {
                        "name": "Ilia Kulikov"
                    },
                    {
                        "name": "Sainbayar Sukhbaatar"
                    },
                    {
                        "name": "Jason Weston"
                    },
                    {
                        "name": "Jing Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jing Xu"
                },
                "author": "Jing Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.13481v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.13481v3",
                "updated": "2025-07-31T17:19:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    19,
                    39,
                    3,
                    212,
                    0
                ],
                "published": "2024-01-24T14:29:39Z",
                "published_parsed": [
                    2024,
                    1,
                    24,
                    14,
                    29,
                    39,
                    2,
                    24,
                    0
                ],
                "title": "How AI Ideas Affect the Creativity, Diversity, and Evolution of Human\n  Ideas: Evidence From a Large, Dynamic Experiment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How AI Ideas Affect the Creativity, Diversity, and Evolution of Human\n  Ideas: Evidence From a Large, Dynamic Experiment"
                },
                "summary": "Exposure to large language model output is rapidly increasing. How will\nseeing AI-generated ideas affect human ideas? We conducted an experiment (800+\nparticipants, 40+ countries) where participants viewed creative ideas that were\nfrom ChatGPT or prior experimental participants and then brainstormed their own\nidea. We varied the number of AI-generated examples (none, low, or high\nexposure) and if the examples were labeled as 'AI' (disclosure). Our dynamic\nexperiment design -- ideas from prior participants in an experimental condition\nare used as stimuli for future participants in the same experimental condition\n-- speaks to the interdependent process of cultural creation: creative ideas\nare built upon prior ideas. Hence, we capture the compounding effects of having\nLLMs 'in the culture loop'. We find that high AI exposure (but not low AI\nexposure) did not affect the creativity of individual ideas but did increase\nthe average amount and rate of change of collective idea diversity. AI made\nideas different, not better. There were no main effects of disclosure. We also\nfound that self-reported creative people were less influenced by knowing an\nidea was from AI and that participants may knowingly adopt AI ideas when the\ntask is difficult. Our findings suggest that introducing AI ideas may increase\ncollective diversity but not individual creativity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposure to large language model output is rapidly increasing. How will\nseeing AI-generated ideas affect human ideas? We conducted an experiment (800+\nparticipants, 40+ countries) where participants viewed creative ideas that were\nfrom ChatGPT or prior experimental participants and then brainstormed their own\nidea. We varied the number of AI-generated examples (none, low, or high\nexposure) and if the examples were labeled as 'AI' (disclosure). Our dynamic\nexperiment design -- ideas from prior participants in an experimental condition\nare used as stimuli for future participants in the same experimental condition\n-- speaks to the interdependent process of cultural creation: creative ideas\nare built upon prior ideas. Hence, we capture the compounding effects of having\nLLMs 'in the culture loop'. We find that high AI exposure (but not low AI\nexposure) did not affect the creativity of individual ideas but did increase\nthe average amount and rate of change of collective idea diversity. AI made\nideas different, not better. There were no main effects of disclosure. We also\nfound that self-reported creative people were less influenced by knowing an\nidea was from AI and that participants may knowingly adopt AI ideas when the\ntask is difficult. Our findings suggest that introducing AI ideas may increase\ncollective diversity but not individual creativity."
                },
                "authors": [
                    {
                        "name": "Joshua Ashkinaze"
                    },
                    {
                        "name": "Julia Mendelsohn"
                    },
                    {
                        "name": "Li Qiwei"
                    },
                    {
                        "name": "Ceren Budak"
                    },
                    {
                        "name": "Eric Gilbert"
                    }
                ],
                "author_detail": {
                    "name": "Eric Gilbert"
                },
                "author": "Eric Gilbert",
                "arxiv_doi": "10.1145/3715928.3737481",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3715928.3737481",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.13481v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.13481v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at ACM Collective Intelligence 2025. Originally posted 2024",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23736v1",
                "updated": "2025-07-31T17:19:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    19,
                    38,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T17:19:38Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    19,
                    38,
                    3,
                    212,
                    0
                ],
                "title": "DICOM De-Identification via Hybrid AI and Rule-Based Framework for\n  Scalable, Uncertainty-Aware Redaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DICOM De-Identification via Hybrid AI and Rule-Based Framework for\n  Scalable, Uncertainty-Aware Redaction"
                },
                "summary": "Access to medical imaging and associated text data has the potential to drive\nmajor advances in healthcare research and patient outcomes. However, the\npresence of Protected Health Information (PHI) and Personally Identifiable\nInformation (PII) in Digital Imaging and Communications in Medicine (DICOM)\nfiles presents a significant barrier to the ethical and secure sharing of\nimaging datasets. This paper presents a hybrid de-identification framework\ndeveloped by Impact Business Information Solutions (IBIS) that combines\nrule-based and AI-driven techniques, and rigorous uncertainty quantification\nfor comprehensive PHI/PII removal from both metadata and pixel data.\n  Our approach begins with a two-tiered rule-based system targeting explicit\nand inferred metadata elements, further augmented by a large language model\n(LLM) fine-tuned for Named Entity Recognition (NER), and trained on a suite of\nsynthetic datasets simulating realistic clinical PHI/PII. For pixel data, we\nemploy an uncertainty-aware Faster R-CNN model to localize embedded text,\nextract candidate PHI via Optical Character Recognition (OCR), and apply the\nNER pipeline for final redaction. Crucially, uncertainty quantification\nprovides confidence measures for AI-based detections to enhance automation\nreliability and enable informed human-in-the-loop verification to manage\nresidual risks.\n  This uncertainty-aware deidentification framework achieves robust performance\nacross benchmark datasets and regulatory standards, including DICOM, HIPAA, and\nTCIA compliance metrics. By combining scalable automation, uncertainty\nquantification, and rigorous quality assurance, our solution addresses critical\nchallenges in medical data de-identification and supports the secure, ethical,\nand trustworthy release of imaging data for research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Access to medical imaging and associated text data has the potential to drive\nmajor advances in healthcare research and patient outcomes. However, the\npresence of Protected Health Information (PHI) and Personally Identifiable\nInformation (PII) in Digital Imaging and Communications in Medicine (DICOM)\nfiles presents a significant barrier to the ethical and secure sharing of\nimaging datasets. This paper presents a hybrid de-identification framework\ndeveloped by Impact Business Information Solutions (IBIS) that combines\nrule-based and AI-driven techniques, and rigorous uncertainty quantification\nfor comprehensive PHI/PII removal from both metadata and pixel data.\n  Our approach begins with a two-tiered rule-based system targeting explicit\nand inferred metadata elements, further augmented by a large language model\n(LLM) fine-tuned for Named Entity Recognition (NER), and trained on a suite of\nsynthetic datasets simulating realistic clinical PHI/PII. For pixel data, we\nemploy an uncertainty-aware Faster R-CNN model to localize embedded text,\nextract candidate PHI via Optical Character Recognition (OCR), and apply the\nNER pipeline for final redaction. Crucially, uncertainty quantification\nprovides confidence measures for AI-based detections to enhance automation\nreliability and enable informed human-in-the-loop verification to manage\nresidual risks.\n  This uncertainty-aware deidentification framework achieves robust performance\nacross benchmark datasets and regulatory standards, including DICOM, HIPAA, and\nTCIA compliance metrics. By combining scalable automation, uncertainty\nquantification, and rigorous quality assurance, our solution addresses critical\nchallenges in medical data de-identification and supports the secure, ethical,\nand trustworthy release of imaging data for research."
                },
                "authors": [
                    {
                        "name": "Kyle Naddeo"
                    },
                    {
                        "name": "Nikolas Koutsoubis"
                    },
                    {
                        "name": "Rahul Krish"
                    },
                    {
                        "name": "Ghulam Rasool"
                    },
                    {
                        "name": "Nidhal Bouaynaya"
                    },
                    {
                        "name": "Tony OSullivan"
                    },
                    {
                        "name": "Raj Krish"
                    }
                ],
                "author_detail": {
                    "name": "Raj Krish"
                },
                "author": "Raj Krish",
                "arxiv_comment": "15 pages, 6 figures,",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23735v1",
                "updated": "2025-07-31T17:18:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    18,
                    55,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T17:18:55Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    18,
                    55,
                    3,
                    212,
                    0
                ],
                "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed AI Agents for Cognitive Underwater Robot Autonomy"
                },
                "summary": "Achieving robust cognitive autonomy in robots navigating complex,\nunpredictable environments remains a fundamental challenge in robotics. This\npaper presents Underwater Robot Self-Organizing Autonomy (UROSA), a\ngroundbreaking architecture leveraging distributed Large Language Model AI\nagents integrated within the Robot Operating System 2 (ROS 2) framework to\nenable advanced cognitive capabilities in Autonomous Underwater Vehicles. UROSA\ndecentralises cognition into specialised AI agents responsible for multimodal\nperception, adaptive reasoning, dynamic mission planning, and real-time\ndecision-making. Central innovations include flexible agents dynamically\nadapting their roles, retrieval-augmented generation utilising vector databases\nfor efficient knowledge management, reinforcement learning-driven behavioural\noptimisation, and autonomous on-the-fly ROS 2 node generation for runtime\nfunctional extensibility. Extensive empirical validation demonstrates UROSA's\npromising adaptability and reliability through realistic underwater missions in\nsimulation and real-world deployments, showing significant advantages over\ntraditional rule-based architectures in handling unforeseen scenarios,\nenvironmental uncertainties, and novel mission objectives. This work not only\nadvances underwater autonomy but also establishes a scalable, safe, and\nversatile cognitive robotics framework capable of generalising to a diverse\narray of real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achieving robust cognitive autonomy in robots navigating complex,\nunpredictable environments remains a fundamental challenge in robotics. This\npaper presents Underwater Robot Self-Organizing Autonomy (UROSA), a\ngroundbreaking architecture leveraging distributed Large Language Model AI\nagents integrated within the Robot Operating System 2 (ROS 2) framework to\nenable advanced cognitive capabilities in Autonomous Underwater Vehicles. UROSA\ndecentralises cognition into specialised AI agents responsible for multimodal\nperception, adaptive reasoning, dynamic mission planning, and real-time\ndecision-making. Central innovations include flexible agents dynamically\nadapting their roles, retrieval-augmented generation utilising vector databases\nfor efficient knowledge management, reinforcement learning-driven behavioural\noptimisation, and autonomous on-the-fly ROS 2 node generation for runtime\nfunctional extensibility. Extensive empirical validation demonstrates UROSA's\npromising adaptability and reliability through realistic underwater missions in\nsimulation and real-world deployments, showing significant advantages over\ntraditional rule-based architectures in handling unforeseen scenarios,\nenvironmental uncertainties, and novel mission objectives. This work not only\nadvances underwater autonomy but also establishes a scalable, safe, and\nversatile cognitive robotics framework capable of generalising to a diverse\narray of real-world applications."
                },
                "authors": [
                    {
                        "name": "Markus Buchholz"
                    },
                    {
                        "name": "Ignacio Carlucho"
                    },
                    {
                        "name": "Michele Grimaldi"
                    },
                    {
                        "name": "Yvan R. Petillot"
                    }
                ],
                "author_detail": {
                    "name": "Yvan R. Petillot"
                },
                "author": "Yvan R. Petillot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07797v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07797v2",
                "updated": "2025-07-31T17:02:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    2,
                    5,
                    3,
                    212,
                    0
                ],
                "published": "2025-05-12T17:48:28Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    17,
                    48,
                    28,
                    0,
                    132,
                    0
                ],
                "title": "A Theoretical Framework for Explaining Reinforcement Learning with\n  Shapley Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Theoretical Framework for Explaining Reinforcement Learning with\n  Shapley Values"
                },
                "summary": "Reinforcement learning agents can achieve super-human performance in complex\ndecision-making tasks, but their behaviour is often difficult to understand and\nexplain. This lack of explanation limits deployment, especially in\nsafety-critical settings where understanding and trust are essential. We\nidentify three core explanatory targets that together provide a comprehensive\nview of reinforcement learning agents: behaviour, outcomes, and predictions. We\ndevelop a unified theoretical framework for explaining these three elements of\nreinforcement learning agents through the influence of individual features that\nthe agent observes in its environment. We derive feature influences by using\nShapley values, which collectively and uniquely satisfy a set of well-motivated\naxioms for fair and consistent credit assignment. The proposed approach,\nShapley Values for Explaining Reinforcement Learning (SVERL), provides a single\ntheoretical framework to comprehensively and meaningfully explain reinforcement\nlearning agents. It yields explanations with precise semantics that are not\nonly interpretable but also mathematically justified, enabling us to identify\nand correct conceptual issues in prior explanations. Through illustrative\nexamples, we show how SVERL produces useful, intuitive explanations of agent\nbehaviour, outcomes, and predictions, which are not apparent from observing\nagent behaviour alone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning agents can achieve super-human performance in complex\ndecision-making tasks, but their behaviour is often difficult to understand and\nexplain. This lack of explanation limits deployment, especially in\nsafety-critical settings where understanding and trust are essential. We\nidentify three core explanatory targets that together provide a comprehensive\nview of reinforcement learning agents: behaviour, outcomes, and predictions. We\ndevelop a unified theoretical framework for explaining these three elements of\nreinforcement learning agents through the influence of individual features that\nthe agent observes in its environment. We derive feature influences by using\nShapley values, which collectively and uniquely satisfy a set of well-motivated\naxioms for fair and consistent credit assignment. The proposed approach,\nShapley Values for Explaining Reinforcement Learning (SVERL), provides a single\ntheoretical framework to comprehensively and meaningfully explain reinforcement\nlearning agents. It yields explanations with precise semantics that are not\nonly interpretable but also mathematically justified, enabling us to identify\nand correct conceptual issues in prior explanations. Through illustrative\nexamples, we show how SVERL produces useful, intuitive explanations of agent\nbehaviour, outcomes, and predictions, which are not apparent from observing\nagent behaviour alone."
                },
                "authors": [
                    {
                        "name": "Daniel Beechey"
                    },
                    {
                        "name": "Thomas M. S. Smith"
                    },
                    {
                        "name": "zgr imek"
                    }
                ],
                "author_detail": {
                    "name": "zgr imek"
                },
                "author": "zgr imek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07797v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07797v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23726v1",
                "updated": "2025-07-31T17:00:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    0,
                    30,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T17:00:30Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    0,
                    30,
                    3,
                    212,
                    0
                ],
                "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving"
                },
                "summary": "LLMs have demonstrated strong mathematical reasoning abilities by leveraging\nreinforcement learning with long chain-of-thought, yet they continue to\nstruggle with theorem proving due to the lack of clear supervision signals when\nsolely using natural language. Dedicated domain-specific languages like Lean\nprovide clear supervision via formal verification of proofs, enabling effective\ntraining through reinforcement learning. In this work, we propose\n\\textbf{Seed-Prover}, a lemma-style whole-proof reasoning model. Seed-Prover\ncan iteratively refine its proof based on Lean feedback, proved lemmas, and\nself-summarization. To solve IMO-level contest problems, we design three\ntest-time inference strategies that enable both deep and broad reasoning.\nSeed-Prover proves $78.1\\%$ of formalized past IMO problems, saturates MiniF2F,\nand achieves over 50\\% on PutnamBench, outperforming the previous\nstate-of-the-art by a large margin. To address the lack of geometry support in\nLean, we introduce a geometry reasoning engine \\textbf{Seed-Geometry}, which\noutperforms previous formal geometry engines. We use these two systems to\nparticipate in IMO 2025 and fully prove 5 out of 6 problems. This work\nrepresents a significant advancement in automated mathematical reasoning,\ndemonstrating the effectiveness of formal verification with long\nchain-of-thought reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have demonstrated strong mathematical reasoning abilities by leveraging\nreinforcement learning with long chain-of-thought, yet they continue to\nstruggle with theorem proving due to the lack of clear supervision signals when\nsolely using natural language. Dedicated domain-specific languages like Lean\nprovide clear supervision via formal verification of proofs, enabling effective\ntraining through reinforcement learning. In this work, we propose\n\\textbf{Seed-Prover}, a lemma-style whole-proof reasoning model. Seed-Prover\ncan iteratively refine its proof based on Lean feedback, proved lemmas, and\nself-summarization. To solve IMO-level contest problems, we design three\ntest-time inference strategies that enable both deep and broad reasoning.\nSeed-Prover proves $78.1\\%$ of formalized past IMO problems, saturates MiniF2F,\nand achieves over 50\\% on PutnamBench, outperforming the previous\nstate-of-the-art by a large margin. To address the lack of geometry support in\nLean, we introduce a geometry reasoning engine \\textbf{Seed-Geometry}, which\noutperforms previous formal geometry engines. We use these two systems to\nparticipate in IMO 2025 and fully prove 5 out of 6 problems. This work\nrepresents a significant advancement in automated mathematical reasoning,\ndemonstrating the effectiveness of formal verification with long\nchain-of-thought reasoning."
                },
                "authors": [
                    {
                        "name": "Luoxin Chen"
                    },
                    {
                        "name": "Jinming Gu"
                    },
                    {
                        "name": "Liankai Huang"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Zhicheng Jiang"
                    },
                    {
                        "name": "Allan Jie"
                    },
                    {
                        "name": "Xiaoran Jin"
                    },
                    {
                        "name": "Xing Jin"
                    },
                    {
                        "name": "Chenggang Li"
                    },
                    {
                        "name": "Kaijing Ma"
                    },
                    {
                        "name": "Cheng Ren"
                    },
                    {
                        "name": "Jiawei Shen"
                    },
                    {
                        "name": "Wenlei Shi"
                    },
                    {
                        "name": "Tong Sun"
                    },
                    {
                        "name": "He Sun"
                    },
                    {
                        "name": "Jiahui Wang"
                    },
                    {
                        "name": "Siran Wang"
                    },
                    {
                        "name": "Zhihong Wang"
                    },
                    {
                        "name": "Chenrui Wei"
                    },
                    {
                        "name": "Shufa Wei"
                    },
                    {
                        "name": "Yonghui Wu"
                    },
                    {
                        "name": "Yuchen Wu"
                    },
                    {
                        "name": "Yihang Xia"
                    },
                    {
                        "name": "Huajian Xin"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Huaiyuan Ying"
                    },
                    {
                        "name": "Hongyi Yuan"
                    },
                    {
                        "name": "Zheng Yuan"
                    },
                    {
                        "name": "Tianyang Zhan"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Tianyun Zhao"
                    },
                    {
                        "name": "Jianqiu Zhao"
                    },
                    {
                        "name": "Yichi Zhou"
                    },
                    {
                        "name": "Thomas Hanwen Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Hanwen Zhu"
                },
                "author": "Thomas Hanwen Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22879v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22879v2",
                "updated": "2025-07-31T16:54:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    16,
                    54,
                    43,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-30T17:55:06Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    17,
                    55,
                    6,
                    2,
                    211,
                    0
                ],
                "title": "RecGPT Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RecGPT Technical Report"
                },
                "summary": "Recommender systems are among the most impactful applications of artificial\nintelligence, serving as critical infrastructure connecting users, merchants,\nand platforms. However, most current industrial systems remain heavily reliant\non historical co-occurrence patterns and log-fitting objectives, i.e.,\noptimizing for past user interactions without explicitly modeling user intent.\nThis log-fitting approach often leads to overfitting to narrow historical\npreferences, failing to capture users' evolving and latent interests. As a\nresult, it reinforces filter bubbles and long-tail phenomena, ultimately\nharming user experience and threatening the sustainability of the whole\nrecommendation ecosystem.\n  To address these challenges, we rethink the overall design paradigm of\nrecommender systems and propose RecGPT, a next-generation framework that places\nuser intent at the center of the recommendation pipeline. By integrating large\nlanguage models (LLMs) into key stages of user interest mining, item retrieval,\nand explanation generation, RecGPT transforms log-fitting recommendation into\nan intent-centric process. To effectively align general-purpose LLMs to the\nabove domain-specific recommendation tasks at scale, RecGPT incorporates a\nmulti-stage training paradigm, which integrates reasoning-enhanced\npre-alignment and self-training evolution, guided by a Human-LLM cooperative\njudge system. Currently, RecGPT has been fully deployed on the Taobao App.\nOnline experiments demonstrate that RecGPT achieves consistent performance\ngains across stakeholders: users benefit from increased content diversity and\nsatisfaction, merchants and the platform gain greater exposure and conversions.\nThese comprehensive improvement results across all stakeholders validates that\nLLM-driven, intent-centric design can foster a more sustainable and mutually\nbeneficial recommendation ecosystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems are among the most impactful applications of artificial\nintelligence, serving as critical infrastructure connecting users, merchants,\nand platforms. However, most current industrial systems remain heavily reliant\non historical co-occurrence patterns and log-fitting objectives, i.e.,\noptimizing for past user interactions without explicitly modeling user intent.\nThis log-fitting approach often leads to overfitting to narrow historical\npreferences, failing to capture users' evolving and latent interests. As a\nresult, it reinforces filter bubbles and long-tail phenomena, ultimately\nharming user experience and threatening the sustainability of the whole\nrecommendation ecosystem.\n  To address these challenges, we rethink the overall design paradigm of\nrecommender systems and propose RecGPT, a next-generation framework that places\nuser intent at the center of the recommendation pipeline. By integrating large\nlanguage models (LLMs) into key stages of user interest mining, item retrieval,\nand explanation generation, RecGPT transforms log-fitting recommendation into\nan intent-centric process. To effectively align general-purpose LLMs to the\nabove domain-specific recommendation tasks at scale, RecGPT incorporates a\nmulti-stage training paradigm, which integrates reasoning-enhanced\npre-alignment and self-training evolution, guided by a Human-LLM cooperative\njudge system. Currently, RecGPT has been fully deployed on the Taobao App.\nOnline experiments demonstrate that RecGPT achieves consistent performance\ngains across stakeholders: users benefit from increased content diversity and\nsatisfaction, merchants and the platform gain greater exposure and conversions.\nThese comprehensive improvement results across all stakeholders validates that\nLLM-driven, intent-centric design can foster a more sustainable and mutually\nbeneficial recommendation ecosystem."
                },
                "authors": [
                    {
                        "name": "Chao Yi"
                    },
                    {
                        "name": "Dian Chen"
                    },
                    {
                        "name": "Gaoyang Guo"
                    },
                    {
                        "name": "Jiakai Tang"
                    },
                    {
                        "name": "Jian Wu"
                    },
                    {
                        "name": "Jing Yu"
                    },
                    {
                        "name": "Mao Zhang"
                    },
                    {
                        "name": "Sunhao Dai"
                    },
                    {
                        "name": "Wen Chen"
                    },
                    {
                        "name": "Wenjun Yang"
                    },
                    {
                        "name": "Yuning Jiang"
                    },
                    {
                        "name": "Zhujin Gao"
                    },
                    {
                        "name": "Bo Zheng"
                    },
                    {
                        "name": "Chi Li"
                    },
                    {
                        "name": "Dimin Wang"
                    },
                    {
                        "name": "Dixuan Wang"
                    },
                    {
                        "name": "Fan Li"
                    },
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "Haibin Chen"
                    },
                    {
                        "name": "Haozhuang Liu"
                    },
                    {
                        "name": "Jialin Zhu"
                    },
                    {
                        "name": "Jiamang Wang"
                    },
                    {
                        "name": "Jiawei Wu"
                    },
                    {
                        "name": "Jin Cui"
                    },
                    {
                        "name": "Ju Huang"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Kan Liu"
                    },
                    {
                        "name": "Lang Tian"
                    },
                    {
                        "name": "Liang Rao"
                    },
                    {
                        "name": "Longbin Li"
                    },
                    {
                        "name": "Lulu Zhao"
                    },
                    {
                        "name": "Na He"
                    },
                    {
                        "name": "Peiyang Wang"
                    },
                    {
                        "name": "Qiqi Huang"
                    },
                    {
                        "name": "Tao Luo"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Xiaoxiao He"
                    },
                    {
                        "name": "Xin Tong"
                    },
                    {
                        "name": "Xu Chen"
                    },
                    {
                        "name": "Xunke Xi"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Yaxuan Wu"
                    },
                    {
                        "name": "Yeqiu Yang"
                    },
                    {
                        "name": "Yi Hu"
                    },
                    {
                        "name": "Yinnan Song"
                    },
                    {
                        "name": "Yuchen Li"
                    },
                    {
                        "name": "Yujie Luo"
                    },
                    {
                        "name": "Yujin Yuan"
                    },
                    {
                        "name": "Yuliang Yan"
                    },
                    {
                        "name": "Zhengyang Wang"
                    },
                    {
                        "name": "Zhibo Xiao"
                    },
                    {
                        "name": "Zhixin Ma"
                    },
                    {
                        "name": "Zile Zhou"
                    },
                    {
                        "name": "Ziqi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ziqi Zhang"
                },
                "author": "Ziqi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22879v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22879v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08184v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08184v3",
                "updated": "2025-07-31T16:45:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    16,
                    45,
                    51,
                    3,
                    212,
                    0
                ],
                "published": "2025-06-09T19:49:11Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    19,
                    49,
                    11,
                    0,
                    160,
                    0
                ],
                "title": "Unable to Forget: Proactive Interference Reveals Working Memory Limits\n  in LLMs Beyond Context Length",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unable to Forget: Proactive Interference Reveals Working Memory Limits\n  in LLMs Beyond Context Length"
                },
                "summary": "Information retrieval in Large Language Models (LLMs) is increasingly\nrecognized as intertwined with generation capabilities rather than mere lookup.\nWhile longer contexts are often assumed to improve retrieval, the effects of\nintra-context interference remain understudied. To address this, we adapt the\nproactive interference (PI) paradigm from cognitive science, where earlier\ninformation disrupts recall of newer updates. In humans, susceptibility to such\ninterference is inversely linked to working memory capacity. We introduce\nPI-LLM, an evaluation that sequentially streams semantically related key-value\nupdates and queries only the final values. Although these final values are\nclearly positioned just before the query, LLM retrieval accuracy declines\nlog-linearly toward zero as interference accumulates; errors arise from\nretrieving previously overwritten values. Attempts to mitigate interference via\nprompt engineering (e.g., instructing models to ignore earlier input) yield\nlimited success. These findings reveal a fundamental constraint on LLMs'\nability to disentangle interference and flexibly manipulate information,\nsuggesting a working memory bottleneck beyond mere context access. This calls\nfor approaches that strengthen models' ability to suppress irrelevant content\nduring retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information retrieval in Large Language Models (LLMs) is increasingly\nrecognized as intertwined with generation capabilities rather than mere lookup.\nWhile longer contexts are often assumed to improve retrieval, the effects of\nintra-context interference remain understudied. To address this, we adapt the\nproactive interference (PI) paradigm from cognitive science, where earlier\ninformation disrupts recall of newer updates. In humans, susceptibility to such\ninterference is inversely linked to working memory capacity. We introduce\nPI-LLM, an evaluation that sequentially streams semantically related key-value\nupdates and queries only the final values. Although these final values are\nclearly positioned just before the query, LLM retrieval accuracy declines\nlog-linearly toward zero as interference accumulates; errors arise from\nretrieving previously overwritten values. Attempts to mitigate interference via\nprompt engineering (e.g., instructing models to ignore earlier input) yield\nlimited success. These findings reveal a fundamental constraint on LLMs'\nability to disentangle interference and flexibly manipulate information,\nsuggesting a working memory bottleneck beyond mere context access. This calls\nfor approaches that strengthen models' ability to suppress irrelevant content\nduring retrieval."
                },
                "authors": [
                    {
                        "name": "Chupei Wang"
                    },
                    {
                        "name": "Jiaqiu Vince Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqiu Vince Sun"
                },
                "arxiv_affiliation": "New York University",
                "author": "Jiaqiu Vince Sun",
                "arxiv_comment": "Accepted at ICML 2025 Workshop on Long Context Foundation Models\n  (ICFM). Code: https://github.com/zhuangziGiantfish/Unable-to-Forget",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08184v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08184v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23709v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23709v1",
                "updated": "2025-07-31T16:30:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    16,
                    30,
                    50,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T16:30:50Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    16,
                    30,
                    50,
                    3,
                    212,
                    0
                ],
                "title": "Explainable Image Classification with Reduced Overconfidence for Tissue\n  Characterisation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable Image Classification with Reduced Overconfidence for Tissue\n  Characterisation"
                },
                "summary": "The deployment of Machine Learning models intraoperatively for tissue\ncharacterisation can assist decision making and guide safe tumour resections.\nFor image classification models, pixel attribution methods are popular to infer\nexplainability. However, overconfidence in deep learning model's predictions\ntranslates to overconfidence in pixel attribution. In this paper, we propose\nthe first approach which incorporates risk estimation into a pixel attribution\nmethod for improved image classification explainability. The proposed method\niteratively applies a classification model with a pixel attribution method to\ncreate a volume of PA maps. This volume is used for the first time, to generate\na pixel-wise distribution of PA values. We introduce a method to generate an\nenhanced PA map by estimating the expectation values of the pixel-wise\ndistributions. In addition, the coefficient of variation (CV) is used to\nestimate pixel-wise risk of this enhanced PA map. Hence, the proposed method\nnot only provides an improved PA map but also produces an estimation of risk on\nthe output PA values. Performance evaluation on probe-based Confocal Laser\nEndomicroscopy (pCLE) data and ImageNet verifies that our improved\nexplainability method outperforms the state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Machine Learning models intraoperatively for tissue\ncharacterisation can assist decision making and guide safe tumour resections.\nFor image classification models, pixel attribution methods are popular to infer\nexplainability. However, overconfidence in deep learning model's predictions\ntranslates to overconfidence in pixel attribution. In this paper, we propose\nthe first approach which incorporates risk estimation into a pixel attribution\nmethod for improved image classification explainability. The proposed method\niteratively applies a classification model with a pixel attribution method to\ncreate a volume of PA maps. This volume is used for the first time, to generate\na pixel-wise distribution of PA values. We introduce a method to generate an\nenhanced PA map by estimating the expectation values of the pixel-wise\ndistributions. In addition, the coefficient of variation (CV) is used to\nestimate pixel-wise risk of this enhanced PA map. Hence, the proposed method\nnot only provides an improved PA map but also produces an estimation of risk on\nthe output PA values. Performance evaluation on probe-based Confocal Laser\nEndomicroscopy (pCLE) data and ImageNet verifies that our improved\nexplainability method outperforms the state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Alfie Roddan"
                    },
                    {
                        "name": "Chi Xu"
                    },
                    {
                        "name": "Serine Ajlouni"
                    },
                    {
                        "name": "Irini Kakaletri"
                    },
                    {
                        "name": "Patra Charalampaki"
                    },
                    {
                        "name": "Stamatia Giannarou"
                    }
                ],
                "author_detail": {
                    "name": "Stamatia Giannarou"
                },
                "author": "Stamatia Giannarou",
                "arxiv_doi": "10.1007/978-3-031-43895-0_54",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-43895-0_54",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.23709v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23709v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "(2023). In: Greenspan, H., et al. (eds) Medical Image Computing\n  and Computer Assisted Intervention MICCAI 2023. Lecture Notes in Computer\n  Science, vol 14221. Springer, Cham",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23701v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23701v1",
                "updated": "2025-07-31T16:22:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    16,
                    22,
                    55,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T16:22:55Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    16,
                    22,
                    55,
                    3,
                    212,
                    0
                ],
                "title": "TextQuests: How Good are LLMs at Text-Based Video Games?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TextQuests: How Good are LLMs at Text-Based Video Games?"
                },
                "summary": "Evaluating AI agents within complex, interactive environments that mirror\nreal-world challenges is critical for understanding their practical\ncapabilities. While existing agent benchmarks effectively assess skills like\ntool use or performance on structured tasks, they often do not fully capture an\nagent's ability to operate autonomously in exploratory environments that demand\nsustained, self-directed reasoning over a long and growing context. To spur the\ndevelopment of agents capable of more robust intrinsic reasoning over long\nhorizons, we introduce TextQuests, a benchmark based on the Infocom suite of\ninteractive fiction games. These text-based adventures, which can take human\nplayers over 30 hours and require hundreds of precise actions to solve, serve\nas an effective proxy for evaluating AI agents on focused, stateful tasks. The\nbenchmark is specifically designed to assess an LLM agent's capacity for\nself-contained problem-solving by precluding the use of external tools, thereby\nfocusing on intrinsic long-context reasoning capabilities in an exploratory\nenvironment characterized by the need for trial-and-error learning and\nsustained problem-solving within a single interactive session. We release\nTextQuests at https://textquests.ai.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating AI agents within complex, interactive environments that mirror\nreal-world challenges is critical for understanding their practical\ncapabilities. While existing agent benchmarks effectively assess skills like\ntool use or performance on structured tasks, they often do not fully capture an\nagent's ability to operate autonomously in exploratory environments that demand\nsustained, self-directed reasoning over a long and growing context. To spur the\ndevelopment of agents capable of more robust intrinsic reasoning over long\nhorizons, we introduce TextQuests, a benchmark based on the Infocom suite of\ninteractive fiction games. These text-based adventures, which can take human\nplayers over 30 hours and require hundreds of precise actions to solve, serve\nas an effective proxy for evaluating AI agents on focused, stateful tasks. The\nbenchmark is specifically designed to assess an LLM agent's capacity for\nself-contained problem-solving by precluding the use of external tools, thereby\nfocusing on intrinsic long-context reasoning capabilities in an exploratory\nenvironment characterized by the need for trial-and-error learning and\nsustained problem-solving within a single interactive session. We release\nTextQuests at https://textquests.ai."
                },
                "authors": [
                    {
                        "name": "Long Phan"
                    },
                    {
                        "name": "Mantas Mazeika"
                    },
                    {
                        "name": "Andy Zou"
                    },
                    {
                        "name": "Dan Hendrycks"
                    }
                ],
                "author_detail": {
                    "name": "Dan Hendrycks"
                },
                "author": "Dan Hendrycks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23701v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23701v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23694v1",
                "updated": "2025-07-31T16:12:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    16,
                    12,
                    22,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T16:12:22Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    16,
                    12,
                    22,
                    3,
                    212,
                    0
                ],
                "title": "A survey of multi-agent geosimulation methodologies: from ABM to LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A survey of multi-agent geosimulation methodologies: from ABM to LLM"
                },
                "summary": "We provide a comprehensive examination of agent-based approaches that codify\nthe principles and linkages underlying multi-agent systems, simulations, and\ninformation systems. Based on two decades of study, this paper confirms a\nframework intended as a formal specification for geosimulation platforms. Our\nfindings show that large language models (LLMs) can be effectively incorporated\nas agent components if they follow a structured architecture specific to\nfundamental agent activities such as perception, memory, planning, and action.\nThis integration is precisely consistent with the architecture that we\nformalize, providing a solid platform for next-generation geosimulation\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We provide a comprehensive examination of agent-based approaches that codify\nthe principles and linkages underlying multi-agent systems, simulations, and\ninformation systems. Based on two decades of study, this paper confirms a\nframework intended as a formal specification for geosimulation platforms. Our\nfindings show that large language models (LLMs) can be effectively incorporated\nas agent components if they follow a structured architecture specific to\nfundamental agent activities such as perception, memory, planning, and action.\nThis integration is precisely consistent with the architecture that we\nformalize, providing a solid platform for next-generation geosimulation\nsystems."
                },
                "authors": [
                    {
                        "name": "Virginia Padilla"
                    },
                    {
                        "name": "Jacinto Dvila"
                    }
                ],
                "author_detail": {
                    "name": "Jacinto Dvila"
                },
                "author": "Jacinto Dvila",
                "arxiv_comment": "20 pages, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T42",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23693v1",
                "updated": "2025-07-31T16:11:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    16,
                    11,
                    26,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T16:11:26Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    16,
                    11,
                    26,
                    3,
                    212,
                    0
                ],
                "title": "CFDagent: A Language-Guided, Zero-Shot Multi-Agent System for Complex\n  Flow Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CFDagent: A Language-Guided, Zero-Shot Multi-Agent System for Complex\n  Flow Simulation"
                },
                "summary": "We introduce CFDagent, a zero-shot, multi-agent system that enables fully\nautonomous computational fluid dynamics (CFD) simulations from natural language\nprompts. CFDagent integrates three specialized LLM-driven agents: (i) the\nPreprocessing Agent that generates 3D geometries from textual or visual inputs\nusing a hybrid text-to-3D diffusion model (Point-E) and automatically meshes\nthe geometries; (ii) the Solver Agent that configures and executes an immersed\nboundary flow solver; and (iii) the Postprocessing Agent that analyzes and\nvisualizes the results, including multimodal renderings. These agents are\ninteractively guided by GPT-4o via conversational prompts, enabling intuitive\nand user-friendly interaction. We validate CFDagent by reproducing canonical\nsphere flows at Reynolds numbers of 100 and 300 using three distinct inputs: a\nsimple text prompt (i.e., \"sphere\"), an image-based input, and a standard\nsphere model. The computed drag and lift coefficients from meshes produced by\neach input approach closely match available data. The proposed system enables\nsynthesization of flow simulations and photorealistic visualizations for\ncomplex geometries. Through extensive tests on canonical and realistic\nscenarios, we demonstrate the robustness, versatility, and practical\napplicability of CFDagent. By bridging generative AI with high-fidelity\nsimulations, CFDagent significantly lowers barriers to expert-level CFD,\nunlocking broad opportunities in education, scientific research, and practical\nengineering applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce CFDagent, a zero-shot, multi-agent system that enables fully\nautonomous computational fluid dynamics (CFD) simulations from natural language\nprompts. CFDagent integrates three specialized LLM-driven agents: (i) the\nPreprocessing Agent that generates 3D geometries from textual or visual inputs\nusing a hybrid text-to-3D diffusion model (Point-E) and automatically meshes\nthe geometries; (ii) the Solver Agent that configures and executes an immersed\nboundary flow solver; and (iii) the Postprocessing Agent that analyzes and\nvisualizes the results, including multimodal renderings. These agents are\ninteractively guided by GPT-4o via conversational prompts, enabling intuitive\nand user-friendly interaction. We validate CFDagent by reproducing canonical\nsphere flows at Reynolds numbers of 100 and 300 using three distinct inputs: a\nsimple text prompt (i.e., \"sphere\"), an image-based input, and a standard\nsphere model. The computed drag and lift coefficients from meshes produced by\neach input approach closely match available data. The proposed system enables\nsynthesization of flow simulations and photorealistic visualizations for\ncomplex geometries. Through extensive tests on canonical and realistic\nscenarios, we demonstrate the robustness, versatility, and practical\napplicability of CFDagent. By bridging generative AI with high-fidelity\nsimulations, CFDagent significantly lowers barriers to expert-level CFD,\nunlocking broad opportunities in education, scientific research, and practical\nengineering applications."
                },
                "authors": [
                    {
                        "name": "Zhaoyue Xu"
                    },
                    {
                        "name": "Long Wang"
                    },
                    {
                        "name": "Chunyu Wang"
                    },
                    {
                        "name": "Yixin Chen"
                    },
                    {
                        "name": "Qingyong Luo"
                    },
                    {
                        "name": "Hua-Dong Yao"
                    },
                    {
                        "name": "Shizhao Wang"
                    },
                    {
                        "name": "Guowei He"
                    }
                ],
                "author_detail": {
                    "name": "Guowei He"
                },
                "author": "Guowei He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23676v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23676v1",
                "updated": "2025-07-31T15:51:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    51,
                    41,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T15:51:41Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    51,
                    41,
                    3,
                    212,
                    0
                ],
                "title": "DepMicroDiff: Diffusion-Based Dependency-Aware Multimodal Imputation for\n  Microbiome Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DepMicroDiff: Diffusion-Based Dependency-Aware Multimodal Imputation for\n  Microbiome Data"
                },
                "summary": "Microbiome data analysis is essential for understanding host health and\ndisease, yet its inherent sparsity and noise pose major challenges for accurate\nimputation, hindering downstream tasks such as biomarker discovery. Existing\nimputation methods, including recent diffusion-based models, often fail to\ncapture the complex interdependencies between microbial taxa and overlook\ncontextual metadata that can inform imputation. We introduce DepMicroDiff, a\nnovel framework that combines diffusion-based generative modeling with a\nDependency-Aware Transformer (DAT) to explicitly capture both mutual pairwise\ndependencies and autoregressive relationships. DepMicroDiff is further enhanced\nby VAE-based pretraining across diverse cancer datasets and conditioning on\npatient metadata encoded via a large language model (LLM). Experiments on TCGA\nmicrobiome datasets show that DepMicroDiff substantially outperforms\nstate-of-the-art baselines, achieving higher Pearson correlation (up to 0.712),\ncosine similarity (up to 0.812), and lower RMSE and MAE across multiple cancer\ntypes, demonstrating its robustness and generalizability for microbiome\nimputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microbiome data analysis is essential for understanding host health and\ndisease, yet its inherent sparsity and noise pose major challenges for accurate\nimputation, hindering downstream tasks such as biomarker discovery. Existing\nimputation methods, including recent diffusion-based models, often fail to\ncapture the complex interdependencies between microbial taxa and overlook\ncontextual metadata that can inform imputation. We introduce DepMicroDiff, a\nnovel framework that combines diffusion-based generative modeling with a\nDependency-Aware Transformer (DAT) to explicitly capture both mutual pairwise\ndependencies and autoregressive relationships. DepMicroDiff is further enhanced\nby VAE-based pretraining across diverse cancer datasets and conditioning on\npatient metadata encoded via a large language model (LLM). Experiments on TCGA\nmicrobiome datasets show that DepMicroDiff substantially outperforms\nstate-of-the-art baselines, achieving higher Pearson correlation (up to 0.712),\ncosine similarity (up to 0.812), and lower RMSE and MAE across multiple cancer\ntypes, demonstrating its robustness and generalizability for microbiome\nimputation."
                },
                "authors": [
                    {
                        "name": "Rabeya Tus Sadia"
                    },
                    {
                        "name": "Qiang Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Cheng"
                },
                "author": "Qiang Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23676v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23676v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23674v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23674v1",
                "updated": "2025-07-31T15:50:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    50,
                    57,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T15:50:57Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    50,
                    57,
                    3,
                    212,
                    0
                ],
                "title": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses"
                },
                "summary": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience."
                },
                "authors": [
                    {
                        "name": "Muhammad Taha Cheema"
                    },
                    {
                        "name": "Abeer Aamir"
                    },
                    {
                        "name": "Khawaja Gul Muhammad"
                    },
                    {
                        "name": "Naveed Anwar Bhatti"
                    },
                    {
                        "name": "Ihsan Ayyub Qazi"
                    },
                    {
                        "name": "Zafar Ayyub Qazi"
                    }
                ],
                "author_detail": {
                    "name": "Zafar Ayyub Qazi"
                },
                "author": "Zafar Ayyub Qazi",
                "arxiv_comment": "13 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23674v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08540v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08540v2",
                "updated": "2025-07-31T15:49:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    49,
                    27,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-11T12:39:25Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    39,
                    25,
                    4,
                    192,
                    0
                ],
                "title": "White-Basilisk: A Hybrid Model for Code Vulnerability Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "White-Basilisk: A Hybrid Model for Code Vulnerability Detection"
                },
                "summary": "The proliferation of software vulnerabilities presents a significant\nchallenge to cybersecurity, necessitating more effective detection\nmethodologies. We introduce White-Basilisk, a novel approach to vulnerability\ndetection that demonstrates superior performance while challenging prevailing\nassumptions in AI model scaling. Utilizing an innovative architecture that\nintegrates Mamba layers, linear self-attention, and a Mixture of Experts\nframework, White-Basilisk achieves state-of-the-art results in vulnerability\ndetection tasks with a parameter count of only 200M. The model's capacity to\nprocess sequences of unprecedented length enables comprehensive analysis of\nextensive codebases in a single pass, surpassing the context limitations of\ncurrent Large Language Models (LLMs). White-Basilisk exhibits robust\nperformance on imbalanced, real-world datasets, while maintaining computational\nefficiency that facilitates deployment across diverse organizational scales.\nThis research not only establishes new benchmarks in code security but also\nprovides empirical evidence that compact, efficiently designed models can\noutperform larger counterparts in specialized tasks, potentially redefining\noptimization strategies in AI development for domain-specific applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of software vulnerabilities presents a significant\nchallenge to cybersecurity, necessitating more effective detection\nmethodologies. We introduce White-Basilisk, a novel approach to vulnerability\ndetection that demonstrates superior performance while challenging prevailing\nassumptions in AI model scaling. Utilizing an innovative architecture that\nintegrates Mamba layers, linear self-attention, and a Mixture of Experts\nframework, White-Basilisk achieves state-of-the-art results in vulnerability\ndetection tasks with a parameter count of only 200M. The model's capacity to\nprocess sequences of unprecedented length enables comprehensive analysis of\nextensive codebases in a single pass, surpassing the context limitations of\ncurrent Large Language Models (LLMs). White-Basilisk exhibits robust\nperformance on imbalanced, real-world datasets, while maintaining computational\nefficiency that facilitates deployment across diverse organizational scales.\nThis research not only establishes new benchmarks in code security but also\nprovides empirical evidence that compact, efficiently designed models can\noutperform larger counterparts in specialized tasks, potentially redefining\noptimization strategies in AI development for domain-specific applications."
                },
                "authors": [
                    {
                        "name": "Ioannis Lamprou"
                    },
                    {
                        "name": "Alexander Shevtsov"
                    },
                    {
                        "name": "Ioannis Arapakis"
                    },
                    {
                        "name": "Sotiris Ioannidis"
                    }
                ],
                "author_detail": {
                    "name": "Sotiris Ioannidis"
                },
                "author": "Sotiris Ioannidis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08540v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08540v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23669v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23669v1",
                "updated": "2025-07-31T15:48:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    48,
                    12,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T15:48:12Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    48,
                    12,
                    3,
                    212,
                    0
                ],
                "title": "Automating AI Failure Tracking: Semantic Association of Reports in AI\n  Incident Database",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating AI Failure Tracking: Semantic Association of Reports in AI\n  Incident Database"
                },
                "summary": "Artificial Intelligence (AI) systems are transforming critical sectors such\nas healthcare, finance, and transportation, enhancing operational efficiency\nand decision-making processes. However, their deployment in high-stakes domains\nhas exposed vulnerabilities that can result in significant societal harm. To\nsystematically study and mitigate these risk, initiatives like the AI Incident\nDatabase (AIID) have emerged, cataloging over 3,000 real-world AI failure\nreports. Currently, associating a new report with the appropriate AI Incident\nrelies on manual expert intervention, limiting scalability and delaying the\nidentification of emerging failure patterns.\n  To address this limitation, we propose a retrieval-based framework that\nautomates the association of new reports with existing AI Incidents through\nsemantic similarity modeling. We formalize the task as a ranking problem, where\neach report-comprising a title and a full textual description-is compared to\npreviously documented AI Incidents based on embedding cosine similarity.\nBenchmarking traditional lexical methods, cross-encoder architectures, and\ntransformer-based sentence embedding models, we find that the latter\nconsistently achieve superior performance. Our analysis further shows that\ncombining titles and descriptions yields substantial improvements in ranking\naccuracy compared to using titles alone. Moreover, retrieval performance\nremains stable across variations in description length, highlighting the\nrobustness of the framework. Finally, we find that retrieval performance\nconsistently improves as the training set expands. Our approach provides a\nscalable and efficient solution for supporting the maintenance of the AIID.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence (AI) systems are transforming critical sectors such\nas healthcare, finance, and transportation, enhancing operational efficiency\nand decision-making processes. However, their deployment in high-stakes domains\nhas exposed vulnerabilities that can result in significant societal harm. To\nsystematically study and mitigate these risk, initiatives like the AI Incident\nDatabase (AIID) have emerged, cataloging over 3,000 real-world AI failure\nreports. Currently, associating a new report with the appropriate AI Incident\nrelies on manual expert intervention, limiting scalability and delaying the\nidentification of emerging failure patterns.\n  To address this limitation, we propose a retrieval-based framework that\nautomates the association of new reports with existing AI Incidents through\nsemantic similarity modeling. We formalize the task as a ranking problem, where\neach report-comprising a title and a full textual description-is compared to\npreviously documented AI Incidents based on embedding cosine similarity.\nBenchmarking traditional lexical methods, cross-encoder architectures, and\ntransformer-based sentence embedding models, we find that the latter\nconsistently achieve superior performance. Our analysis further shows that\ncombining titles and descriptions yields substantial improvements in ranking\naccuracy compared to using titles alone. Moreover, retrieval performance\nremains stable across variations in description length, highlighting the\nrobustness of the framework. Finally, we find that retrieval performance\nconsistently improves as the training set expands. Our approach provides a\nscalable and efficient solution for supporting the maintenance of the AIID."
                },
                "authors": [
                    {
                        "name": "Diego Russo"
                    },
                    {
                        "name": "Gian Marco Orlando"
                    },
                    {
                        "name": "Valerio La Gatta"
                    },
                    {
                        "name": "Vincenzo Moscato"
                    }
                ],
                "author_detail": {
                    "name": "Vincenzo Moscato"
                },
                "author": "Vincenzo Moscato",
                "arxiv_comment": "Accepted at the 28th European Conference on Artificial Intelligence\n  (ECAI 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23669v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23669v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21903v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21903v2",
                "updated": "2025-07-31T15:33:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    33,
                    32,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-29T15:14:39Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    15,
                    14,
                    39,
                    1,
                    210,
                    0
                ],
                "title": "Who's important? -- SUnSET: Synergistic Understanding of Stakeholder,\n  Events and Time for Timeline Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Who's important? -- SUnSET: Synergistic Understanding of Stakeholder,\n  Events and Time for Timeline Generation"
                },
                "summary": "As news reporting becomes increasingly global and decentralized online,\ntracking related events across multiple sources presents significant\nchallenges. Existing news summarization methods typically utilizes Large\nLanguage Models and Graphical methods on article-based summaries. However, this\nis not effective since it only considers the textual content of similarly dated\narticles to understand the gist of the event. To counteract the lack of\nanalysis on the parties involved, it is essential to come up with a novel\nframework to gauge the importance of stakeholders and the connection of related\nevents through the relevant entities involved. Therefore, we present SUnSET:\nSynergistic Understanding of Stakeholder, Events and Time for the task of\nTimeline Summarization (TLS). We leverage powerful Large Language Models (LLMs)\nto build SET triplets and introduced the use of stakeholder-based ranking to\nconstruct a $Relevancy$ metric, which can be extended into general situations.\nOur experimental results outperform all prior baselines and emerged as the new\nState-of-the-Art, highlighting the impact of stakeholder information within\nnews article.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As news reporting becomes increasingly global and decentralized online,\ntracking related events across multiple sources presents significant\nchallenges. Existing news summarization methods typically utilizes Large\nLanguage Models and Graphical methods on article-based summaries. However, this\nis not effective since it only considers the textual content of similarly dated\narticles to understand the gist of the event. To counteract the lack of\nanalysis on the parties involved, it is essential to come up with a novel\nframework to gauge the importance of stakeholders and the connection of related\nevents through the relevant entities involved. Therefore, we present SUnSET:\nSynergistic Understanding of Stakeholder, Events and Time for the task of\nTimeline Summarization (TLS). We leverage powerful Large Language Models (LLMs)\nto build SET triplets and introduced the use of stakeholder-based ranking to\nconstruct a $Relevancy$ metric, which can be extended into general situations.\nOur experimental results outperform all prior baselines and emerged as the new\nState-of-the-Art, highlighting the impact of stakeholder information within\nnews article."
                },
                "authors": [
                    {
                        "name": "Tiviatis Sim"
                    },
                    {
                        "name": "Kaiwen Yang"
                    },
                    {
                        "name": "Shen Xin"
                    },
                    {
                        "name": "Kenji Kawaguchi"
                    }
                ],
                "author_detail": {
                    "name": "Kenji Kawaguchi"
                },
                "author": "Kenji Kawaguchi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21903v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21903v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23648v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23648v1",
                "updated": "2025-07-31T15:25:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    25,
                    49,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T15:25:49Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    25,
                    49,
                    3,
                    212,
                    0
                ],
                "title": "Towards Field-Ready AI-based Malaria Diagnosis: A Continual Learning\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Field-Ready AI-based Malaria Diagnosis: A Continual Learning\n  Approach"
                },
                "summary": "Malaria remains a major global health challenge, particularly in low-resource\nsettings where access to expert microscopy may be limited. Deep learning-based\ncomputer-aided diagnosis (CAD) systems have been developed and demonstrate\npromising performance on thin blood smear images. However, their clinical\ndeployment may be hindered by limited generalization across sites with varying\nconditions. Yet very few practical solutions have been proposed. In this work,\nwe investigate continual learning (CL) as a strategy to enhance the robustness\nof malaria CAD models to domain shifts. We frame the problem as a\ndomain-incremental learning scenario, where a YOLO-based object detector must\nadapt to new acquisition sites while retaining performance on previously seen\ndomains. We evaluate four CL strategies, two rehearsal-based and two\nregularization-based methods, on real-life conditions thanks to a multi-site\nclinical dataset of thin blood smear images. Our results suggest that CL, and\nrehearsal-based methods in particular, can significantly improve performance.\nThese findings highlight the potential of continual learning to support the\ndevelopment of deployable, field-ready CAD tools for malaria.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Malaria remains a major global health challenge, particularly in low-resource\nsettings where access to expert microscopy may be limited. Deep learning-based\ncomputer-aided diagnosis (CAD) systems have been developed and demonstrate\npromising performance on thin blood smear images. However, their clinical\ndeployment may be hindered by limited generalization across sites with varying\nconditions. Yet very few practical solutions have been proposed. In this work,\nwe investigate continual learning (CL) as a strategy to enhance the robustness\nof malaria CAD models to domain shifts. We frame the problem as a\ndomain-incremental learning scenario, where a YOLO-based object detector must\nadapt to new acquisition sites while retaining performance on previously seen\ndomains. We evaluate four CL strategies, two rehearsal-based and two\nregularization-based methods, on real-life conditions thanks to a multi-site\nclinical dataset of thin blood smear images. Our results suggest that CL, and\nrehearsal-based methods in particular, can significantly improve performance.\nThese findings highlight the potential of continual learning to support the\ndevelopment of deployable, field-ready CAD tools for malaria."
                },
                "authors": [
                    {
                        "name": "Louise Guillon"
                    },
                    {
                        "name": "Soheib Biga"
                    },
                    {
                        "name": "Yendoube E. Kantchire"
                    },
                    {
                        "name": "Mouhamadou Lamine Sane"
                    },
                    {
                        "name": "Grgoire Pasquier"
                    },
                    {
                        "name": "Kossi Yakpa"
                    },
                    {
                        "name": "Stphane E. Sossou"
                    },
                    {
                        "name": "Marc Thellier"
                    },
                    {
                        "name": "Laurent Bonnardot"
                    },
                    {
                        "name": "Laurence Lachaud"
                    },
                    {
                        "name": "Renaud Piarroux"
                    },
                    {
                        "name": "Ameyo M. Dorkenoo"
                    }
                ],
                "author_detail": {
                    "name": "Ameyo M. Dorkenoo"
                },
                "author": "Ameyo M. Dorkenoo",
                "arxiv_comment": "MICCAI 2025 AMAI Workshop, Accepted, Submitted Manuscript Version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23648v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23648v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18102v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18102v2",
                "updated": "2025-07-31T15:25:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    25,
                    9,
                    3,
                    212,
                    0
                ],
                "published": "2025-05-23T16:57:34Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    16,
                    57,
                    34,
                    4,
                    143,
                    0
                ],
                "title": "How Can I Publish My LLM Benchmark Without Giving the True Answers Away?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Can I Publish My LLM Benchmark Without Giving the True Answers Away?"
                },
                "summary": "Publishing a large language model (LLM) benchmark on the Internet risks\ncontaminating future LLMs: the benchmark may be unintentionally (or\nintentionally) used to train or select a model. A common mitigation is to keep\nthe benchmark private and let participants submit their models or predictions\nto the organizers. However, this strategy will require trust in a single\norganization and still permits test-set overfitting through repeated queries.\nTo overcome this issue, we propose a way to publish benchmarks without\ncompletely disclosing the ground-truth answers to the questions, while still\nmaintaining the ability to openly evaluate LLMs. Our main idea is to inject\nrandomness to the answers by preparing several logically correct answers, and\nonly include one of them as the solution in the benchmark. This reduces the\nbest possible accuracy, i.e., Bayes accuracy, of the benchmark. Not only is\nthis helpful to keep us from disclosing the ground truth, but this approach\nalso offers a test for detecting data contamination. In principle, even fully\ncapable models should not surpass the Bayes accuracy. If a model surpasses this\nceiling despite this expectation, this is a strong signal of data\ncontamination. We present experimental evidence that our method can detect data\ncontamination accurately on a wide range of benchmarks, models, and training\nmethodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Publishing a large language model (LLM) benchmark on the Internet risks\ncontaminating future LLMs: the benchmark may be unintentionally (or\nintentionally) used to train or select a model. A common mitigation is to keep\nthe benchmark private and let participants submit their models or predictions\nto the organizers. However, this strategy will require trust in a single\norganization and still permits test-set overfitting through repeated queries.\nTo overcome this issue, we propose a way to publish benchmarks without\ncompletely disclosing the ground-truth answers to the questions, while still\nmaintaining the ability to openly evaluate LLMs. Our main idea is to inject\nrandomness to the answers by preparing several logically correct answers, and\nonly include one of them as the solution in the benchmark. This reduces the\nbest possible accuracy, i.e., Bayes accuracy, of the benchmark. Not only is\nthis helpful to keep us from disclosing the ground truth, but this approach\nalso offers a test for detecting data contamination. In principle, even fully\ncapable models should not surpass the Bayes accuracy. If a model surpasses this\nceiling despite this expectation, this is a strong signal of data\ncontamination. We present experimental evidence that our method can detect data\ncontamination accurately on a wide range of benchmarks, models, and training\nmethodologies."
                },
                "authors": [
                    {
                        "name": "Takashi Ishida"
                    },
                    {
                        "name": "Thanawat Lodkaew"
                    },
                    {
                        "name": "Ikko Yamane"
                    }
                ],
                "author_detail": {
                    "name": "Ikko Yamane"
                },
                "author": "Ikko Yamane",
                "arxiv_comment": "Extended version of the paper presented as an Oral at the ICML 2025\n  Workshop on the Impact of Memorization on Trustworthy Foundation Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18102v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18102v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23643v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23643v1",
                "updated": "2025-07-31T15:22:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    22,
                    23,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T15:22:23Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    22,
                    23,
                    3,
                    212,
                    0
                ],
                "title": "FFGAF-SNN: The Forward-Forward Based Gradient Approximation Free\n  Training Framework for Spiking Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FFGAF-SNN: The Forward-Forward Based Gradient Approximation Free\n  Training Framework for Spiking Neural Networks"
                },
                "summary": "Spiking Neural Networks (SNNs) offer a biologically plausible framework for\nenergy-efficient neuromorphic computing. However, it is a challenge to train\nSNNs due to their non-differentiability, efficiently. Existing gradient\napproximation approaches frequently sacrifice accuracy and face deployment\nlimitations on edge devices due to the substantial computational requirements\nof backpropagation. To address these challenges, we propose a Forward-Forward\n(FF) based gradient approximation-free training framework for Spiking Neural\nNetworks, which treats spiking activations as black-box modules, thereby\neliminating the need for gradient approximation while significantly reducing\ncomputational complexity. Furthermore, we introduce a class-aware complexity\nadaptation mechanism that dynamically optimizes the loss function based on\ninter-class difficulty metrics, enabling efficient allocation of network\nresources across different categories. Experimental results demonstrate that\nour proposed training framework achieves test accuracies of 99.58%, 92.13%, and\n75.64% on the MNIST, Fashion-MNIST, and CIFAR-10 datasets, respectively,\nsurpassing all existing FF-based SNN approaches. Additionally, our proposed\nmethod exhibits significant advantages in terms of memory access and\ncomputational power consumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Neural Networks (SNNs) offer a biologically plausible framework for\nenergy-efficient neuromorphic computing. However, it is a challenge to train\nSNNs due to their non-differentiability, efficiently. Existing gradient\napproximation approaches frequently sacrifice accuracy and face deployment\nlimitations on edge devices due to the substantial computational requirements\nof backpropagation. To address these challenges, we propose a Forward-Forward\n(FF) based gradient approximation-free training framework for Spiking Neural\nNetworks, which treats spiking activations as black-box modules, thereby\neliminating the need for gradient approximation while significantly reducing\ncomputational complexity. Furthermore, we introduce a class-aware complexity\nadaptation mechanism that dynamically optimizes the loss function based on\ninter-class difficulty metrics, enabling efficient allocation of network\nresources across different categories. Experimental results demonstrate that\nour proposed training framework achieves test accuracies of 99.58%, 92.13%, and\n75.64% on the MNIST, Fashion-MNIST, and CIFAR-10 datasets, respectively,\nsurpassing all existing FF-based SNN approaches. Additionally, our proposed\nmethod exhibits significant advantages in terms of memory access and\ncomputational power consumption."
                },
                "authors": [
                    {
                        "name": "Changqing Xu"
                    },
                    {
                        "name": "Ziqiang Yang"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Xinfang Liao"
                    },
                    {
                        "name": "Guiqi Mo"
                    },
                    {
                        "name": "Hao Zeng"
                    },
                    {
                        "name": "Yintang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yintang Yang"
                },
                "author": "Yintang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23643v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23633v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23633v1",
                "updated": "2025-07-31T15:11:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    11,
                    38,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T15:11:38Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    11,
                    38,
                    3,
                    212,
                    0
                ],
                "title": "MemoCue: Empowering LLM-Based Agents for Human Memory Recall via\n  Strategy-Guided Querying",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemoCue: Empowering LLM-Based Agents for Human Memory Recall via\n  Strategy-Guided Querying"
                },
                "summary": "Agent-assisted memory recall is one critical research problem in the field of\nhuman-computer interaction. In conventional methods, the agent can retrieve\ninformation from its equipped memory module to help the person recall\nincomplete or vague memories. The limited size of memory module hinders the\nacquisition of complete memories and impacts the memory recall performance in\npractice. Memory theories suggest that the person's relevant memory can be\nproactively activated through some effective cues. Inspired by this, we propose\na novel strategy-guided agent-assisted memory recall method, allowing the agent\nto transform an original query into a cue-rich one via the judiciously designed\nstrategy to help the person recall memories. To this end, there are two key\nchallenges. (1) How to choose the appropriate recall strategy for diverse\nforgetting scenarios with distinct memory-recall characteristics? (2) How to\nobtain the high-quality responses leveraging recall strategies, given only\nabstract and sparsely annotated strategy patterns? To address the challenges,\nwe propose a Recall Router framework. Specifically, we design a 5W Recall Map\nto classify memory queries into five typical scenarios and define fifteen\nrecall strategy patterns across the corresponding scenarios. We then propose a\nhierarchical recall tree combined with the Monte Carlo Tree Search algorithm to\noptimize the selection of strategy and the generation of strategy responses. We\nconstruct an instruction tuning dataset and fine-tune multiple open-source\nlarge language models (LLMs) to develop MemoCue, an agent that excels in\nproviding memory-inspired responses. Experiments on three representative\ndatasets show that MemoCue surpasses LLM-based methods by 17.74% in recall\ninspiration. Further human evaluation highlights its advantages in\nmemory-recall applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent-assisted memory recall is one critical research problem in the field of\nhuman-computer interaction. In conventional methods, the agent can retrieve\ninformation from its equipped memory module to help the person recall\nincomplete or vague memories. The limited size of memory module hinders the\nacquisition of complete memories and impacts the memory recall performance in\npractice. Memory theories suggest that the person's relevant memory can be\nproactively activated through some effective cues. Inspired by this, we propose\na novel strategy-guided agent-assisted memory recall method, allowing the agent\nto transform an original query into a cue-rich one via the judiciously designed\nstrategy to help the person recall memories. To this end, there are two key\nchallenges. (1) How to choose the appropriate recall strategy for diverse\nforgetting scenarios with distinct memory-recall characteristics? (2) How to\nobtain the high-quality responses leveraging recall strategies, given only\nabstract and sparsely annotated strategy patterns? To address the challenges,\nwe propose a Recall Router framework. Specifically, we design a 5W Recall Map\nto classify memory queries into five typical scenarios and define fifteen\nrecall strategy patterns across the corresponding scenarios. We then propose a\nhierarchical recall tree combined with the Monte Carlo Tree Search algorithm to\noptimize the selection of strategy and the generation of strategy responses. We\nconstruct an instruction tuning dataset and fine-tune multiple open-source\nlarge language models (LLMs) to develop MemoCue, an agent that excels in\nproviding memory-inspired responses. Experiments on three representative\ndatasets show that MemoCue surpasses LLM-based methods by 17.74% in recall\ninspiration. Further human evaluation highlights its advantages in\nmemory-recall applications."
                },
                "authors": [
                    {
                        "name": "Qian Zhao"
                    },
                    {
                        "name": "Zhuo Sun"
                    },
                    {
                        "name": "Bin Guo"
                    },
                    {
                        "name": "Zhiwen Yu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiwen Yu"
                },
                "author": "Zhiwen Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23633v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23633v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23617v1",
                "updated": "2025-07-31T14:57:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    14,
                    57,
                    5,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T14:57:05Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    14,
                    57,
                    5,
                    3,
                    212,
                    0
                ],
                "title": "Quasi-continuous sub-$$K strontium source without a high-finesse\n  cavity stabilized laser",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quasi-continuous sub-$$K strontium source without a high-finesse\n  cavity stabilized laser"
                },
                "summary": "We demonstrate a quasi-continuous sub-$\\mu$K strontium source achieved\nwithout the use of a high-finesse cavity-locked laser. Our frequency reference\nis based on a dispersion-optimized, fiber-based frequency comb that enables\nsub-kHz linewidths. The long-term stability of the comb is defined by an\nexternal RF reference: either a 10 MHz RF signal from the Dutch Metrology\nInstitute (VSL), or a tunable RF source whose long-term stability is maintained\nby monitoring and stabilizing the position of a narrow-line magneto-optical\ntrap (MOT). The comb-stabilized system is benchmarked against a conventional\ncavity-locked laser and achieves comparable performance in broadband and\nsingle-frequency MOTs using the narrow $^1$S$_0$ $\\rightarrow$ $^3$P$_1$ laser\ncooling transition. We generate high-flux, sub-$\\mu$K samples of all three\nbosonic strontium isotopes and demonstrate quasi-continuous outcoupling from\nthe MOT. These results highlight the system's suitability for compact, robust,\nand field-deployable continuous cold atom devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We demonstrate a quasi-continuous sub-$\\mu$K strontium source achieved\nwithout the use of a high-finesse cavity-locked laser. Our frequency reference\nis based on a dispersion-optimized, fiber-based frequency comb that enables\nsub-kHz linewidths. The long-term stability of the comb is defined by an\nexternal RF reference: either a 10 MHz RF signal from the Dutch Metrology\nInstitute (VSL), or a tunable RF source whose long-term stability is maintained\nby monitoring and stabilizing the position of a narrow-line magneto-optical\ntrap (MOT). The comb-stabilized system is benchmarked against a conventional\ncavity-locked laser and achieves comparable performance in broadband and\nsingle-frequency MOTs using the narrow $^1$S$_0$ $\\rightarrow$ $^3$P$_1$ laser\ncooling transition. We generate high-flux, sub-$\\mu$K samples of all three\nbosonic strontium isotopes and demonstrate quasi-continuous outcoupling from\nthe MOT. These results highlight the system's suitability for compact, robust,\nand field-deployable continuous cold atom devices."
                },
                "authors": [
                    {
                        "name": "Sana Boughdachi"
                    },
                    {
                        "name": "Benedikt Heizenreder"
                    },
                    {
                        "name": "Ananya Sitaram"
                    },
                    {
                        "name": "Erik Dierikx"
                    },
                    {
                        "name": "Yan Xie"
                    },
                    {
                        "name": "Sander Klemann"
                    },
                    {
                        "name": "Paul Klop"
                    },
                    {
                        "name": "Jeroen Koelemeij"
                    },
                    {
                        "name": "Rafa Wilk"
                    },
                    {
                        "name": "Florian Schreck"
                    },
                    {
                        "name": "Andreas Brodschelm"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Brodschelm"
                },
                "author": "Andreas Brodschelm",
                "arxiv_comment": "12 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.atom-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.quant-gas",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23611v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23611v1",
                "updated": "2025-07-31T14:49:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    14,
                    49,
                    3,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T14:49:03Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    14,
                    49,
                    3,
                    3,
                    212,
                    0
                ],
                "title": "LLM-Based Identification of Infostealer Infection Vectors from\n  Screenshots: The Case of Aurora",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Based Identification of Infostealer Infection Vectors from\n  Screenshots: The Case of Aurora"
                },
                "summary": "Infostealers exfiltrate credentials, session cookies, and sensitive data from\ninfected systems. With over 29 million stealer logs reported in 2024, manual\nanalysis and mitigation at scale are virtually unfeasible/unpractical. While\nmost research focuses on proactive malware detection, a significant gap remains\nin leveraging reactive analysis of stealer logs and their associated artifacts.\nSpecifically, infection artifacts such as screenshots, image captured at the\npoint of compromise, are largely overlooked by the current literature. This\npaper introduces a novel approach leveraging Large Language Models (LLMs), more\nspecifically gpt-4o-mini, to analyze infection screenshots to extract potential\nIndicators of Compromise (IoCs), map infection vectors, and track campaigns.\nFocusing on the Aurora infostealer, we demonstrate how LLMs can process\nscreenshots to identify infection vectors, such as malicious URLs, installer\nfiles, and exploited software themes. Our method extracted 337 actionable URLs\nand 246 relevant files from 1000 screenshots, revealing key malware\ndistribution methods and social engineering tactics. By correlating extracted\nfilenames, URLs, and infection themes, we identified three distinct malware\ncampaigns, demonstrating the potential of LLM-driven analysis for uncovering\ninfection workflows and enhancing threat intelligence. By shifting malware\nanalysis from traditional log-based detection methods to a reactive,\nartifact-driven approach that leverages infection screenshots, this research\npresents a scalable method for identifying infection vectors and enabling early\nintervention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Infostealers exfiltrate credentials, session cookies, and sensitive data from\ninfected systems. With over 29 million stealer logs reported in 2024, manual\nanalysis and mitigation at scale are virtually unfeasible/unpractical. While\nmost research focuses on proactive malware detection, a significant gap remains\nin leveraging reactive analysis of stealer logs and their associated artifacts.\nSpecifically, infection artifacts such as screenshots, image captured at the\npoint of compromise, are largely overlooked by the current literature. This\npaper introduces a novel approach leveraging Large Language Models (LLMs), more\nspecifically gpt-4o-mini, to analyze infection screenshots to extract potential\nIndicators of Compromise (IoCs), map infection vectors, and track campaigns.\nFocusing on the Aurora infostealer, we demonstrate how LLMs can process\nscreenshots to identify infection vectors, such as malicious URLs, installer\nfiles, and exploited software themes. Our method extracted 337 actionable URLs\nand 246 relevant files from 1000 screenshots, revealing key malware\ndistribution methods and social engineering tactics. By correlating extracted\nfilenames, URLs, and infection themes, we identified three distinct malware\ncampaigns, demonstrating the potential of LLM-driven analysis for uncovering\ninfection workflows and enhancing threat intelligence. By shifting malware\nanalysis from traditional log-based detection methods to a reactive,\nartifact-driven approach that leverages infection screenshots, this research\npresents a scalable method for identifying infection vectors and enabling early\nintervention."
                },
                "authors": [
                    {
                        "name": "Estelle Ruellan"
                    },
                    {
                        "name": "Eric Clay"
                    },
                    {
                        "name": "Nicholas Ascoli"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas Ascoli"
                },
                "author": "Nicholas Ascoli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23611v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23611v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23599v1",
                "updated": "2025-07-31T14:39:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    14,
                    39,
                    31,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T14:39:31Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    14,
                    39,
                    31,
                    3,
                    212,
                    0
                ],
                "title": "DA-Occ: Efficient 3D Voxel Occupancy Prediction via Directional 2D for\n  Geometric Structure Preservation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DA-Occ: Efficient 3D Voxel Occupancy Prediction via Directional 2D for\n  Geometric Structure Preservation"
                },
                "summary": "Efficient and high-accuracy 3D occupancy prediction is crucial for ensuring\nthe performance of autonomous driving (AD) systems. However, many current\nmethods focus on high accuracy at the expense of real-time processing needs. To\naddress this challenge of balancing accuracy and inference speed, we propose a\ndirectional pure 2D approach. Our method involves slicing 3D voxel features to\npreserve complete vertical geometric information. This strategy compensates for\nthe loss of height cues in Bird's-Eye View (BEV) representations, thereby\nmaintaining the integrity of the 3D geometric structure. By employing a\ndirectional attention mechanism, we efficiently extract geometric features from\ndifferent orientations, striking a balance between accuracy and computational\nefficiency. Experimental results highlight the significant advantages of our\napproach for autonomous driving. On the Occ3D-nuScenes, the proposed method\nachieves an mIoU of 39.3% and an inference speed of 27.7 FPS, effectively\nbalancing accuracy and efficiency. In simulations on edge devices, the\ninference speed reaches 14.8 FPS, further demonstrating the method's\napplicability for real-time deployment in resource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and high-accuracy 3D occupancy prediction is crucial for ensuring\nthe performance of autonomous driving (AD) systems. However, many current\nmethods focus on high accuracy at the expense of real-time processing needs. To\naddress this challenge of balancing accuracy and inference speed, we propose a\ndirectional pure 2D approach. Our method involves slicing 3D voxel features to\npreserve complete vertical geometric information. This strategy compensates for\nthe loss of height cues in Bird's-Eye View (BEV) representations, thereby\nmaintaining the integrity of the 3D geometric structure. By employing a\ndirectional attention mechanism, we efficiently extract geometric features from\ndifferent orientations, striking a balance between accuracy and computational\nefficiency. Experimental results highlight the significant advantages of our\napproach for autonomous driving. On the Occ3D-nuScenes, the proposed method\nachieves an mIoU of 39.3% and an inference speed of 27.7 FPS, effectively\nbalancing accuracy and efficiency. In simulations on edge devices, the\ninference speed reaches 14.8 FPS, further demonstrating the method's\napplicability for real-time deployment in resource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Yuchen Zhou"
                    },
                    {
                        "name": "Yan Luo"
                    },
                    {
                        "name": "Xiangang Wang"
                    },
                    {
                        "name": "Xingjian Gu"
                    },
                    {
                        "name": "Mingzhou Lu"
                    }
                ],
                "author_detail": {
                    "name": "Mingzhou Lu"
                },
                "author": "Mingzhou Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15299v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15299v3",
                "updated": "2025-07-31T14:38:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    14,
                    38,
                    33,
                    3,
                    212,
                    0
                ],
                "published": "2025-03-19T15:21:48Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    21,
                    48,
                    2,
                    78,
                    0
                ],
                "title": "Inside-Out: Hidden Factual Knowledge in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inside-Out: Hidden Factual Knowledge in LLMs"
                },
                "summary": "This work presents a framework for assessing whether large language models\n(LLMs) encode more factual knowledge in their parameters than what they express\nin their outputs. While a few studies hint at this possibility, none has\nclearly defined or demonstrated this phenomenon. We first propose a formal\ndefinition of knowledge, quantifying it for a given question as the fraction of\ncorrect-incorrect answer pairs where the correct one is ranked higher. This\ngives rise to external and internal knowledge, depending on the information\nused to score individual answer candidates: either the model's observable\ntoken-level probabilities or its intermediate computations. Hidden knowledge\narises when internal knowledge exceeds external knowledge. We then present a\ncase study, applying this framework to three popular open-weights LLMs in a\nclosed-book QA setup. Our results indicate that: (1) LLMs consistently encode\nmore factual knowledge internally than what they express externally, with an\naverage relative gap of 40%. (2) Surprisingly, some knowledge is so deeply\nhidden that a model can internally know an answer perfectly, yet fail to\ngenerate it even once, despite large-scale repeated sampling of 1,000 answers.\nThis reveals fundamental limitations in the generation capabilities of LLMs,\nwhich (3) put a practical constraint on scaling test-time compute via repeated\nanswer sampling in closed-book QA: significant performance improvements remain\ninaccessible because some answers are practically never sampled, yet if they\nwere, we would be guaranteed to rank them first.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a framework for assessing whether large language models\n(LLMs) encode more factual knowledge in their parameters than what they express\nin their outputs. While a few studies hint at this possibility, none has\nclearly defined or demonstrated this phenomenon. We first propose a formal\ndefinition of knowledge, quantifying it for a given question as the fraction of\ncorrect-incorrect answer pairs where the correct one is ranked higher. This\ngives rise to external and internal knowledge, depending on the information\nused to score individual answer candidates: either the model's observable\ntoken-level probabilities or its intermediate computations. Hidden knowledge\narises when internal knowledge exceeds external knowledge. We then present a\ncase study, applying this framework to three popular open-weights LLMs in a\nclosed-book QA setup. Our results indicate that: (1) LLMs consistently encode\nmore factual knowledge internally than what they express externally, with an\naverage relative gap of 40%. (2) Surprisingly, some knowledge is so deeply\nhidden that a model can internally know an answer perfectly, yet fail to\ngenerate it even once, despite large-scale repeated sampling of 1,000 answers.\nThis reveals fundamental limitations in the generation capabilities of LLMs,\nwhich (3) put a practical constraint on scaling test-time compute via repeated\nanswer sampling in closed-book QA: significant performance improvements remain\ninaccessible because some answers are practically never sampled, yet if they\nwere, we would be guaranteed to rank them first."
                },
                "authors": [
                    {
                        "name": "Zorik Gekhman"
                    },
                    {
                        "name": "Eyal Ben David"
                    },
                    {
                        "name": "Hadas Orgad"
                    },
                    {
                        "name": "Eran Ofek"
                    },
                    {
                        "name": "Yonatan Belinkov"
                    },
                    {
                        "name": "Idan Szpektor"
                    },
                    {
                        "name": "Jonathan Herzig"
                    },
                    {
                        "name": "Roi Reichart"
                    }
                ],
                "author_detail": {
                    "name": "Roi Reichart"
                },
                "author": "Roi Reichart",
                "arxiv_comment": "Accepted to COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15299v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15299v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19115v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19115v2",
                "updated": "2025-07-31T14:34:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    14,
                    34,
                    0,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-25T09:50:48Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    9,
                    50,
                    48,
                    4,
                    206,
                    0
                ],
                "title": "Automated Code Review Using Large Language Models at Ericsson: An\n  Experience Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Code Review Using Large Language Models at Ericsson: An\n  Experience Report"
                },
                "summary": "Code review is one of the primary means of assuring the quality of released\nsoftware along with testing and static analysis. However, code review requires\nexperienced developers who may not always have the time to perform an in-depth\nreview of code. Thus, automating code review can help alleviate the cognitive\nburden on experienced software developers allowing them to focus on their\nprimary activities of writing code to add new features and fix bugs. In this\npaper, we describe our experience in using Large Language Models towards\nautomating the code review process in Ericsson. We describe the development of\na lightweight tool using LLMs and static program analysis. We then describe our\npreliminary experiments with experienced developers in evaluating our code\nreview tool and the encouraging results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code review is one of the primary means of assuring the quality of released\nsoftware along with testing and static analysis. However, code review requires\nexperienced developers who may not always have the time to perform an in-depth\nreview of code. Thus, automating code review can help alleviate the cognitive\nburden on experienced software developers allowing them to focus on their\nprimary activities of writing code to add new features and fix bugs. In this\npaper, we describe our experience in using Large Language Models towards\nautomating the code review process in Ericsson. We describe the development of\na lightweight tool using LLMs and static program analysis. We then describe our\npreliminary experiments with experienced developers in evaluating our code\nreview tool and the encouraging results."
                },
                "authors": [
                    {
                        "name": "Shweta Ramesh"
                    },
                    {
                        "name": "Joy Bose"
                    },
                    {
                        "name": "Hamender Singh"
                    },
                    {
                        "name": "A K Raghavan"
                    },
                    {
                        "name": "Sujoy Roychowdhury"
                    },
                    {
                        "name": "Giriprasad Sridhara"
                    },
                    {
                        "name": "Nishrith Saini"
                    },
                    {
                        "name": "Ricardo Britto"
                    }
                ],
                "author_detail": {
                    "name": "Ricardo Britto"
                },
                "author": "Ricardo Britto",
                "arxiv_comment": "6 pages, 4 figures, 1 table. Accepted in ICSME 2025 conference in\n  Auckland",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19115v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19115v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23589v1",
                "updated": "2025-07-31T14:25:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    14,
                    25,
                    54,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T14:25:54Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    14,
                    25,
                    54,
                    3,
                    212,
                    0
                ],
                "title": "Can LLM-Reasoning Models Replace Classical Planning? A Benchmark Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLM-Reasoning Models Replace Classical Planning? A Benchmark Study"
                },
                "summary": "Recent advancements in Large Language Models have sparked interest in their\npotential for robotic task planning. While these models demonstrate strong\ngenerative capabilities, their effectiveness in producing structured and\nexecutable plans remains uncertain. This paper presents a systematic evaluation\nof a broad spectrum of current state of the art language models, each directly\nprompted using Planning Domain Definition Language domain and problem files,\nand compares their planning performance with the Fast Downward planner across a\nvariety of benchmarks. In addition to measuring success rates, we assess how\nfaithfully the generated plans translate into sequences of actions that can\nactually be executed, identifying both strengths and limitations of using these\nmodels in this setting. Our findings show that while the models perform well on\nsimpler planning tasks, they continue to struggle with more complex scenarios\nthat require precise resource management, consistent state tracking, and strict\nconstraint compliance. These results underscore fundamental challenges in\napplying language models to robotic planning in real world environments. By\noutlining the gaps that emerge during execution, we aim to guide future\nresearch toward combined approaches that integrate language models with\nclassical planners in order to enhance the reliability and scalability of\nplanning in autonomous robotics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models have sparked interest in their\npotential for robotic task planning. While these models demonstrate strong\ngenerative capabilities, their effectiveness in producing structured and\nexecutable plans remains uncertain. This paper presents a systematic evaluation\nof a broad spectrum of current state of the art language models, each directly\nprompted using Planning Domain Definition Language domain and problem files,\nand compares their planning performance with the Fast Downward planner across a\nvariety of benchmarks. In addition to measuring success rates, we assess how\nfaithfully the generated plans translate into sequences of actions that can\nactually be executed, identifying both strengths and limitations of using these\nmodels in this setting. Our findings show that while the models perform well on\nsimpler planning tasks, they continue to struggle with more complex scenarios\nthat require precise resource management, consistent state tracking, and strict\nconstraint compliance. These results underscore fundamental challenges in\napplying language models to robotic planning in real world environments. By\noutlining the gaps that emerge during execution, we aim to guide future\nresearch toward combined approaches that integrate language models with\nclassical planners in order to enhance the reliability and scalability of\nplanning in autonomous robotics."
                },
                "authors": [
                    {
                        "name": "Kai Goebel"
                    },
                    {
                        "name": "Patrik Zips"
                    }
                ],
                "author_detail": {
                    "name": "Patrik Zips"
                },
                "author": "Patrik Zips",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23581v1",
                "updated": "2025-07-31T14:11:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    14,
                    11,
                    16,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T14:11:16Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    14,
                    11,
                    16,
                    3,
                    212,
                    0
                ],
                "title": "GraphRAG-R1: Graph Retrieval-Augmented Generation with\n  Process-Constrained Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphRAG-R1: Graph Retrieval-Augmented Generation with\n  Process-Constrained Reinforcement Learning"
                },
                "summary": "Graph Retrieval-Augmented Generation (GraphRAG) has shown great effectiveness\nin enhancing the reasoning abilities of LLMs by leveraging graph structures for\nknowledge representation and modeling complex real-world relationships.\nHowever, existing GraphRAG methods still face significant bottlenecks when\nhandling complex problems that require multi-hop reasoning, as their query and\nretrieval phases are largely based on pre-defined heuristics and do not fully\nutilize the reasoning potentials of LLMs. To address this problem, we propose\nGraphRAG-R1, an adaptive GraphRAG framework by training LLMs with\nprocess-constrained outcome-based reinforcement learning (RL) to enhance the\nmulti-hop reasoning ability. Our method can decompose complex problems,\nautonomously invoke retrieval tools to acquire necessary information, and\nperform effective reasoning. Specifically, we utilize a modified version of\nGroup Relative Policy Optimization (GRPO) that supports rollout-with-thinking\ncapability. Next, we design two process-constrained reward functions. To handle\nthe shallow retrieval problem, we design a Progressive Retrieval Attenuation\n(PRA) reward to encourage essential retrievals. Then, to handle the\nover-thinking problem, we design Cost-Aware F1 (CAF) reward to balance the\nmodel performance with computational costs. We further design a phase-dependent\ntraining strategy, containing three training stages corresponding to cold start\nand these two rewards. Lastly, our method adopts a hybrid graph-textual\nretrieval to improve the reasoning capacity. Extensive experimental results\ndemonstrate that GraphRAG-R1 boosts LLM capabilities in solving complex\nreasoning problems compared to state-of-the-art GraphRAG methods on both\nin-domain and out-of-domain datasets. Furthermore, our framework can be\nflexibly integrated with various existing retrieval methods, consistently\ndelivering performance improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Retrieval-Augmented Generation (GraphRAG) has shown great effectiveness\nin enhancing the reasoning abilities of LLMs by leveraging graph structures for\nknowledge representation and modeling complex real-world relationships.\nHowever, existing GraphRAG methods still face significant bottlenecks when\nhandling complex problems that require multi-hop reasoning, as their query and\nretrieval phases are largely based on pre-defined heuristics and do not fully\nutilize the reasoning potentials of LLMs. To address this problem, we propose\nGraphRAG-R1, an adaptive GraphRAG framework by training LLMs with\nprocess-constrained outcome-based reinforcement learning (RL) to enhance the\nmulti-hop reasoning ability. Our method can decompose complex problems,\nautonomously invoke retrieval tools to acquire necessary information, and\nperform effective reasoning. Specifically, we utilize a modified version of\nGroup Relative Policy Optimization (GRPO) that supports rollout-with-thinking\ncapability. Next, we design two process-constrained reward functions. To handle\nthe shallow retrieval problem, we design a Progressive Retrieval Attenuation\n(PRA) reward to encourage essential retrievals. Then, to handle the\nover-thinking problem, we design Cost-Aware F1 (CAF) reward to balance the\nmodel performance with computational costs. We further design a phase-dependent\ntraining strategy, containing three training stages corresponding to cold start\nand these two rewards. Lastly, our method adopts a hybrid graph-textual\nretrieval to improve the reasoning capacity. Extensive experimental results\ndemonstrate that GraphRAG-R1 boosts LLM capabilities in solving complex\nreasoning problems compared to state-of-the-art GraphRAG methods on both\nin-domain and out-of-domain datasets. Furthermore, our framework can be\nflexibly integrated with various existing retrieval methods, consistently\ndelivering performance improvements."
                },
                "authors": [
                    {
                        "name": "Chuanyue Yu"
                    },
                    {
                        "name": "Kuo Zhao"
                    },
                    {
                        "name": "Yuhan Li"
                    },
                    {
                        "name": "Heng Chang"
                    },
                    {
                        "name": "Mingjian Feng"
                    },
                    {
                        "name": "Xiangzhe Jiang"
                    },
                    {
                        "name": "Yufei Sun"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Yuzhi Zhang"
                    },
                    {
                        "name": "Jianxin Li"
                    },
                    {
                        "name": "Ziwei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ziwei Zhang"
                },
                "author": "Ziwei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02744v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02744v3",
                "updated": "2025-07-31T14:02:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    14,
                    2,
                    13,
                    3,
                    212,
                    0
                ],
                "published": "2024-10-03T17:55:17Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    55,
                    17,
                    3,
                    277,
                    0
                ],
                "title": "Neutral Residues: Revisiting Adapters for Model Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neutral Residues: Revisiting Adapters for Model Extension"
                },
                "summary": "We address the problem of extending a pretrained large language model to a\nnew domain that was not seen during training. Standard techniques, such as\nfinetuning or low-rank adaptation (LoRA) are successful at domain adaptation,\nbut do not formally add capacity to the model. This often leads to a trade-off,\nbetween performing well on the new domain vs. degrading performance on the\noriginal domain. Here, we revisit and improve adapters to extend LLMs from\nthree angles: data, architecture and training procedure, which are\nadvantageously considered jointly. The resulting method, called neutral\nresidues, modifies adapters in a way that leads each new residual block to\noutput near-zeros on the original domain. This solution leads to strong results\nwhen adapting a state-of-the-art model originally trained on English to a new\nlanguage. Neutral residues significantly outperform competing approaches such\nas finetuning, LoRA or vanilla adapters in terms of the trade-off between\nlearning the new language and not forgetting English.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the problem of extending a pretrained large language model to a\nnew domain that was not seen during training. Standard techniques, such as\nfinetuning or low-rank adaptation (LoRA) are successful at domain adaptation,\nbut do not formally add capacity to the model. This often leads to a trade-off,\nbetween performing well on the new domain vs. degrading performance on the\noriginal domain. Here, we revisit and improve adapters to extend LLMs from\nthree angles: data, architecture and training procedure, which are\nadvantageously considered jointly. The resulting method, called neutral\nresidues, modifies adapters in a way that leads each new residual block to\noutput near-zeros on the original domain. This solution leads to strong results\nwhen adapting a state-of-the-art model originally trained on English to a new\nlanguage. Neutral residues significantly outperform competing approaches such\nas finetuning, LoRA or vanilla adapters in terms of the trade-off between\nlearning the new language and not forgetting English."
                },
                "authors": [
                    {
                        "name": "Franck Signe Talla"
                    },
                    {
                        "name": "Edouard Grave"
                    },
                    {
                        "name": "Herv Jgou"
                    }
                ],
                "author_detail": {
                    "name": "Herv Jgou"
                },
                "author": "Herv Jgou",
                "arxiv_comment": "Accepted at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02744v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02744v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07426v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07426v3",
                "updated": "2025-07-31T13:57:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    13,
                    57,
                    25,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-10T04:39:55Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    4,
                    39,
                    55,
                    3,
                    191,
                    0
                ],
                "title": "DrugMCTS: a drug repurposing framework combining multi-agent, RAG and\n  Monte Carlo Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DrugMCTS: a drug repurposing framework combining multi-agent, RAG and\n  Monte Carlo Tree Search"
                },
                "summary": "Recent advances in large language models have demonstrated considerable\npotential in scientific domains such as drug repositioning. However, their\neffectiveness remains constrained when reasoning extends beyond the knowledge\nacquired during pretraining. Conventional approaches, such as fine-tuning or\nretrieval-augmented generation, face limitations in either imposing high\ncomputational overhead or failing to fully exploit structured scientific data.\nTo overcome these challenges, we propose DrugMCTS, a novel framework that\nsynergistically integrates RAG, multi-agent collaboration, and Monte Carlo Tree\nSearch for drug repositioning. The framework employs five specialized agents\ntasked with retrieving and analyzing molecular and protein information, thereby\nenabling structured and iterative reasoning. Extensive experiments on the\nDrugBank and KIBA datasets demonstrate that DrugMCTS achieves substantially\nhigher recall and robustness compared to both general-purpose LLMs and deep\nlearning baselines. Our results highlight the importance of structured\nreasoning, agent-based collaboration, and feedback-driven search mechanisms in\nadvancing LLM applications for drug repositioning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models have demonstrated considerable\npotential in scientific domains such as drug repositioning. However, their\neffectiveness remains constrained when reasoning extends beyond the knowledge\nacquired during pretraining. Conventional approaches, such as fine-tuning or\nretrieval-augmented generation, face limitations in either imposing high\ncomputational overhead or failing to fully exploit structured scientific data.\nTo overcome these challenges, we propose DrugMCTS, a novel framework that\nsynergistically integrates RAG, multi-agent collaboration, and Monte Carlo Tree\nSearch for drug repositioning. The framework employs five specialized agents\ntasked with retrieving and analyzing molecular and protein information, thereby\nenabling structured and iterative reasoning. Extensive experiments on the\nDrugBank and KIBA datasets demonstrate that DrugMCTS achieves substantially\nhigher recall and robustness compared to both general-purpose LLMs and deep\nlearning baselines. Our results highlight the importance of structured\nreasoning, agent-based collaboration, and feedback-driven search mechanisms in\nadvancing LLM applications for drug repositioning."
                },
                "authors": [
                    {
                        "name": "Zerui Yang"
                    },
                    {
                        "name": "Yuwei Wan"
                    },
                    {
                        "name": "Siyu Yan"
                    },
                    {
                        "name": "Yudai Matsuda"
                    },
                    {
                        "name": "Tong Xie"
                    },
                    {
                        "name": "Bram Hoex"
                    },
                    {
                        "name": "Linqi Song"
                    }
                ],
                "author_detail": {
                    "name": "Linqi Song"
                },
                "author": "Linqi Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07426v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07426v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23562v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23562v1",
                "updated": "2025-07-31T13:49:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    13,
                    49,
                    44,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T13:49:44Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    13,
                    49,
                    44,
                    3,
                    212,
                    0
                ],
                "title": "Hardware-Aware Fine-Tuning of Spiking Q-Networks on the SpiNNaker2\n  Neuromorphic Platform",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hardware-Aware Fine-Tuning of Spiking Q-Networks on the SpiNNaker2\n  Neuromorphic Platform"
                },
                "summary": "Spiking Neural Networks (SNNs) promise orders-of-magnitude lower power\nconsumption and low-latency inference on neuromorphic hardware for a wide range\nof robotic tasks. In this work, we present an energy-efficient implementation\nof a reinforcement learning (RL) algorithm using quantized SNNs to solve two\nclassical control tasks. The network is trained using the Q-learning algorithm,\nthen fine-tuned and quantized to low-bit (8-bit) precision for embedded\ndeployment on the SpiNNaker2 neuromorphic chip. To evaluate the comparative\nadvantage of SpiNNaker2 over conventional computing platforms, we analyze\ninference latency, dynamic power consumption, and energy cost per inference for\nour SNN models, comparing performance against a GTX 1650 GPU baseline. Our\nresults demonstrate SpiNNaker2's strong potential for scalable, low-energy\nneuromorphic computing, achieving up to 32x reduction in energy consumption.\nInference latency remains on par with GPU-based execution, with improvements\nobserved in certain task settings, reinforcing SpiNNaker2's viability for\nreal-time neuromorphic control and making the neuromorphic approach a\ncompelling direction for efficient deep Q-learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Neural Networks (SNNs) promise orders-of-magnitude lower power\nconsumption and low-latency inference on neuromorphic hardware for a wide range\nof robotic tasks. In this work, we present an energy-efficient implementation\nof a reinforcement learning (RL) algorithm using quantized SNNs to solve two\nclassical control tasks. The network is trained using the Q-learning algorithm,\nthen fine-tuned and quantized to low-bit (8-bit) precision for embedded\ndeployment on the SpiNNaker2 neuromorphic chip. To evaluate the comparative\nadvantage of SpiNNaker2 over conventional computing platforms, we analyze\ninference latency, dynamic power consumption, and energy cost per inference for\nour SNN models, comparing performance against a GTX 1650 GPU baseline. Our\nresults demonstrate SpiNNaker2's strong potential for scalable, low-energy\nneuromorphic computing, achieving up to 32x reduction in energy consumption.\nInference latency remains on par with GPU-based execution, with improvements\nobserved in certain task settings, reinforcing SpiNNaker2's viability for\nreal-time neuromorphic control and making the neuromorphic approach a\ncompelling direction for efficient deep Q-learning."
                },
                "authors": [
                    {
                        "name": "Sirine Arfa"
                    },
                    {
                        "name": "Bernhard Vogginger"
                    },
                    {
                        "name": "Christian Mayr"
                    }
                ],
                "author_detail": {
                    "name": "Christian Mayr"
                },
                "author": "Christian Mayr",
                "arxiv_comment": "8 pages, 5 figures, 3 tables",
                "arxiv_journal_ref": "ACM ICONS 2025 - International Conference on Neuromorphic Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23562v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23562v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23554v1",
                "updated": "2025-07-31T13:42:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    13,
                    42,
                    14,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T13:42:14Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    13,
                    42,
                    14,
                    3,
                    212,
                    0
                ],
                "title": "DICE: Dynamic In-Context Example Selection in LLM Agents via Efficient\n  Knowledge Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DICE: Dynamic In-Context Example Selection in LLM Agents via Efficient\n  Knowledge Transfer"
                },
                "summary": "Large language model-based agents, empowered by in-context learning (ICL),\nhave demonstrated strong capabilities in complex reasoning and tool-use tasks.\nHowever, existing works have shown that the effectiveness of ICL is highly\nsensitive to the choice of demonstrations, with suboptimal examples often\nleading to unstable or degraded performance. While prior work has explored\nexample selection, including in some agentic or multi-step settings, existing\napproaches typically rely on heuristics or task-specific designs and lack a\ngeneral, theoretically grounded criterion for what constitutes an effective\ndemonstration across reasoning steps. Therefore, it is non-trivial to develop a\nprincipled, general-purpose method for selecting demonstrations that\nconsistently benefit agent performance. In this paper, we address this\nchallenge with DICE, Dynamic In-Context Example Selection for LLM Agents, a\ntheoretically grounded ICL framework for agentic tasks that selects the most\nrelevant demonstrations at each step of reasoning. Our approach decomposes\ndemonstration knowledge into transferable and non-transferable components\nthrough a causal lens, showing how the latter can introduce spurious\ndependencies that impair generalization. We further propose a stepwise\nselection criterion with a formal guarantee of improved agent performance.\nImportantly, DICE is a general, framework-agnostic solution that can be\nintegrated as a plug-in module into existing agentic frameworks without any\nadditional training cost. Extensive experiments across diverse domains\ndemonstrate our method's effectiveness and generality, highlighting the\nimportance of principled, context-aware demo selection for robust and efficient\nLLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model-based agents, empowered by in-context learning (ICL),\nhave demonstrated strong capabilities in complex reasoning and tool-use tasks.\nHowever, existing works have shown that the effectiveness of ICL is highly\nsensitive to the choice of demonstrations, with suboptimal examples often\nleading to unstable or degraded performance. While prior work has explored\nexample selection, including in some agentic or multi-step settings, existing\napproaches typically rely on heuristics or task-specific designs and lack a\ngeneral, theoretically grounded criterion for what constitutes an effective\ndemonstration across reasoning steps. Therefore, it is non-trivial to develop a\nprincipled, general-purpose method for selecting demonstrations that\nconsistently benefit agent performance. In this paper, we address this\nchallenge with DICE, Dynamic In-Context Example Selection for LLM Agents, a\ntheoretically grounded ICL framework for agentic tasks that selects the most\nrelevant demonstrations at each step of reasoning. Our approach decomposes\ndemonstration knowledge into transferable and non-transferable components\nthrough a causal lens, showing how the latter can introduce spurious\ndependencies that impair generalization. We further propose a stepwise\nselection criterion with a formal guarantee of improved agent performance.\nImportantly, DICE is a general, framework-agnostic solution that can be\nintegrated as a plug-in module into existing agentic frameworks without any\nadditional training cost. Extensive experiments across diverse domains\ndemonstrate our method's effectiveness and generality, highlighting the\nimportance of principled, context-aware demo selection for robust and efficient\nLLM agents."
                },
                "authors": [
                    {
                        "name": "Ruoyu Wang"
                    },
                    {
                        "name": "Junda Wu"
                    },
                    {
                        "name": "Yu Xia"
                    },
                    {
                        "name": "Tong Yu"
                    },
                    {
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "name": "Julian McAuley"
                    },
                    {
                        "name": "Lina Yao"
                    }
                ],
                "author_detail": {
                    "name": "Lina Yao"
                },
                "author": "Lina Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18337v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18337v4",
                "updated": "2025-07-31T13:39:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    13,
                    39,
                    18,
                    3,
                    212,
                    0
                ],
                "published": "2024-11-27T13:35:32Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    13,
                    35,
                    32,
                    2,
                    332,
                    0
                ],
                "title": "Can LLMs assist with Ambiguity? A Quantitative Evaluation of various\n  Large Language Models on Word Sense Disambiguation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs assist with Ambiguity? A Quantitative Evaluation of various\n  Large Language Models on Word Sense Disambiguation"
                },
                "summary": "Ambiguous words are often found in modern digital communications. Lexical\nambiguity challenges traditional Word Sense Disambiguation (WSD) methods, due\nto limited data. Consequently, the efficiency of translation, information\nretrieval, and question-answering systems is hindered by these limitations.\nThis study investigates the use of Large Language Models (LLMs) to improve WSD\nusing a novel approach combining a systematic prompt augmentation mechanism\nwith a knowledge base (KB) consisting of different sense interpretations. The\nproposed method incorporates a human-in-loop approach for prompt augmentation\nwhere prompt is supported by Part-of-Speech (POS) tagging, synonyms of\nambiguous words, aspect-based sense filtering and few-shot prompting to guide\nthe LLM. By utilizing a few-shot Chain of Thought (COT) prompting-based\napproach, this work demonstrates a substantial improvement in performance. The\nevaluation was conducted using FEWS test data and sense tags. This research\nadvances accurate word interpretation in social media and digital\ncommunication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ambiguous words are often found in modern digital communications. Lexical\nambiguity challenges traditional Word Sense Disambiguation (WSD) methods, due\nto limited data. Consequently, the efficiency of translation, information\nretrieval, and question-answering systems is hindered by these limitations.\nThis study investigates the use of Large Language Models (LLMs) to improve WSD\nusing a novel approach combining a systematic prompt augmentation mechanism\nwith a knowledge base (KB) consisting of different sense interpretations. The\nproposed method incorporates a human-in-loop approach for prompt augmentation\nwhere prompt is supported by Part-of-Speech (POS) tagging, synonyms of\nambiguous words, aspect-based sense filtering and few-shot prompting to guide\nthe LLM. By utilizing a few-shot Chain of Thought (COT) prompting-based\napproach, this work demonstrates a substantial improvement in performance. The\nevaluation was conducted using FEWS test data and sense tags. This research\nadvances accurate word interpretation in social media and digital\ncommunication."
                },
                "authors": [
                    {
                        "name": "T. G. D. K. Sumanathilaka"
                    },
                    {
                        "name": "Nicholas Micallef"
                    },
                    {
                        "name": "Julian Hough"
                    }
                ],
                "author_detail": {
                    "name": "Julian Hough"
                },
                "author": "Julian Hough",
                "arxiv_comment": "12 pages,6 tables, 1 figure, Proceedings of the 1st International\n  Conference on NLP & AI for Cyber Security",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18337v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18337v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23541v1",
                "updated": "2025-07-31T13:31:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    13,
                    31,
                    1,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T13:31:01Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    13,
                    31,
                    1,
                    3,
                    212,
                    0
                ],
                "title": "Med-R$^3$: Enhancing Medical Retrieval-Augmented Reasoning of LLMs via\n  Progressive Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Med-R$^3$: Enhancing Medical Retrieval-Augmented Reasoning of LLMs via\n  Progressive Reinforcement Learning"
                },
                "summary": "In medical scenarios, effectively retrieving external knowledge and\nleveraging it for rigorous logical reasoning is of significant importance.\nDespite their potential, existing work has predominantly focused on enhancing\neither retrieval or reasoning capabilities of the models in isolation, with\nlittle attention given to their joint optimization, which leads to limited\ncoordination between the two processes. Additionally, current methods rely\nheavily on supervised fine-tuning (SFT), which can cause models to memorize\nexisting problem-solving pathways, thereby restricting their generalization\nability when confronted with novel problem contexts. Furthermore, while some\nstudies have explored to improve retrieval-augmented reasoning in general\ndomains via reinforcement learning, their reward function designs do not\nadequately capture the specific demands of the medical domain. To address these\nchallenges, we introduce **Med-R$^3$**, a **Med**ical **R**etrieval-augmented\n**R**easoning framework driven by progressive **R**einforcement learning. In\nthis framework, we first develop the model's ability to perform logical\nreasoning over medical problems. Subsequently, on the basis of this foundation,\nwe adaptively optimize the retrieval capability to better align with the\ncharacteristics of knowledge corpus and external information utilization\nthroughout the reasoning process. Finally, we conduct joint optimization of the\nmodel's retrieval and reasoning coordination. Extensive experiments indicate\nthat **Med-R$^3$** could achieve state-of-the-art performances, with\nLLaMA3.1-8B-Instruct + Med-R$^3$ surpassing closed-sourced GPT-4o-mini by\n3.93\\% at a comparable parameter scale, while Qwen2.5-14B augmented with\nMed-R$^3$ shows a more substantial gain of 13.53\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In medical scenarios, effectively retrieving external knowledge and\nleveraging it for rigorous logical reasoning is of significant importance.\nDespite their potential, existing work has predominantly focused on enhancing\neither retrieval or reasoning capabilities of the models in isolation, with\nlittle attention given to their joint optimization, which leads to limited\ncoordination between the two processes. Additionally, current methods rely\nheavily on supervised fine-tuning (SFT), which can cause models to memorize\nexisting problem-solving pathways, thereby restricting their generalization\nability when confronted with novel problem contexts. Furthermore, while some\nstudies have explored to improve retrieval-augmented reasoning in general\ndomains via reinforcement learning, their reward function designs do not\nadequately capture the specific demands of the medical domain. To address these\nchallenges, we introduce **Med-R$^3$**, a **Med**ical **R**etrieval-augmented\n**R**easoning framework driven by progressive **R**einforcement learning. In\nthis framework, we first develop the model's ability to perform logical\nreasoning over medical problems. Subsequently, on the basis of this foundation,\nwe adaptively optimize the retrieval capability to better align with the\ncharacteristics of knowledge corpus and external information utilization\nthroughout the reasoning process. Finally, we conduct joint optimization of the\nmodel's retrieval and reasoning coordination. Extensive experiments indicate\nthat **Med-R$^3$** could achieve state-of-the-art performances, with\nLLaMA3.1-8B-Instruct + Med-R$^3$ surpassing closed-sourced GPT-4o-mini by\n3.93\\% at a comparable parameter scale, while Qwen2.5-14B augmented with\nMed-R$^3$ shows a more substantial gain of 13.53\\%."
                },
                "authors": [
                    {
                        "name": "Keer Lu"
                    },
                    {
                        "name": "Zheng Liang"
                    },
                    {
                        "name": "Youquan Li"
                    },
                    {
                        "name": "Jiejun Tan"
                    },
                    {
                        "name": "Da Pan"
                    },
                    {
                        "name": "Shusen Zhang"
                    },
                    {
                        "name": "Guosheng Dong"
                    },
                    {
                        "name": "Huang Leng"
                    }
                ],
                "author_detail": {
                    "name": "Huang Leng"
                },
                "author": "Huang Leng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23540v1",
                "updated": "2025-07-31T13:30:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    13,
                    30,
                    47,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T13:30:47Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    13,
                    30,
                    47,
                    3,
                    212,
                    0
                ],
                "title": "A Unified Perception-Language-Action Framework for Adaptive Autonomous\n  Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Perception-Language-Action Framework for Adaptive Autonomous\n  Driving"
                },
                "summary": "Autonomous driving systems face significant challenges in achieving\nhuman-like adaptability, robustness, and interpretability in complex,\nopen-world environments. These challenges stem from fragmented architectures,\nlimited generalization to novel scenarios, and insufficient semantic extraction\nfrom perception. To address these limitations, we propose a unified\nPerception-Language-Action (PLA) framework that integrates multi-sensor fusion\n(cameras, LiDAR, radar) with a large language model (LLM)-augmented\nVision-Language-Action (VLA) architecture, specifically a GPT-4.1-powered\nreasoning core. This framework unifies low-level sensory processing with\nhigh-level contextual reasoning, tightly coupling perception with natural\nlanguage-based semantic understanding and decision-making to enable\ncontext-aware, explainable, and safety-bounded autonomous driving. Evaluations\non an urban intersection scenario with a construction zone demonstrate superior\nperformance in trajectory tracking, speed prediction, and adaptive planning.\nThe results highlight the potential of language-augmented cognitive frameworks\nfor advancing the safety, interpretability, and scalability of autonomous\ndriving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous driving systems face significant challenges in achieving\nhuman-like adaptability, robustness, and interpretability in complex,\nopen-world environments. These challenges stem from fragmented architectures,\nlimited generalization to novel scenarios, and insufficient semantic extraction\nfrom perception. To address these limitations, we propose a unified\nPerception-Language-Action (PLA) framework that integrates multi-sensor fusion\n(cameras, LiDAR, radar) with a large language model (LLM)-augmented\nVision-Language-Action (VLA) architecture, specifically a GPT-4.1-powered\nreasoning core. This framework unifies low-level sensory processing with\nhigh-level contextual reasoning, tightly coupling perception with natural\nlanguage-based semantic understanding and decision-making to enable\ncontext-aware, explainable, and safety-bounded autonomous driving. Evaluations\non an urban intersection scenario with a construction zone demonstrate superior\nperformance in trajectory tracking, speed prediction, and adaptive planning.\nThe results highlight the potential of language-augmented cognitive frameworks\nfor advancing the safety, interpretability, and scalability of autonomous\ndriving systems."
                },
                "authors": [
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Erik Leo Ha"
                    },
                    {
                        "name": "Kuo-Yi Chao"
                    },
                    {
                        "name": "Nenad Petrovic"
                    },
                    {
                        "name": "Yinglei Song"
                    },
                    {
                        "name": "Chengdong Wu"
                    },
                    {
                        "name": "Alois Knoll"
                    }
                ],
                "author_detail": {
                    "name": "Alois Knoll"
                },
                "author": "Alois Knoll",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23539v1",
                "updated": "2025-07-31T13:29:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    13,
                    29,
                    43,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T13:29:43Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    13,
                    29,
                    43,
                    3,
                    212,
                    0
                ],
                "title": "Improved Algorithms for Kernel Matrix-Vector Multiplication Under\n  Sparsity Assumptions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improved Algorithms for Kernel Matrix-Vector Multiplication Under\n  Sparsity Assumptions"
                },
                "summary": "Motivated by the problem of fast processing of attention matrices, we study\nfast algorithms for computing matrix-vector products for asymmetric Gaussian\nKernel matrices $K\\in \\mathbb{R}^{n\\times n}$. $K$'s columns are indexed by a\nset of $n$ keys $k_1,k_2\\ldots, k_n\\in \\mathbb{R}^d$, rows by a set of $n$\nqueries $q_1,q_2,\\ldots,q_n\\in \\mathbb{R}^d $, and its $i,j$ entry is $K_{ij} =\ne^{-\\|q_i-k_j\\|_2^2/2\\sigma^2}$ for some bandwidth parameter $\\sigma>0$. Given\na vector $x\\in \\mathbb{R}^n$ and error parameter $\\epsilon>0$, our task is to\noutput a $y\\in \\mathbb{R}^n$ such that $\\|Kx-y\\|_2\\leq \\epsilon \\|x\\|_2$ in\ntime subquadratic in $n$ and linear in $d$. Our algorithms rely on the\nfollowing modelling assumption about the matrices $K$: the sum of the entries\nof $K$ scales linearly in $n$, as opposed to worst case quadratic growth. We\nvalidate this assumption experimentally, for Gaussian kernel matrices\nencountered in various settings such as fast attention computation in LLMs. We\nobtain the first subquadratic-time algorithm that works under this assumption,\nfor unrestricted vectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by the problem of fast processing of attention matrices, we study\nfast algorithms for computing matrix-vector products for asymmetric Gaussian\nKernel matrices $K\\in \\mathbb{R}^{n\\times n}$. $K$'s columns are indexed by a\nset of $n$ keys $k_1,k_2\\ldots, k_n\\in \\mathbb{R}^d$, rows by a set of $n$\nqueries $q_1,q_2,\\ldots,q_n\\in \\mathbb{R}^d $, and its $i,j$ entry is $K_{ij} =\ne^{-\\|q_i-k_j\\|_2^2/2\\sigma^2}$ for some bandwidth parameter $\\sigma>0$. Given\na vector $x\\in \\mathbb{R}^n$ and error parameter $\\epsilon>0$, our task is to\noutput a $y\\in \\mathbb{R}^n$ such that $\\|Kx-y\\|_2\\leq \\epsilon \\|x\\|_2$ in\ntime subquadratic in $n$ and linear in $d$. Our algorithms rely on the\nfollowing modelling assumption about the matrices $K$: the sum of the entries\nof $K$ scales linearly in $n$, as opposed to worst case quadratic growth. We\nvalidate this assumption experimentally, for Gaussian kernel matrices\nencountered in various settings such as fast attention computation in LLMs. We\nobtain the first subquadratic-time algorithm that works under this assumption,\nfor unrestricted vectors."
                },
                "authors": [
                    {
                        "name": "Piotr Indyk"
                    },
                    {
                        "name": "Michael Kapralov"
                    },
                    {
                        "name": "Kshiteej Sheth"
                    },
                    {
                        "name": "Tal Wagner"
                    }
                ],
                "author_detail": {
                    "name": "Tal Wagner"
                },
                "author": "Tal Wagner",
                "arxiv_comment": "Published in ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23536v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23536v1",
                "updated": "2025-07-31T13:23:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    13,
                    23,
                    21,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T13:23:21Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    13,
                    23,
                    21,
                    3,
                    212,
                    0
                ],
                "title": "From LLMs to Edge: Parameter-Efficient Fine-Tuning on Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From LLMs to Edge: Parameter-Efficient Fine-Tuning on Edge Devices"
                },
                "summary": "Parameter-efficient fine-tuning (PEFT) methods reduce the computational costs\nof updating deep learning models by minimizing the number of additional\nparameters used to adapt a model to a down- stream task. While extensively\nresearched in large language models (LLMs), their application to smaller models\nused on edge devices, such as convolutional neural networks, remains\nunderexplored. This paper benchmarks and analyzes popular PEFT methods on\nconvolutional architectures typically deployed in resource-constrained edge\nenvironments. We evaluate LoRA, DoRA, and GaLore for updating standard and\ndepthwise convolutional architectures to handle distribution shifts and\naccommodate unseen classes. We utilize recently proposed PyTorch profilers to\ncompare the updated model performance and computational costs of these PEFT\nmethods with traditional fine-tuning approaches. With resource efficiency in\nmind, we investigate their update behavior across different rank dimensions. We\nfind that the evaluated PEFT methods are only half as memory-efficient when\napplied to depthwise-separable convolution architectures, compared to their\nefficiency with LLMs. Conversely, when targeting convolu- tional architectures\noptimized for edge deployment, adapter-based PEFT methods can reduce floating\npoint operations (FLOPs) during model updates by up to 95%. These insights\noffer valuable guidance for selecting PEFT methods based on hardware\nconstraints, performance requirements, and application needs. Our code is\nonline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-efficient fine-tuning (PEFT) methods reduce the computational costs\nof updating deep learning models by minimizing the number of additional\nparameters used to adapt a model to a down- stream task. While extensively\nresearched in large language models (LLMs), their application to smaller models\nused on edge devices, such as convolutional neural networks, remains\nunderexplored. This paper benchmarks and analyzes popular PEFT methods on\nconvolutional architectures typically deployed in resource-constrained edge\nenvironments. We evaluate LoRA, DoRA, and GaLore for updating standard and\ndepthwise convolutional architectures to handle distribution shifts and\naccommodate unseen classes. We utilize recently proposed PyTorch profilers to\ncompare the updated model performance and computational costs of these PEFT\nmethods with traditional fine-tuning approaches. With resource efficiency in\nmind, we investigate their update behavior across different rank dimensions. We\nfind that the evaluated PEFT methods are only half as memory-efficient when\napplied to depthwise-separable convolution architectures, compared to their\nefficiency with LLMs. Conversely, when targeting convolu- tional architectures\noptimized for edge deployment, adapter-based PEFT methods can reduce floating\npoint operations (FLOPs) during model updates by up to 95%. These insights\noffer valuable guidance for selecting PEFT methods based on hardware\nconstraints, performance requirements, and application needs. Our code is\nonline."
                },
                "authors": [
                    {
                        "name": "Georg Slamanig"
                    },
                    {
                        "name": "Francesco Corti"
                    },
                    {
                        "name": "Olga Saukh"
                    }
                ],
                "author_detail": {
                    "name": "Olga Saukh"
                },
                "author": "Olga Saukh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23536v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23536v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15621v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15621v2",
                "updated": "2025-07-31T12:41:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    12,
                    41,
                    25,
                    3,
                    212,
                    0
                ],
                "published": "2025-03-19T18:10:12Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    18,
                    10,
                    12,
                    2,
                    78,
                    0
                ],
                "title": "LLaVA-MORE: A Comparative Study of LLMs and Visual Backbones for\n  Enhanced Visual Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaVA-MORE: A Comparative Study of LLMs and Visual Backbones for\n  Enhanced Visual Instruction Tuning"
                },
                "summary": "Recent progress in Multimodal Large Language Models (MLLMs) has highlighted\nthe critical roles of both the visual backbone and the underlying language\nmodel. While prior work has primarily focused on scaling these components to\nbillions of parameters, the trade-offs between model size, architecture, and\nperformance remain underexplored. Additionally, inconsistencies in training\ndata and evaluation protocols have hindered direct comparisons, making it\ndifficult to derive optimal design choices. In this paper, we introduce\nLLaVA-MORE, a new family of MLLMs that integrates recent language models with\ndiverse visual backbones. To ensure fair comparisons, we employ a unified\ntraining protocol applied consistently across all architectures. Our analysis\nsystematically explores both small- and medium-scale LLMs -- including Phi-4,\nLLaMA-3.1, and Gemma-2 -- to evaluate multimodal reasoning, generation, and\ninstruction following, while examining the relationship between model size and\nperformance. Beyond evaluating the LLM impact on final results, we conduct a\ncomprehensive study of various visual encoders, ranging from CLIP-based\narchitectures to alternatives such as DINOv2, SigLIP, and SigLIP2. Additional\nexperiments investigate the effects of increased image resolution and\nvariations in pre-training datasets. Overall, our results provide insights into\nthe design of more effective MLLMs, offering a reproducible evaluation\nframework that facilitates direct comparisons and can guide future model\ndevelopment. Our source code and trained models are publicly available at:\nhttps://github.com/aimagelab/LLaVA-MORE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in Multimodal Large Language Models (MLLMs) has highlighted\nthe critical roles of both the visual backbone and the underlying language\nmodel. While prior work has primarily focused on scaling these components to\nbillions of parameters, the trade-offs between model size, architecture, and\nperformance remain underexplored. Additionally, inconsistencies in training\ndata and evaluation protocols have hindered direct comparisons, making it\ndifficult to derive optimal design choices. In this paper, we introduce\nLLaVA-MORE, a new family of MLLMs that integrates recent language models with\ndiverse visual backbones. To ensure fair comparisons, we employ a unified\ntraining protocol applied consistently across all architectures. Our analysis\nsystematically explores both small- and medium-scale LLMs -- including Phi-4,\nLLaMA-3.1, and Gemma-2 -- to evaluate multimodal reasoning, generation, and\ninstruction following, while examining the relationship between model size and\nperformance. Beyond evaluating the LLM impact on final results, we conduct a\ncomprehensive study of various visual encoders, ranging from CLIP-based\narchitectures to alternatives such as DINOv2, SigLIP, and SigLIP2. Additional\nexperiments investigate the effects of increased image resolution and\nvariations in pre-training datasets. Overall, our results provide insights into\nthe design of more effective MLLMs, offering a reproducible evaluation\nframework that facilitates direct comparisons and can guide future model\ndevelopment. Our source code and trained models are publicly available at:\nhttps://github.com/aimagelab/LLaVA-MORE."
                },
                "authors": [
                    {
                        "name": "Federico Cocchi"
                    },
                    {
                        "name": "Nicholas Moratelli"
                    },
                    {
                        "name": "Davide Caffagni"
                    },
                    {
                        "name": "Sara Sarto"
                    },
                    {
                        "name": "Lorenzo Baraldi"
                    },
                    {
                        "name": "Marcella Cornia"
                    },
                    {
                        "name": "Rita Cucchiara"
                    }
                ],
                "author_detail": {
                    "name": "Rita Cucchiara"
                },
                "author": "Rita Cucchiara",
                "arxiv_comment": "ICCV 2025 Workshop on What is Next in Multimodal Foundation Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15621v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15621v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01694v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01694v2",
                "updated": "2025-07-31T12:30:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    12,
                    30,
                    18,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-02T13:20:52Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    13,
                    20,
                    52,
                    2,
                    183,
                    0
                ],
                "title": "Graph Representation-based Model Poisoning on Federated Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Representation-based Model Poisoning on Federated Large Language\n  Models"
                },
                "summary": "Federated large language models (FedLLMs) enable powerful generative\ncapabilities within wireless networks while preserving data privacy.\nNonetheless, FedLLMs remain vulnerable to model poisoning attacks. This article\nfirst reviews recent advancements in model poisoning techniques and existing\ndefense mechanisms for FedLLMs, underscoring critical limitations, especially\nwhen dealing with non-IID textual data distributions. Current defense\nstrategies predominantly employ distance or similarity-based outlier detection\nmechanisms, relying on the assumption that malicious updates markedly differ\nfrom benign statistical patterns. However, this assumption becomes inadequate\nagainst adaptive adversaries targeting billion-parameter LLMs. The article\nfurther investigates graph representation-based model poisoning (GRMP), an\nemerging attack paradigm that exploits higher-order correlations among benign\nclient gradients to craft malicious updates indistinguishable from legitimate\nones. GRMP can effectively circumvent advanced defense systems, causing\nsubstantial degradation in model accuracy and overall performance. Moreover,\nthe article outlines a forward-looking research roadmap that emphasizes the\nnecessity of graph-aware secure aggregation methods, specialized vulnerability\nmetrics tailored for FedLLMs, and evaluation frameworks to enhance the\nrobustness of federated language model deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated large language models (FedLLMs) enable powerful generative\ncapabilities within wireless networks while preserving data privacy.\nNonetheless, FedLLMs remain vulnerable to model poisoning attacks. This article\nfirst reviews recent advancements in model poisoning techniques and existing\ndefense mechanisms for FedLLMs, underscoring critical limitations, especially\nwhen dealing with non-IID textual data distributions. Current defense\nstrategies predominantly employ distance or similarity-based outlier detection\nmechanisms, relying on the assumption that malicious updates markedly differ\nfrom benign statistical patterns. However, this assumption becomes inadequate\nagainst adaptive adversaries targeting billion-parameter LLMs. The article\nfurther investigates graph representation-based model poisoning (GRMP), an\nemerging attack paradigm that exploits higher-order correlations among benign\nclient gradients to craft malicious updates indistinguishable from legitimate\nones. GRMP can effectively circumvent advanced defense systems, causing\nsubstantial degradation in model accuracy and overall performance. Moreover,\nthe article outlines a forward-looking research roadmap that emphasizes the\nnecessity of graph-aware secure aggregation methods, specialized vulnerability\nmetrics tailored for FedLLMs, and evaluation frameworks to enhance the\nrobustness of federated language model deployments."
                },
                "authors": [
                    {
                        "name": "Hanlin Cai"
                    },
                    {
                        "name": "Haofan Dong"
                    },
                    {
                        "name": "Houtianfu Wang"
                    },
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Ozgur B. Akan"
                    }
                ],
                "author_detail": {
                    "name": "Ozgur B. Akan"
                },
                "author": "Ozgur B. Akan",
                "arxiv_comment": "7 pages, 5 figures (Submitted to IEEE Communication Magazine)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01694v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01694v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23486v1",
                "updated": "2025-07-31T12:10:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    12,
                    10,
                    0,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T12:10:00Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    12,
                    10,
                    0,
                    3,
                    212,
                    0
                ],
                "title": "A Novel Evaluation Benchmark for Medical LLMs: Illuminating Safety and\n  Effectiveness in Clinical Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Evaluation Benchmark for Medical LLMs: Illuminating Safety and\n  Effectiveness in Clinical Domains"
                },
                "summary": "Large language models (LLMs) hold promise in clinical decision support but\nface major challenges in safety evaluation and effectiveness validation. We\ndeveloped the Clinical Safety-Effectiveness Dual-Track Benchmark (CSEDB), a\nmultidimensional framework built on clinical expert consensus, encompassing 30\ncriteria covering critical areas like critical illness recognition, guideline\nadherence, and medication safety, with weighted consequence measures.\nThirty-two specialist physicians developed and reviewed 2,069 open-ended Q&A\nitems aligned with these criteria, spanning 26 clinical departments to simulate\nreal-world scenarios. Benchmark testing of six LLMs revealed moderate overall\nperformance (average total score 57.2%, safety 54.7%, effectiveness 62.3%),\nwith a significant 13.3% performance drop in high-risk scenarios (p < 0.0001).\nDomain-specific medical LLMs showed consistent performance advantages over\ngeneral-purpose models, with relatively higher top scores in safety (0.912) and\neffectiveness (0.861). The findings of this study not only provide a\nstandardized metric for evaluating the clinical application of medical LLMs,\nfacilitating comparative analyses, risk exposure identification, and\nimprovement directions across different scenarios, but also hold the potential\nto promote safer and more effective deployment of large language models in\nhealthcare environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) hold promise in clinical decision support but\nface major challenges in safety evaluation and effectiveness validation. We\ndeveloped the Clinical Safety-Effectiveness Dual-Track Benchmark (CSEDB), a\nmultidimensional framework built on clinical expert consensus, encompassing 30\ncriteria covering critical areas like critical illness recognition, guideline\nadherence, and medication safety, with weighted consequence measures.\nThirty-two specialist physicians developed and reviewed 2,069 open-ended Q&A\nitems aligned with these criteria, spanning 26 clinical departments to simulate\nreal-world scenarios. Benchmark testing of six LLMs revealed moderate overall\nperformance (average total score 57.2%, safety 54.7%, effectiveness 62.3%),\nwith a significant 13.3% performance drop in high-risk scenarios (p < 0.0001).\nDomain-specific medical LLMs showed consistent performance advantages over\ngeneral-purpose models, with relatively higher top scores in safety (0.912) and\neffectiveness (0.861). The findings of this study not only provide a\nstandardized metric for evaluating the clinical application of medical LLMs,\nfacilitating comparative analyses, risk exposure identification, and\nimprovement directions across different scenarios, but also hold the potential\nto promote safer and more effective deployment of large language models in\nhealthcare environments."
                },
                "authors": [
                    {
                        "name": "Shirui Wang"
                    },
                    {
                        "name": "Zhihui Tang"
                    },
                    {
                        "name": "Huaxia Yang"
                    },
                    {
                        "name": "Qiuhong Gong"
                    },
                    {
                        "name": "Tiantian Gu"
                    },
                    {
                        "name": "Hongyang Ma"
                    },
                    {
                        "name": "Yongxin Wang"
                    },
                    {
                        "name": "Wubin Sun"
                    },
                    {
                        "name": "Zeliang Lian"
                    },
                    {
                        "name": "Kehang Mao"
                    },
                    {
                        "name": "Yinan Jiang"
                    },
                    {
                        "name": "Zhicheng Huang"
                    },
                    {
                        "name": "Lingyun Ma"
                    },
                    {
                        "name": "Wenjie Shen"
                    },
                    {
                        "name": "Yajie Ji"
                    },
                    {
                        "name": "Yunhui Tan"
                    },
                    {
                        "name": "Chunbo Wang"
                    },
                    {
                        "name": "Yunlu Gao"
                    },
                    {
                        "name": "Qianling Ye"
                    },
                    {
                        "name": "Rui Lin"
                    },
                    {
                        "name": "Mingyu Chen"
                    },
                    {
                        "name": "Lijuan Niu"
                    },
                    {
                        "name": "Zhihao Wang"
                    },
                    {
                        "name": "Peng Yu"
                    },
                    {
                        "name": "Mengran Lang"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Huimin Zhang"
                    },
                    {
                        "name": "Haitao Shen"
                    },
                    {
                        "name": "Long Chen"
                    },
                    {
                        "name": "Qiguang Zhao"
                    },
                    {
                        "name": "Si-Xuan Liu"
                    },
                    {
                        "name": "Lina Zhou"
                    },
                    {
                        "name": "Hua Gao"
                    },
                    {
                        "name": "Dongqiang Ye"
                    },
                    {
                        "name": "Lingmin Meng"
                    },
                    {
                        "name": "Youtao Yu"
                    },
                    {
                        "name": "Naixin Liang"
                    },
                    {
                        "name": "Jianxiong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Jianxiong Wu"
                },
                "author": "Jianxiong Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23470v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23470v1",
                "updated": "2025-07-31T11:49:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    11,
                    49,
                    1,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T11:49:01Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    11,
                    49,
                    1,
                    3,
                    212,
                    0
                ],
                "title": "Automated Feedback on Student-Generated UML and ER Diagrams Using Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Feedback on Student-Generated UML and ER Diagrams Using Large\n  Language Models"
                },
                "summary": "UML and ER diagrams are foundational in computer science education but come\nwith challenges for learners due to the need for abstract thinking, contextual\nunderstanding, and mastery of both syntax and semantics. These complexities are\ndifficult to address through traditional teaching methods, which often struggle\nto provide scalable, personalized feedback, especially in large classes. We\nintroduce DUET (Diagrammatic UML & ER Tutor), a prototype of an LLM-based tool,\nwhich converts a reference diagram and a student-submitted diagram into a\ntextual representation and provides structured feedback based on the\ndifferences. It uses a multi-stage LLM pipeline to compare diagrams and\ngenerate reflective feedback. Furthermore, the tool enables analytical insights\nfor educators, aiming to foster self-directed learning and inform instructional\nstrategies. We evaluated DUET through semi-structured interviews with six\nparticipants, including two educators and four teaching assistants. They\nidentified strengths such as accessibility, scalability, and learning support\nalongside limitations, including reliability and potential misuse. Participants\nalso suggested potential improvements, such as bulk upload functionality and\ninteractive clarification features. DUET presents a promising direction for\nintegrating LLMs into modeling education and offers a foundation for future\nclassroom integration and empirical evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UML and ER diagrams are foundational in computer science education but come\nwith challenges for learners due to the need for abstract thinking, contextual\nunderstanding, and mastery of both syntax and semantics. These complexities are\ndifficult to address through traditional teaching methods, which often struggle\nto provide scalable, personalized feedback, especially in large classes. We\nintroduce DUET (Diagrammatic UML & ER Tutor), a prototype of an LLM-based tool,\nwhich converts a reference diagram and a student-submitted diagram into a\ntextual representation and provides structured feedback based on the\ndifferences. It uses a multi-stage LLM pipeline to compare diagrams and\ngenerate reflective feedback. Furthermore, the tool enables analytical insights\nfor educators, aiming to foster self-directed learning and inform instructional\nstrategies. We evaluated DUET through semi-structured interviews with six\nparticipants, including two educators and four teaching assistants. They\nidentified strengths such as accessibility, scalability, and learning support\nalongside limitations, including reliability and potential misuse. Participants\nalso suggested potential improvements, such as bulk upload functionality and\ninteractive clarification features. DUET presents a promising direction for\nintegrating LLMs into modeling education and offers a foundation for future\nclassroom integration and empirical evaluation."
                },
                "authors": [
                    {
                        "name": "Sebastian Grtl"
                    },
                    {
                        "name": "Gloria Schimetta"
                    },
                    {
                        "name": "David Kerschbaumer"
                    },
                    {
                        "name": "Michael Liut"
                    },
                    {
                        "name": "Alexander Steinmaurer"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Steinmaurer"
                },
                "author": "Alexander Steinmaurer",
                "arxiv_comment": "Learnersourcing: Student-generated Content @ Scale Workshop at L@S\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23470v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23470v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06254v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06254v2",
                "updated": "2025-07-31T11:48:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    11,
                    48,
                    39,
                    3,
                    212,
                    0
                ],
                "published": "2024-11-09T19:03:56Z",
                "published_parsed": [
                    2024,
                    11,
                    9,
                    19,
                    3,
                    56,
                    5,
                    314,
                    0
                ],
                "title": "KeyB2: Selecting Key Blocks is Also Important for Long Document Ranking\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KeyB2: Selecting Key Blocks is Also Important for Long Document Ranking\n  with Large Language Models"
                },
                "summary": "The emergence of large language models (LLMs) such as Llama has significantly\nadvanced neural information retrieval (IR). However, applying LLMs to long\ndocument reranking remains computationally expensive and may be ineffective.\nMoreover, the internal behavior of LLMs during document relevance judgment is\nstill underexplored. In this paper, we begin with an in-depth analysis of\ndecoder-only LLM attention patterns and find that several attention heads\nconsistently align with relevance signals, yet this alignment deteriorates as\nirrelevant content increases. Motivated by this observation, we revisit and\nextend the block selection paradigm, introducing KeyB2, a scalable reranking\nframework that combines block pre-selection with powerful decoder-only LLMs.\nKeyB2 generalizes the selection stage to support BM25, cross-encoder, and\nbi-encoder, and adapts LLM to compute fine-grained relevance scores. We further\nintroduce a new bi-encoder strategy that performs strongly and efficiently.\nExtensive experiments on TREC DL 2019/2023 document task, Robust04, and MLDR-zh\ndemonstrate that KeyB2 outperforms baselines including RankLLaMA,\nRankLLaMA-MaxP/AvgP, and KeyB, achieving new state-of-the-art (SOTA) results on\nTREC DL 2019 document reranking task. In addition, KeyB2 reduces reranking\nlatency compared with RankLLaMA by over 83% and memory usage by over 74%,\npositioning it as a practical and effective solution for long document ranking\nwith LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models (LLMs) such as Llama has significantly\nadvanced neural information retrieval (IR). However, applying LLMs to long\ndocument reranking remains computationally expensive and may be ineffective.\nMoreover, the internal behavior of LLMs during document relevance judgment is\nstill underexplored. In this paper, we begin with an in-depth analysis of\ndecoder-only LLM attention patterns and find that several attention heads\nconsistently align with relevance signals, yet this alignment deteriorates as\nirrelevant content increases. Motivated by this observation, we revisit and\nextend the block selection paradigm, introducing KeyB2, a scalable reranking\nframework that combines block pre-selection with powerful decoder-only LLMs.\nKeyB2 generalizes the selection stage to support BM25, cross-encoder, and\nbi-encoder, and adapts LLM to compute fine-grained relevance scores. We further\nintroduce a new bi-encoder strategy that performs strongly and efficiently.\nExtensive experiments on TREC DL 2019/2023 document task, Robust04, and MLDR-zh\ndemonstrate that KeyB2 outperforms baselines including RankLLaMA,\nRankLLaMA-MaxP/AvgP, and KeyB, achieving new state-of-the-art (SOTA) results on\nTREC DL 2019 document reranking task. In addition, KeyB2 reduces reranking\nlatency compared with RankLLaMA by over 83% and memory usage by over 74%,\npositioning it as a practical and effective solution for long document ranking\nwith LLMs."
                },
                "authors": [
                    {
                        "name": "Minghan Li"
                    },
                    {
                        "name": "Eric Gaussier"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Guodong Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guodong Zhou"
                },
                "author": "Guodong Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06254v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06254v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23465v1",
                "updated": "2025-07-31T11:41:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    11,
                    41,
                    4,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T11:41:04Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    11,
                    41,
                    4,
                    3,
                    212,
                    0
                ],
                "title": "Role-Aware Language Models for Secure and Contextualized Access Control\n  in Organizations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role-Aware Language Models for Secure and Contextualized Access Control\n  in Organizations"
                },
                "summary": "As large language models (LLMs) are increasingly deployed in enterprise\nsettings, controlling model behavior based on user roles becomes an essential\nrequirement. Existing safety methods typically assume uniform access and focus\non preventing harmful or toxic outputs, without addressing role-specific access\nconstraints. In this work, we investigate whether LLMs can be fine-tuned to\ngenerate responses that reflect the access privileges associated with different\norganizational roles. We explore three modeling strategies: a BERT-based\nclassifier, an LLM-based classifier, and role-conditioned generation. To\nevaluate these approaches, we construct two complementary datasets. The first\nis adapted from existing instruction-tuning corpora through clustering and role\nlabeling, while the second is synthetically generated to reflect realistic,\nrole-sensitive enterprise scenarios. We assess model performance across varying\norganizational structures and analyze robustness to prompt injection, role\nmismatch, and jailbreak attempts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are increasingly deployed in enterprise\nsettings, controlling model behavior based on user roles becomes an essential\nrequirement. Existing safety methods typically assume uniform access and focus\non preventing harmful or toxic outputs, without addressing role-specific access\nconstraints. In this work, we investigate whether LLMs can be fine-tuned to\ngenerate responses that reflect the access privileges associated with different\norganizational roles. We explore three modeling strategies: a BERT-based\nclassifier, an LLM-based classifier, and role-conditioned generation. To\nevaluate these approaches, we construct two complementary datasets. The first\nis adapted from existing instruction-tuning corpora through clustering and role\nlabeling, while the second is synthetically generated to reflect realistic,\nrole-sensitive enterprise scenarios. We assess model performance across varying\norganizational structures and analyze robustness to prompt injection, role\nmismatch, and jailbreak attempts."
                },
                "authors": [
                    {
                        "name": "Saeed Almheiri"
                    },
                    {
                        "name": "Yerulan Kongrat"
                    },
                    {
                        "name": "Adrian Santosh"
                    },
                    {
                        "name": "Ruslan Tasmukhanov"
                    },
                    {
                        "name": "Josemaria Vera"
                    },
                    {
                        "name": "Muhammad Dehan Al Kautsar"
                    },
                    {
                        "name": "Fajri Koto"
                    }
                ],
                "author_detail": {
                    "name": "Fajri Koto"
                },
                "author": "Fajri Koto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23453v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23453v1",
                "updated": "2025-07-31T11:29:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    11,
                    29,
                    42,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T11:29:42Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    11,
                    29,
                    42,
                    3,
                    212,
                    0
                ],
                "title": "Counterfactual Evaluation for Blind Attack Detection in LLM-based\n  Evaluation Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterfactual Evaluation for Blind Attack Detection in LLM-based\n  Evaluation Systems"
                },
                "summary": "This paper investigates defenses for LLM-based evaluation systems against\nprompt injection. We formalize a class of threats called blind attacks, where a\ncandidate answer is crafted independently of the true answer to deceive the\nevaluator. To counter such attacks, we propose a framework that augments\nStandard Evaluation (SE) with Counterfactual Evaluation (CFE), which\nre-evaluates the submission against a deliberately false ground-truth answer.\nAn attack is detected if the system validates an answer under both standard and\ncounterfactual conditions. Experiments show that while standard evaluation is\nhighly vulnerable, our SE+CFE framework significantly improves security by\nboosting attack detection with minimal performance trade-offs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates defenses for LLM-based evaluation systems against\nprompt injection. We formalize a class of threats called blind attacks, where a\ncandidate answer is crafted independently of the true answer to deceive the\nevaluator. To counter such attacks, we propose a framework that augments\nStandard Evaluation (SE) with Counterfactual Evaluation (CFE), which\nre-evaluates the submission against a deliberately false ground-truth answer.\nAn attack is detected if the system validates an answer under both standard and\ncounterfactual conditions. Experiments show that while standard evaluation is\nhighly vulnerable, our SE+CFE framework significantly improves security by\nboosting attack detection with minimal performance trade-offs."
                },
                "authors": [
                    {
                        "name": "Lijia Liu"
                    },
                    {
                        "name": "Takumi Kondo"
                    },
                    {
                        "name": "Kyohei Atarashi"
                    },
                    {
                        "name": "Koh Takeuchi"
                    },
                    {
                        "name": "Jiyi Li"
                    },
                    {
                        "name": "Shigeru Saito"
                    },
                    {
                        "name": "Hisashi Kashima"
                    }
                ],
                "author_detail": {
                    "name": "Hisashi Kashima"
                },
                "author": "Hisashi Kashima",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23453v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23453v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23440v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23440v1",
                "updated": "2025-07-31T11:18:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    11,
                    18,
                    42,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T11:18:42Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    11,
                    18,
                    42,
                    3,
                    212,
                    0
                ],
                "title": "Self-Foveate: Enhancing Diversity and Difficulty of Synthesized\n  Instructions from Unsupervised Text via Multi-Level Foveation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Foveate: Enhancing Diversity and Difficulty of Synthesized\n  Instructions from Unsupervised Text via Multi-Level Foveation"
                },
                "summary": "Large language models (LLMs) with instruction following capabilities have\ndemonstrated impressive problem-solving abilities. While synthesizing\ninstructional data from unsupervised text has become a common approach for\ntraining such models, conventional methods rely heavily on human effort for\ndata annotation. Although existing automated synthesis paradigms have\nalleviated this constraint, they still exhibit significant limitations in\nensuring adequate diversity and difficulty of synthesized instructions. To\naddress these challenges, we propose Self-Foveate, an innovative LLM-driven\nmethod for instruction synthesis. This approach introduces a\n\"Micro-Scatter-Macro\" multi-level foveation methodology that effectively guides\nthe LLM to deeply excavate fine-grained information embedded in unsupervised\ntext, thereby enhancing both the diversity and difficulty of synthesized\ninstructions. Comprehensive experiments across multiple unsupervised corpora\nand diverse model architectures validate the effectiveness and superiority of\nour proposed method. We publicly release our data and codes:\nhttps://github.com/Mubuky/Self-Foveate",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with instruction following capabilities have\ndemonstrated impressive problem-solving abilities. While synthesizing\ninstructional data from unsupervised text has become a common approach for\ntraining such models, conventional methods rely heavily on human effort for\ndata annotation. Although existing automated synthesis paradigms have\nalleviated this constraint, they still exhibit significant limitations in\nensuring adequate diversity and difficulty of synthesized instructions. To\naddress these challenges, we propose Self-Foveate, an innovative LLM-driven\nmethod for instruction synthesis. This approach introduces a\n\"Micro-Scatter-Macro\" multi-level foveation methodology that effectively guides\nthe LLM to deeply excavate fine-grained information embedded in unsupervised\ntext, thereby enhancing both the diversity and difficulty of synthesized\ninstructions. Comprehensive experiments across multiple unsupervised corpora\nand diverse model architectures validate the effectiveness and superiority of\nour proposed method. We publicly release our data and codes:\nhttps://github.com/Mubuky/Self-Foveate"
                },
                "authors": [
                    {
                        "name": "Mingzhe Li"
                    },
                    {
                        "name": "Xin Lu"
                    },
                    {
                        "name": "Yanyan Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yanyan Zhao"
                },
                "author": "Yanyan Zhao",
                "arxiv_comment": "Accepted by Findings of ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23440v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23440v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14928v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14928v3",
                "updated": "2025-07-31T11:11:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    11,
                    11,
                    30,
                    3,
                    212,
                    0
                ],
                "published": "2025-04-21T07:48:20Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    7,
                    48,
                    20,
                    0,
                    111,
                    0
                ],
                "title": "EducationQ: Evaluating LLMs' Teaching Capabilities Through Multi-Agent\n  Dialogue Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EducationQ: Evaluating LLMs' Teaching Capabilities Through Multi-Agent\n  Dialogue Framework"
                },
                "summary": "Large language models (LLMs) increasingly serve as educational tools, yet\nevaluating their teaching capabilities remains challenging due to the\nresource-intensive, context-dependent, and methodologically complex nature of\nteacher-student interactions. We introduce EducationQ, a multi-agent dialogue\nframework that efficiently assesses teaching capabilities through simulated\ndynamic educational scenarios, featuring specialized agents for teaching,\nlearning, and evaluation. Testing 14 LLMs across major AI Organizations\n(OpenAI, Meta, Google, Anthropic, and others) on 1,498 questions spanning 13\ndisciplines and 10 difficulty levels reveals that teaching effectiveness does\nnot correlate linearly with model scale or general reasoning capabilities -\nwith some smaller open-source models outperforming larger commercial\ncounterparts in teaching contexts. This finding highlights a critical gap in\ncurrent evaluations that prioritize knowledge recall over interactive pedagogy.\nOur mixed-methods evaluation, combining quantitative metrics with qualitative\nanalysis and expert case studies, identifies distinct pedagogical strengths\nemployed by top-performing models (e.g., sophisticated questioning strategies,\nadaptive feedback mechanisms). Human expert evaluations show 78% agreement with\nour automated qualitative analysis of effective teaching behaviors, validating\nour methodology. EducationQ demonstrates that LLMs-as-teachers require\nspecialized optimization beyond simple scaling, suggesting next-generation\neducational AI prioritize targeted enhancement of specific pedagogical\neffectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) increasingly serve as educational tools, yet\nevaluating their teaching capabilities remains challenging due to the\nresource-intensive, context-dependent, and methodologically complex nature of\nteacher-student interactions. We introduce EducationQ, a multi-agent dialogue\nframework that efficiently assesses teaching capabilities through simulated\ndynamic educational scenarios, featuring specialized agents for teaching,\nlearning, and evaluation. Testing 14 LLMs across major AI Organizations\n(OpenAI, Meta, Google, Anthropic, and others) on 1,498 questions spanning 13\ndisciplines and 10 difficulty levels reveals that teaching effectiveness does\nnot correlate linearly with model scale or general reasoning capabilities -\nwith some smaller open-source models outperforming larger commercial\ncounterparts in teaching contexts. This finding highlights a critical gap in\ncurrent evaluations that prioritize knowledge recall over interactive pedagogy.\nOur mixed-methods evaluation, combining quantitative metrics with qualitative\nanalysis and expert case studies, identifies distinct pedagogical strengths\nemployed by top-performing models (e.g., sophisticated questioning strategies,\nadaptive feedback mechanisms). Human expert evaluations show 78% agreement with\nour automated qualitative analysis of effective teaching behaviors, validating\nour methodology. EducationQ demonstrates that LLMs-as-teachers require\nspecialized optimization beyond simple scaling, suggesting next-generation\neducational AI prioritize targeted enhancement of specific pedagogical\neffectiveness."
                },
                "authors": [
                    {
                        "name": "Yao Shi"
                    },
                    {
                        "name": "Rongkeng Liang"
                    },
                    {
                        "name": "Yong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Xu"
                },
                "author": "Yong Xu",
                "arxiv_doi": "10.18653/v1/2025.acl-long.1576",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2025.acl-long.1576",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.14928v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14928v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Paper URL: https://aclanthology.org/2025.acl-long.1576 ;Presentation\n  Video: https://www.youtube.com/watch?v=j63ooKE50I0",
                "arxiv_journal_ref": "Proceedings of the 63rd Annual Meeting of the Association for\n  Computational Linguistics (Volume 1: Long Papers) (2025) 32799-32828",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23429v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23429v1",
                "updated": "2025-07-31T11:09:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    11,
                    9,
                    50,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T11:09:50Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    11,
                    9,
                    50,
                    3,
                    212,
                    0
                ],
                "title": "Chatting with your ERP: A Recipe",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chatting with your ERP: A Recipe"
                },
                "summary": "This paper presents the design, implementation, and evaluation behind a Large\nLanguage Model (LLM) agent that chats with an industrial production-grade ERP\nsystem. The agent is capable of interpreting natural language queries and\ntranslating them into executable SQL statements, leveraging open-weight LLMs. A\nnovel dual-agent architecture combining reasoning and critique stages was\nproposed to improve query generation reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the design, implementation, and evaluation behind a Large\nLanguage Model (LLM) agent that chats with an industrial production-grade ERP\nsystem. The agent is capable of interpreting natural language queries and\ntranslating them into executable SQL statements, leveraging open-weight LLMs. A\nnovel dual-agent architecture combining reasoning and critique stages was\nproposed to improve query generation reliability."
                },
                "authors": [
                    {
                        "name": "Jorge Ruiz Gmez"
                    },
                    {
                        "name": "Lidia Andrs Susinos"
                    },
                    {
                        "name": "Jorge Alamo Oliv"
                    },
                    {
                        "name": "Sonia Rey Osorno"
                    },
                    {
                        "name": "Manuel Luis Gonzalez Hernndez"
                    }
                ],
                "author_detail": {
                    "name": "Manuel Luis Gonzalez Hernndez"
                },
                "author": "Manuel Luis Gonzalez Hernndez",
                "arxiv_comment": "11 pages, includes 3 tables summarizing schema and model performance.\n  Submitted on July 31, 2025. Targets integration of LLM agents with ERP\n  systems using open-weight models and Ollama deployment",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23429v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 68P20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.2.5; H.2.8; H.5.m",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18497v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18497v2",
                "updated": "2025-07-31T11:02:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    11,
                    2,
                    54,
                    3,
                    212,
                    0
                ],
                "published": "2025-05-24T04:24:59Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    4,
                    24,
                    59,
                    5,
                    144,
                    0
                ],
                "title": "The Pragmatic Mind of Machines: Tracing the Emergence of Pragmatic\n  Competence in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Pragmatic Mind of Machines: Tracing the Emergence of Pragmatic\n  Competence in Large Language Models"
                },
                "summary": "Current large language models (LLMs) have demonstrated emerging capabilities\nin social intelligence tasks, including implicature resolution and\ntheory-of-mind reasoning, both of which require substantial pragmatic\nunderstanding. However, how LLMs acquire this pragmatic competence throughout\nthe training process remains poorly understood. In this work, we introduce\nALTPRAG, a dataset grounded in the pragmatic concept of alternatives, to\nevaluate whether LLMs at different training stages can accurately infer nuanced\nspeaker intentions. Each instance pairs two equally plausible yet pragmatically\ndivergent continuations and requires the model to (i) infer the speaker's\nintended meaning and (ii) explain when and why a speaker would choose one\nutterance over its alternative, thus directly probing pragmatic competence\nthrough contrastive reasoning. We systematically evaluate 22 LLMs across 3 key\ntraining stages: after pre-training, supervised fine-tuning (SFT), and\npreference optimization, to examine the development of pragmatic competence.\nOur results show that even base models exhibit notable sensitivity to pragmatic\ncues, which improves consistently with increases in model and data scale.\nAdditionally, SFT and RLHF contribute further gains, particularly in\ncognitive-pragmatic scenarios. These findings highlight pragmatic competence as\nan emergent and compositional property of LLM training and offer new insights\nfor aligning models with human communicative norms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current large language models (LLMs) have demonstrated emerging capabilities\nin social intelligence tasks, including implicature resolution and\ntheory-of-mind reasoning, both of which require substantial pragmatic\nunderstanding. However, how LLMs acquire this pragmatic competence throughout\nthe training process remains poorly understood. In this work, we introduce\nALTPRAG, a dataset grounded in the pragmatic concept of alternatives, to\nevaluate whether LLMs at different training stages can accurately infer nuanced\nspeaker intentions. Each instance pairs two equally plausible yet pragmatically\ndivergent continuations and requires the model to (i) infer the speaker's\nintended meaning and (ii) explain when and why a speaker would choose one\nutterance over its alternative, thus directly probing pragmatic competence\nthrough contrastive reasoning. We systematically evaluate 22 LLMs across 3 key\ntraining stages: after pre-training, supervised fine-tuning (SFT), and\npreference optimization, to examine the development of pragmatic competence.\nOur results show that even base models exhibit notable sensitivity to pragmatic\ncues, which improves consistently with increases in model and data scale.\nAdditionally, SFT and RLHF contribute further gains, particularly in\ncognitive-pragmatic scenarios. These findings highlight pragmatic competence as\nan emergent and compositional property of LLM training and offer new insights\nfor aligning models with human communicative norms."
                },
                "authors": [
                    {
                        "name": "Kefan Yu"
                    },
                    {
                        "name": "Qingcheng Zeng"
                    },
                    {
                        "name": "Weihao Xuan"
                    },
                    {
                        "name": "Wanxin Li"
                    },
                    {
                        "name": "Jingyi Wu"
                    },
                    {
                        "name": "Rob Voigt"
                    }
                ],
                "author_detail": {
                    "name": "Rob Voigt"
                },
                "author": "Rob Voigt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18497v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18497v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10375v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10375v2",
                "updated": "2025-07-31T10:44:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    10,
                    44,
                    12,
                    3,
                    212,
                    0
                ],
                "published": "2024-06-14T19:07:03Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    19,
                    7,
                    3,
                    4,
                    166,
                    0
                ],
                "title": "Mokav: Execution-driven Differential Testing with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mokav: Execution-driven Differential Testing with LLMs"
                },
                "summary": "It is essential to detect functional differences between programs in various\nsoftware engineering tasks, such as automated program repair, mutation testing,\nand code refactoring. The problem of detecting functional differences between\ntwo programs can be reduced to searching for a difference exposing test (DET):\na test input that results in different outputs on the subject programs. In this\npaper, we propose Mokav, a novel execution-driven tool that leverages LLMs to\ngenerate DETs. Mokav takes two versions of a program (P and Q) and an example\ntest input. When successful, Mokav generates a valid DET, a test input that\nleads to provably different outputs on P and Q. Mokav iteratively prompts an\nLLM with a specialized prompt to generate new test inputs. At each iteration,\nMokav provides execution-based feedback from previously generated tests until\nthe LLM produces a DET. We evaluate Mokav on 1535 pairs of Python programs\ncollected from the Codeforces competition platform and 32 pairs of programs\nfrom the QuixBugs dataset. Our experiments show that Mokav outperforms the\nstate-of-the-art, Pynguin and Differential Prompting, by a large margin. Mokav\ncan generate DETs for 81.7% (1,255/1535) of the program pairs in our benchmark\n(versus 4.9% for Pynguin and 37.3% for Differential Prompting). We demonstrate\nthat the iterative and execution-driven feedback components of the system\ncontribute to its high effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is essential to detect functional differences between programs in various\nsoftware engineering tasks, such as automated program repair, mutation testing,\nand code refactoring. The problem of detecting functional differences between\ntwo programs can be reduced to searching for a difference exposing test (DET):\na test input that results in different outputs on the subject programs. In this\npaper, we propose Mokav, a novel execution-driven tool that leverages LLMs to\ngenerate DETs. Mokav takes two versions of a program (P and Q) and an example\ntest input. When successful, Mokav generates a valid DET, a test input that\nleads to provably different outputs on P and Q. Mokav iteratively prompts an\nLLM with a specialized prompt to generate new test inputs. At each iteration,\nMokav provides execution-based feedback from previously generated tests until\nthe LLM produces a DET. We evaluate Mokav on 1535 pairs of Python programs\ncollected from the Codeforces competition platform and 32 pairs of programs\nfrom the QuixBugs dataset. Our experiments show that Mokav outperforms the\nstate-of-the-art, Pynguin and Differential Prompting, by a large margin. Mokav\ncan generate DETs for 81.7% (1,255/1535) of the program pairs in our benchmark\n(versus 4.9% for Pynguin and 37.3% for Differential Prompting). We demonstrate\nthat the iterative and execution-driven feedback components of the system\ncontribute to its high effectiveness."
                },
                "authors": [
                    {
                        "name": "Khashayar Etemadi"
                    },
                    {
                        "name": "Bardia Mohammadi"
                    },
                    {
                        "name": "Zhendong Su"
                    },
                    {
                        "name": "Martin Monperrus"
                    }
                ],
                "author_detail": {
                    "name": "Martin Monperrus"
                },
                "author": "Martin Monperrus",
                "arxiv_doi": "10.1016/j.jss.2025.112571",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jss.2025.112571",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.10375v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10375v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23410v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23410v1",
                "updated": "2025-07-31T10:33:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    10,
                    33,
                    47,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T10:33:47Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    10,
                    33,
                    47,
                    3,
                    212,
                    0
                ],
                "title": "Towards LLM-Enhanced Product Line Scoping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards LLM-Enhanced Product Line Scoping"
                },
                "summary": "The idea of product line scoping is to identify the set of features and\nconfigurations that a product line should include, i.e., offer for\nconfiguration purposes. In this context, a major scoping task is to find a\nbalance between commercial relevance and technical feasibility. Traditional\nproduct line scoping approaches rely on formal feature models and require a\nmanual analysis which can be quite time-consuming. In this paper, we sketch how\nLarge Language Models (LLMs) can be applied to support product line scoping\ntasks with a natural language interaction based scoping process. Using a\nworking example from the smarthome domain, we sketch how LLMs can be applied to\nevaluate different feature model alternatives. We discuss open research\nchallenges regarding the integration of LLMs with product line scoping.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The idea of product line scoping is to identify the set of features and\nconfigurations that a product line should include, i.e., offer for\nconfiguration purposes. In this context, a major scoping task is to find a\nbalance between commercial relevance and technical feasibility. Traditional\nproduct line scoping approaches rely on formal feature models and require a\nmanual analysis which can be quite time-consuming. In this paper, we sketch how\nLarge Language Models (LLMs) can be applied to support product line scoping\ntasks with a natural language interaction based scoping process. Using a\nworking example from the smarthome domain, we sketch how LLMs can be applied to\nevaluate different feature model alternatives. We discuss open research\nchallenges regarding the integration of LLMs with product line scoping."
                },
                "authors": [
                    {
                        "name": "Alexander Felfernig"
                    },
                    {
                        "name": "Damian Garber"
                    },
                    {
                        "name": "Viet-Man Le"
                    },
                    {
                        "name": "Sebastian Lubos"
                    },
                    {
                        "name": "Thi Ngoc Trang Tran"
                    }
                ],
                "author_detail": {
                    "name": "Thi Ngoc Trang Tran"
                },
                "author": "Thi Ngoc Trang Tran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23410v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23410v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06989v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06989v2",
                "updated": "2025-07-31T10:26:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    10,
                    26,
                    35,
                    3,
                    212,
                    0
                ],
                "published": "2025-03-10T07:10:38Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    7,
                    10,
                    38,
                    0,
                    69,
                    0
                ],
                "title": "Probabilistic Modeling of Jailbreak on Multimodal LLMs: From\n  Quantification to Application",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic Modeling of Jailbreak on Multimodal LLMs: From\n  Quantification to Application"
                },
                "summary": "Recently, Multimodal Large Language Models (MLLMs) have demonstrated their\nsuperior ability in understanding multimodal content. However, they remain\nvulnerable to jailbreak attacks, which exploit weaknesses in their safety\nalignment to generate harmful responses. Previous studies categorize jailbreaks\nas successful or failed based on whether responses contain malicious content.\nHowever, given the stochastic nature of MLLM responses, this binary\nclassification of an input's ability to jailbreak MLLMs is inappropriate.\nDerived from this viewpoint, we introduce jailbreak probability to quantify the\njailbreak potential of an input, which represents the likelihood that MLLMs\ngenerated a malicious response when prompted with this input. We approximate\nthis probability through multiple queries to MLLMs. After modeling the\nrelationship between input hidden states and their corresponding jailbreak\nprobability using Jailbreak Probability Prediction Network (JPPN), we use\ncontinuous jailbreak probability for optimization. Specifically, we propose\nJailbreak-Probability-based Attack (JPA) that optimizes adversarial\nperturbations on input image to maximize jailbreak probability, and further\nenhance it as Multimodal JPA (MJPA) by including monotonic text rephrasing. To\ncounteract attacks, we also propose Jailbreak-Probability-based Finetuning\n(JPF), which minimizes jailbreak probability through MLLM parameter updates.\nExtensive experiments show that (1) (M)JPA yields significant improvements when\nattacking a wide range of models under both white and black box settings. (2)\nJPF vastly reduces jailbreaks by at most over 60\\%. Both of the above results\ndemonstrate the significance of introducing jailbreak probability to make\nnuanced distinctions among input jailbreak abilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Multimodal Large Language Models (MLLMs) have demonstrated their\nsuperior ability in understanding multimodal content. However, they remain\nvulnerable to jailbreak attacks, which exploit weaknesses in their safety\nalignment to generate harmful responses. Previous studies categorize jailbreaks\nas successful or failed based on whether responses contain malicious content.\nHowever, given the stochastic nature of MLLM responses, this binary\nclassification of an input's ability to jailbreak MLLMs is inappropriate.\nDerived from this viewpoint, we introduce jailbreak probability to quantify the\njailbreak potential of an input, which represents the likelihood that MLLMs\ngenerated a malicious response when prompted with this input. We approximate\nthis probability through multiple queries to MLLMs. After modeling the\nrelationship between input hidden states and their corresponding jailbreak\nprobability using Jailbreak Probability Prediction Network (JPPN), we use\ncontinuous jailbreak probability for optimization. Specifically, we propose\nJailbreak-Probability-based Attack (JPA) that optimizes adversarial\nperturbations on input image to maximize jailbreak probability, and further\nenhance it as Multimodal JPA (MJPA) by including monotonic text rephrasing. To\ncounteract attacks, we also propose Jailbreak-Probability-based Finetuning\n(JPF), which minimizes jailbreak probability through MLLM parameter updates.\nExtensive experiments show that (1) (M)JPA yields significant improvements when\nattacking a wide range of models under both white and black box settings. (2)\nJPF vastly reduces jailbreaks by at most over 60\\%. Both of the above results\ndemonstrate the significance of introducing jailbreak probability to make\nnuanced distinctions among input jailbreak abilities."
                },
                "authors": [
                    {
                        "name": "Wenzhuo Xu"
                    },
                    {
                        "name": "Zhipeng Wei"
                    },
                    {
                        "name": "Xiongtao Sun"
                    },
                    {
                        "name": "Zonghao Ying"
                    },
                    {
                        "name": "Deyue Zhang"
                    },
                    {
                        "name": "Dongdong Yang"
                    },
                    {
                        "name": "Xiangzheng Zhang"
                    },
                    {
                        "name": "Quanchen Zou"
                    }
                ],
                "author_detail": {
                    "name": "Quanchen Zou"
                },
                "author": "Quanchen Zou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06989v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06989v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16725v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16725v2",
                "updated": "2025-07-31T10:20:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    10,
                    20,
                    56,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-22T16:08:12Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    16,
                    8,
                    12,
                    1,
                    203,
                    0
                ],
                "title": "RAVine: Reality-Aligned Evaluation for Agentic Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAVine: Reality-Aligned Evaluation for Agentic Search"
                },
                "summary": "Agentic search, as a more autonomous and adaptive paradigm of retrieval\naugmentation, is driving the evolution of intelligent search systems. However,\nexisting evaluation frameworks fail to align well with the goals of agentic\nsearch. First, the complex queries commonly used in current benchmarks often\ndeviate from realistic user search scenarios. Second, prior approaches tend to\nintroduce noise when extracting ground truth for end-to-end evaluations,\nleading to distorted assessments at a fine-grained level. Third, most current\nframeworks focus solely on the quality of final answers, neglecting the\nevaluation of the iterative process inherent to agentic search. To address\nthese limitations, we propose RAVine -- a Reality-Aligned eValuation framework\nfor agentic LLMs with search. RAVine targets multi-point queries and long-form\nanswers that better reflect user intents, and introduces an attributable ground\ntruth construction strategy to enhance the accuracy of fine-grained evaluation.\nMoreover, RAVine examines model's interaction with search tools throughout the\niterative process, and accounts for factors of efficiency. We benchmark a\nseries of models using RAVine and derive several insights, which we hope will\ncontribute to advancing the development of agentic search systems. The code and\ndatasets are available at https://github.com/SwordFaith/RAVine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic search, as a more autonomous and adaptive paradigm of retrieval\naugmentation, is driving the evolution of intelligent search systems. However,\nexisting evaluation frameworks fail to align well with the goals of agentic\nsearch. First, the complex queries commonly used in current benchmarks often\ndeviate from realistic user search scenarios. Second, prior approaches tend to\nintroduce noise when extracting ground truth for end-to-end evaluations,\nleading to distorted assessments at a fine-grained level. Third, most current\nframeworks focus solely on the quality of final answers, neglecting the\nevaluation of the iterative process inherent to agentic search. To address\nthese limitations, we propose RAVine -- a Reality-Aligned eValuation framework\nfor agentic LLMs with search. RAVine targets multi-point queries and long-form\nanswers that better reflect user intents, and introduces an attributable ground\ntruth construction strategy to enhance the accuracy of fine-grained evaluation.\nMoreover, RAVine examines model's interaction with search tools throughout the\niterative process, and accounts for factors of efficiency. We benchmark a\nseries of models using RAVine and derive several insights, which we hope will\ncontribute to advancing the development of agentic search systems. The code and\ndatasets are available at https://github.com/SwordFaith/RAVine."
                },
                "authors": [
                    {
                        "name": "Yilong Xu"
                    },
                    {
                        "name": "Xiang Long"
                    },
                    {
                        "name": "Zhi Zheng"
                    },
                    {
                        "name": "Jinhua Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jinhua Gao"
                },
                "author": "Jinhua Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16725v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16725v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23399v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23399v1",
                "updated": "2025-07-31T10:13:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    10,
                    13,
                    48,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T10:13:48Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    10,
                    13,
                    48,
                    3,
                    212,
                    0
                ],
                "title": "Beyond the Cloud: Assessing the Benefits and Drawbacks of Local LLM\n  Deployment for Translators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Cloud: Assessing the Benefits and Drawbacks of Local LLM\n  Deployment for Translators"
                },
                "summary": "The rapid proliferation of Large Language Models presents both opportunities\nand challenges for the translation field. While commercial, cloud-based AI\nchatbots have garnered significant attention in translation studies, concerns\nregarding data privacy, security, and equitable access necessitate exploration\nof alternative deployment models. This paper investigates the feasibility and\nperformance of locally deployable, free language models as a viable alternative\nto proprietary, cloud-based AI solutions. This study evaluates three\nopen-source models installed on CPU-based platforms and compared against\ncommercially available online chat-bots. The evaluation focuses on functional\nperformance rather than a comparative analysis of human-machine translation\nquality, an area already subject to extensive research. The platforms assessed\nwere chosen for their accessibility and ease of use across various operating\nsystems. While local deployment introduces its own challenges, the benefits of\nenhanced data control, improved privacy, and reduced dependency on cloud\nservices are compelling. The findings of this study contribute to a growing\nbody of knowledge concerning the democratization of AI technology and inform\nfuture research and development efforts aimed at making LLMs more accessible\nand practical for a wider range of users, specifically focusing on the needs of\nindividual translators and small businesses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid proliferation of Large Language Models presents both opportunities\nand challenges for the translation field. While commercial, cloud-based AI\nchatbots have garnered significant attention in translation studies, concerns\nregarding data privacy, security, and equitable access necessitate exploration\nof alternative deployment models. This paper investigates the feasibility and\nperformance of locally deployable, free language models as a viable alternative\nto proprietary, cloud-based AI solutions. This study evaluates three\nopen-source models installed on CPU-based platforms and compared against\ncommercially available online chat-bots. The evaluation focuses on functional\nperformance rather than a comparative analysis of human-machine translation\nquality, an area already subject to extensive research. The platforms assessed\nwere chosen for their accessibility and ease of use across various operating\nsystems. While local deployment introduces its own challenges, the benefits of\nenhanced data control, improved privacy, and reduced dependency on cloud\nservices are compelling. The findings of this study contribute to a growing\nbody of knowledge concerning the democratization of AI technology and inform\nfuture research and development efforts aimed at making LLMs more accessible\nand practical for a wider range of users, specifically focusing on the needs of\nindividual translators and small businesses."
                },
                "authors": [
                    {
                        "name": "Peter Sandrini"
                    }
                ],
                "author_detail": {
                    "name": "Peter Sandrini"
                },
                "author": "Peter Sandrini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23399v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23399v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; K.4.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23386v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23386v1",
                "updated": "2025-07-31T10:01:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    10,
                    1,
                    11,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T10:01:11Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    10,
                    1,
                    11,
                    3,
                    212,
                    0
                ],
                "title": "Causal2Vec: Improving Decoder-only LLMs as Versatile Embedding Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal2Vec: Improving Decoder-only LLMs as Versatile Embedding Models"
                },
                "summary": "Decoder-only large language models (LLMs) are increasingly used to build\nembedding models that effectively encode the semantic information of natural\nlanguage texts into dense vector representations for various embedding tasks.\nHowever, many existing methods primarily focus on removing the causal attention\nmask in LLMs to enable bidirectional attention, potentially undermining the\nmodel's ability to extract semantic information acquired during pretraining.\nAdditionally, leading unidirectional approaches often rely on extra input text\nto overcome the inherent limitations of causal attention, inevitably increasing\ncomputational costs. In this work, we propose Causal2Vec, a general-purpose\nembedding model tailored to enhance the performance of decoder-only LLMs\nwithout altering their original architectures or introducing significant\ncomputational overhead. Specifically, we first employ a lightweight BERT-style\nmodel to pre-encode the input text into a single Contextual token, which is\nthen prepended to the LLM's input sequence, allowing each token to capture\ncontextualized information even without attending to future tokens.\nFurthermore, to mitigate the recency bias introduced by last-token pooling and\nhelp LLMs better leverage the semantic information encoded in the Contextual\ntoken, we concatenate the last hidden states of Contextual and EOS tokens as\nthe final text embedding. In practice, Causal2Vec achieves state-of-the-art\nperformance on the Massive Text Embeddings Benchmark (MTEB) among models\ntrained solely on publicly available retrieval datasets, while reducing the\nrequired sequence length by up to 85% and inference time by up to 82% compared\nto best-performing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoder-only large language models (LLMs) are increasingly used to build\nembedding models that effectively encode the semantic information of natural\nlanguage texts into dense vector representations for various embedding tasks.\nHowever, many existing methods primarily focus on removing the causal attention\nmask in LLMs to enable bidirectional attention, potentially undermining the\nmodel's ability to extract semantic information acquired during pretraining.\nAdditionally, leading unidirectional approaches often rely on extra input text\nto overcome the inherent limitations of causal attention, inevitably increasing\ncomputational costs. In this work, we propose Causal2Vec, a general-purpose\nembedding model tailored to enhance the performance of decoder-only LLMs\nwithout altering their original architectures or introducing significant\ncomputational overhead. Specifically, we first employ a lightweight BERT-style\nmodel to pre-encode the input text into a single Contextual token, which is\nthen prepended to the LLM's input sequence, allowing each token to capture\ncontextualized information even without attending to future tokens.\nFurthermore, to mitigate the recency bias introduced by last-token pooling and\nhelp LLMs better leverage the semantic information encoded in the Contextual\ntoken, we concatenate the last hidden states of Contextual and EOS tokens as\nthe final text embedding. In practice, Causal2Vec achieves state-of-the-art\nperformance on the Massive Text Embeddings Benchmark (MTEB) among models\ntrained solely on publicly available retrieval datasets, while reducing the\nrequired sequence length by up to 85% and inference time by up to 82% compared\nto best-performing methods."
                },
                "authors": [
                    {
                        "name": "Ailiang Lin"
                    },
                    {
                        "name": "Zhuoyun Li"
                    },
                    {
                        "name": "Kotaro Funakoshi"
                    }
                ],
                "author_detail": {
                    "name": "Kotaro Funakoshi"
                },
                "author": "Kotaro Funakoshi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23386v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23386v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13373v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13373v2",
                "updated": "2025-07-31T10:00:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    10,
                    0,
                    51,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-12T13:31:01Z",
                "published_parsed": [
                    2025,
                    7,
                    12,
                    13,
                    31,
                    1,
                    5,
                    193,
                    0
                ],
                "title": "Butter: Frequency Consistency and Hierarchical Fusion for Autonomous\n  Driving Object Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Butter: Frequency Consistency and Hierarchical Fusion for Autonomous\n  Driving Object Detection"
                },
                "summary": "Hierarchical feature representations play a pivotal role in computer vision,\nparticularly in object detection for autonomous driving. Multi-level semantic\nunderstanding is crucial for accurately identifying pedestrians, vehicles, and\ntraffic signs in dynamic environments. However, existing architectures, such as\nYOLO and DETR, struggle to maintain feature consistency across different scales\nwhile balancing detection precision and computational efficiency. To address\nthese challenges, we propose Butter, a novel object detection framework\ndesigned to enhance hierarchical feature representations for improving\ndetection robustness. Specifically, Butter introduces two key innovations:\nFrequency-Adaptive Feature Consistency Enhancement (FAFCE) Component, which\nrefines multi-scale feature consistency by leveraging adaptive frequency\nfiltering to enhance structural and boundary precision, and Progressive\nHierarchical Feature Fusion Network (PHFFNet) Module, which progressively\nintegrates multi-level features to mitigate semantic gaps and strengthen\nhierarchical feature learning. Through extensive experiments on BDD100K, KITTI,\nand Cityscapes, Butter demonstrates superior feature representation\ncapabilities, leading to notable improvements in detection accuracy while\nreducing model complexity. By focusing on hierarchical feature refinement and\nintegration, Butter provides an advanced approach to object detection that\nachieves a balance between accuracy, deployability, and computational\nefficiency in real-time autonomous driving scenarios. Our model and\nimplementation are publicly available at https://github.com/Aveiro-Lin/Butter,\nfacilitating further research and validation within the autonomous driving\ncommunity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical feature representations play a pivotal role in computer vision,\nparticularly in object detection for autonomous driving. Multi-level semantic\nunderstanding is crucial for accurately identifying pedestrians, vehicles, and\ntraffic signs in dynamic environments. However, existing architectures, such as\nYOLO and DETR, struggle to maintain feature consistency across different scales\nwhile balancing detection precision and computational efficiency. To address\nthese challenges, we propose Butter, a novel object detection framework\ndesigned to enhance hierarchical feature representations for improving\ndetection robustness. Specifically, Butter introduces two key innovations:\nFrequency-Adaptive Feature Consistency Enhancement (FAFCE) Component, which\nrefines multi-scale feature consistency by leveraging adaptive frequency\nfiltering to enhance structural and boundary precision, and Progressive\nHierarchical Feature Fusion Network (PHFFNet) Module, which progressively\nintegrates multi-level features to mitigate semantic gaps and strengthen\nhierarchical feature learning. Through extensive experiments on BDD100K, KITTI,\nand Cityscapes, Butter demonstrates superior feature representation\ncapabilities, leading to notable improvements in detection accuracy while\nreducing model complexity. By focusing on hierarchical feature refinement and\nintegration, Butter provides an advanced approach to object detection that\nachieves a balance between accuracy, deployability, and computational\nefficiency in real-time autonomous driving scenarios. Our model and\nimplementation are publicly available at https://github.com/Aveiro-Lin/Butter,\nfacilitating further research and validation within the autonomous driving\ncommunity."
                },
                "authors": [
                    {
                        "name": "Xiaojian Lin"
                    },
                    {
                        "name": "Wenxin Zhang"
                    },
                    {
                        "name": "Yuchu Jiang"
                    },
                    {
                        "name": "Wangyu Wu"
                    },
                    {
                        "name": "Yiran Guo"
                    },
                    {
                        "name": "Kangxu Wang"
                    },
                    {
                        "name": "Zongzheng Zhang"
                    },
                    {
                        "name": "Guijin Wang"
                    },
                    {
                        "name": "Lei Jin"
                    },
                    {
                        "name": "Hao Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhao"
                },
                "author": "Hao Zhao",
                "arxiv_comment": "10 pages, 6 figures. Supplementary material: 8 pages, 7 figures.\n  Accepted at ACM Multimedia 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13373v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13373v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.8; I.2.10; H.5.1; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14422v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14422v3",
                "updated": "2025-07-31T09:49:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    9,
                    49,
                    19,
                    3,
                    212,
                    0
                ],
                "published": "2025-05-20T14:31:53Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    31,
                    53,
                    1,
                    140,
                    0
                ],
                "title": "MindVote: When AI Meets the Wild West of Social Media Opinion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MindVote: When AI Meets the Wild West of Social Media Opinion"
                },
                "summary": "Large Language Models (LLMs) are increasingly used as scalable tools for\npilot testing, predicting public opinion distributions before deploying costly\nsurveys. To serve as effective pilot testing tools, the performance of these\nLLMs is typically benchmarked against their ability to reproduce the outcomes\nof past structured surveys. This evaluation paradigm, however, is misaligned\nwith the dynamic, context-rich social media environments where public opinion\nis increasingly formed and expressed. By design, surveys strip away the social,\ncultural, and temporal context that shapes public opinion, and LLM benchmarks\nbuilt on this paradigm inherit these critical limitations. To bridge this gap,\nwe introduce MindVote, the first benchmark for public opinion distribution\nprediction grounded in authentic social media discourse. MindVote is\nconstructed from 3,918 naturalistic polls sourced from Reddit and Weibo,\nspanning 23 topics and enriched with detailed annotations for platform,\ntopical, and temporal context. Using this benchmark, we conduct a comprehensive\nevaluation of 15 LLMs. MindVote provides a robust, ecologically valid framework\nto move beyond survey-based evaluations and advance the development of more\nsocially intelligent AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used as scalable tools for\npilot testing, predicting public opinion distributions before deploying costly\nsurveys. To serve as effective pilot testing tools, the performance of these\nLLMs is typically benchmarked against their ability to reproduce the outcomes\nof past structured surveys. This evaluation paradigm, however, is misaligned\nwith the dynamic, context-rich social media environments where public opinion\nis increasingly formed and expressed. By design, surveys strip away the social,\ncultural, and temporal context that shapes public opinion, and LLM benchmarks\nbuilt on this paradigm inherit these critical limitations. To bridge this gap,\nwe introduce MindVote, the first benchmark for public opinion distribution\nprediction grounded in authentic social media discourse. MindVote is\nconstructed from 3,918 naturalistic polls sourced from Reddit and Weibo,\nspanning 23 topics and enriched with detailed annotations for platform,\ntopical, and temporal context. Using this benchmark, we conduct a comprehensive\nevaluation of 15 LLMs. MindVote provides a robust, ecologically valid framework\nto move beyond survey-based evaluations and advance the development of more\nsocially intelligent AI systems."
                },
                "authors": [
                    {
                        "name": "Xutao Mao"
                    },
                    {
                        "name": "Ezra Xuanru Tao"
                    },
                    {
                        "name": "Leyao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Leyao Wang"
                },
                "author": "Leyao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14422v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14422v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23377v1",
                "updated": "2025-07-31T09:45:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    9,
                    45,
                    55,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T09:45:55Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    9,
                    45,
                    55,
                    3,
                    212,
                    0
                ],
                "title": "LLM4Rail: An LLM-Augmented Railway Service Consulting Platform",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4Rail: An LLM-Augmented Railway Service Consulting Platform"
                },
                "summary": "Large language models (LLMs) have significantly reshaped different walks of\nbusiness. To meet the increasing demands for individualized railway service, we\ndevelop LLM4Rail - a novel LLM-augmented railway service consulting platform.\nEmpowered by LLM, LLM4Rail can provide custom modules for ticketing, railway\nfood & drink recommendations, weather information, and chitchat. In LLM4Rail,\nwe propose the iterative \"Question-Thought-Action-Observation (QTAO)\" prompting\nframework. It meticulously integrates verbal reasoning with task-oriented\nactions, that is, reasoning to guide action selection, to effectively retrieve\nexternal observations relevant to railway operation and service to generate\naccurate responses. To provide personalized onboard dining services, we first\nconstruct the Chinese Railway Food and Drink (CRFD-25) - a publicly accessible\ntakeout dataset tailored for railway services. CRFD-25 covers a wide range of\nsignature dishes categorized by cities, cuisines, age groups, and spiciness\nlevels. We further introduce an LLM-based zero-shot conversational recommender\nfor railway catering. To address the unconstrained nature of open\nrecommendations, the feature similarity-based post-processing step is\nintroduced to ensure all the recommended items are aligned with CRFD-25\ndataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have significantly reshaped different walks of\nbusiness. To meet the increasing demands for individualized railway service, we\ndevelop LLM4Rail - a novel LLM-augmented railway service consulting platform.\nEmpowered by LLM, LLM4Rail can provide custom modules for ticketing, railway\nfood & drink recommendations, weather information, and chitchat. In LLM4Rail,\nwe propose the iterative \"Question-Thought-Action-Observation (QTAO)\" prompting\nframework. It meticulously integrates verbal reasoning with task-oriented\nactions, that is, reasoning to guide action selection, to effectively retrieve\nexternal observations relevant to railway operation and service to generate\naccurate responses. To provide personalized onboard dining services, we first\nconstruct the Chinese Railway Food and Drink (CRFD-25) - a publicly accessible\ntakeout dataset tailored for railway services. CRFD-25 covers a wide range of\nsignature dishes categorized by cities, cuisines, age groups, and spiciness\nlevels. We further introduce an LLM-based zero-shot conversational recommender\nfor railway catering. To address the unconstrained nature of open\nrecommendations, the feature similarity-based post-processing step is\nintroduced to ensure all the recommended items are aligned with CRFD-25\ndataset."
                },
                "authors": [
                    {
                        "name": "Zhuo Li"
                    },
                    {
                        "name": "Xianghuai Deng"
                    },
                    {
                        "name": "Chiwei Feng"
                    },
                    {
                        "name": "Hanmeng Li"
                    },
                    {
                        "name": "Shenjie Wang"
                    },
                    {
                        "name": "Haichao Zhang"
                    },
                    {
                        "name": "Teng Jia"
                    },
                    {
                        "name": "Conlin Chen"
                    },
                    {
                        "name": "Louis Linchun Wu"
                    },
                    {
                        "name": "Jia Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jia Wang"
                },
                "author": "Jia Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23370v1",
                "updated": "2025-07-31T09:37:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    9,
                    37,
                    22,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T09:37:22Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    9,
                    37,
                    22,
                    3,
                    212,
                    0
                ],
                "title": "Trae Agent: An LLM-based Agent for Software Engineering with Test-time\n  Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trae Agent: An LLM-based Agent for Software Engineering with Test-time\n  Scaling"
                },
                "summary": "Software issue resolution is a critical challenge in software engineering and\nhas garnered increasing attention in recent years. With the rapid advancement\nof large language models (LLMs), substantial progress has been made in\naddressing real-world software engineering tasks. Recent studies have\nintroduced ensemble reasoning techniques to enhance the performance of\nLLM-based issue resolution. However, existing prompting-based methods still\nface limitations in effectively exploring large ensemble spaces and lack the\ncapacity for repository-level understanding, both of which constrain their\noverall effectiveness. In this paper, we propose Trae Agent, the first\nagent-based ensemble reasoning approach for repository-level issue resolution.\nTrae Agent formulates our goal as an optimal solution search problem and\naddresses two key challenges, i.e., large ensemble spaces and repository-level\nunderstanding, through modular agents for generation, pruning, and selection.\nWe conduct extensive experiments using three leading LLMs on the widely-adopted\nSWE-bench benchmark, comparing Trae Agent against four state-of-the-art\nensemble reasoning techniques. Experimental results demonstrate that Trae Agent\nconsistently achieves superior performance, with an average improvement of\n10.22% over all baselines in terms of Pass@1. Trae Agent has achieved first\nplace on the SWE-bench Verified leaderboard, with a notable Pass@1 score of\n75.20%. We are pleased to release Trae Agent as an open-source project to\nsupport the research community, with all resources available at\nhttps://github.com/bytedance/trae-agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software issue resolution is a critical challenge in software engineering and\nhas garnered increasing attention in recent years. With the rapid advancement\nof large language models (LLMs), substantial progress has been made in\naddressing real-world software engineering tasks. Recent studies have\nintroduced ensemble reasoning techniques to enhance the performance of\nLLM-based issue resolution. However, existing prompting-based methods still\nface limitations in effectively exploring large ensemble spaces and lack the\ncapacity for repository-level understanding, both of which constrain their\noverall effectiveness. In this paper, we propose Trae Agent, the first\nagent-based ensemble reasoning approach for repository-level issue resolution.\nTrae Agent formulates our goal as an optimal solution search problem and\naddresses two key challenges, i.e., large ensemble spaces and repository-level\nunderstanding, through modular agents for generation, pruning, and selection.\nWe conduct extensive experiments using three leading LLMs on the widely-adopted\nSWE-bench benchmark, comparing Trae Agent against four state-of-the-art\nensemble reasoning techniques. Experimental results demonstrate that Trae Agent\nconsistently achieves superior performance, with an average improvement of\n10.22% over all baselines in terms of Pass@1. Trae Agent has achieved first\nplace on the SWE-bench Verified leaderboard, with a notable Pass@1 score of\n75.20%. We are pleased to release Trae Agent as an open-source project to\nsupport the research community, with all resources available at\nhttps://github.com/bytedance/trae-agent."
                },
                "authors": [
                    {
                        "name": "Trae Research Team"
                    },
                    {
                        "name": "Pengfei Gao"
                    },
                    {
                        "name": "Zhao Tian"
                    },
                    {
                        "name": "Xiangxin Meng"
                    },
                    {
                        "name": "Xinchen Wang"
                    },
                    {
                        "name": "Ruida Hu"
                    },
                    {
                        "name": "Yuanan Xiao"
                    },
                    {
                        "name": "Yizhou Liu"
                    },
                    {
                        "name": "Zhao Zhang"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Cuiyun Gao"
                    },
                    {
                        "name": "Yun Lin"
                    },
                    {
                        "name": "Yingfei Xiong"
                    },
                    {
                        "name": "Chao Peng"
                    },
                    {
                        "name": "Xia Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xia Liu"
                },
                "author": "Xia Liu",
                "arxiv_comment": "Pengfei Gao and Zhao Tian contributed equally to this technical\n  report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07106v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07106v2",
                "updated": "2025-07-31T09:33:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    9,
                    33,
                    35,
                    3,
                    212,
                    0
                ],
                "published": "2025-06-08T12:28:38Z",
                "published_parsed": [
                    2025,
                    6,
                    8,
                    12,
                    28,
                    38,
                    6,
                    159,
                    0
                ],
                "title": "Theorem-of-Thought: A Multi-Agent Framework for Abductive, Deductive,\n  and Inductive Reasoning in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theorem-of-Thought: A Multi-Agent Framework for Abductive, Deductive,\n  and Inductive Reasoning in Language Models"
                },
                "summary": "Large language models (LLMs) have shown strong performance across natural\nlanguage reasoning tasks, yet their reasoning processes remain brittle and\ndifficult to interpret. Prompting techniques like Chain-of-Thought (CoT)\nenhance reliability by eliciting intermediate reasoning steps or aggregating\nmultiple outputs. However, they lack mechanisms for enforcing logical structure\nand assessing internal coherence. We introduce Theorem-of-Thought (ToTh), a\nnovel framework that models reasoning as collaboration among three parallel\nagents, each simulating a distinct mode of inference: abductive, deductive, and\ninductive. Each agent produces a reasoning trace, which is structured into a\nformal reasoning graph. To evaluate consistency, we apply Bayesian belief\npropagation guided by natural language inference (NLI), assigning confidence\nscores to each step. The most coherent graph is selected to derive the final\nanswer. Experiments on symbolic (WebOfLies) and numerical (MultiArith)\nreasoning benchmarks show that ToTh consistently outperforms CoT,\nSelf-Consistency, and CoT-Decoding across multiple LLMs, while producing\ninterpretable and logically grounded reasoning chains. Our findings suggest a\npromising direction for building more robust and cognitively inspired LLM\nreasoning. The implementation is available at\nhttps://github.com/KurbanIntelligenceLab/theorem-of-thought.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown strong performance across natural\nlanguage reasoning tasks, yet their reasoning processes remain brittle and\ndifficult to interpret. Prompting techniques like Chain-of-Thought (CoT)\nenhance reliability by eliciting intermediate reasoning steps or aggregating\nmultiple outputs. However, they lack mechanisms for enforcing logical structure\nand assessing internal coherence. We introduce Theorem-of-Thought (ToTh), a\nnovel framework that models reasoning as collaboration among three parallel\nagents, each simulating a distinct mode of inference: abductive, deductive, and\ninductive. Each agent produces a reasoning trace, which is structured into a\nformal reasoning graph. To evaluate consistency, we apply Bayesian belief\npropagation guided by natural language inference (NLI), assigning confidence\nscores to each step. The most coherent graph is selected to derive the final\nanswer. Experiments on symbolic (WebOfLies) and numerical (MultiArith)\nreasoning benchmarks show that ToTh consistently outperforms CoT,\nSelf-Consistency, and CoT-Decoding across multiple LLMs, while producing\ninterpretable and logically grounded reasoning chains. Our findings suggest a\npromising direction for building more robust and cognitively inspired LLM\nreasoning. The implementation is available at\nhttps://github.com/KurbanIntelligenceLab/theorem-of-thought."
                },
                "authors": [
                    {
                        "name": "Samir Abdaljalil"
                    },
                    {
                        "name": "Hasan Kurban"
                    },
                    {
                        "name": "Khalid Qaraqe"
                    },
                    {
                        "name": "Erchin Serpedin"
                    }
                ],
                "author_detail": {
                    "name": "Erchin Serpedin"
                },
                "author": "Erchin Serpedin",
                "arxiv_comment": "ACL 2025 KnowFM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07106v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07106v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23365v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23365v1",
                "updated": "2025-07-31T09:25:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    9,
                    25,
                    55,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T09:25:55Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    9,
                    25,
                    55,
                    3,
                    212,
                    0
                ],
                "title": "\"I made this (sort of)\": Negotiating authorship, confronting\n  fraudulence, and exploring new musical spaces with prompt-based AI music\n  generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"I made this (sort of)\": Negotiating authorship, confronting\n  fraudulence, and exploring new musical spaces with prompt-based AI music\n  generation"
                },
                "summary": "I reflect on my experience creating two music albums centered on\nstate-of-the-art prompt-based AI music generation platforms. The first album\nexplicitly poses the question: What happens when I collide my junk mail with\nthese platforms? The second album is a direct response to the first, and toys\nwith the inability of state-of-the-art prompt-based AI music generation\nplatforms to generate music that is not ``practiced'', ``polished'', and\n``produced''. I seed a large language model (LLM) with information about these\nalbums and have it interview me, which results in the exploration of several\ndeeper questions: To what extent am I the author? Where am I in the resulting\nmusic? How is my musical identity changing as I am faced with machines that are\nin some ways far more talented than I? What new musical spaces does my work\nopen, for me or anyone/thing else? I conclude by reflecting on my reflections,\nas well as LLM-mediated self-reflection as method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I reflect on my experience creating two music albums centered on\nstate-of-the-art prompt-based AI music generation platforms. The first album\nexplicitly poses the question: What happens when I collide my junk mail with\nthese platforms? The second album is a direct response to the first, and toys\nwith the inability of state-of-the-art prompt-based AI music generation\nplatforms to generate music that is not ``practiced'', ``polished'', and\n``produced''. I seed a large language model (LLM) with information about these\nalbums and have it interview me, which results in the exploration of several\ndeeper questions: To what extent am I the author? Where am I in the resulting\nmusic? How is my musical identity changing as I am faced with machines that are\nin some ways far more talented than I? What new musical spaces does my work\nopen, for me or anyone/thing else? I conclude by reflecting on my reflections,\nas well as LLM-mediated self-reflection as method."
                },
                "authors": [
                    {
                        "name": "Bob L. T. Sturm"
                    }
                ],
                "author_detail": {
                    "name": "Bob L. T. Sturm"
                },
                "author": "Bob L. T. Sturm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23365v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; J.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21875v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21875v2",
                "updated": "2025-07-31T09:23:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    9,
                    23,
                    52,
                    3,
                    212,
                    0
                ],
                "published": "2025-06-27T03:18:45Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    3,
                    18,
                    45,
                    4,
                    178,
                    0
                ],
                "title": "WildSpeech-Bench: Benchmarking Audio LLMs in Natural Speech Conversation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildSpeech-Bench: Benchmarking Audio LLMs in Natural Speech Conversation"
                },
                "summary": "Recent multi-modal Large Language Models (LLMs) such as GPT-4o have\ndemonstrated strong capabilities of direct speech interaction. However, the\nlack of specialized and comprehensive benchmarks for end-to-end speech LLM\nevaluation hinders optimizing the user experience of Audio LLMs in real-world\napplications. Existing evaluation methods often adapt text-based benchmarks,\noverlooking speech's unique characteristics and challenges, including prosody,\nhomophones, stuttering, and differing user expectations. Here, we present a\nnovel approach to thoroughly evaluate LLMs in practical speech conversations.\nWe systematically curate real-world chat data relevant to spoken scenarios,\nintroduce diversity in speaker attributes and acoustic conditions, and augment\nthe dataset with speech-specific phenomena. We further design a query-aware\nevaluation method to use customized evaluation checklists and prompts to\nenhance the accuracy of automatic evaluation. We conduct comprehensive testing\nand detailed analysis of various mainstream speech models, revealing\nsignificant differences in model performance across different speech scenarios.\nThe use of query-aware evaluation further enables a finer-grained assessment\nunder various speech-specific scenarios. Our benchmark can provide valuable\ninsights for speech model development and evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent multi-modal Large Language Models (LLMs) such as GPT-4o have\ndemonstrated strong capabilities of direct speech interaction. However, the\nlack of specialized and comprehensive benchmarks for end-to-end speech LLM\nevaluation hinders optimizing the user experience of Audio LLMs in real-world\napplications. Existing evaluation methods often adapt text-based benchmarks,\noverlooking speech's unique characteristics and challenges, including prosody,\nhomophones, stuttering, and differing user expectations. Here, we present a\nnovel approach to thoroughly evaluate LLMs in practical speech conversations.\nWe systematically curate real-world chat data relevant to spoken scenarios,\nintroduce diversity in speaker attributes and acoustic conditions, and augment\nthe dataset with speech-specific phenomena. We further design a query-aware\nevaluation method to use customized evaluation checklists and prompts to\nenhance the accuracy of automatic evaluation. We conduct comprehensive testing\nand detailed analysis of various mainstream speech models, revealing\nsignificant differences in model performance across different speech scenarios.\nThe use of query-aware evaluation further enables a finer-grained assessment\nunder various speech-specific scenarios. Our benchmark can provide valuable\ninsights for speech model development and evaluation."
                },
                "authors": [
                    {
                        "name": "Jian Zhang"
                    },
                    {
                        "name": "Linhao Zhang"
                    },
                    {
                        "name": "Bokai Lei"
                    },
                    {
                        "name": "Chuhan Wu"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "Xiao Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Zhou"
                },
                "author": "Xiao Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21875v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21875v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11952v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11952v3",
                "updated": "2025-07-31T09:14:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    9,
                    14,
                    25,
                    3,
                    212,
                    0
                ],
                "published": "2025-04-16T10:29:30Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    10,
                    29,
                    30,
                    2,
                    106,
                    0
                ],
                "title": "Robust and Fine-Grained Detection of AI Generated Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust and Fine-Grained Detection of AI Generated Texts"
                },
                "summary": "An ideal detection system for machine generated content is supposed to work\nwell on any generator as many more advanced LLMs come into existence day by\nday. Existing systems often struggle with accurately identifying AI-generated\ncontent over shorter texts. Further, not all texts might be entirely authored\nby a human or LLM, hence we focused more over partial cases i.e human-LLM\nco-authored texts. Our paper introduces a set of models built for the task of\ntoken classification which are trained on an extensive collection of\nhuman-machine co-authored texts, which performed well over texts of unseen\ndomains, unseen generators, texts by non-native speakers and those with\nadversarial inputs. We also introduce a new dataset of over 2.4M such texts\nmostly co-authored by several popular proprietary LLMs over 23 languages. We\nalso present findings of our models' performance over each texts of each domain\nand generator. Additional findings include comparison of performance against\neach adversarial method, length of input texts and characteristics of generated\ntexts compared to the original human authored texts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An ideal detection system for machine generated content is supposed to work\nwell on any generator as many more advanced LLMs come into existence day by\nday. Existing systems often struggle with accurately identifying AI-generated\ncontent over shorter texts. Further, not all texts might be entirely authored\nby a human or LLM, hence we focused more over partial cases i.e human-LLM\nco-authored texts. Our paper introduces a set of models built for the task of\ntoken classification which are trained on an extensive collection of\nhuman-machine co-authored texts, which performed well over texts of unseen\ndomains, unseen generators, texts by non-native speakers and those with\nadversarial inputs. We also introduce a new dataset of over 2.4M such texts\nmostly co-authored by several popular proprietary LLMs over 23 languages. We\nalso present findings of our models' performance over each texts of each domain\nand generator. Additional findings include comparison of performance against\neach adversarial method, length of input texts and characteristics of generated\ntexts compared to the original human authored texts."
                },
                "authors": [
                    {
                        "name": "Ram Mohan Rao Kadiyala"
                    },
                    {
                        "name": "Siddartha Pullakhandam"
                    },
                    {
                        "name": "Kanwal Mehreen"
                    },
                    {
                        "name": "Drishti Sharma"
                    },
                    {
                        "name": "Siddhant Gupta"
                    },
                    {
                        "name": "Jebish Purbey"
                    },
                    {
                        "name": "Ashay Srivastava"
                    },
                    {
                        "name": "Subhasya TippaReddy"
                    },
                    {
                        "name": "Arvind Reddy Bobbili"
                    },
                    {
                        "name": "Suraj Telugara Chandrashekhar"
                    },
                    {
                        "name": "Modabbir Adeeb"
                    },
                    {
                        "name": "Srinadh Vura"
                    },
                    {
                        "name": "Suman Debnath"
                    },
                    {
                        "name": "Hamza Farooq"
                    }
                ],
                "author_detail": {
                    "name": "Hamza Farooq"
                },
                "author": "Hamza Farooq",
                "arxiv_comment": "18 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11952v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11952v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23361v1",
                "updated": "2025-07-31T09:13:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    9,
                    13,
                    42,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T09:13:42Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    9,
                    13,
                    42,
                    3,
                    212,
                    0
                ],
                "title": "SWE-Exp: Experience-Driven Software Issue Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWE-Exp: Experience-Driven Software Issue Resolution"
                },
                "summary": "Recent advances in large language model (LLM) agents have shown remarkable\nprogress in software issue resolution, leveraging advanced techniques such as\nmulti-agent collaboration and Monte Carlo Tree Search (MCTS). However, current\nagents act as memoryless explorers - treating each problem separately without\nretaining or reusing knowledge from previous repair experiences. This leads to\nredundant exploration of failed trajectories and missed chances to adapt\nsuccessful issue resolution methods to similar problems. To address this\nproblem, we introduce SWE-Exp, an experience - enhanced approach that distills\nconcise and actionable experience from prior agent trajectories, enabling\ncontinuous learning across issues. Our method introduces a multi-faceted\nexperience bank that captures both successful and failed repair attempts.\nSpecifically, it extracts reusable issue resolution knowledge at different\nlevels - from high-level problem comprehension to specific code changes.\nExperiments show that SWE-Exp achieves state-of-the-art resolution rate (41.6%\nPass@1) on SWE-bench-Verified under open-source agent frameworks. Our approach\nestablishes a new paradigm in which automated software engineering agents\nsystematically accumulate and leverage repair expertise, fundamentally shifting\nfrom trial-and-error exploration to strategic, experience-driven issue\nresolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language model (LLM) agents have shown remarkable\nprogress in software issue resolution, leveraging advanced techniques such as\nmulti-agent collaboration and Monte Carlo Tree Search (MCTS). However, current\nagents act as memoryless explorers - treating each problem separately without\nretaining or reusing knowledge from previous repair experiences. This leads to\nredundant exploration of failed trajectories and missed chances to adapt\nsuccessful issue resolution methods to similar problems. To address this\nproblem, we introduce SWE-Exp, an experience - enhanced approach that distills\nconcise and actionable experience from prior agent trajectories, enabling\ncontinuous learning across issues. Our method introduces a multi-faceted\nexperience bank that captures both successful and failed repair attempts.\nSpecifically, it extracts reusable issue resolution knowledge at different\nlevels - from high-level problem comprehension to specific code changes.\nExperiments show that SWE-Exp achieves state-of-the-art resolution rate (41.6%\nPass@1) on SWE-bench-Verified under open-source agent frameworks. Our approach\nestablishes a new paradigm in which automated software engineering agents\nsystematically accumulate and leverage repair expertise, fundamentally shifting\nfrom trial-and-error exploration to strategic, experience-driven issue\nresolution."
                },
                "authors": [
                    {
                        "name": "Silin Chen"
                    },
                    {
                        "name": "Shaoxin Lin"
                    },
                    {
                        "name": "Xiaodong Gu"
                    },
                    {
                        "name": "Yuling Shi"
                    },
                    {
                        "name": "Heng Lian"
                    },
                    {
                        "name": "Longfei Yun"
                    },
                    {
                        "name": "Dong Chen"
                    },
                    {
                        "name": "Weiguo Sun"
                    },
                    {
                        "name": "Lin Cao"
                    },
                    {
                        "name": "Qianxiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qianxiang Wang"
                },
                "author": "Qianxiang Wang",
                "arxiv_comment": "Our code and data are available at\n  https://github.com/YerbaPage/SWE-Exp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23358v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23358v1",
                "updated": "2025-07-31T09:08:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    9,
                    8,
                    59,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T09:08:59Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    9,
                    8,
                    59,
                    3,
                    212,
                    0
                ],
                "title": "Text-to-SQL Task-oriented Dialogue Ontology Construction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL Task-oriented Dialogue Ontology Construction"
                },
                "summary": "Large language models (LLMs) are widely used as general-purpose knowledge\nsources, but they rely on parametric knowledge, limiting explainability and\ntrustworthiness. In task-oriented dialogue (TOD) systems, this separation is\nexplicit, using an external database structured by an explicit ontology to\nensure explainability and controllability. However, building such ontologies\nrequires manual labels or supervised training. We introduce TeQoDO: a\nText-to-SQL task-oriented Dialogue Ontology construction method. Here, an LLM\nautonomously builds a TOD ontology from scratch without supervision using its\ninherent SQL programming capabilities combined with dialogue theory provided in\nthe prompt. We show that TeQoDO outperforms transfer learning approaches, and\nits constructed ontology is competitive on a downstream dialogue state tracking\ntask. Ablation studies demonstrate the key role of dialogue theory. TeQoDO also\nscales to allow construction of much larger ontologies, which we investigate on\na Wikipedia and ArXiv dataset. We view this as a step towards broader\napplication of ontologies to increase LLM explainability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely used as general-purpose knowledge\nsources, but they rely on parametric knowledge, limiting explainability and\ntrustworthiness. In task-oriented dialogue (TOD) systems, this separation is\nexplicit, using an external database structured by an explicit ontology to\nensure explainability and controllability. However, building such ontologies\nrequires manual labels or supervised training. We introduce TeQoDO: a\nText-to-SQL task-oriented Dialogue Ontology construction method. Here, an LLM\nautonomously builds a TOD ontology from scratch without supervision using its\ninherent SQL programming capabilities combined with dialogue theory provided in\nthe prompt. We show that TeQoDO outperforms transfer learning approaches, and\nits constructed ontology is competitive on a downstream dialogue state tracking\ntask. Ablation studies demonstrate the key role of dialogue theory. TeQoDO also\nscales to allow construction of much larger ontologies, which we investigate on\na Wikipedia and ArXiv dataset. We view this as a step towards broader\napplication of ontologies to increase LLM explainability."
                },
                "authors": [
                    {
                        "name": "Renato Vukovic"
                    },
                    {
                        "name": "Carel van Niekerk"
                    },
                    {
                        "name": "Michael Heck"
                    },
                    {
                        "name": "Benjamin Ruppik"
                    },
                    {
                        "name": "Hsien-Chin Lin"
                    },
                    {
                        "name": "Shutong Feng"
                    },
                    {
                        "name": "Nurul Lubis"
                    },
                    {
                        "name": "Milica Gasic"
                    }
                ],
                "author_detail": {
                    "name": "Milica Gasic"
                },
                "author": "Milica Gasic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23358v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23356v1",
                "updated": "2025-07-31T09:06:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    9,
                    6,
                    20,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T09:06:20Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    9,
                    6,
                    20,
                    3,
                    212,
                    0
                ],
                "title": "Quality Evaluation of COBOL to Java Code Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quality Evaluation of COBOL to Java Code Transformation"
                },
                "summary": "We present an automated evaluation system for assessing COBOL-to-Java code\ntranslation within IBM's watsonx Code Assistant for Z (WCA4Z). The system\naddresses key challenges in evaluating LLM-based translators, including model\nopacity and the complexity of translation quality assessment. Our approach\ncombines analytic checkers with LLM-as-a-judge (LaaJ) techniques to deliver\nscalable, multi-faceted evaluations. The system supports continuous integration\nworkflows, enables large-scale benchmarking, and reduces reliance on manual\nreview. We describe the system architecture, evaluation strategies, and\nreporting mechanisms that provide actionable insights for developers and\nproject managers, facilitating the evolution of high-quality, modernized\ncodebases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an automated evaluation system for assessing COBOL-to-Java code\ntranslation within IBM's watsonx Code Assistant for Z (WCA4Z). The system\naddresses key challenges in evaluating LLM-based translators, including model\nopacity and the complexity of translation quality assessment. Our approach\ncombines analytic checkers with LLM-as-a-judge (LaaJ) techniques to deliver\nscalable, multi-faceted evaluations. The system supports continuous integration\nworkflows, enables large-scale benchmarking, and reduces reliance on manual\nreview. We describe the system architecture, evaluation strategies, and\nreporting mechanisms that provide actionable insights for developers and\nproject managers, facilitating the evolution of high-quality, modernized\ncodebases."
                },
                "authors": [
                    {
                        "name": "Shmulik Froimovich"
                    },
                    {
                        "name": "Raviv Gal"
                    },
                    {
                        "name": "Wesam Ibraheem"
                    },
                    {
                        "name": "Avi Ziv"
                    }
                ],
                "author_detail": {
                    "name": "Avi Ziv"
                },
                "author": "Avi Ziv",
                "arxiv_comment": "Submitted to ASE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07695v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07695v2",
                "updated": "2025-07-31T08:57:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    8,
                    57,
                    40,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-10T12:19:03Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    12,
                    19,
                    3,
                    3,
                    191,
                    0
                ],
                "title": "KeyKnowledgeRAG (K^2RAG): An Enhanced RAG method for improved LLM\n  question-answering capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KeyKnowledgeRAG (K^2RAG): An Enhanced RAG method for improved LLM\n  question-answering capabilities"
                },
                "summary": "Fine-tuning is an immensely resource-intensive process when retraining Large\nLanguage Models (LLMs) to incorporate a larger body of knowledge. Although many\nfine-tuning techniques have been developed to reduce the time and computational\ncost involved, the challenge persists as LLMs continue to grow in size and\ncomplexity. To address this, a new approach to knowledge expansion in LLMs is\nneeded. Retrieval-Augmented Generation (RAG) offers one such alternative by\nstoring external knowledge in a database and retrieving relevant chunks to\nsupport question answering. However, naive implementations of RAG face\nsignificant limitations in scalability and answer accuracy. This paper\nintroduces KeyKnowledgeRAG (K2RAG), a novel framework designed to overcome\nthese limitations. Inspired by the divide-and-conquer paradigm, K2RAG\nintegrates dense and sparse vector search, knowledge graphs, and text\nsummarization to improve retrieval quality and system efficiency. The framework\nalso includes a preprocessing step that summarizes the training data,\nsignificantly reducing the training time. K2RAG was evaluated using the\nMultiHopRAG dataset, where the proposed pipeline was trained on the document\ncorpus and tested on a separate evaluation set. Results demonstrated notable\nimprovements over common naive RAG implementations. K2RAG achieved the highest\nmean answer similarity score of 0.57, and reached the highest third quartile\n(Q3) similarity of 0.82, indicating better alignment with ground-truth answers.\nIn addition to improved accuracy, the framework proved highly efficient. The\nsummarization step reduced the average training time of individual components\nby 93%, and execution speed was up to 40% faster than traditional knowledge\ngraph-based RAG systems. K2RAG also demonstrated superior scalability,\nrequiring three times less VRAM than several naive RAG implementations tested\nin this study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning is an immensely resource-intensive process when retraining Large\nLanguage Models (LLMs) to incorporate a larger body of knowledge. Although many\nfine-tuning techniques have been developed to reduce the time and computational\ncost involved, the challenge persists as LLMs continue to grow in size and\ncomplexity. To address this, a new approach to knowledge expansion in LLMs is\nneeded. Retrieval-Augmented Generation (RAG) offers one such alternative by\nstoring external knowledge in a database and retrieving relevant chunks to\nsupport question answering. However, naive implementations of RAG face\nsignificant limitations in scalability and answer accuracy. This paper\nintroduces KeyKnowledgeRAG (K2RAG), a novel framework designed to overcome\nthese limitations. Inspired by the divide-and-conquer paradigm, K2RAG\nintegrates dense and sparse vector search, knowledge graphs, and text\nsummarization to improve retrieval quality and system efficiency. The framework\nalso includes a preprocessing step that summarizes the training data,\nsignificantly reducing the training time. K2RAG was evaluated using the\nMultiHopRAG dataset, where the proposed pipeline was trained on the document\ncorpus and tested on a separate evaluation set. Results demonstrated notable\nimprovements over common naive RAG implementations. K2RAG achieved the highest\nmean answer similarity score of 0.57, and reached the highest third quartile\n(Q3) similarity of 0.82, indicating better alignment with ground-truth answers.\nIn addition to improved accuracy, the framework proved highly efficient. The\nsummarization step reduced the average training time of individual components\nby 93%, and execution speed was up to 40% faster than traditional knowledge\ngraph-based RAG systems. K2RAG also demonstrated superior scalability,\nrequiring three times less VRAM than several naive RAG implementations tested\nin this study."
                },
                "authors": [
                    {
                        "name": "Hruday Markondapatnaikuni"
                    },
                    {
                        "name": "Basem Suleiman"
                    },
                    {
                        "name": "Abdelkarim Erradi"
                    },
                    {
                        "name": "Shijing Chen"
                    }
                ],
                "author_detail": {
                    "name": "Shijing Chen"
                },
                "author": "Shijing Chen",
                "arxiv_comment": "21 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07695v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07695v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23348v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23348v1",
                "updated": "2025-07-31T08:54:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    8,
                    54,
                    46,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T08:54:46Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    8,
                    54,
                    46,
                    3,
                    212,
                    0
                ],
                "title": "SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution"
                },
                "summary": "Issue resolution has made remarkable progress thanks to the advanced\nreasoning capabilities of large language models (LLMs). Recently, agent-based\nframeworks such as SWE-agent have further advanced this progress by enabling\nautonomous, tool-using agents to tackle complex software engineering tasks.\nWhile existing agent-based issue resolution approaches are primarily based on\nagents' independent explorations, they often get stuck in local solutions and\nfail to identify issue patterns that span across different parts of the\ncodebase. To address this limitation, we propose SWE-Debate, a competitive\nmulti-agent debate framework that encourages diverse reasoning paths and\nachieves more consolidated issue localization. SWE-Debate first creates\nmultiple fault propagation traces as localization proposals by traversing a\ncode dependency graph. Then, it organizes a three-round debate among\nspecialized agents, each embodying distinct reasoning perspectives along the\nfault propagation trace. This structured competition enables agents to\ncollaboratively converge on a consolidated fix plan. Finally, this consolidated\nfix plan is integrated into an MCTS-based code modification agent for patch\ngeneration. Experiments on the SWE-bench benchmark show that SWE-Debate\nachieves new state-of-the-art results in open-source agent frameworks and\noutperforms baselines by a large margin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Issue resolution has made remarkable progress thanks to the advanced\nreasoning capabilities of large language models (LLMs). Recently, agent-based\nframeworks such as SWE-agent have further advanced this progress by enabling\nautonomous, tool-using agents to tackle complex software engineering tasks.\nWhile existing agent-based issue resolution approaches are primarily based on\nagents' independent explorations, they often get stuck in local solutions and\nfail to identify issue patterns that span across different parts of the\ncodebase. To address this limitation, we propose SWE-Debate, a competitive\nmulti-agent debate framework that encourages diverse reasoning paths and\nachieves more consolidated issue localization. SWE-Debate first creates\nmultiple fault propagation traces as localization proposals by traversing a\ncode dependency graph. Then, it organizes a three-round debate among\nspecialized agents, each embodying distinct reasoning perspectives along the\nfault propagation trace. This structured competition enables agents to\ncollaboratively converge on a consolidated fix plan. Finally, this consolidated\nfix plan is integrated into an MCTS-based code modification agent for patch\ngeneration. Experiments on the SWE-bench benchmark show that SWE-Debate\nachieves new state-of-the-art results in open-source agent frameworks and\noutperforms baselines by a large margin."
                },
                "authors": [
                    {
                        "name": "Han Li"
                    },
                    {
                        "name": "Yuling Shi"
                    },
                    {
                        "name": "Shaoxin Lin"
                    },
                    {
                        "name": "Xiaodong Gu"
                    },
                    {
                        "name": "Heng Lian"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Yantao Jia"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Qianxiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qianxiang Wang"
                },
                "author": "Qianxiang Wang",
                "arxiv_comment": "Our code and data are available at\n  https://github.com/YerbaPage/SWE-Debate",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23348v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23348v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09753v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09753v3",
                "updated": "2025-07-31T08:49:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    8,
                    49,
                    18,
                    3,
                    212,
                    0
                ],
                "published": "2025-04-13T23:10:13Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    23,
                    10,
                    13,
                    6,
                    103,
                    0
                ],
                "title": "Improving Multilingual Capabilities with Cultural and Local Knowledge in\n  Large Language Models While Enhancing Native Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Multilingual Capabilities with Cultural and Local Knowledge in\n  Large Language Models While Enhancing Native Performance"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable capabilities, but their\ndevelopment has primarily focused on English and other high-resource languages,\nleaving many languages underserved. We present our latest Hindi-English\nbi-lingual LLM \\textbf{Mantra-14B} with ~3\\% average improvement in benchmark\nscores over both languages, outperforming models twice its size. Using a\ncurated dataset composed of English and Hindi instruction data of 485K samples,\nwe instruction tuned models such as Qwen-2.5-14B-Instruct and Phi-4 to improve\nperformance over both English and Hindi. Our experiments encompassing seven\ndifferent LLMs of varying parameter sizes and over 140 training attempts with\nvarying English-Hindi training data ratios demonstrated that it is possible to\nsignificantly improve multilingual performance without compromising native\nperformance. Further, our approach avoids resource-intensive techniques like\nvocabulary expansion or architectural modifications, thus keeping the model\nsize small. Our results indicate that modest fine-tuning with culturally and\nlocally informed data can bridge performance gaps without incurring significant\ncomputational overhead. We release our training code, datasets, and models\nunder mit and apache licenses to aid further research towards under-represented\nand low-resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable capabilities, but their\ndevelopment has primarily focused on English and other high-resource languages,\nleaving many languages underserved. We present our latest Hindi-English\nbi-lingual LLM \\textbf{Mantra-14B} with ~3\\% average improvement in benchmark\nscores over both languages, outperforming models twice its size. Using a\ncurated dataset composed of English and Hindi instruction data of 485K samples,\nwe instruction tuned models such as Qwen-2.5-14B-Instruct and Phi-4 to improve\nperformance over both English and Hindi. Our experiments encompassing seven\ndifferent LLMs of varying parameter sizes and over 140 training attempts with\nvarying English-Hindi training data ratios demonstrated that it is possible to\nsignificantly improve multilingual performance without compromising native\nperformance. Further, our approach avoids resource-intensive techniques like\nvocabulary expansion or architectural modifications, thus keeping the model\nsize small. Our results indicate that modest fine-tuning with culturally and\nlocally informed data can bridge performance gaps without incurring significant\ncomputational overhead. We release our training code, datasets, and models\nunder mit and apache licenses to aid further research towards under-represented\nand low-resource languages."
                },
                "authors": [
                    {
                        "name": "Ram Mohan Rao Kadiyala"
                    },
                    {
                        "name": "Siddartha Pullakhandam"
                    },
                    {
                        "name": "Siddhant Gupta"
                    },
                    {
                        "name": "Drishti Sharma"
                    },
                    {
                        "name": "Jebish Purbey"
                    },
                    {
                        "name": "Kanwal Mehreen"
                    },
                    {
                        "name": "Muhammad Arham"
                    },
                    {
                        "name": "Suman Debnath"
                    },
                    {
                        "name": "Hamza Farooq"
                    }
                ],
                "author_detail": {
                    "name": "Hamza Farooq"
                },
                "author": "Hamza Farooq",
                "arxiv_comment": "24 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09753v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09753v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23336v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23336v1",
                "updated": "2025-07-31T08:32:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    8,
                    32,
                    37,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T08:32:37Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    8,
                    32,
                    37,
                    3,
                    212,
                    0
                ],
                "title": "DSBC : Data Science task Benchmarking with Context engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DSBC : Data Science task Benchmarking with Context engineering"
                },
                "summary": "Recent advances in large language models (LLMs) have significantly impacted\ndata science workflows, giving rise to specialized data science agents designed\nto automate analytical tasks. Despite rapid adoption, systematic benchmarks\nevaluating the efficacy and limitations of these agents remain scarce. In this\npaper, we introduce a comprehensive benchmark specifically crafted to reflect\nreal-world user interactions with data science agents by observing usage of our\ncommercial applications. We evaluate three LLMs: Claude-4.0-Sonnet,\nGemini-2.5-Flash, and OpenAI-o4-Mini across three approaches: zero-shot with\ncontext engineering, multi-step with context engineering, and with SmolAgent.\nOur benchmark assesses performance across a diverse set of eight data science\ntask categories, additionally exploring the sensitivity of models to common\nprompting issues, such as data leakage and slightly ambiguous instructions. We\nfurther investigate the influence of temperature parameters on overall and\ntask-specific outcomes for each model and approach. Our findings reveal\ndistinct performance disparities among the evaluated models and methodologies,\nhighlighting critical factors that affect practical deployment. The benchmark\ndataset and evaluation framework introduced herein aim to provide a foundation\nfor future research of more robust and effective data science agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have significantly impacted\ndata science workflows, giving rise to specialized data science agents designed\nto automate analytical tasks. Despite rapid adoption, systematic benchmarks\nevaluating the efficacy and limitations of these agents remain scarce. In this\npaper, we introduce a comprehensive benchmark specifically crafted to reflect\nreal-world user interactions with data science agents by observing usage of our\ncommercial applications. We evaluate three LLMs: Claude-4.0-Sonnet,\nGemini-2.5-Flash, and OpenAI-o4-Mini across three approaches: zero-shot with\ncontext engineering, multi-step with context engineering, and with SmolAgent.\nOur benchmark assesses performance across a diverse set of eight data science\ntask categories, additionally exploring the sensitivity of models to common\nprompting issues, such as data leakage and slightly ambiguous instructions. We\nfurther investigate the influence of temperature parameters on overall and\ntask-specific outcomes for each model and approach. Our findings reveal\ndistinct performance disparities among the evaluated models and methodologies,\nhighlighting critical factors that affect practical deployment. The benchmark\ndataset and evaluation framework introduced herein aim to provide a foundation\nfor future research of more robust and effective data science agents."
                },
                "authors": [
                    {
                        "name": "Ram Mohan Rao Kadiyala"
                    },
                    {
                        "name": "Siddhant Gupta"
                    },
                    {
                        "name": "Jebish Purbey"
                    },
                    {
                        "name": "Giulio Martini"
                    },
                    {
                        "name": "Suman Debnath"
                    },
                    {
                        "name": "Hamza Farooq"
                    }
                ],
                "author_detail": {
                    "name": "Hamza Farooq"
                },
                "author": "Hamza Farooq",
                "arxiv_comment": "32 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23336v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23336v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00956v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00956v2",
                "updated": "2025-07-31T08:31:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    8,
                    31,
                    51,
                    3,
                    212,
                    0
                ],
                "published": "2025-06-01T11:00:24Z",
                "published_parsed": [
                    2025,
                    6,
                    1,
                    11,
                    0,
                    24,
                    6,
                    152,
                    0
                ],
                "title": "Continual-MEGA: A Large-scale Benchmark for Generalizable Continual\n  Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual-MEGA: A Large-scale Benchmark for Generalizable Continual\n  Anomaly Detection"
                },
                "summary": "In this paper, we introduce a new benchmark for continual learning in anomaly\ndetection, aimed at better reflecting real-world deployment scenarios. Our\nbenchmark, Continual-MEGA, includes a large and diverse dataset that\nsignificantly expands existing evaluation settings by combining carefully\ncurated existing datasets with our newly proposed dataset, ContinualAD. In\naddition to standard continual learning with expanded quantity, we propose a\nnovel scenario that measures zero-shot generalization to unseen classes, those\nnot observed during continual adaptation. This setting poses a new problem\nsetting that continual adaptation also enhances zero-shot performance. We also\npresent a unified baseline algorithm that improves robustness in few-shot\ndetection and maintains strong generalization. Through extensive evaluations,\nwe report three key findings: (1) existing methods show substantial room for\nimprovement, particularly in pixel-level defect localization; (2) our proposed\nmethod consistently outperforms prior approaches; and (3) the newly introduced\nContinualAD dataset enhances the performance of strong anomaly detection\nmodels. We release the benchmark and code in\nhttps://github.com/Continual-Mega/Continual-Mega.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a new benchmark for continual learning in anomaly\ndetection, aimed at better reflecting real-world deployment scenarios. Our\nbenchmark, Continual-MEGA, includes a large and diverse dataset that\nsignificantly expands existing evaluation settings by combining carefully\ncurated existing datasets with our newly proposed dataset, ContinualAD. In\naddition to standard continual learning with expanded quantity, we propose a\nnovel scenario that measures zero-shot generalization to unseen classes, those\nnot observed during continual adaptation. This setting poses a new problem\nsetting that continual adaptation also enhances zero-shot performance. We also\npresent a unified baseline algorithm that improves robustness in few-shot\ndetection and maintains strong generalization. Through extensive evaluations,\nwe report three key findings: (1) existing methods show substantial room for\nimprovement, particularly in pixel-level defect localization; (2) our proposed\nmethod consistently outperforms prior approaches; and (3) the newly introduced\nContinualAD dataset enhances the performance of strong anomaly detection\nmodels. We release the benchmark and code in\nhttps://github.com/Continual-Mega/Continual-Mega."
                },
                "authors": [
                    {
                        "name": "Geonu Lee"
                    },
                    {
                        "name": "Yujeong Oh"
                    },
                    {
                        "name": "Geonhui Jang"
                    },
                    {
                        "name": "Soyoung Lee"
                    },
                    {
                        "name": "Jeonghyo Song"
                    },
                    {
                        "name": "Sungmin Cha"
                    },
                    {
                        "name": "YoungJoon Yoo"
                    }
                ],
                "author_detail": {
                    "name": "YoungJoon Yoo"
                },
                "author": "YoungJoon Yoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00956v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00956v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23334v1",
                "updated": "2025-07-31T08:31:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    8,
                    31,
                    5,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T08:31:05Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    8,
                    31,
                    5,
                    3,
                    212,
                    0
                ],
                "title": "MUST-RAG: MUSical Text Question Answering with Retrieval Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MUST-RAG: MUSical Text Question Answering with Retrieval Augmented\n  Generation"
                },
                "summary": "Recent advancements in Large language models (LLMs) have demonstrated\nremarkable capabilities across diverse domains. While they exhibit strong\nzero-shot performance on various tasks, LLMs' effectiveness in music-related\napplications remains limited due to the relatively small proportion of\nmusic-specific knowledge in their training data. To address this limitation, we\npropose MusT-RAG, a comprehensive framework based on Retrieval Augmented\nGeneration (RAG) to adapt general-purpose LLMs for text-only music question\nanswering (MQA) tasks. RAG is a technique that provides external knowledge to\nLLMs by retrieving relevant context information when generating answers to\nquestions. To optimize RAG for the music domain, we (1) propose MusWikiDB, a\nmusic-specialized vector database for the retrieval stage, and (2) utilizes\ncontext information during both inference and fine-tuning processes to\neffectively transform general-purpose LLMs into music-specific models. Our\nexperiment demonstrates that MusT-RAG significantly outperforms traditional\nfine-tuning approaches in enhancing LLMs' music domain adaptation capabilities,\nshowing consistent improvements across both in-domain and out-of-domain MQA\nbenchmarks. Additionally, our MusWikiDB proves substantially more effective\nthan general Wikipedia corpora, delivering superior performance and\ncomputational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large language models (LLMs) have demonstrated\nremarkable capabilities across diverse domains. While they exhibit strong\nzero-shot performance on various tasks, LLMs' effectiveness in music-related\napplications remains limited due to the relatively small proportion of\nmusic-specific knowledge in their training data. To address this limitation, we\npropose MusT-RAG, a comprehensive framework based on Retrieval Augmented\nGeneration (RAG) to adapt general-purpose LLMs for text-only music question\nanswering (MQA) tasks. RAG is a technique that provides external knowledge to\nLLMs by retrieving relevant context information when generating answers to\nquestions. To optimize RAG for the music domain, we (1) propose MusWikiDB, a\nmusic-specialized vector database for the retrieval stage, and (2) utilizes\ncontext information during both inference and fine-tuning processes to\neffectively transform general-purpose LLMs into music-specific models. Our\nexperiment demonstrates that MusT-RAG significantly outperforms traditional\nfine-tuning approaches in enhancing LLMs' music domain adaptation capabilities,\nshowing consistent improvements across both in-domain and out-of-domain MQA\nbenchmarks. Additionally, our MusWikiDB proves substantially more effective\nthan general Wikipedia corpora, delivering superior performance and\ncomputational efficiency."
                },
                "authors": [
                    {
                        "name": "Daeyong Kwon"
                    },
                    {
                        "name": "SeungHeon Doh"
                    },
                    {
                        "name": "Juhan Nam"
                    }
                ],
                "author_detail": {
                    "name": "Juhan Nam"
                },
                "author": "Juhan Nam",
                "arxiv_comment": "8 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11167v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11167v3",
                "updated": "2025-07-31T08:14:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    8,
                    14,
                    30,
                    3,
                    212,
                    0
                ],
                "published": "2024-12-15T12:30:52Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    12,
                    30,
                    52,
                    6,
                    350,
                    0
                ],
                "title": "Cultural Palette: Pluralising Culture Alignment via Multi-agent Palette",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cultural Palette: Pluralising Culture Alignment via Multi-agent Palette"
                },
                "summary": "Large language models (LLMs) face challenges in aligning with diverse\ncultural values despite their remarkable performance in generation, which stems\nfrom inherent monocultural biases and difficulties in capturing nuanced\ncultural semantics. Existing methods struggle to adapt to unknown culture after\nfine-tuning. Inspired by cultural geography across five continents, we propose\nCultural Palette, a multi-agent framework that redefines cultural alignment as\nan adaptive \"color-blending\" process for country-specific adaptation. Our\napproach harnesses cultural geography across five continents (Africa, America,\nAsia, Europe, Oceania) through three key steps: First, we synthesize the\nPentachromatic Cultural Palette Dataset using GPT-4o, refining\ncontinental-level dialogues with Hofstede's cultural dimensions to establish\nfoundational cultural representations. Second, five continent-level alignment\nagents form specialized cultural communities that generate region-specific\ndraft responses. Third, a Meta Agent employs Cultural MoErges to dynamically\nblend these cultural \"colors\" through attention-gated parameter merging, akin\nto mixing pigments on a palette, resolving conflicts while preserving cultural\nnuances to produce the final culturally-aligned response. Extensive experiments\nacross various countries demonstrate that Cultural Palette surpasses existing\nbaselines in cultural alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face challenges in aligning with diverse\ncultural values despite their remarkable performance in generation, which stems\nfrom inherent monocultural biases and difficulties in capturing nuanced\ncultural semantics. Existing methods struggle to adapt to unknown culture after\nfine-tuning. Inspired by cultural geography across five continents, we propose\nCultural Palette, a multi-agent framework that redefines cultural alignment as\nan adaptive \"color-blending\" process for country-specific adaptation. Our\napproach harnesses cultural geography across five continents (Africa, America,\nAsia, Europe, Oceania) through three key steps: First, we synthesize the\nPentachromatic Cultural Palette Dataset using GPT-4o, refining\ncontinental-level dialogues with Hofstede's cultural dimensions to establish\nfoundational cultural representations. Second, five continent-level alignment\nagents form specialized cultural communities that generate region-specific\ndraft responses. Third, a Meta Agent employs Cultural MoErges to dynamically\nblend these cultural \"colors\" through attention-gated parameter merging, akin\nto mixing pigments on a palette, resolving conflicts while preserving cultural\nnuances to produce the final culturally-aligned response. Extensive experiments\nacross various countries demonstrate that Cultural Palette surpasses existing\nbaselines in cultural alignment."
                },
                "authors": [
                    {
                        "name": "Jiahao Yuan"
                    },
                    {
                        "name": "Zixiang Di"
                    },
                    {
                        "name": "Shangzixin Zhao"
                    },
                    {
                        "name": "Zhiqing Cui"
                    },
                    {
                        "name": "Hanqing Wang"
                    },
                    {
                        "name": "Guisong Yang"
                    },
                    {
                        "name": "Usman Naseem"
                    }
                ],
                "author_detail": {
                    "name": "Usman Naseem"
                },
                "author": "Usman Naseem",
                "arxiv_comment": "20 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11167v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11167v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23326v1",
                "updated": "2025-07-31T08:14:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    8,
                    14,
                    26,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T08:14:26Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    8,
                    14,
                    26,
                    3,
                    212,
                    0
                ],
                "title": "Learning Semantic Directions for Feature Augmentation in\n  Domain-Generalized Medical Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Semantic Directions for Feature Augmentation in\n  Domain-Generalized Medical Segmentation"
                },
                "summary": "Medical image segmentation plays a crucial role in clinical workflows, but\ndomain shift often leads to performance degradation when models are applied to\nunseen clinical domains. This challenge arises due to variations in imaging\nconditions, scanner types, and acquisition protocols, limiting the practical\ndeployment of segmentation models. Unlike natural images, medical images\ntypically exhibit consistent anatomical structures across patients, with\ndomain-specific variations mainly caused by imaging conditions. This unique\ncharacteristic makes medical image segmentation particularly challenging.\n  To address this challenge, we propose a domain generalization framework\ntailored for medical image segmentation. Our approach improves robustness to\ndomain-specific variations by introducing implicit feature perturbations guided\nby domain statistics. Specifically, we employ a learnable semantic direction\nselector and a covariance-based semantic intensity sampler to modulate\ndomain-variant features while preserving task-relevant anatomical consistency.\nFurthermore, we design an adaptive consistency constraint that is selectively\napplied only when feature adjustment leads to degraded segmentation\nperformance. This constraint encourages the adjusted features to align with the\noriginal predictions, thereby stabilizing feature selection and improving the\nreliability of the segmentation.\n  Extensive experiments on two public multi-center benchmarks show that our\nframework consistently outperforms existing domain generalization approaches,\nachieving robust and generalizable segmentation performance across diverse\nclinical domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical image segmentation plays a crucial role in clinical workflows, but\ndomain shift often leads to performance degradation when models are applied to\nunseen clinical domains. This challenge arises due to variations in imaging\nconditions, scanner types, and acquisition protocols, limiting the practical\ndeployment of segmentation models. Unlike natural images, medical images\ntypically exhibit consistent anatomical structures across patients, with\ndomain-specific variations mainly caused by imaging conditions. This unique\ncharacteristic makes medical image segmentation particularly challenging.\n  To address this challenge, we propose a domain generalization framework\ntailored for medical image segmentation. Our approach improves robustness to\ndomain-specific variations by introducing implicit feature perturbations guided\nby domain statistics. Specifically, we employ a learnable semantic direction\nselector and a covariance-based semantic intensity sampler to modulate\ndomain-variant features while preserving task-relevant anatomical consistency.\nFurthermore, we design an adaptive consistency constraint that is selectively\napplied only when feature adjustment leads to degraded segmentation\nperformance. This constraint encourages the adjusted features to align with the\noriginal predictions, thereby stabilizing feature selection and improving the\nreliability of the segmentation.\n  Extensive experiments on two public multi-center benchmarks show that our\nframework consistently outperforms existing domain generalization approaches,\nachieving robust and generalizable segmentation performance across diverse\nclinical domains."
                },
                "authors": [
                    {
                        "name": "Yingkai Wang"
                    },
                    {
                        "name": "Yaoyao Zhu"
                    },
                    {
                        "name": "Xiuding Cai"
                    },
                    {
                        "name": "Yuhao Xiao"
                    },
                    {
                        "name": "Haotian Wu"
                    },
                    {
                        "name": "Yu Yao"
                    }
                ],
                "author_detail": {
                    "name": "Yu Yao"
                },
                "author": "Yu Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.08403v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.08403v6",
                "updated": "2025-07-31T08:08:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    8,
                    8,
                    47,
                    3,
                    212,
                    0
                ],
                "published": "2024-02-13T12:04:43Z",
                "published_parsed": [
                    2024,
                    2,
                    13,
                    12,
                    4,
                    43,
                    1,
                    44,
                    0
                ],
                "title": "LLMs and the Human Condition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs and the Human Condition"
                },
                "summary": "Theory based AI research has had a hard time recently and the aim here is to\npropose a model of what LLMs are actually doing when they impress us with their\nlanguage skills. The model integrates three established theories of human\ndecision-making from philosophy, sociology, and computer science. The paper\nstarts with the collective understanding of reasoning from the early days of AI\nresearch - primarily because that model is how we humans think we think, and is\nthe most accessible. It then describes what is commonly thought of as \"reactive\nsystems\" which is the position taken by many philosophers and indeed many\ncontemporary AI researchers. The third component to the proposed model is from\nsociology and based on the idea that human intelligence is a collective skill\nfor which individuals are merely actors. The resulting model provides an\nalternate view of ``mind reading'' in human communication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theory based AI research has had a hard time recently and the aim here is to\npropose a model of what LLMs are actually doing when they impress us with their\nlanguage skills. The model integrates three established theories of human\ndecision-making from philosophy, sociology, and computer science. The paper\nstarts with the collective understanding of reasoning from the early days of AI\nresearch - primarily because that model is how we humans think we think, and is\nthe most accessible. It then describes what is commonly thought of as \"reactive\nsystems\" which is the position taken by many philosophers and indeed many\ncontemporary AI researchers. The third component to the proposed model is from\nsociology and based on the idea that human intelligence is a collective skill\nfor which individuals are merely actors. The resulting model provides an\nalternate view of ``mind reading'' in human communication."
                },
                "authors": [
                    {
                        "name": "Peter Wallis"
                    }
                ],
                "author_detail": {
                    "name": "Peter Wallis"
                },
                "author": "Peter Wallis",
                "arxiv_comment": "Edits and rearranged to provide significantly tighter writing - no\n  change to the content. Now targeting a one day workshop on multi agent\n  systems and LLMs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.08403v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.08403v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23324v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23324v1",
                "updated": "2025-07-31T08:07:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    8,
                    7,
                    50,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T08:07:50Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    8,
                    7,
                    50,
                    3,
                    212,
                    0
                ],
                "title": "Assessing the Alignment of Automated Vehicle Decisions with Human\n  Reasons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the Alignment of Automated Vehicle Decisions with Human\n  Reasons"
                },
                "summary": "A key challenge in deploying automated vehicles (AVs) is ensuring they make\nappropriate decisions in ethically challenging everyday driving situations.\nWhile much attention has been paid to rare, high-stakes dilemmas such as\ntrolley problems, similar tensions also arise in routine scenarios, such as\nnavigating empty intersections, where multiple human considerations, including\nlegality and comfort, often conflict. Current AV planning systems typically\nrely on rigid rules, which struggle to balance these competing considerations\nand can lead to behaviour that misaligns with human expectations. This paper\nproposes a novel reasons-based trajectory evaluation framework that\noperationalises the tracking condition of Meaningful Human Control (MHC). The\nframework models the reasons of human agents, such as regulatory compliance, as\nquantifiable functions and evaluates how well candidate AV trajectories align\nwith these reasons. By assigning adjustable weights to agent priorities and\nintegrating a balance function to discourage the exclusion of any agent, the\nframework supports interpretable decision evaluation. Through a\nreal-world-inspired overtaking scenario, we show how this approach reveals\ntensions, for instance between regulatory compliance, efficiency, and comfort.\nThe framework functions as a modular evaluation layer over existing planning\nalgorithms. It offers a transparent tool for assessing ethical alignment in\neveryday scenarios and provides a practical step toward implementing MHC in\nreal-world AV deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A key challenge in deploying automated vehicles (AVs) is ensuring they make\nappropriate decisions in ethically challenging everyday driving situations.\nWhile much attention has been paid to rare, high-stakes dilemmas such as\ntrolley problems, similar tensions also arise in routine scenarios, such as\nnavigating empty intersections, where multiple human considerations, including\nlegality and comfort, often conflict. Current AV planning systems typically\nrely on rigid rules, which struggle to balance these competing considerations\nand can lead to behaviour that misaligns with human expectations. This paper\nproposes a novel reasons-based trajectory evaluation framework that\noperationalises the tracking condition of Meaningful Human Control (MHC). The\nframework models the reasons of human agents, such as regulatory compliance, as\nquantifiable functions and evaluates how well candidate AV trajectories align\nwith these reasons. By assigning adjustable weights to agent priorities and\nintegrating a balance function to discourage the exclusion of any agent, the\nframework supports interpretable decision evaluation. Through a\nreal-world-inspired overtaking scenario, we show how this approach reveals\ntensions, for instance between regulatory compliance, efficiency, and comfort.\nThe framework functions as a modular evaluation layer over existing planning\nalgorithms. It offers a transparent tool for assessing ethical alignment in\neveryday scenarios and provides a practical step toward implementing MHC in\nreal-world AV deployment."
                },
                "authors": [
                    {
                        "name": "Lucas Elbert Suryana"
                    },
                    {
                        "name": "Saeed Rahmani"
                    },
                    {
                        "name": "Simeon Craig Calvert"
                    },
                    {
                        "name": "Arkady Zgonnikov"
                    },
                    {
                        "name": "Bart van Arem"
                    }
                ],
                "author_detail": {
                    "name": "Bart van Arem"
                },
                "author": "Bart van Arem",
                "arxiv_comment": "This version incorporates revisions based on peer-review feedback\n  from a prior submission. The work has not yet been accepted and is being\n  prepared for resubmission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23324v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23324v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23319v1",
                "updated": "2025-07-31T08:02:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    8,
                    2,
                    4,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T08:02:04Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    8,
                    2,
                    4,
                    3,
                    212,
                    0
                ],
                "title": "What's Taboo for You? - An Empirical Evaluation of LLMs Behavior Toward\n  Sensitive Content",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What's Taboo for You? - An Empirical Evaluation of LLMs Behavior Toward\n  Sensitive Content"
                },
                "summary": "Proprietary Large Language Models (LLMs) have shown tendencies toward\npoliteness, formality, and implicit content moderation. While previous research\nhas primarily focused on explicitly training models to moderate and detoxify\nsensitive content, there has been limited exploration of whether LLMs\nimplicitly sanitize language without explicit instructions. This study\nempirically analyzes the implicit moderation behavior of GPT-4o-mini when\nparaphrasing sensitive content and evaluates the extent of sensitivity shifts.\nOur experiments indicate that GPT-4o-mini systematically moderates content\ntoward less sensitive classes, with substantial reductions in derogatory and\ntaboo language. Also, we evaluate the zero-shot capabilities of LLMs in\nclassifying sentence sensitivity, comparing their performances against\ntraditional methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proprietary Large Language Models (LLMs) have shown tendencies toward\npoliteness, formality, and implicit content moderation. While previous research\nhas primarily focused on explicitly training models to moderate and detoxify\nsensitive content, there has been limited exploration of whether LLMs\nimplicitly sanitize language without explicit instructions. This study\nempirically analyzes the implicit moderation behavior of GPT-4o-mini when\nparaphrasing sensitive content and evaluates the extent of sensitivity shifts.\nOur experiments indicate that GPT-4o-mini systematically moderates content\ntoward less sensitive classes, with substantial reductions in derogatory and\ntaboo language. Also, we evaluate the zero-shot capabilities of LLMs in\nclassifying sentence sensitivity, comparing their performances against\ntraditional methods."
                },
                "authors": [
                    {
                        "name": "Alfio Ferrara"
                    },
                    {
                        "name": "Sergio Picascia"
                    },
                    {
                        "name": "Laura Pinnavaia"
                    },
                    {
                        "name": "Vojimir Ranitovic"
                    },
                    {
                        "name": "Elisabetta Rocchetti"
                    },
                    {
                        "name": "Alice Tuveri"
                    }
                ],
                "author_detail": {
                    "name": "Alice Tuveri"
                },
                "author": "Alice Tuveri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23315v1",
                "updated": "2025-07-31T07:47:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    7,
                    47,
                    30,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T07:47:30Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    7,
                    47,
                    30,
                    3,
                    212,
                    0
                ],
                "title": "Impact of Hyperparameter Optimization on the Accuracy of Lightweight\n  Deep Learning Models for Real-Time Image Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impact of Hyperparameter Optimization on the Accuracy of Lightweight\n  Deep Learning Models for Real-Time Image Classification"
                },
                "summary": "Lightweight convolutional and transformer-based models have become vital for\nreal-time image classification in resource-constrained applications, such as\nembedded systems and edge devices. This work analyzes the influence of\nhyperparameter adjustment on the accuracy and convergence behavior of seven\nefficient deep learning architectures: EfficientNetV2-S, ConvNeXt-T, MobileViT\nv2 (XXS/XS/S), MobileNetV3-L, TinyViT-21M, and RepVGG-A2. All models are\ntrained on the ImageNet-1K dataset under consistent training settings, with an\nemphasis on real-time practicality. An comprehensive ablation study is\nundertaken to separate the effect of critical hyperparameters, including\nlearning rate schedules, batch sizes, input resolution, data augmentation,\nregularization approaches, and optimizer choice. To assess appropriateness for\nreal-time applications, each model is assessed not only in terms of Top-1 and\nTop-5 classification accuracy, but also in terms of inference time, parameter\ncount, model size, and frames-per-second (FPS) on a GPU-accelerated edge\ndeployment simulation. Results demonstrate that cosine learning rate decay and\nadjustable batch size may greatly boost both accuracy and convergence speed,\nwhile keeping low latency and memory cost. Notably, RepVGG-A2 achieves over 80%\nTop-1 accuracy with efficient inference performance, offering a compelling\nbalance between accuracy and deployment cost for VGG-style models. The results\ngive practical guidance for constructing resource-efficient deep learning\nmodels appropriate for real-time image processing pipelines. All code and\ntraining logs are publicly accessible at\nhttps://github.com/VineetKumarRakesh/lcnn-opt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight convolutional and transformer-based models have become vital for\nreal-time image classification in resource-constrained applications, such as\nembedded systems and edge devices. This work analyzes the influence of\nhyperparameter adjustment on the accuracy and convergence behavior of seven\nefficient deep learning architectures: EfficientNetV2-S, ConvNeXt-T, MobileViT\nv2 (XXS/XS/S), MobileNetV3-L, TinyViT-21M, and RepVGG-A2. All models are\ntrained on the ImageNet-1K dataset under consistent training settings, with an\nemphasis on real-time practicality. An comprehensive ablation study is\nundertaken to separate the effect of critical hyperparameters, including\nlearning rate schedules, batch sizes, input resolution, data augmentation,\nregularization approaches, and optimizer choice. To assess appropriateness for\nreal-time applications, each model is assessed not only in terms of Top-1 and\nTop-5 classification accuracy, but also in terms of inference time, parameter\ncount, model size, and frames-per-second (FPS) on a GPU-accelerated edge\ndeployment simulation. Results demonstrate that cosine learning rate decay and\nadjustable batch size may greatly boost both accuracy and convergence speed,\nwhile keeping low latency and memory cost. Notably, RepVGG-A2 achieves over 80%\nTop-1 accuracy with efficient inference performance, offering a compelling\nbalance between accuracy and deployment cost for VGG-style models. The results\ngive practical guidance for constructing resource-efficient deep learning\nmodels appropriate for real-time image processing pipelines. All code and\ntraining logs are publicly accessible at\nhttps://github.com/VineetKumarRakesh/lcnn-opt."
                },
                "authors": [
                    {
                        "name": "Vineet Kumar Rakesh"
                    },
                    {
                        "name": "Soumya Mazumdar"
                    },
                    {
                        "name": "Tapas Samanta"
                    },
                    {
                        "name": "Sarbajit Pal"
                    },
                    {
                        "name": "Amitabha Das"
                    }
                ],
                "author_detail": {
                    "name": "Amitabha Das"
                },
                "author": "Amitabha Das",
                "arxiv_comment": "13 pages, 4 figures, 4 tables. Includes ablation study and evaluation\n  on 7 lightweight deep learning models. Code and logs available at\n  https://github.com/VineetKumarRakesh/lcnn-opt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22069v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22069v2",
                "updated": "2025-07-31T07:33:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    7,
                    33,
                    11,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-16T03:11:43Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    3,
                    11,
                    43,
                    2,
                    197,
                    0
                ],
                "title": "A Compute-Matched Re-Evaluation of TroVE on MATH",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Compute-Matched Re-Evaluation of TroVE on MATH"
                },
                "summary": "Reusing established theorems and formulas is central to mathematical problem\nsolving, serving as essential building blocks for tackling increasingly complex\nchallenges. Recent work, TroVE, argues that code-generating Large Language\nModels (LLMs) can benefit similarly on the MATH benchmark by inducing and\nreusing higher-level toolboxes. By allocating computational budget across an\nensemble of three modes -- directly generating code, creating tools, and\nreusing tools -- TroVE claims to outperform a PRIMITIVE baseline that only\nperforms direct generation. However, recent analysis (Berlot-Attwell et al.,\n2024) casts doubt on these gains, noting that the tools created are often\ntrivial or rarely reused, suggesting that improvements may stem from\nself-consistency or self-correction. In this work, we re-evaluate TroVE on\nMATH, analyze the impact of each of its modes, and show that its benefit does\nnot come from these mechanisms, but simply from a higher computational budget\nspent for TroVE compared to PRIMITIVE. To this end, we also perform a small\ncorrection in the original implementation of TroVE's selection mechanism,\nboosting TroVE's performance on MATH by 3\\% in accuracy. After matching for\ncompute, the benefit of TroVE reduces to a marginal improvement of 1\\%,\nsuggesting that this toolbox approach does not provide a significant benefit on\nMATH.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reusing established theorems and formulas is central to mathematical problem\nsolving, serving as essential building blocks for tackling increasingly complex\nchallenges. Recent work, TroVE, argues that code-generating Large Language\nModels (LLMs) can benefit similarly on the MATH benchmark by inducing and\nreusing higher-level toolboxes. By allocating computational budget across an\nensemble of three modes -- directly generating code, creating tools, and\nreusing tools -- TroVE claims to outperform a PRIMITIVE baseline that only\nperforms direct generation. However, recent analysis (Berlot-Attwell et al.,\n2024) casts doubt on these gains, noting that the tools created are often\ntrivial or rarely reused, suggesting that improvements may stem from\nself-consistency or self-correction. In this work, we re-evaluate TroVE on\nMATH, analyze the impact of each of its modes, and show that its benefit does\nnot come from these mechanisms, but simply from a higher computational budget\nspent for TroVE compared to PRIMITIVE. To this end, we also perform a small\ncorrection in the original implementation of TroVE's selection mechanism,\nboosting TroVE's performance on MATH by 3\\% in accuracy. After matching for\ncompute, the benefit of TroVE reduces to a marginal improvement of 1\\%,\nsuggesting that this toolbox approach does not provide a significant benefit on\nMATH."
                },
                "authors": [
                    {
                        "name": "Tobias Sesterhenn"
                    },
                    {
                        "name": "Ian Berlot-Attwell"
                    },
                    {
                        "name": "Janis Zenkner"
                    },
                    {
                        "name": "Christian Bartelt"
                    }
                ],
                "author_detail": {
                    "name": "Christian Bartelt"
                },
                "author": "Christian Bartelt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22069v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22069v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12365v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12365v2",
                "updated": "2025-07-31T07:09:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    7,
                    9,
                    27,
                    3,
                    212,
                    0
                ],
                "published": "2025-06-14T05:55:19Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    5,
                    55,
                    19,
                    5,
                    165,
                    0
                ],
                "title": "Advances in LLMs with Focus on Reasoning, Adaptability, Efficiency and\n  Ethics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances in LLMs with Focus on Reasoning, Adaptability, Efficiency and\n  Ethics"
                },
                "summary": "This survey paper outlines the key developments in the field of Large\nLanguage Models (LLMs), including enhancements to their reasoning skills,\nadaptability to various tasks, increased computational efficiency, and the\nability to make ethical decisions. The techniques that have been most effective\nin bridging the gap between human and machine communications include the\nChain-of-Thought prompting, Instruction Tuning, and Reinforcement Learning from\nHuman Feedback. The improvements in multimodal learning and few-shot or\nzero-shot techniques have further empowered LLMs to handle complex jobs with\nminor input. A significant focus is placed on efficiency, detailing scaling\nstrategies, optimization techniques, and the influential Mixture-of-Experts\n(MoE) architecture, which strategically routes inputs to specialized\nsubnetworks to boost predictive accuracy, while optimizing resource allocation.\nThis survey also offers a broader perspective on recent advancements in LLMs,\ngoing beyond isolated aspects such as model architecture or ethical concerns.\nAdditionally, it explores the role of LLMs in Agentic AI and their use as\nAutonomous Decision-Making Systems, and categorizes emerging methods that\nenhance LLM reasoning, efficiency, and ethical alignment. The survey also\nidentifies underexplored areas such as interpretability, cross-modal\nintegration, and sustainability. While significant advancements have been made\nin LLMs, challenges such as high computational costs, biases, and ethical risks\nremain. Overcoming these requires a focus on bias mitigation, transparent\ndecision-making, and explicit ethical guidelines. Future research will\ngenerally focus on enhancing the model's ability to handle multiple inputs,\nthereby making it more intelligent, safe, and reliable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This survey paper outlines the key developments in the field of Large\nLanguage Models (LLMs), including enhancements to their reasoning skills,\nadaptability to various tasks, increased computational efficiency, and the\nability to make ethical decisions. The techniques that have been most effective\nin bridging the gap between human and machine communications include the\nChain-of-Thought prompting, Instruction Tuning, and Reinforcement Learning from\nHuman Feedback. The improvements in multimodal learning and few-shot or\nzero-shot techniques have further empowered LLMs to handle complex jobs with\nminor input. A significant focus is placed on efficiency, detailing scaling\nstrategies, optimization techniques, and the influential Mixture-of-Experts\n(MoE) architecture, which strategically routes inputs to specialized\nsubnetworks to boost predictive accuracy, while optimizing resource allocation.\nThis survey also offers a broader perspective on recent advancements in LLMs,\ngoing beyond isolated aspects such as model architecture or ethical concerns.\nAdditionally, it explores the role of LLMs in Agentic AI and their use as\nAutonomous Decision-Making Systems, and categorizes emerging methods that\nenhance LLM reasoning, efficiency, and ethical alignment. The survey also\nidentifies underexplored areas such as interpretability, cross-modal\nintegration, and sustainability. While significant advancements have been made\nin LLMs, challenges such as high computational costs, biases, and ethical risks\nremain. Overcoming these requires a focus on bias mitigation, transparent\ndecision-making, and explicit ethical guidelines. Future research will\ngenerally focus on enhancing the model's ability to handle multiple inputs,\nthereby making it more intelligent, safe, and reliable."
                },
                "authors": [
                    {
                        "name": "Asifullah Khan"
                    },
                    {
                        "name": "Muhammad Zaeem Khan"
                    },
                    {
                        "name": "Saleha Jamshed"
                    },
                    {
                        "name": "Sadia Ahmad"
                    },
                    {
                        "name": "Aleesha Zainab"
                    },
                    {
                        "name": "Kaynat Khatib"
                    },
                    {
                        "name": "Faria Bibi"
                    },
                    {
                        "name": "Abdul Rehman"
                    }
                ],
                "author_detail": {
                    "name": "Abdul Rehman"
                },
                "author": "Abdul Rehman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12365v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12365v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14313v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14313v3",
                "updated": "2025-07-31T06:53:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    6,
                    53,
                    46,
                    3,
                    212,
                    0
                ],
                "published": "2024-06-20T13:43:38Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    13,
                    43,
                    38,
                    3,
                    172,
                    0
                ],
                "title": "Iterative Repair with Weak Verifiers for Few-shot Transfer in KBQA with\n  Unanswerability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative Repair with Weak Verifiers for Few-shot Transfer in KBQA with\n  Unanswerability"
                },
                "summary": "Real-world applications of KBQA require models to handle unanswerable\nquestions with a limited volume of in-domain labeled training data. We propose\nthe novel task of few-shot transfer for KBQA with unanswerable questions and\ncontribute two new datasets for performance evaluation. We present FUn-FuSIC -\na novel solution for our task that extends FuSIC KBQA, the state-of-the-art\nfew-shot transfer model for answerable-only KBQA. We first note that\nFuSIC-KBQA's iterative repair makes a strong assumption that all questions are\nunanswerable. As a remedy, we propose Feedback for Unanswerability (FUn), which\nuses iterative repair using feedback from a suite of strong and weak verifiers,\nand an adaptation of self consistency for unanswerabilty to better assess the\nanswerability of a question. Our experiments show that FUn-FuSIC significantly\noutperforms suitable adaptations of multiple LLM based and supervised SoTA\nmodels on our task, while establishing a new SoTA for answerable few-shot\ntransfer as well.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world applications of KBQA require models to handle unanswerable\nquestions with a limited volume of in-domain labeled training data. We propose\nthe novel task of few-shot transfer for KBQA with unanswerable questions and\ncontribute two new datasets for performance evaluation. We present FUn-FuSIC -\na novel solution for our task that extends FuSIC KBQA, the state-of-the-art\nfew-shot transfer model for answerable-only KBQA. We first note that\nFuSIC-KBQA's iterative repair makes a strong assumption that all questions are\nunanswerable. As a remedy, we propose Feedback for Unanswerability (FUn), which\nuses iterative repair using feedback from a suite of strong and weak verifiers,\nand an adaptation of self consistency for unanswerabilty to better assess the\nanswerability of a question. Our experiments show that FUn-FuSIC significantly\noutperforms suitable adaptations of multiple LLM based and supervised SoTA\nmodels on our task, while establishing a new SoTA for answerable few-shot\ntransfer as well."
                },
                "authors": [
                    {
                        "name": "Riya Sawhney"
                    },
                    {
                        "name": "Samrat Yadav"
                    },
                    {
                        "name": "Indrajit Bhattacharya"
                    },
                    {
                        "name": "Mausam"
                    }
                ],
                "author_detail": {
                    "name": "Mausam"
                },
                "author": "Mausam",
                "arxiv_journal_ref": "Findings 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14313v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14313v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23628v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23628v2",
                "updated": "2025-07-31T06:40:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    6,
                    40,
                    31,
                    3,
                    212,
                    0
                ],
                "published": "2025-05-29T16:34:58Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    34,
                    58,
                    3,
                    149,
                    0
                ],
                "title": "AutoSchemaKG: Autonomous Knowledge Graph Construction through Dynamic\n  Schema Induction from Web-Scale Corpora",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoSchemaKG: Autonomous Knowledge Graph Construction through Dynamic\n  Schema Induction from Web-Scale Corpora"
                },
                "summary": "We present AutoSchemaKG, a framework for fully autonomous knowledge graph\nconstruction that eliminates the need for predefined schemas. Our system\nleverages large language models to simultaneously extract knowledge triples and\ninduce comprehensive schemas directly from text, modeling both entities and\nevents while employing conceptualization to organize instances into semantic\ncategories. Processing over 50 million documents, we construct ATLAS (Automated\nTriple Linking And Schema induction), a family of knowledge graphs with 900+\nmillion nodes and 5.9 billion edges. This approach outperforms state-of-the-art\nbaselines on multi-hop QA tasks and enhances LLM factuality. Notably, our\nschema induction achieves 95\\% semantic alignment with human-crafted schemas\nwith zero manual intervention, demonstrating that billion-scale knowledge\ngraphs with dynamically induced schemas can effectively complement parametric\nknowledge in large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present AutoSchemaKG, a framework for fully autonomous knowledge graph\nconstruction that eliminates the need for predefined schemas. Our system\nleverages large language models to simultaneously extract knowledge triples and\ninduce comprehensive schemas directly from text, modeling both entities and\nevents while employing conceptualization to organize instances into semantic\ncategories. Processing over 50 million documents, we construct ATLAS (Automated\nTriple Linking And Schema induction), a family of knowledge graphs with 900+\nmillion nodes and 5.9 billion edges. This approach outperforms state-of-the-art\nbaselines on multi-hop QA tasks and enhances LLM factuality. Notably, our\nschema induction achieves 95\\% semantic alignment with human-crafted schemas\nwith zero manual intervention, demonstrating that billion-scale knowledge\ngraphs with dynamically induced schemas can effectively complement parametric\nknowledge in large language models."
                },
                "authors": [
                    {
                        "name": "Jiaxin Bai"
                    },
                    {
                        "name": "Wei Fan"
                    },
                    {
                        "name": "Qi Hu"
                    },
                    {
                        "name": "Qing Zong"
                    },
                    {
                        "name": "Chunyang Li"
                    },
                    {
                        "name": "Hong Ting Tsang"
                    },
                    {
                        "name": "Hongyu Luo"
                    },
                    {
                        "name": "Yauwai Yim"
                    },
                    {
                        "name": "Haoyu Huang"
                    },
                    {
                        "name": "Xiao Zhou"
                    },
                    {
                        "name": "Feng Qin"
                    },
                    {
                        "name": "Tianshi Zheng"
                    },
                    {
                        "name": "Xi Peng"
                    },
                    {
                        "name": "Xin Yao"
                    },
                    {
                        "name": "Huiwen Yang"
                    },
                    {
                        "name": "Leijie Wu"
                    },
                    {
                        "name": "Yi Ji"
                    },
                    {
                        "name": "Gong Zhang"
                    },
                    {
                        "name": "Renhai Chen"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "arxiv_comment": "9 pages, preprint, code:\n  https://github.com/HKUST-KnowComp/AutoSchemaKG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23628v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23628v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23279v1",
                "updated": "2025-07-31T06:35:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    6,
                    35,
                    33,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T06:35:33Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    6,
                    35,
                    33,
                    3,
                    212,
                    0
                ],
                "title": "Unveiling Super Experts in Mixture-of-Experts Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Super Experts in Mixture-of-Experts Large Language Models"
                },
                "summary": "Sparsely activated Mixture-of-Experts (MoE) models have shown promise in\nenhancing the learning capacity of large language models (LLMs). Leveraging the\nintrinsic importance differences among experts, recent research has explored\nexpert-level compression techniques to improve the efficiency of MoE LLMs.\nHowever, existing approaches often rely on empirical criteria to identify\ncritical experts, lacking a deeper exploration and understanding of the\nheterogeneous importance of experts. In this study, we present the first\ndiscovery and investigation of a distinct subset of experts that play a crucial\nrole in the underlying mechanisms during the model's forward inference. These\nexperts are prevalent in open-source MoE LLMs, and despite their limited\nnumber, pruning them leads to a significant decline in model performance (e.g.,\npruning three causes Qwen3-30B-A3B to produce repetitive and uninformative\noutputs). We refer to these experts as Super Experts (SEs). Our comprehensive\nanalysis provides progressively deeper insights into SEs. (i) SEs are\ncharacterized by rare but extreme activation outliers in the output of the\ndown_proj, which give rise to massive activations in the hidden states between\ndecoder layers. Moreover, the distribution of SEs remains model-specific and is\nunaffected by post-training processes. (ii) By pruning SEs, we assess their\nsignificance across a variety of tasks, revealing their considerable impact on\nthe model's overall performance, particularly in mathematical reasoning. (iii)\nWe further enhance our understanding of the influence of SEs compression. Our\nfindings confirm that MoE LLMs rely on SEs to induce attention sinks, which are\ncrucial for the distribution of attention scores but are significantly\ndisrupted by SE pruning. The code is available at\nhttps://github.com/ZunhaiSu/Super-Experts-Profilling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparsely activated Mixture-of-Experts (MoE) models have shown promise in\nenhancing the learning capacity of large language models (LLMs). Leveraging the\nintrinsic importance differences among experts, recent research has explored\nexpert-level compression techniques to improve the efficiency of MoE LLMs.\nHowever, existing approaches often rely on empirical criteria to identify\ncritical experts, lacking a deeper exploration and understanding of the\nheterogeneous importance of experts. In this study, we present the first\ndiscovery and investigation of a distinct subset of experts that play a crucial\nrole in the underlying mechanisms during the model's forward inference. These\nexperts are prevalent in open-source MoE LLMs, and despite their limited\nnumber, pruning them leads to a significant decline in model performance (e.g.,\npruning three causes Qwen3-30B-A3B to produce repetitive and uninformative\noutputs). We refer to these experts as Super Experts (SEs). Our comprehensive\nanalysis provides progressively deeper insights into SEs. (i) SEs are\ncharacterized by rare but extreme activation outliers in the output of the\ndown_proj, which give rise to massive activations in the hidden states between\ndecoder layers. Moreover, the distribution of SEs remains model-specific and is\nunaffected by post-training processes. (ii) By pruning SEs, we assess their\nsignificance across a variety of tasks, revealing their considerable impact on\nthe model's overall performance, particularly in mathematical reasoning. (iii)\nWe further enhance our understanding of the influence of SEs compression. Our\nfindings confirm that MoE LLMs rely on SEs to induce attention sinks, which are\ncrucial for the distribution of attention scores but are significantly\ndisrupted by SE pruning. The code is available at\nhttps://github.com/ZunhaiSu/Super-Experts-Profilling."
                },
                "authors": [
                    {
                        "name": "Zunhai Su"
                    },
                    {
                        "name": "Qingyuan Li"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "YuLei Qian"
                    },
                    {
                        "name": "Yuchen Xie"
                    },
                    {
                        "name": "Kehong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Kehong Yuan"
                },
                "author": "Kehong Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23276v1",
                "updated": "2025-07-31T06:32:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    6,
                    32,
                    6,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T06:32:06Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    6,
                    32,
                    6,
                    3,
                    212,
                    0
                ],
                "title": "How Far Are AI Scientists from Changing the World?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Far Are AI Scientists from Changing the World?"
                },
                "summary": "The emergence of large language models (LLMs) is propelling automated\nscientific discovery to the next level, with LLM-based Artificial Intelligence\n(AI) Scientist systems now taking the lead in scientific research. Several\ninfluential works have already appeared in the field of AI Scientist systems,\nwith AI-generated research papers having been accepted at the ICLR 2025\nworkshop, suggesting that a human-level AI Scientist capable of uncovering\nphenomena previously unknown to humans, may soon become a reality. In this\nsurvey, we focus on the central question: How far are AI scientists from\nchanging the world and reshaping the scientific research paradigm? To answer\nthis question, we provide a prospect-driven review that comprehensively\nanalyzes the current achievements of AI Scientist systems, identifying key\nbottlenecks and the critical components required for the emergence of a\nscientific agent capable of producing ground-breaking discoveries that solve\ngrand challenges. We hope this survey will contribute to a clearer\nunderstanding of limitations of current AI Scientist systems, showing where we\nare, what is missing, and what the ultimate goals for scientific AI should be.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models (LLMs) is propelling automated\nscientific discovery to the next level, with LLM-based Artificial Intelligence\n(AI) Scientist systems now taking the lead in scientific research. Several\ninfluential works have already appeared in the field of AI Scientist systems,\nwith AI-generated research papers having been accepted at the ICLR 2025\nworkshop, suggesting that a human-level AI Scientist capable of uncovering\nphenomena previously unknown to humans, may soon become a reality. In this\nsurvey, we focus on the central question: How far are AI scientists from\nchanging the world and reshaping the scientific research paradigm? To answer\nthis question, we provide a prospect-driven review that comprehensively\nanalyzes the current achievements of AI Scientist systems, identifying key\nbottlenecks and the critical components required for the emergence of a\nscientific agent capable of producing ground-breaking discoveries that solve\ngrand challenges. We hope this survey will contribute to a clearer\nunderstanding of limitations of current AI Scientist systems, showing where we\nare, what is missing, and what the ultimate goals for scientific AI should be."
                },
                "authors": [
                    {
                        "name": "Qiujie Xie"
                    },
                    {
                        "name": "Yixuan Weng"
                    },
                    {
                        "name": "Minjun Zhu"
                    },
                    {
                        "name": "Fuchen Shen"
                    },
                    {
                        "name": "Shulin Huang"
                    },
                    {
                        "name": "Zhen Lin"
                    },
                    {
                        "name": "Jiahui Zhou"
                    },
                    {
                        "name": "Zilan Mao"
                    },
                    {
                        "name": "Zijie Yang"
                    },
                    {
                        "name": "Linyi Yang"
                    },
                    {
                        "name": "Jian Wu"
                    },
                    {
                        "name": "Yue Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhang"
                },
                "author": "Yue Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13201v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13201v2",
                "updated": "2025-07-31T06:22:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    6,
                    22,
                    28,
                    3,
                    212,
                    0
                ],
                "published": "2025-04-15T03:50:04Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    3,
                    50,
                    4,
                    1,
                    105,
                    0
                ],
                "title": "CEE: An Inference-Time Jailbreak Defense for Embodied Intelligence via\n  Subspace Concept Rotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CEE: An Inference-Time Jailbreak Defense for Embodied Intelligence via\n  Subspace Concept Rotation"
                },
                "summary": "Large Language Models (LLMs) are increasingly becoming the cognitive core of\nEmbodied Intelligence (EI) systems, such as robots and autonomous vehicles.\nHowever, this integration also exposes them to serious jailbreak risks, where\nmalicious instructions can be transformed into dangerous physical actions.\nExisting defense mechanisms suffer from notable drawbacks--including high\ntraining costs, significant inference delays, and complex hyperparameter\ntuning--which limit their practical applicability. To address these challenges,\nwe propose a novel and efficient inference-time defense framework: Concept\nEnhancement Engineering (CEE). CEE enhances the model's inherent safety\nmechanisms by directly manipulating its internal representations, requiring\nneither additional training nor external modules, thereby improving defense\nefficiency. Furthermore, CEE introduces a rotation-based control mechanism that\nenables stable and linearly tunable behavioral control of the model. This\ndesign eliminates the need for tedious manual tuning and avoids the output\ndegradation issues commonly observed in other representation engineering\nmethods. Extensive experiments across multiple EI safety benchmarks and diverse\nattack scenarios demonstrate that CEE significantly improves the defense\nsuccess rates of various multimodal LLMs. It effectively mitigates safety risks\nwhile preserving high-quality generation and inference efficiency, offering a\npromising solution for deploying safer embodied intelligence systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly becoming the cognitive core of\nEmbodied Intelligence (EI) systems, such as robots and autonomous vehicles.\nHowever, this integration also exposes them to serious jailbreak risks, where\nmalicious instructions can be transformed into dangerous physical actions.\nExisting defense mechanisms suffer from notable drawbacks--including high\ntraining costs, significant inference delays, and complex hyperparameter\ntuning--which limit their practical applicability. To address these challenges,\nwe propose a novel and efficient inference-time defense framework: Concept\nEnhancement Engineering (CEE). CEE enhances the model's inherent safety\nmechanisms by directly manipulating its internal representations, requiring\nneither additional training nor external modules, thereby improving defense\nefficiency. Furthermore, CEE introduces a rotation-based control mechanism that\nenables stable and linearly tunable behavioral control of the model. This\ndesign eliminates the need for tedious manual tuning and avoids the output\ndegradation issues commonly observed in other representation engineering\nmethods. Extensive experiments across multiple EI safety benchmarks and diverse\nattack scenarios demonstrate that CEE significantly improves the defense\nsuccess rates of various multimodal LLMs. It effectively mitigates safety risks\nwhile preserving high-quality generation and inference efficiency, offering a\npromising solution for deploying safer embodied intelligence systems."
                },
                "authors": [
                    {
                        "name": "Jirui Yang"
                    },
                    {
                        "name": "Zheyu Lin"
                    },
                    {
                        "name": "Zhihui Lu"
                    },
                    {
                        "name": "Yinggui Wang"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Tao Wei"
                    },
                    {
                        "name": "Xin Du"
                    },
                    {
                        "name": "Shuhan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Shuhan Yang"
                },
                "author": "Shuhan Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13201v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13201v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23261v1",
                "updated": "2025-07-31T05:52:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    5,
                    52,
                    30,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T05:52:30Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    5,
                    52,
                    30,
                    3,
                    212,
                    0
                ],
                "title": "DynaSwarm: Dynamically Graph Structure Selection for LLM-based\n  Multi-agent System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynaSwarm: Dynamically Graph Structure Selection for LLM-based\n  Multi-agent System"
                },
                "summary": "Current multi-agent systems (MAS) frameworks often rely on manually designed\nand static collaboration graph structures, limiting adaptability and\nperformance. To address these limitations, we propose DynaSwarm, a dynamic\nframework that enhances LLM-based MAS through two key innovations: (1) an\nactor-critic reinforcement learning (A2C) mechanism to optimize graph\nstructures with improved stability over prior RL methods, and (2) a dynamic\ngraph selector that adaptively chooses the optimal graph structure for each\ninput sample via parameter-efficient LLM fine-tuning. DynaSwarm eliminates the\nneed for rigid, one-fits-all graph architectures, instead leveraging\nsample-specific idiosyncrasies to dynamically route queries through specialized\nagent networks. (c) We propose to fine-tune the demonstration retriever to\nfully exploit the power of in-context learning (ICL). Extensive experiments on\nquestion answering, mathematical reasoning, and coding tasks demonstrate that\nDynaSwarm consistently outperforms state-of-the-art single-agent and MAS\nbaselines across multiple LLM backbones. Our findings highlight the importance\nof sample-aware structural flexibility in LLM MAS designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current multi-agent systems (MAS) frameworks often rely on manually designed\nand static collaboration graph structures, limiting adaptability and\nperformance. To address these limitations, we propose DynaSwarm, a dynamic\nframework that enhances LLM-based MAS through two key innovations: (1) an\nactor-critic reinforcement learning (A2C) mechanism to optimize graph\nstructures with improved stability over prior RL methods, and (2) a dynamic\ngraph selector that adaptively chooses the optimal graph structure for each\ninput sample via parameter-efficient LLM fine-tuning. DynaSwarm eliminates the\nneed for rigid, one-fits-all graph architectures, instead leveraging\nsample-specific idiosyncrasies to dynamically route queries through specialized\nagent networks. (c) We propose to fine-tune the demonstration retriever to\nfully exploit the power of in-context learning (ICL). Extensive experiments on\nquestion answering, mathematical reasoning, and coding tasks demonstrate that\nDynaSwarm consistently outperforms state-of-the-art single-agent and MAS\nbaselines across multiple LLM backbones. Our findings highlight the importance\nof sample-aware structural flexibility in LLM MAS designs."
                },
                "authors": [
                    {
                        "name": "Hui Yi Leong"
                    },
                    {
                        "name": "Yuqing Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yuqing Wu"
                },
                "author": "Yuqing Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22688v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22688v3",
                "updated": "2025-07-31T05:49:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    5,
                    49,
                    44,
                    3,
                    212,
                    0
                ],
                "published": "2025-03-05T09:47:02Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    9,
                    47,
                    2,
                    2,
                    64,
                    0
                ],
                "title": "CodeIF-Bench: Evaluating Instruction-Following Capabilities of Large\n  Language Models in Interactive Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeIF-Bench: Evaluating Instruction-Following Capabilities of Large\n  Language Models in Interactive Code Generation"
                },
                "summary": "Large Language Models (LLMs) have demonstrated exceptional performance in\ncode generation tasks and have become indispensable programming assistants for\ndevelopers. However, existing code generation benchmarks primarily assess the\nfunctional correctness of code generated by LLMs in single-turn interactions.\nThey offer limited insight into LLMs' abilities to generate code that strictly\nfollows users' instructions in multi-turn interaction scenarios. In this paper,\nwe introduce CodeIF-Bench, a benchmark for evaluating the instruction-following\ncapabilities of LLMs in interactive code generation. Specifically, CodeIF-Bench\nincorporates nine types of verifiable instructions aligned with the real-world\nsoftware development requirements, which can be independently and objectively\nvalidated through specified test cases, facilitating the evaluation of\ninstruction-following capability in multi-turn interactions. In both\n\\textit{Static Conversation} and \\textit{Dynamic Conversation} settings, we\nevaluate the performance of 7 state-of-the-art LLMs and summarize the important\nfactors influencing the instruction-following ability of LLMs in multi-turn\ninteractions, as well as potential directions for improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated exceptional performance in\ncode generation tasks and have become indispensable programming assistants for\ndevelopers. However, existing code generation benchmarks primarily assess the\nfunctional correctness of code generated by LLMs in single-turn interactions.\nThey offer limited insight into LLMs' abilities to generate code that strictly\nfollows users' instructions in multi-turn interaction scenarios. In this paper,\nwe introduce CodeIF-Bench, a benchmark for evaluating the instruction-following\ncapabilities of LLMs in interactive code generation. Specifically, CodeIF-Bench\nincorporates nine types of verifiable instructions aligned with the real-world\nsoftware development requirements, which can be independently and objectively\nvalidated through specified test cases, facilitating the evaluation of\ninstruction-following capability in multi-turn interactions. In both\n\\textit{Static Conversation} and \\textit{Dynamic Conversation} settings, we\nevaluate the performance of 7 state-of-the-art LLMs and summarize the important\nfactors influencing the instruction-following ability of LLMs in multi-turn\ninteractions, as well as potential directions for improvement."
                },
                "authors": [
                    {
                        "name": "Peiding Wang"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Lin Shi"
                    },
                    {
                        "name": "Minxiao Li"
                    },
                    {
                        "name": "Bo Shen"
                    },
                    {
                        "name": "An Fu"
                    }
                ],
                "author_detail": {
                    "name": "An Fu"
                },
                "author": "An Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22688v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22688v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23248v1",
                "updated": "2025-07-31T05:16:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    5,
                    16,
                    43,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T05:16:43Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    5,
                    16,
                    43,
                    3,
                    212,
                    0
                ],
                "title": "Evaluating LLMs' Multilingual Capabilities for Bengali: Benchmark\n  Creation and Performance Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLMs' Multilingual Capabilities for Bengali: Benchmark\n  Creation and Performance Analysis"
                },
                "summary": "Bengali is an underrepresented language in NLP research. However, it remains\na challenge due to its unique linguistic structure and computational\nconstraints. In this work, we systematically investigate the challenges that\nhinder Bengali NLP performance by focusing on the absence of standardized\nevaluation benchmarks. We then evaluated 10 recent open source Large Language\nModels (LLMs) in 8 of the translated datasets and performed a comprehensive\nerror analysis to pinpoint their primary failure modes. Our findings reveal\nconsistent performance gaps for Bengali compared to English, particularly for\nsmaller models and specific model families like Mistral. We also identified\npromising robustness in certain architectures, such as DeepSeek, that maintain\nmore stable performance across languages. Our analysis reveals an inverse\nrelationship between tokenization efficiency and LLM accuracy where models tend\nto perform worse when inputs are excessively tokenized, whereas more efficient\n\\& concise tokenization results in improved performance. These findings\nhighlight critical areas where current models fall short and underscore the\nneed for improved dataset quality and evaluation methodologies tailored to\nmultilingual contexts. This work will catalyze further research on NLP for\nunderrepresented languages, helping to democratize access to advanced language\ntechnologies worldwide. The code and dataset used in this research is publicly\navailable at https://github.com/BengaliAI/bn-llm-benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bengali is an underrepresented language in NLP research. However, it remains\na challenge due to its unique linguistic structure and computational\nconstraints. In this work, we systematically investigate the challenges that\nhinder Bengali NLP performance by focusing on the absence of standardized\nevaluation benchmarks. We then evaluated 10 recent open source Large Language\nModels (LLMs) in 8 of the translated datasets and performed a comprehensive\nerror analysis to pinpoint their primary failure modes. Our findings reveal\nconsistent performance gaps for Bengali compared to English, particularly for\nsmaller models and specific model families like Mistral. We also identified\npromising robustness in certain architectures, such as DeepSeek, that maintain\nmore stable performance across languages. Our analysis reveals an inverse\nrelationship between tokenization efficiency and LLM accuracy where models tend\nto perform worse when inputs are excessively tokenized, whereas more efficient\n\\& concise tokenization results in improved performance. These findings\nhighlight critical areas where current models fall short and underscore the\nneed for improved dataset quality and evaluation methodologies tailored to\nmultilingual contexts. This work will catalyze further research on NLP for\nunderrepresented languages, helping to democratize access to advanced language\ntechnologies worldwide. The code and dataset used in this research is publicly\navailable at https://github.com/BengaliAI/bn-llm-benchmark."
                },
                "authors": [
                    {
                        "name": "Shimanto Bhowmik"
                    },
                    {
                        "name": "Tawsif Tashwar Dipto"
                    },
                    {
                        "name": "Md Sazzad Islam"
                    },
                    {
                        "name": "Sheryl Hsu"
                    },
                    {
                        "name": "Tahsin Reasat"
                    }
                ],
                "author_detail": {
                    "name": "Tahsin Reasat"
                },
                "author": "Tahsin Reasat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23247v1",
                "updated": "2025-07-31T05:10:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    5,
                    10,
                    38,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T05:10:38Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    5,
                    10,
                    38,
                    3,
                    212,
                    0
                ],
                "title": "P-ReMIS: Pragmatic Reasoning in Mental Health and a Social Implication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "P-ReMIS: Pragmatic Reasoning in Mental Health and a Social Implication"
                },
                "summary": "There has been an increase in recent advancements in the explainability and\ndevelopment of personalized chatbots for mental health. However, the reasoning\naspects for explainability and dialogue discourse have not been explored\npreviously for mental health. Hence, we are investigating the pragmatic\nreasoning capability of large language models (LLMs) in this domain. We\nintroduce P-ReMe dataset, and propose a modified definition for the pragmatic\nphenomena of implicature (implied meaning) and presupposition (implicit\nassumption) in mental health. Following the definition, we formulate two tasks\nin implicature and one task in presupposition. To benchmark the dataset and the\npresented tasks, we consider four models - Llama3.1, Mistral, MentaLLaMa, and\nQwen. The results of the experiments suggest that Mistral and Qwen show\nsubstantial reasoning capabilities in the domain. In addition, we also propose\nStiPRompts to study the stigma around mental health with the state-of-the-art\nLLMs, GPT-4o mini, Deepseek-chat, and Claude-3.5-haiku. Our evaluated findings\nshow that Claude-3.5-haiku deals with the stigma more responsibly compared to\nthe other two LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There has been an increase in recent advancements in the explainability and\ndevelopment of personalized chatbots for mental health. However, the reasoning\naspects for explainability and dialogue discourse have not been explored\npreviously for mental health. Hence, we are investigating the pragmatic\nreasoning capability of large language models (LLMs) in this domain. We\nintroduce P-ReMe dataset, and propose a modified definition for the pragmatic\nphenomena of implicature (implied meaning) and presupposition (implicit\nassumption) in mental health. Following the definition, we formulate two tasks\nin implicature and one task in presupposition. To benchmark the dataset and the\npresented tasks, we consider four models - Llama3.1, Mistral, MentaLLaMa, and\nQwen. The results of the experiments suggest that Mistral and Qwen show\nsubstantial reasoning capabilities in the domain. In addition, we also propose\nStiPRompts to study the stigma around mental health with the state-of-the-art\nLLMs, GPT-4o mini, Deepseek-chat, and Claude-3.5-haiku. Our evaluated findings\nshow that Claude-3.5-haiku deals with the stigma more responsibly compared to\nthe other two LLMs."
                },
                "authors": [
                    {
                        "name": "Sneha Oram"
                    },
                    {
                        "name": "Pushpak Bhattacharyya"
                    }
                ],
                "author_detail": {
                    "name": "Pushpak Bhattacharyya"
                },
                "author": "Pushpak Bhattacharyya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17761v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17761v5",
                "updated": "2025-07-31T05:05:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    5,
                    5,
                    45,
                    3,
                    212,
                    0
                ],
                "published": "2025-04-24T17:25:12Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    17,
                    25,
                    12,
                    3,
                    114,
                    0
                ],
                "title": "Step1X-Edit: A Practical Framework for General Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step1X-Edit: A Practical Framework for General Image Editing"
                },
                "summary": "In recent years, image editing models have witnessed remarkable and rapid\ndevelopment. The recent unveiling of cutting-edge multimodal models such as\nGPT-4o and Gemini2 Flash has introduced highly promising image editing\ncapabilities. These models demonstrate an impressive aptitude for fulfilling a\nvast majority of user-driven editing requirements, marking a significant\nadvancement in the field of image manipulation. However, there is still a large\ngap between the open-source algorithm with these closed-source models. Thus, in\nthis paper, we aim to release a state-of-the-art image editing model, called\nStep1X-Edit, which can provide comparable performance against the closed-source\nmodels like GPT-4o and Gemini2 Flash. More specifically, we adopt the\nMultimodal LLM to process the reference image and the user's editing\ninstruction. A latent embedding has been extracted and integrated with a\ndiffusion image decoder to obtain the target image. To train the model, we\nbuild a data generation pipeline to produce a high-quality dataset. For\nevaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world\nuser instructions. Experimental results on GEdit-Bench demonstrate that\nStep1X-Edit outperforms existing open-source baselines by a substantial margin\nand approaches the performance of leading proprietary models, thereby making\nsignificant contributions to the field of image editing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, image editing models have witnessed remarkable and rapid\ndevelopment. The recent unveiling of cutting-edge multimodal models such as\nGPT-4o and Gemini2 Flash has introduced highly promising image editing\ncapabilities. These models demonstrate an impressive aptitude for fulfilling a\nvast majority of user-driven editing requirements, marking a significant\nadvancement in the field of image manipulation. However, there is still a large\ngap between the open-source algorithm with these closed-source models. Thus, in\nthis paper, we aim to release a state-of-the-art image editing model, called\nStep1X-Edit, which can provide comparable performance against the closed-source\nmodels like GPT-4o and Gemini2 Flash. More specifically, we adopt the\nMultimodal LLM to process the reference image and the user's editing\ninstruction. A latent embedding has been extracted and integrated with a\ndiffusion image decoder to obtain the target image. To train the model, we\nbuild a data generation pipeline to produce a high-quality dataset. For\nevaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world\nuser instructions. Experimental results on GEdit-Bench demonstrate that\nStep1X-Edit outperforms existing open-source baselines by a substantial margin\nand approaches the performance of leading proprietary models, thereby making\nsignificant contributions to the field of image editing."
                },
                "authors": [
                    {
                        "name": "Shiyu Liu"
                    },
                    {
                        "name": "Yucheng Han"
                    },
                    {
                        "name": "Peng Xing"
                    },
                    {
                        "name": "Fukun Yin"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Jiaqi Liao"
                    },
                    {
                        "name": "Yingming Wang"
                    },
                    {
                        "name": "Honghao Fu"
                    },
                    {
                        "name": "Chunrui Han"
                    },
                    {
                        "name": "Guopeng Li"
                    },
                    {
                        "name": "Yuang Peng"
                    },
                    {
                        "name": "Quan Sun"
                    },
                    {
                        "name": "Jingwei Wu"
                    },
                    {
                        "name": "Yan Cai"
                    },
                    {
                        "name": "Zheng Ge"
                    },
                    {
                        "name": "Ranchen Ming"
                    },
                    {
                        "name": "Lei Xia"
                    },
                    {
                        "name": "Xianfang Zeng"
                    },
                    {
                        "name": "Yibo Zhu"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Gang Yu"
                    },
                    {
                        "name": "Daxin Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Daxin Jiang"
                },
                "author": "Daxin Jiang",
                "arxiv_comment": "code: https://github.com/stepfun-ai/Step1X-Edit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17761v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17761v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15444v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15444v4",
                "updated": "2025-07-31T04:52:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    4,
                    52,
                    43,
                    3,
                    212,
                    0
                ],
                "published": "2024-05-30T18:07:13Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    18,
                    7,
                    13,
                    3,
                    151,
                    0
                ],
                "title": "Cutting Through the Noise: Boosting LLM Performance on Math Word\n  Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cutting Through the Noise: Boosting LLM Performance on Math Word\n  Problems"
                },
                "summary": "Large Language Models (LLMs) excel at various tasks, including solving math\nword problems (MWPs), but struggle with real-world problems containing\nirrelevant information. To address this, we propose a prompting framework that\ngenerates adversarial variants of MWPs by adding irrelevant variables. We\nintroduce a dataset, PROBLEMATHIC, containing both adversarial and\nnon-adversarial MWPs. Our experiments reveal that LLMs are susceptible to\ndistraction by numerical noise, resulting in an average relative performance\ndrop of ~26% on adversarial MWPs. To mitigate this, we fine-tune LLMs (Llama-2,\nMistral) on the adversarial samples from our dataset. Fine-tuning on\nadversarial training instances improves performance on adversarial MWPs by ~8%,\nindicating increased robustness to noise and improved ability to identify\nrelevant data for reasoning. Finally, to assess the generalizability of our\nprompting framework, we introduce GSM-8K-Adv, an adversarial variant of the\nGSM-8K benchmark. LLMs continue to struggle when faced with adversarial\ninformation, reducing performance by up to 6%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at various tasks, including solving math\nword problems (MWPs), but struggle with real-world problems containing\nirrelevant information. To address this, we propose a prompting framework that\ngenerates adversarial variants of MWPs by adding irrelevant variables. We\nintroduce a dataset, PROBLEMATHIC, containing both adversarial and\nnon-adversarial MWPs. Our experiments reveal that LLMs are susceptible to\ndistraction by numerical noise, resulting in an average relative performance\ndrop of ~26% on adversarial MWPs. To mitigate this, we fine-tune LLMs (Llama-2,\nMistral) on the adversarial samples from our dataset. Fine-tuning on\nadversarial training instances improves performance on adversarial MWPs by ~8%,\nindicating increased robustness to noise and improved ability to identify\nrelevant data for reasoning. Finally, to assess the generalizability of our\nprompting framework, we introduce GSM-8K-Adv, an adversarial variant of the\nGSM-8K benchmark. LLMs continue to struggle when faced with adversarial\ninformation, reducing performance by up to 6%."
                },
                "authors": [
                    {
                        "name": "Ujjwala Anantheswaran"
                    },
                    {
                        "name": "Himanshu Gupta"
                    },
                    {
                        "name": "Kevin Scaria"
                    },
                    {
                        "name": "Shreyas Verma"
                    },
                    {
                        "name": "Chitta Baral"
                    },
                    {
                        "name": "Swaroop Mishra"
                    }
                ],
                "author_detail": {
                    "name": "Swaroop Mishra"
                },
                "author": "Swaroop Mishra",
                "arxiv_comment": "Published at ICLR 2025 Workshop on Reasoning and Planning for LLMs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15444v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15444v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00068v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00068v2",
                "updated": "2025-07-31T04:41:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    4,
                    41,
                    18,
                    3,
                    212,
                    0
                ],
                "published": "2025-05-29T15:15:42Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    15,
                    42,
                    3,
                    149,
                    0
                ],
                "title": "Framing Political Bias in Multilingual LLMs Across Pakistani Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Framing Political Bias in Multilingual LLMs Across Pakistani Languages"
                },
                "summary": "Large Language Models (LLMs) increasingly shape public discourse, yet most\nevaluations of political and economic bias have focused on high-resource,\nWestern languages and contexts. This leaves critical blind spots in\nlow-resource, multilingual regions such as Pakistan, where linguistic identity\nis closely tied to political, religious, and regional ideologies. We present a\nsystematic evaluation of political bias in 13 state-of-the-art LLMs across five\nPakistani languages: Urdu, Punjabi, Sindhi, Pashto, and Balochi. Our framework\nintegrates a culturally adapted Political Compass Test (PCT) with multi-level\nframing analysis, capturing both ideological stance (economic/social axes) and\nstylistic framing (content, tone, emphasis). Prompts are aligned with 11\nsocio-political themes specific to the Pakistani context. Results show that\nwhile LLMs predominantly reflect liberal-left orientations consistent with\nWestern training data, they exhibit more authoritarian framing in regional\nlanguages, highlighting language-conditioned ideological modulation. We also\nidentify consistent model-specific bias patterns across languages. These\nfindings show the need for culturally grounded, multilingual bias auditing\nframeworks in global NLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) increasingly shape public discourse, yet most\nevaluations of political and economic bias have focused on high-resource,\nWestern languages and contexts. This leaves critical blind spots in\nlow-resource, multilingual regions such as Pakistan, where linguistic identity\nis closely tied to political, religious, and regional ideologies. We present a\nsystematic evaluation of political bias in 13 state-of-the-art LLMs across five\nPakistani languages: Urdu, Punjabi, Sindhi, Pashto, and Balochi. Our framework\nintegrates a culturally adapted Political Compass Test (PCT) with multi-level\nframing analysis, capturing both ideological stance (economic/social axes) and\nstylistic framing (content, tone, emphasis). Prompts are aligned with 11\nsocio-political themes specific to the Pakistani context. Results show that\nwhile LLMs predominantly reflect liberal-left orientations consistent with\nWestern training data, they exhibit more authoritarian framing in regional\nlanguages, highlighting language-conditioned ideological modulation. We also\nidentify consistent model-specific bias patterns across languages. These\nfindings show the need for culturally grounded, multilingual bias auditing\nframeworks in global NLP."
                },
                "authors": [
                    {
                        "name": "Afrozah Nadeem"
                    },
                    {
                        "name": "Mark Dras"
                    },
                    {
                        "name": "Usman Naseem"
                    }
                ],
                "author_detail": {
                    "name": "Usman Naseem"
                },
                "author": "Usman Naseem",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00068v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00068v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23236v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23236v1",
                "updated": "2025-07-31T04:25:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    4,
                    25,
                    59,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T04:25:59Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    4,
                    25,
                    59,
                    3,
                    212,
                    0
                ],
                "title": "BS-1-to-N: Diffusion-Based Environment-Aware Cross-BS Channel Knowledge\n  Map Generation for Cell-Free Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BS-1-to-N: Diffusion-Based Environment-Aware Cross-BS Channel Knowledge\n  Map Generation for Cell-Free Networks"
                },
                "summary": "Channel knowledge map (CKM) inference across base stations (BSs) is the key\nto achieving efficient environmentaware communications. This paper proposes an\nenvironmentaware cross-BS CKM inference method called BS-1-to-N based on the\ngenerative diffusion model. To this end, we first design the BS location\nembedding (BSLE) method tailored for cross-BS CKM inference to embed BS\nlocation information in the feature vector of CKM. Further, we utilize the\ncross- and self-attention mechanism for the proposed BS-1-to-N model to\nrespectively learn the relationships between source and target BSs, as well as\nthat among target BSs. Therefore, given the locations of the source and target\nBSs, together with the source CKMs as control conditions, cross-BS CKM\ninference can be performed for an arbitrary number of source and target BSs.\nSpecifically, in architectures with massive distributed nodes like cell-free\nnetworks, traditional methods of sequentially traversing each BS for CKM\nconstruction are prohibitively costly. By contrast, the proposed BS-1-to-N\nmodel is able to achieve efficient CKM inference for a target BS at any\npotential location based on the CKMs of source BSs. This is achieved by\nexploiting the fact that within a given area, different BSs share the same\nwireless environment that leads to their respective CKMs. Therefore, similar to\nmulti-view synthesis, CKMs of different BSs are representations of the same\nwireless environment from different BS locations. By mining the implicit\ncorrelation between CKM and BS location based on the wireless environment, the\nproposed BS-1-to-N method achieves efficient CKM inference across BSs. We\nprovide extensive comparisons of CKM inference between the proposed BS-1-to-N\ngenerative model versus benchmarking schemes, and provide one use case study to\ndemonstrate its practical application for the optimization of BS deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Channel knowledge map (CKM) inference across base stations (BSs) is the key\nto achieving efficient environmentaware communications. This paper proposes an\nenvironmentaware cross-BS CKM inference method called BS-1-to-N based on the\ngenerative diffusion model. To this end, we first design the BS location\nembedding (BSLE) method tailored for cross-BS CKM inference to embed BS\nlocation information in the feature vector of CKM. Further, we utilize the\ncross- and self-attention mechanism for the proposed BS-1-to-N model to\nrespectively learn the relationships between source and target BSs, as well as\nthat among target BSs. Therefore, given the locations of the source and target\nBSs, together with the source CKMs as control conditions, cross-BS CKM\ninference can be performed for an arbitrary number of source and target BSs.\nSpecifically, in architectures with massive distributed nodes like cell-free\nnetworks, traditional methods of sequentially traversing each BS for CKM\nconstruction are prohibitively costly. By contrast, the proposed BS-1-to-N\nmodel is able to achieve efficient CKM inference for a target BS at any\npotential location based on the CKMs of source BSs. This is achieved by\nexploiting the fact that within a given area, different BSs share the same\nwireless environment that leads to their respective CKMs. Therefore, similar to\nmulti-view synthesis, CKMs of different BSs are representations of the same\nwireless environment from different BS locations. By mining the implicit\ncorrelation between CKM and BS location based on the wireless environment, the\nproposed BS-1-to-N method achieves efficient CKM inference across BSs. We\nprovide extensive comparisons of CKM inference between the proposed BS-1-to-N\ngenerative model versus benchmarking schemes, and provide one use case study to\ndemonstrate its practical application for the optimization of BS deployment."
                },
                "authors": [
                    {
                        "name": "Zhuoyin Dai"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Yong Zeng"
                    },
                    {
                        "name": "Xiaoli Xu"
                    },
                    {
                        "name": "Xinyi Wang"
                    },
                    {
                        "name": "Zesong Fei"
                    }
                ],
                "author_detail": {
                    "name": "Zesong Fei"
                },
                "author": "Zesong Fei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23236v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18666v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18666v3",
                "updated": "2025-07-31T04:00:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    4,
                    0,
                    48,
                    3,
                    212,
                    0
                ],
                "published": "2025-03-24T13:31:48Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    13,
                    31,
                    48,
                    0,
                    83,
                    0
                ],
                "title": "AgentSpec: Customizable Runtime Enforcement for Safe and Reliable LLM\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentSpec: Customizable Runtime Enforcement for Safe and Reliable LLM\n  Agents"
                },
                "summary": "Agents built on LLMs are increasingly deployed across diverse domains,\nautomating complex decision-making and task execution. However, their autonomy\nintroduces safety risks, including security vulnerabilities, legal violations,\nand unintended harmful actions. Existing mitigation methods, such as\nmodel-based safeguards and early enforcement strategies, fall short in\nrobustness, interpretability, and adaptability. To address these challenges, we\npropose AgentSpec, a lightweight domain-specific language for specifying and\nenforcing runtime constraints on LLM agents. With AgentSpec, users define\nstructured rules that incorporate triggers, predicates, and enforcement\nmechanisms, ensuring agents operate within predefined safety boundaries. We\nimplement AgentSpec across multiple domains, including code execution, embodied\nagents, and autonomous driving, demonstrating its adaptability and\neffectiveness. Our evaluation shows that AgentSpec successfully prevents unsafe\nexecutions in over 90% of code agent cases, eliminates all hazardous actions in\nembodied agent tasks, and enforces 100% compliance by autonomous vehicles\n(AVs). Despite its strong safety guarantees, AgentSpec remains computationally\nlightweight, with overheads in milliseconds. By combining interpretability,\nmodularity, and efficiency, AgentSpec provides a practical and scalable\nsolution for enforcing LLM agent safety across diverse applications. We also\nautomate the generation of rules using LLMs and assess their effectiveness. Our\nevaluation shows that the rules generated by OpenAI o1 achieve a precision of\n95.56% and recall of 70.96% for embodied agents, successfully identify 87.26%\nof the risky code, and prevent AVs from breaking laws in 5 out of 8 scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agents built on LLMs are increasingly deployed across diverse domains,\nautomating complex decision-making and task execution. However, their autonomy\nintroduces safety risks, including security vulnerabilities, legal violations,\nand unintended harmful actions. Existing mitigation methods, such as\nmodel-based safeguards and early enforcement strategies, fall short in\nrobustness, interpretability, and adaptability. To address these challenges, we\npropose AgentSpec, a lightweight domain-specific language for specifying and\nenforcing runtime constraints on LLM agents. With AgentSpec, users define\nstructured rules that incorporate triggers, predicates, and enforcement\nmechanisms, ensuring agents operate within predefined safety boundaries. We\nimplement AgentSpec across multiple domains, including code execution, embodied\nagents, and autonomous driving, demonstrating its adaptability and\neffectiveness. Our evaluation shows that AgentSpec successfully prevents unsafe\nexecutions in over 90% of code agent cases, eliminates all hazardous actions in\nembodied agent tasks, and enforces 100% compliance by autonomous vehicles\n(AVs). Despite its strong safety guarantees, AgentSpec remains computationally\nlightweight, with overheads in milliseconds. By combining interpretability,\nmodularity, and efficiency, AgentSpec provides a practical and scalable\nsolution for enforcing LLM agent safety across diverse applications. We also\nautomate the generation of rules using LLMs and assess their effectiveness. Our\nevaluation shows that the rules generated by OpenAI o1 achieve a precision of\n95.56% and recall of 70.96% for embodied agents, successfully identify 87.26%\nof the risky code, and prevent AVs from breaking laws in 5 out of 8 scenarios."
                },
                "authors": [
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Christopher M. Poskitt"
                    },
                    {
                        "name": "Jun Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jun Sun"
                },
                "author": "Jun Sun",
                "arxiv_comment": "Accepted by the 48th IEEE/ACM International Conference on Software\n  Engineering (ICSE 2026)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18666v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18666v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23229v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23229v1",
                "updated": "2025-07-31T03:50:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    3,
                    50,
                    16,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T03:50:16Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    3,
                    50,
                    16,
                    3,
                    212,
                    0
                ],
                "title": "Fine-Grained Privacy Extraction from Retrieval-Augmented Generation\n  Systems via Knowledge Asymmetry Exploitation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Grained Privacy Extraction from Retrieval-Augmented Generation\n  Systems via Knowledge Asymmetry Exploitation"
                },
                "summary": "Retrieval-augmented generation (RAG) systems enhance large language models\n(LLMs) by integrating external knowledge bases, but this advancement introduces\nsignificant privacy risks. Existing privacy attacks on RAG systems can trigger\ndata leakage but often fail to accurately isolate knowledge-base-derived\nsentences within mixed responses. They also lack robustness when applied across\nmultiple domains. This paper addresses these challenges by presenting a novel\nblack-box attack framework that exploits knowledge asymmetry between RAG and\nstandard LLMs to achieve fine-grained privacy extraction across heterogeneous\nknowledge landscapes. We propose a chain-of-thought reasoning strategy that\ncreates adaptive prompts to steer RAG systems away from sensitive content.\nSpecifically, we first decompose adversarial queries to maximize information\ndisparity and then apply a semantic relationship scoring to resolve lexical and\nsyntactic ambiguities. We finally train a neural network on these feature\nscores to precisely identify sentences containing private information. Unlike\nprior work, our framework generalizes to unseen domains through iterative\nrefinement without pre-defined knowledge. Experimental results show that we\nachieve over 91% privacy extraction rate in single-domain and 83% in\nmulti-domain scenarios, reducing sensitive sentence exposure by over 65% in\ncase studies. This work bridges the gap between attack and defense in RAG\nsystems, enabling precise extraction of private information while providing a\nfoundation for adaptive mitigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) systems enhance large language models\n(LLMs) by integrating external knowledge bases, but this advancement introduces\nsignificant privacy risks. Existing privacy attacks on RAG systems can trigger\ndata leakage but often fail to accurately isolate knowledge-base-derived\nsentences within mixed responses. They also lack robustness when applied across\nmultiple domains. This paper addresses these challenges by presenting a novel\nblack-box attack framework that exploits knowledge asymmetry between RAG and\nstandard LLMs to achieve fine-grained privacy extraction across heterogeneous\nknowledge landscapes. We propose a chain-of-thought reasoning strategy that\ncreates adaptive prompts to steer RAG systems away from sensitive content.\nSpecifically, we first decompose adversarial queries to maximize information\ndisparity and then apply a semantic relationship scoring to resolve lexical and\nsyntactic ambiguities. We finally train a neural network on these feature\nscores to precisely identify sentences containing private information. Unlike\nprior work, our framework generalizes to unseen domains through iterative\nrefinement without pre-defined knowledge. Experimental results show that we\nachieve over 91% privacy extraction rate in single-domain and 83% in\nmulti-domain scenarios, reducing sensitive sentence exposure by over 65% in\ncase studies. This work bridges the gap between attack and defense in RAG\nsystems, enabling precise extraction of private information while providing a\nfoundation for adaptive mitigation."
                },
                "authors": [
                    {
                        "name": "Yufei Chen"
                    },
                    {
                        "name": "Yao Wang"
                    },
                    {
                        "name": "Haibin Zhang"
                    },
                    {
                        "name": "Tao Gu"
                    }
                ],
                "author_detail": {
                    "name": "Tao Gu"
                },
                "author": "Tao Gu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23229v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23229v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23227v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23227v1",
                "updated": "2025-07-31T03:49:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    3,
                    49,
                    31,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T03:49:31Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    3,
                    49,
                    31,
                    3,
                    212,
                    0
                ],
                "title": "Enabling Few-Shot Alzheimer's Disease Diagnosis on Tabular Biomarker\n  Data with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Few-Shot Alzheimer's Disease Diagnosis on Tabular Biomarker\n  Data with LLMs"
                },
                "summary": "Early and accurate diagnosis of Alzheimer's disease (AD), a complex\nneurodegenerative disorder, requires analysis of heterogeneous biomarkers\n(e.g., neuroimaging, genetic risk factors, cognitive tests, and cerebrospinal\nfluid proteins) typically represented in a tabular format. With flexible\nfew-shot reasoning, multimodal integration, and natural-language-based\ninterpretability, large language models (LLMs) offer unprecedented\nopportunities for prediction with structured biomedical data. We propose a\nnovel framework called TAP-GPT, Tabular Alzheimer's Prediction GPT, that adapts\nTableGPT2, a multimodal tabular-specialized LLM originally developed for\nbusiness intelligence tasks, for AD diagnosis using structured biomarker data\nwith small sample sizes. Our approach constructs few-shot tabular prompts using\nin-context learning examples from structured biomedical data and finetunes\nTableGPT2 using the parameter-efficient qLoRA adaption for a clinical binary\nclassification task of AD or cognitively normal (CN). The TAP-GPT framework\nharnesses the powerful tabular understanding ability of TableGPT2 and the\nencoded prior knowledge of LLMs to outperform more advanced general-purpose\nLLMs and a tabular foundation model (TFM) developed for prediction tasks. To\nour knowledge, this is the first application of LLMs to the prediction task\nusing tabular biomarker data, paving the way for future LLM-driven multi-agent\nframeworks in biomedical informatics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early and accurate diagnosis of Alzheimer's disease (AD), a complex\nneurodegenerative disorder, requires analysis of heterogeneous biomarkers\n(e.g., neuroimaging, genetic risk factors, cognitive tests, and cerebrospinal\nfluid proteins) typically represented in a tabular format. With flexible\nfew-shot reasoning, multimodal integration, and natural-language-based\ninterpretability, large language models (LLMs) offer unprecedented\nopportunities for prediction with structured biomedical data. We propose a\nnovel framework called TAP-GPT, Tabular Alzheimer's Prediction GPT, that adapts\nTableGPT2, a multimodal tabular-specialized LLM originally developed for\nbusiness intelligence tasks, for AD diagnosis using structured biomarker data\nwith small sample sizes. Our approach constructs few-shot tabular prompts using\nin-context learning examples from structured biomedical data and finetunes\nTableGPT2 using the parameter-efficient qLoRA adaption for a clinical binary\nclassification task of AD or cognitively normal (CN). The TAP-GPT framework\nharnesses the powerful tabular understanding ability of TableGPT2 and the\nencoded prior knowledge of LLMs to outperform more advanced general-purpose\nLLMs and a tabular foundation model (TFM) developed for prediction tasks. To\nour knowledge, this is the first application of LLMs to the prediction task\nusing tabular biomarker data, paving the way for future LLM-driven multi-agent\nframeworks in biomedical informatics."
                },
                "authors": [
                    {
                        "name": "Sophie Kearney"
                    },
                    {
                        "name": "Shu Yang"
                    },
                    {
                        "name": "Zixuan Wen"
                    },
                    {
                        "name": "Bojian Hou"
                    },
                    {
                        "name": "Duy Duong-Tran"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Jason Moore"
                    },
                    {
                        "name": "Marylyn Ritchie"
                    },
                    {
                        "name": "Li Shen"
                    }
                ],
                "author_detail": {
                    "name": "Li Shen"
                },
                "author": "Li Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23227v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23227v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]